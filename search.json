[
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "I used to give workshops regularly when I worked in academia, and I have kept the content here. Some were not so much workshops as talks without any expectation of hands-on exercises or similar, so may not be as useful without the in-person context. Some of these, especially programming specific ones, are likely too dated to be useful anymore. The modeling focused ones may still have relevant content however."
  },
  {
    "objectID": "workshops.html#last-efforts",
    "href": "workshops.html#last-efforts",
    "title": "Workshops",
    "section": "Last efforts",
    "text": "Last efforts\nThese were among the last workshops I gave:\n\nDistill for R Markdown\nExploratory Data Analysis Tools\nMixed Models with R\nMore Mixed Models\nPatchwork and gganimate\nLibrary Learning Analytics Workshop\nGetting More from RStudio\nLatent Variable Models\nGeneralized Additive Models\nMixed Models\n\n\nTexts\nThese are the texts that serve as the basis for the workshops.\n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Structural Equation Modeling\nThis document regards a recent workshop given on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The document should be useful to anyone interested in the techniques covered, though it is R-based, with special emphasis on the lavaan package. \n Easy Bayes with rstanarm and brms\nThis workshop provides an overview of the rstanarm and brms packages. Basic modeling syntax is provided, as well as diagnostic checking, model comparison (posterior predictive checks , WAIC/LOO ), and how to get more from the models (marginal effects , posterior probabilities posterior probabilities, etc.). \n Factor Analysis and Related Methods\nThis workshop will expose participants to a variety of related techniques that might fall under the heading of ‘factor analysis’, latent variable modeling, dimension reduction and similar, such as principal components analysis, factor analysis, and measurement models, with possible exposure to and demonstration of latent Dirichlet allocation, mixture models, item response theory, and others. Brief overviews with examples of the more common techniques will be provided. \n Introduction to R Markdown\nThis workshop will introduce participants to the basics of R Markdown. After an introduction to concepts related to reproducible programming and research, demonstrations of standard markdown as well as overviews of different formats will be provided, including exercises. This document has been superseded by Practical Data Science, and will no longer be updated. \n Text Analysis with R\nThis document covers a wide range of topics, including how to process text generally, and demonstrations of sentiment analysis, parts-of-speech tagging, and topic modeling. Exercises are provided for some topics. It has practically no relevance in the modern large language model era."
  },
  {
    "objectID": "workshops.html#been-awhile",
    "href": "workshops.html#been-awhile",
    "title": "Workshops",
    "section": "Been awhile…",
    "text": "Been awhile…\nThese haven’t been given recently and are increasingly out date, but some content may be useful.\n My God, it’s full of STARs! Using astrology to get more from your data.\nTalk on structured additive regression models, and generalized additive models in particular. \n Become a Bayesian in 10 Minutes\nThis document regards a talk aimed at giving an introduction Bayesian modeling in R via the Stan programming language. It doesn’t assume too much statistically or any prior Bayesian experience. For those with such experience, they can quickly work with the code or packages discussed. I post them here because they exist and provide a quick overview, but you’d get more from the more extensive document. \n Engaging the Web with R\nDocument regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach.  \n Ceci n’est pas une %&gt;%\nExploring your data with R. A workshop that introduces some newer modes of data wrangling within R, with an eye toward visualization. Focus on dplyr and magrittr packages. No longer available as the javascript the slides were based on kept producing vulnerabilities for my website. Nowadays, using pipes is standard anyway. \n Getting More from RStudio\nAn afternoon talk on how to use RStudio for more than just coding."
  },
  {
    "objectID": "posts/2024-05-20/index.html",
    "href": "posts/2024-05-20/index.html",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  },
  {
    "objectID": "posts/2024-05-20/index.html#book-in-progess",
    "href": "posts/2024-05-20/index.html#book-in-progess",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html",
    "href": "posts/2022-09-deep-linear-models/index.html",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#introduction",
    "href": "posts/2022-09-deep-linear-models/index.html#introduction",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "title": "Deep Linear Models",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s get the primary packages loaded first.\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport torch\n\nNext, we’ll use the well-known titanic dataset, and to start things off, we’ll need to get a sense of what we’re dealing with. The basic idea is that we’d like to predict survival based on key features like sex, age, ticket class and more.\n\n# non-kaggle-requiring url here: https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/data/dl-linear-regression/titanic/train.csv\ndf_titanic_train = pd.read_csv('../../data/dl-linear-regression/titanic/train.csv')\n# df_titanic_train\n\n\ndf_titanic_train.describe()\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "href": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "title": "Deep Linear Models",
    "section": "Initial Data Processing",
    "text": "Initial Data Processing\nThe data is not ready for modeling as is, so we’ll do some additional processing to get it ready. We’ll check out the missing values and replace them with modes3.\n\ndf_titanic_train.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\nmodes = df_titanic_train.mode().iloc[0]\n\ndf_titanic_train.fillna(modes, inplace = True)\n\ndf_titanic_train.describe(include = (np.number))\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]\n\n\nWith features, sometimes it is worthwhile to log transform data for potentially more efficient optimization search. Since we have zeros, we add 1 before taking the log.\n\ndf_titanic_train['Fare'].hist()\n\n\nNow the transformed data looks a little more manageable. More to the point, we won’t potentially have huge coefficients relative to other covariates because of the range of the data.\n\ndf_titanic_train['LogFare'] = np.log1p(df_titanic_train['Fare'])\n\n# df_titanic_train['LogFare'].hist()\n\n\nThe Pclass (passenger class) feature is actually categorical.\n\npclasses = sorted(df_titanic_train.Pclass.unique())\npclasses\n\n[np.int64(1), np.int64(2), np.int64(3)]\n\n\nHere are the other categorical features.\n\ndf_titanic_train.describe(include = [object])\n\n                           Name   Sex  Ticket    Cabin Embarked\ncount                       891   891     891      891      891\nunique                      891     2     681      147        3\ntop     Braund, Mr. Owen Harris  male  347082  B96 B98        S\nfreq                          1   577       7      691      646\n\n\nIn order to use categorical variables, they need to be changed to numbers4, so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use embeddings5, particularly for things that have lots of unique categories.\n\ndf_titanic_train = pd.get_dummies(df_titanic_train, columns = [\"Sex\", \"Pclass\", \"Embarked\"])\n\nLet’s take a look at our data now.\n\ndf_titanic_train.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\ndf_titanic_train.head()\n\n   PassengerId  Survived  ... Embarked_Q  Embarked_S\n0            1         0  ...      False        True\n1            2         1  ...      False       False\n2            3         1  ...      False        True\n3            4         1  ...      False        True\n4            5         0  ...      False        True\n\n[5 rows x 18 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "title": "Deep Linear Models",
    "section": "Getting Started with pytorch",
    "text": "Getting Started with pytorch\n\nSetup\nNow we are ready to prep things for specific use with pytorch. I will not use the same terminology as in Jeremy’s original post, so for us, target = ‘dependent variable’ and X is our feature matrix6. Both of these will be pytorch tensors, which for our purposes is just another word for an array of arbitrary size.\n\nfrom torch import tensor\ndevice = torch.device('cpu')\ntarget = tensor(df_titanic_train.Survived)\n\n\ndummies = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nall_features = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummies \n\nX = df_titanic_train[all_features].apply(pd.to_numeric).astype(float)\nX = tensor(X.values, dtype = torch.float)\n\nX.shape\n\ntorch.Size([891, 12])"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "href": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "title": "Deep Linear Models",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nWe have our feature matrix and target variable prepped. The next step is to map the features to the target by means of predicted values. In linear regression, we typically call the weights that produce the predictions coefficients, but in standard deep/machine learning terminology, they are usually called weights, or more generally, parameters. Here, we generate some random values between -.5 and .5 to get started7:.\n\ntorch.manual_seed(442)\n\n&lt;torch._C.Generator object at 0x16d8c3330&gt;\n\nn_coeff = X.shape[1]\ncoeffs = torch.rand(n_coeff) - 0.5  # default would produce values from 0 to 1\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625])\n\n\nThe original post did a form of min-max scaling to the features, basically putting everything on a potentially [0, 1] scale. Here we’ll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.\n\n# vals,indices = X.max(dim=0)\n# X = X / vals\nX_means = X.mean(dim = 0, keepdim = True)\nX_sds   = X.std(dim = 0)\n\nX_sc = (X - X_means) / X_sds\n\n# X_sc.mean(dim = 0)  # all means = 0 \n# X_sc.std(dim = 0)   # all sd = 1\n\nAs noted in the original post and worth iterating here for our statistical modeling crowd, we don’t estimate an intercept for this model and keep all the dummy coded features. The following takes our coefficients, multiplies them by their respective feature, and sums them.\n\npreds = (X_sc * coeffs).sum(axis = 1)\npreds[:10]\n\ntensor([ 0.6000, -1.9341,  0.2080,  0.1723, -0.0032,  0.3088, -0.5066,  1.6219,\n         0.6990, -1.2584])\n\n\nWe can calculate our loss, the difference in our predictions versus the target values, in many ways. Here we get the mean squared error.\n\nloss = torch.square(preds - target).mean()\nloss\n\ntensor(1.3960)\n\n\nNow we’ll create functions that do the previous steps, and finally, give it a test run! In the original fastai formulation, they use mean absolute error for the loss, which actually is just the L1loss that is available in torch. For a change of pace, we’ll keep our mean squared error, which is sometimes called L2 loss (this will create different results from the original notebook). I create the option within the function for you to do either. Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.\n\ndef calc_preds(X, weights):\n    return((X * weights).sum(axis = 1))\n\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n    \n    # torch.abs(calc_preds(X, coeffs)-target).mean()  # original\n\n    if which == 'l2':\n      loss = torch.nn.MSELoss()\n    else: \n      loss = torch.nn.L1Loss()\n      \n    L = loss(preds, target.float())\n      \n    return(L)\n\ncalc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which = 'l1')\n\n(tensor(1.3960), tensor(0.8891))\n\n\n\nDoing a Gradient Descent Step\nWe can continue our journey onward to actually estimating the weights rather than specifying them directly, since we definitely don’t want to just keep guessing! This is an iterative process where we still start with an initial (random) guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps epochs, and getting our next guess requires calculating what’s called a gradient. Here are some resources for more detail:\n\nHow Does a Neural Net Really Work?: great intro by Jeremy Howard\nSome by-hand code using gradient descent for linear regression, R, Python: By yours truly\n\nIn any case, this is basic functionality within pytorch, and it will keep track of each step taken.\n\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\n\nloss = calc_loss(X_sc, coeffs, target)\nloss\n\ntensor(1.3960, grad_fn=&lt;MseLossBackward0&gt;)\n\n\nWe use backward to calculate the gradients and inspect them.\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-0.9311,  0.6245,  0.4957, -0.7423,  0.6008, -0.6008, -0.9158,  0.0938,\n         0.7127, -1.7183,  0.1715,  1.3974])\n\n\nEach time backward is called, the gradients are added to the previous values. We can see here that they’ve now doubled.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-1.8621,  1.2491,  0.9914, -1.4847,  1.2015, -1.2015, -1.8317,  0.1877,\n         1.4254, -3.4366,  0.3431,  2.7947])\n\n\nWhat we want instead is to set them back to zero after they are used for our estimation step. The following does this.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)     # sub subtracts in place\n    coeffs.grad.zero_()                # zeros out in place\n    print(calc_loss(X, coeffs, target))\n\ntensor([-0.1836, -0.0488,  0.0922, -0.0035, -0.4435, -0.1345,  0.7624,  0.2854,\n         0.0661,  0.0763,  0.1588, -0.0567], requires_grad=True)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\ntensor(37.9424)\n\n\n\n\nTraining the Linear Model\nWe typically would split our data into training and test. We can do so here, or keep this data as training and import test.csv for the test set. The latter is actually used for the Kaggle submission, but that’s not a goal here. We’ll use scikit-learn for the splitting.\n\nfrom sklearn.model_selection import train_test_split\n\n# test size .2 in keeping with fastai RandomSplitter default\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n  X_sc, \n  target.float(), \n  test_size = 0.2, \n  random_state = 808\n)\n  \n\nlen(train_x), len(valid_x) # might be one off of the original notebook\n\n(712, 179)\n\n\nAs before, we’ll create functions to help automate our steps:\n\none to initialize the weights\na function to update weights\none to do a full epoch (using weights to calculate loss, updating weights)\none to train the entire model (run multiple times/epochs)\n\nAs mentioned, the approach here is to create functions that are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each verbose value epoch (e.g. verbose = 10 means you’ll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (possibly not too big a deal depending on your IDE).\n\ndef init_weights(n_wts): \n    return (torch.rand(n_wts) - 0.5).requires_grad_()\n\ndef update_weights(weights, lr):\n    weights.sub_(weights.grad * lr)\n    weights.grad.zero_()\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\ndef train_model(X, target, epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1])\n    \n    for i in range(epochs): \n        one_epoch(X, coeffs, target, lr = lr, i = i, verbose = verbose)\n    return coeffs\n\nTry out the functions if you like (not shown).\n\ncalc_loss(X_sc, init_weights(X_sc.shape[1]), target).backward()\n\n\none_epoch(train_x, init_weights(train_x.shape[1]), train_y, .01)\n\nNow train the model for multiple epochs. The loss drops very quickly before becoming more steady.\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, verbose = 5, lr = .2)\n\n 1.375618\n  0.296216\n  0.284019\n  0.281221\n  0.280271\n  0.279923\n  0.279794\n  0.279746\n  0.279728\n  0.279721\n \n\n\nLet’s create a function to show our estimated parameters/weights/coefficients in a pretty fashion.\n\ndef show_coeffs(estimates): \n  coef_dict = dict(zip(all_features, estimates.requires_grad_(False).numpy()))\n  return pd.DataFrame(coef_dict, index = ['value']).T\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.090825\nSibSp      -0.054449\nParch      -0.016111\nLogFare     0.046320\nSex_male   -0.406538\nSex_female -0.171426\nPclass_1    0.408707\nPclass_2    0.335766\nPclass_3    0.329800\nEmbarked_C  0.057091\nEmbarked_Q  0.032813\nEmbarked_S  0.039464\n\n\n\n\nMeasuring Accuracy\nIt’s one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data. The following function will convert our estimates to a binary value like our target, and compares them to the target. Depending on how you did your training setup, it might be pretty bad or at least better than guessing.\n\ndef acc(X, weights, target): \n    return (target.bool() == (calc_preds(X, weights) &gt; 0.5)).float().mean()\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7051), tensor(0.6425))\n\n\n\n\nUsing sigmoid\nNothing about the previous setup actually produces a result on the probability scale, so basing a cutoff of .5 is meaningless. you can inspect them and might see values are above 1 or below zero, which we generally don’t want8. However we do have a solution. The sigmoid function9 allows us to transform our predictions to values between 0 and 1, i.e. probabilities in this context, and in particular, the probability of survival. Then our acc function will be more appropriate, where any probability &gt; .5 will be given a value of 1 (or True technically), while others will be 0/False.\n\ndef calc_preds(X, weights):\n    return torch.sigmoid((X*weights).sum(axis = 1))\n\nWe also will do more iterations, and fiddle with the learning rate (a.k.a. step size)\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  epochs = 500,\n  lr = 1,\n  verbose = 100\n)\n\n 0.314158\n  0.154329\n  0.154237\n  0.154232\n  0.154232\n \n\n\nNot too shabby!\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7823), tensor(0.7989))\n\n\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.516476\nSibSp      -0.423656\nParch      -0.179623\nLogFare     0.396468\nSex_male   -0.927410\nSex_female  0.349448\nPclass_1    0.713895\nPclass_2    0.320935\nPclass_3    0.078919\nEmbarked_C  0.107378\nEmbarked_Q  0.082943\nEmbarked_S -0.036137\n\n\nIn implementing the sigmoid, let’s go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster)10. To do this, the coefficients will need to be a column vector, so we change our init_coeffs function slightly11.\n\ndef calc_preds(X, weights): \n    return torch.sigmoid(X@weights)\n\ndef init_coeffs(n_wts): \n    return (torch.rand(n_wts, 1) * 0.1).requires_grad_()\n\nNow our functions are more like the mathematical notation we’d usually see for linear regression.\n\\[\\hat{y} = X\\beta\\]\n\n\nCompare to Linear/Logistic Regression\nBefore getting too excited, let’s compare our results to basic linear and logistic regression. The linear regression is more like our model before using the sigmoid transformation, while the logistic is more like when we used it. Depending on your settings, the logistic regression is probably doing better at this point.\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\n\n\nreg = linear_model.LinearRegression()\nreg.fit(train_x, train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\nacc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)\n\n(tensor(0.7989), tensor(0.7821))\n\n\n\nreg = linear_model.LogisticRegression()\nreg.fit(train_x, train_y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n0.7821229050279329\n\n\nIt looks like our coefficient estimates are similar to the logistic regression ones.\n\nshow_coeffs(coeffs_est).assign(logreg = np.round(reg.coef_.T, 4))\n\n               value  logreg\nAge        -0.516476 -0.4799\nSibSp      -0.423656 -0.4191\nParch      -0.179623 -0.1265\nLogFare     0.396468  0.3441\nSex_male   -0.927410 -0.6262\nSex_female  0.349448  0.6262\nPclass_1    0.713895  0.3941\nPclass_2    0.320935  0.0675\nPclass_3    0.078919 -0.3945\nEmbarked_C  0.107378  0.0546\nEmbarked_Q  0.082943  0.0655\nEmbarked_S -0.036137 -0.0890"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "href": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "title": "Deep Linear Models",
    "section": "A Neural Network",
    "text": "A Neural Network\n\nAt this point we’ve basically reproduced a general linear model. A neural network, on the other hand, has from one to many hidden layers of varying types in between input and output. Let’s say we have a single layer with two nodes. For a fully connected or dense network, we’d need weights to map our features to each node of the hidden layer (n_wts * n_hidden parameters total), and then another set of weights to map the hidden nodes to our next layer, which is our output, basically creating the predicted values. You can think of it as a second hidden layer with a single output node. With additional hidden nodes we add more complexity, but also flexibility, to the model. But this may come at a price, e.g. more difficulty with training due to the additional parameters that have to be estimated.\nSo basically we need matrices of weights, and the following function allows us to create those. We also add a bias/intercept/constant for the hidden-to-output processing. In the first layer, we divide the weights by n_hidden to create sums at the next layer that are of similar magnitude as the inputs. In general though, there are many ways to initialize weights.\n\ndef init_weights(n_wts, n_hidden = 20):\n    layer1 = (torch.rand(n_wts, n_hidden) - 0.5) / n_hidden # n_wts x n_hidden matrix of weights\n    layer2 = torch.rand(n_hidden, 1) - 0.3                  # n_hidden weights\n    const  = torch.rand(1)[0]                               # constant\n    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\nNow we revise our calc_preds function to incorporate all the weights. Initially, we extract the different sets of weights that are estimated by the model. For the original inputs, we multiply them by the layer 1 weights and sum. Then we apply a transformation to them to induce nonlinearity. Typical approaches are the sigmoid function we used before, hyperbolic tangent, and, probably the most common, the relu. The original notebook used relu, while I use a more recent one called Mish, which is a variant of relu. The hidden layer nodes then get multiplied by their respective weights and summed with the constant added. We then use our sigmoid function to get the probability scale as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(X, weights):\n    l1, l2, const = weights\n    res = F.mish(X@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res).flatten()\n\nWith additional sets of weights, we use an update loop.\n\ndef update_weights(weights, lr):\n    for layer in weights:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, lr = 1, verbose = 10)\n\n 0.325837\n  0.155810\n  0.141485\n  0.137652\n  0.136034\n \n\n\nAt this point we’re doing a little bit better in general, and even better than standard logistic regression on the test set!\n\nacc(train_x, coeffs_est, train_y), \\\nacc(valid_x, coeffs_est, valid_y), \\\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n(tensor(0.8160), tensor(0.8045), 0.7821229050279329)"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "href": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "title": "Deep Linear Models",
    "section": "Deep Learning",
    "text": "Deep Learning\nWe previously used a single hidden layer, but we want to go deeper! That’s the whole point of deep learning right? The following modifies our previous functions to allow for an arbitrary number of layers. You’ll note there are some hacks to get the weights in a good way for each layer12, but you normally wouldn’t have to do that on your own, since most tools will provide sensible modifications.\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\n# change loss to binary\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n\n    loss = torch.nn.BCELoss()\n\n    L = loss(preds, target.float())\n\n    return(L)\n\n\ndef init_weights(n_wts, hiddens):  \n    sizes = [n_wts] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]\n    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers, consts\n\n\ndef calc_preds(X, weights):\n    layers, consts = weights\n    n = len(layers)\n    res = X\n    \n    for i, l in enumerate(layers):\n        res = res@l + consts[i]\n    \n    if i != n-1: \n      res = F.mish(res)\n      \n    \n    return torch.sigmoid(res).flatten()\n\ndef update_weights(weights, lr):\n    layers, consts = weights\n    for layer in layers + consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1], hiddens)\n    \n    for i in range(epochs): \n        if verbose != 0:\n            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)\n    \n    return coeffs\n\nWith everything set up, let’s do some deep linear modeling! You can play around with the number of hidden layers, number of nodes and other settings. Feel free to explore!\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  hiddens = [500, 250, 100],\n  epochs  = 500,\n  lr      = 1e-4,\n  verbose = 10\n)\n\n 5.123790\n  0.666971\n  0.653124\n  0.640325\n  0.628476\n  0.617496\n  0.607313\n  0.597861\n  0.589081\n  0.580918\n  0.573322\n  0.566249\n  0.559658\n  0.553510\n  0.547772\n  0.542413\n  0.537403\n  0.532715\n  0.528326\n  0.524212\n  0.520354\n  0.516733\n  0.513330\n  0.510130\n  0.507118\n  0.504281\n  0.501605\n  0.499080\n  0.496695\n  0.494439\n  0.492305\n  0.490283\n  0.488366\n  0.486547\n  0.484820\n  0.483178\n  0.481616\n  0.480129\n  0.478712\n  0.477361\n  0.476072\n  0.474840\n  0.473663\n  0.472538\n  0.471461\n  0.470429\n  0.469440\n  0.468493\n  0.467583\n  0.466710\n \n\n\nHooray! Our best model yet (at least tied).\n\npd.DataFrame({\n    'acc_train': acc(train_x, coeffs_est, train_y).flatten(), \n    'acc_test': acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm': accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int))\n}, index = ['value'])\n\n       acc_train  acc_test  acc_test_glm\nvalue    0.77809  0.804469      0.782123"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "href": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "title": "Deep Linear Models",
    "section": "The Elephant in the Room",
    "text": "The Elephant in the Room\nAs noted in my previous posts [1, 2], probably your biggest challenge in implementing a deep learning model for tabular data, one with mixed data types and other complexities, is beating an off the shelf boosting model. Here is a quick demo with lightgbm.\n\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(\n  # n_estimators = 500,  # the sorts of parameters you can play with (many more!)\n  # max_depth    = 4,\n  # reg_alpha    = .1\n)\n\nmodel.fit(train_x, train_y)\n\nLGBMClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifieriFittedLGBMClassifier() \n\nmodel.score(valid_x, valid_y.numpy())\n\n0.8491620111731844\n\n\n\n# sklearn example\n# from sklearn.ensemble import HistGradientBoostingClassifier\n# \n# res = HistGradientBoostingClassifier().fit(train_x.numpy(), train_y.numpy())\n# \n# res.score(valid_x.numpy(), valid_y.numpy())\n\nNo tuning at all, and we’re already doing significantly better. Granted, if you use a packaged DL model for tabular data like the one in fastai, you should be doing better than our little demo. Even then though, you may still find the boosting results tough to beat.\n\ndf_accs = pd.DataFrame({ \n    'acc_test_dl':   acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm':  accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int)),\n    'acc_test_lgbm': model.score(valid_x, valid_y.numpy())\n}, index = ['value']).round(4)\n\ndf_accs\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue       0.8045        0.7821         0.8492\n\n\n\ndf_perc_improvement = 100 * (df_accs / df_accs.iloc[0,1] - 1)  # % improvement\ndf_perc_improvement\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue     2.864075           0.0       8.579466"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#summary",
    "href": "posts/2022-09-deep-linear-models/index.html#summary",
    "title": "Deep Linear Models",
    "section": "Summary",
    "text": "Summary\nThis was a lot of work to do slightly better than a logistic regression! However, there is a lot going on with a typical DL model that would likely prove even better. But it also serves as a reminder to have a suitable baseline, and as we saw with the lightgbm model, it can take little effort to do very well without deep learning. Hopefully though, the peek behind the scenes to do some ‘deep’ linear modeling can make it more accessible for you."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "href": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "title": "Deep Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI won’t actually use fastai, since they aren’t up to supporting M1/2 Macs very well. I think it was only used for the train/test data split anyway. I would rant a bit about this, but a lot of fastai is geared toward non-local computing, and the fault is really with Apple and NVidia as near as I can tell.↩︎\nI’m also not going to go into broadcasting, submitting to Kaggle, and other things that I don’t think are necessary for our purposes here.↩︎\nJust as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here, and sometimes that’s enough.↩︎\nEven though every modeling tool requires this, strangely very few in the Python world offer options for automatic handling of such things, but it’s getting better.↩︎\nWe actually aren’t too far removed from this in our model coming up, the main difference is that we don’t treat the categorical feature part of the model separately.↩︎\nI’ll not perpetuate calling features/predictor variables that are clearly not independent as independent. That nomenclature really only works for randomized experiments, and that is definitely not the case here.↩︎\nYou could use torch.randn to get standard normal values, and often times we’ll even start with just zeros if we really are just doing a standard linear model.↩︎\nUnless you are an economist, in which case you call it a linear probability model and ignore the ridiculous predictions because you have very fine standard errors.↩︎\nA lot of R folks seem unaware that the base R plogis function accomplishes this.↩︎\nThe @ operator is essentially the dot product, so x@y is np.dot(x, y)↩︎\nThe fastai demo also changes the target to a column vector, but this doesn’t seem necessary.↩︎\nAnd they probably aren’t as good for the changes I’ve made.↩︎"
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "title": "Deep Learning for Tabular Data",
    "section": "TLDR: the meta-analysis",
    "text": "TLDR: the meta-analysis\nI collected most of the results from the summarized articles here and those covered in the previous post to see if we come to any general conclusions about which methods are best or work best in certain settings. In the following tables, I excluded those I knew to be image data, as well as datasets where I thought results were indistinguishable across all models tested (e.g. less than 1% difference in accuracy). This left comparisons for 92 datasets across six articles. However, it’s important to note that these were not independent datasets or studies. For example, Gorishniy et al. are the source of two papers and essentially the same testing situations, and other datasets were common across papers (e.g. Higgs Boson). In the rare situations there was a tie, I gave the nod to boosting methods as a. the whole point is to do better than those, b. they are the easier model to implement, and c. they are not always given the same advantages in these studies (e.g. pre-processing).\n\nFeature Type\nThe following shows results by feature type.\n\nHeterogeneous: at least 10% of categorical or numeric data with the rest of the other\nMinimal combo: means any feature inclusion of a different type. In the second table I collapse to ‘any heterogeneous’.\nBoost: Any boosting method (most of the time it’s XGBoost but could include lightGBM or other variant)\nMLP: multilayer perceptron or some variant\nDL_complex: A DL method more complex than MLP and which is typically the focus of the paper\n\nThe results suggest that current DL approaches’ strength is mostly with purely numeric data, and for heterogeneous data, simpler MLP or Boosting will generally prevail. I initially thought that boosting would do even better with heterogeneous data, and I still suspect that with more heterogeneous data and on more equal footing, results would tilt even more.\n\n\n\n\nTable 1: Feature Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nHeterogeneous\nMin. Combo\n\n\n\n\nBoost\n2\n10\n14\n6\n\n\nMLP\n2\n4\n9\n11\n\n\nDL_complex\n0\n22\n7\n5\n\n\n\n\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nAny Combo\n\n\n\n\nBoost\n2\n10\n20\n\n\nMLP\n2\n4\n20\n\n\nDL_complex\n0\n22\n12\n\n\n\n\n\n\n\n\n\n\n\n\nSample/Feature Set Size\nThe following suggests that complex DL methods are going to require a lot of data to perform better. This isn’t that surprising but the difference here is quite dramatic. Interestingly, MLP methods worked well for fewer features. N total in this case means total size reported (not just training).\n\n\n\n\nTable 2: Sample Size\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nN features\nN total\n\n\n\n\nBoost\n209\n133,309\n\n\nDL_complex\n207\n530,976\n\n\nMLP\n114\n114,164\n\n\n\n\n\n\n\n\n\n\n\n\nTarget Type\nIn the following we compare binary (bin), multiclass (mc), and numeric (num) target results1, but there’s no strong conclusion for this. The main thing to glean from this is that these papers do not test numeric targets nearly enough. Across dozens of disciplines and countless datasets that I’ve come across in various settings, if anything, this ratio should be reversed.\n\n\n\n\nTable 3: Target Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nbin\nmc\nnum\n\n\n\n\nBoost\n17\n10\n5\n\n\nDL_complex\n17\n11\n6\n\n\nMLP\n10\n14\n2\n\n\n\n\n\n\n\n\n\n\n\n\nCombinations\nIn the following I look at any heterogeneous, smaller data (N &lt; 200,000). A complex DL model will likely not do great in this setting.\n\n\n\n\nTable 4: Combinations\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nn\n\n\n\n\nBoost\n19\n\n\nDL_complex\n8\n\n\nMLP\n19\n\n\n\n\n\n\n\n\n\n\nNow, on to the details of some of the recent results that were included."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "On Embeddings for Numerical Features in Tabular Deep Learning",
    "text": "On Embeddings for Numerical Features in Tabular Deep Learning\n\nAuthors: Gorishniy, Rubachev, & Babenko\nYear: 2022\nArxiv Link\n\n\nOverview\nYura Gorishniy, Rubachev, and Babenko (2022) pit several architectures against one another, such as standard multilayer perceptron (MLP), ResNet, and their own transformer approach (see Yuri Gorishniy et al. (2021)). Their previous work, which was summarized in my earlier post, was focused on the architecture, while here they focus on embedding approaches. The primary idea is to take the value of some feature and expand it to some embedding space, then use the embedding in lieu of the raw feature. It can essentially be seen as a pre-processing task.\nOne approach they use is piecewise linear encoding (PLE), which they at one point describe as ‘a continuous alternative to the one-hot encoding’2. Another embedding they use is basically a fourier transform.\n\n\nData\n\n12 public datasets mostly from previous works on tabular DL and Kaggle competitions.\nSizes were from ~10K to &gt;1M.\nTarget variables were binary, multiclass, or numeric.\nThe number of features ranged from 8 to 200.\n\n9 of 12 data sets had only numeric features, two had a single categorical feature, and unfortunately, only one of these might be called truly heterogeneous, i.e., with a notable mix of categorical and numeric features3.\n\n\n\nModels Explored\n\nCatBoost\nXGBoost\nMLP, MLP*\nResNet, ResNet*\nTransformer*\n\n* Using proposed embeddings\n\n\nQuick Summary\n\nA mix of results with no clear/obvious winners (results are less distinguishable if one keeps to the actual precision of the performance metrics, and even less so if talking about statistical differences in performance).\n\nSeveral datasets showed no practical difference across any model (e.g. all accuracy results within ~.01 of each other).\n\nEmbedding-based approaches generally tend to improve over their non-embedding counter parts (e.g. MLP + embedding &gt; MLP), this was possibly the clearest result of the paper.\nI’m not sure we could say the same for ResNet, where results were similar with or without embedding\nXGBoost was best on the one truly heterogeneous dataset.\n\n\n\nIn general this was an interesting paper, and I liked the simple embedding approaches used. It was nice to see that they may be useful in some contexts. The fourier transform is something that analysts (including our team at Strong) have used in boosting, so I’m a bit curious why they don’t do Boosting + embeddings for comparison for that or both embedding types. These embeddings can be seen as a pre-processing step, so nothing would keep someone from using them for any model.\nAnother interesting aspect was how little difference there was in model performance. It seemed half the datasets showed extremely small differences between any model type."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "title": "Deep Learning for Tabular Data",
    "section": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training",
    "text": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training\n\nAuthors: Somepalli, Goldblum, Schwarzschild, Bayan-Bruss, & Goldstein\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper applies BERT-style attention over rows and columns, along with embedding/data augmentation. They distinguish the standard attention over features, with intersample attention of rows. In addition, they use CutMix for data augmentation (originally devised for images), which basically combines pairs of observations to create a new observation4. Their model is called SAINT, the Self-Attention and Intersample Attention Transformer.\n\n\nData\n\n16 data sets\nAll classification, 2 multiclass\n6 are heterogeneous, 2 notably so\nSizes 200 to almost 500K\n\n\n\nModels Explored\n\nLogistic Regression (!)\nRandom Forest\nBoosting\n\nCatBoost\nXGBoost\nLightGBM\n\nMLP\nTabNet\nVIME\nTabTransformer\nSAINT\n\n\n\nQuick Summary\n\nIt seems the SAINT does quite well on some of the data, and average AUROC across all datasets is higher than XGB.\nMain table shows only 9 datasets though, which they call ‘representative’ but it’s not clear what that means when you only have 16 to start. One dataset showed near perfect classification for all models so will not be considered. Of the 15 total remaining:\n\nSAINT wins 10 (including 3 heterogeneous)\nBoosting wins 5 (including 2 heterogeneous)\n\nSAINT benefits from data augmentation. This could have been applied to any of the other models, but doesn’t appear to have been done.\nAt least they also used some form of logistic regression as a baseline, though I couldn’t find details on its implementation (e.g. regularization, including interactions). I don’t think this sort of simple baseline is utilized enough.\n\nThis is an interesting result, but somewhat dampened by lack of including numeric targets and more heterogeneous data. The authors include small data settings which is great, and are careful to not generalize despite some good results, which I can appreciate.\nI really like the fact they also compare a simple logistic regression to these models, because if you’re not able to perform notably better relative to the simplest model one could do, then why would we care? The fact that logistic regression is at times competitive and even beats boosting/SAINT methods occasionally gives me pause though. Perhaps some of these data are not sufficiently complex to be useful in distinguishing these methods? It is realistic though. While it’s best not to assume as such, sometimes a linear model is appropriate given the features and target at hand."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
    "text": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning\n\nAuthors: Kossen, Band, Lyle, Gomez, Rainforth, & Gal\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper introduces Non-Parametric Transformers, which focus on holistic processing of multiple inputs, and attempts to consider an entire dataset as input as opposed to a single row. Their model attempts to learn relations between data points to aid prediction. They use a mask to identify prediction points from the non-masked data, i.e. the entire \\(X_{\\textrm{not masked}}\\text{ }\\) data used to predict \\(X_{\\textrm{masked}}\\text{ }\\). The X matrix actually includes the target (also masked vs. not). At prediction, the model is able to make use of the correlations of inputs of training to ultimately make a prediction.\n\n\nData\n\n10 datasets from UCI, 2 are image (CIFAR MNIST)\n4 binary, 2 multiclass, 4 numeric targets\n\n\n\nModels Explored\n\nNPT\nBoosting\n\nGB\nXGB\nCatBoost\nLightGBM\n\nRandom Forest\nTabNet\nKnn\n\n\n\nQuick Summary\n\nGood performance of these models, but not too different from best boosting model for any type of data.\n\nNPT best on binary classification, but similar to CatBoost\nSame as XGB and similar to MLP on multiclass\nBoosting slightly better on numeric targets, but NPT similar\n\nAs seen several times now, TabNet continues to underperform\nk-nn regression worst (not surprising)\n\nWhen I first read the abstract where they say “We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input.”, I immediately was like ‘What about this, that, and those?’. The key phrase was ‘deep learning’, because the authors note later that this has a very long history in the statistical modeling realm. I was glad to see in their background of the research that they explicitly noted the models that came to my mind, like gaussian processes, kernel regression, etc. Beyond that, many are familiar with techniques like knn-regression and predictive mean matching, so it’s definitely not new to consider more than a single data point for prediction. I thought it was good of them to add k-nn regression to the model mix, even though it was not going to do well compared to the other approaches.\nThough the author’s acknowledge a clear thread/history here, I’m not sure this result is the fundamental shift they claim, versus a further extension/expansion into the DL domain. Even techniques that may work on a single input at a time may ultimately be taking advantage of correlations among the inputs (e.g. spatial correlations in images). Also, automatic learning of feature interactions is standard even in basic regularized regression settings, but here their focus is on observation interactions (but see k-nn regression)."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "title": "Deep Learning for Tabular Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn the two reviews on DL for tabular data that I’ve done, it appears there is more work in store for DL methods applied to tabular data. While it’d be nice to have any technique that would substantially improve prediction for such settings, I do have a suspicion results are likely rosier than they are, since that is just about the case for any newly touted technique, and at least in some cases, I don’t think we’re even making apple to apple comparisons.\nThat said, I do feel like some ground has been made for DL applications for tabular data, in that architectures can now more consistently performing as well as boosting methods in certain settings, especially if we include MLP. In the end though, results don’t appear strong enough to warrant a switch from boosting for truly heterogeneous data, or even tabular data in general. I feel like someday we’ll maybe have a breakthrough, but in the meantime, we can just agree that messy data is hard stuff to model, and the best tool is whichever one works for your specific situation."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "title": "Deep Learning for Tabular Data",
    "section": "Guidelines for future research",
    "text": "Guidelines for future research\nI was thinking about what would be a convincing result, the type of setting and setup where if a DL technique was consistently performing statistically better than boosting methods, I’d be impressed. So I’ve made a list of things I’d like to see more of, and which would make for a better story if the DL method were to beat out other techniques.\n\nAlways use heterogeneous data. For giggles let’s say 20%+ of the minority feature type.\nFeatures should at least be minimally correlated, if not notably so.\nImage data results are not interesting (why would we use boosting on this in practice?).\nNumeric targets should at least be as much of focus as categorical targets.\nInclude ‘small’ datasets.\nInclude very structured data (e.g. clustered with repeated observations, geographical points, time series).\nUse a flexible generalized additive or similar penalized regression with interactions as a baseline statistical model.\nMaybe add survival targets to the mix.\nIf using a pre-processing step that is done outside of modeling, this likely should be applied to non-DL methods for better comparison, especially, if we’re only considering predictive accuracy and don’t care too much about interpretation.\nNote your model variants before analyzing any data. Tweaking/torturing model architecture after results don’t pan out is akin to p-hacking in the statistical realm, and likewise wastes both researcher and reader’s time.\nRegarding results…\n\nDon’t claim differences that you don’t have precision to do so, or at least back them up with an actual statistical test.\nIf margin of error in the metrics is overlapping, while statistically they could be different, practically they probably aren’t to most readers. Don’t make a big deal about it.\nIt is unlikely anyone will be interested in three decimal place differences for rmse/acc type metrics, and statistically, results often don’t even support two decimal precision.\nReport how you are obtaining uncertainty in any error estimates.\nIf straightforward, try to give an estimate of total tuning/run times.\n\nWith the datasets\n\nName datasets exactly how they are named at the source you obtained them from, provide direct links\nProvide a breakdown for both feature and target types\nProvide clear delineation of total/training/validation/test sizes"
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "title": "Deep Learning for Tabular Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t refer to numeric targets as ‘regression’ because that’s silly for so many reasons. 😄↩︎\nA quick look suggests it’s not too dissimilar from a b-spline.↩︎\nI’ll let you go ahead and make your own prediction about which method was best on that data set.↩︎\nIt’s not clear to me how well this CutUp approach would actually preserve feature correlations. My gut tells me the feature correlations of this approach would be reduced relative to the observed, since the variability of the new observations is likely reduced. This ultimately may not matter for predictive purposes or their ultimate use in embeddings. However, I wonder if something like SMOTE, random (bootstrap) sampling, other DL methods like autoencoders, or similar approaches might do the same or better.↩︎"
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html",
    "href": "posts/2021-07-15-dl-for-tabular/index.html",
    "title": "This is definitely not all you need",
    "section": "",
    "text": "I’ve been a little perplexed at the lack of attention of deep learning (DL) toward what I consider to be ‘default’ data in my world, often referred to as tabular data, where typically we have a two dimensional input of observations (rows) and features (columns) and inputs are of varying type, scale and source. Despite the ubiquity of such data in data science generally, and despite momentous advances in areas like computer vision and natural language processing, at this time, it’s not very clear what the status of DL for tabular data is.\nThere have been developments in the area recently though, with some modeling approaches, such as TabNet, gaining traction. In June of 2021, I actually came across three papers on Arxiv that were making related claims about the efficacy of DL for tabular data. As in many academic and practical (and life) pursuits, results of these studies are nuanced, so I thought I’d help myself and others by summarizing here."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#motivation",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#motivation",
    "title": "This is definitely not all you need",
    "section": "",
    "text": "I’ve been a little perplexed at the lack of attention of deep learning (DL) toward what I consider to be ‘default’ data in my world, often referred to as tabular data, where typically we have a two dimensional input of observations (rows) and features (columns) and inputs are of varying type, scale and source. Despite the ubiquity of such data in data science generally, and despite momentous advances in areas like computer vision and natural language processing, at this time, it’s not very clear what the status of DL for tabular data is.\nThere have been developments in the area recently though, with some modeling approaches, such as TabNet, gaining traction. In June of 2021, I actually came across three papers on Arxiv that were making related claims about the efficacy of DL for tabular data. As in many academic and practical (and life) pursuits, results of these studies are nuanced, so I thought I’d help myself and others by summarizing here."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#goal",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#goal",
    "title": "This is definitely not all you need",
    "section": "Goal",
    "text": "Goal\nI want to know if, e.g. time and/or resources are limited, whether it will be worth diving into a DL model if I have a satisfactory simpler/easier one ready to implement that does pretty well. Perhaps this answer is already, ‘if it ain’t broke, don’t fix it’, but given the advancements in other data domains, it would be good to assess what the current state of DL with tabular data is."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#caveats",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#caveats",
    "title": "This is definitely not all you need",
    "section": "Caveats",
    "text": "Caveats\n\nI’m not going to do more than give a cursory summary of the articles, and provide no in-depth explanation of the models. For more detail, see the corresponding articles and references for the models therein. You are not going to learn how to use TabNet, NODE, transformers, etc., for tabular data.\nThere are other decent articles on the topic not covered here. Some are referenced in these more recent offerings, so feel free to peruse."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#quick-take",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#quick-take",
    "title": "This is definitely not all you need",
    "section": "Quick Take",
    "text": "Quick Take\nIn case you don’t want any detail, here’s a quick summary based on my impressions from these articles. Right now, if you want to use DL on tabular data, don’t make a fuss of it. A simple architecture, even a standard multi-layer perceptron, will likely do as well as more complicated ones. In general though, the amount of effort put into prep/tuning may not be worth it for many typical tabular data settings, for example, relative to a suitably flexible statistical model (e.g. GAMM) or a default fast boosting implementation like XGBoost. However, DL models are already thinking ‘big data’, so for very large data situations, a DL model might make a great choice, as others may not be computationally very viable. It also will not be surprising at all that in the near future some big hurdle may be overcome as we saw with DL applications in other fields, in which case some form of DL may be ‘all you need’.\nNow, on to the rest!"
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#tabular-data-deep-learning-is-not-all-you-need",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#tabular-data-deep-learning-is-not-all-you-need",
    "title": "This is definitely not all you need",
    "section": "Tabular Data: Deep Learning is Not All You Need",
    "text": "Tabular Data: Deep Learning is Not All You Need\n\nPaper Info\n\nWho: Shwartz-Ziv & Armon\nWhere: Intel\nWhen: 2021-06-21 V1\nArxiv Link\n\n\n\nFrom the Abstract\n\nWe analyze the deep models proposed in four recent papers across eleven datasets, nine of which were used in these papers, to answer these questions. We show that in most cases, each model performs best on the datasets used in its respective paper but significantly worse on other datasets. Moreover, our study shows that XGBoost (Chen and Guestrin, 2016) usually outperforms the deep models on these datasets. Furthermore, we demonstrate that the hyperparameter search process was much shorter for XGBoost.\n\n\n\nOverview\nFor each model they used the data that was implemented in the original model papers by the authors (e.g. the dataset used in the TabNet article), and also used their suggested parameter settings. They tested all the models against their own data, plus the other papers’ data, plus two additional data sets that were not used in any of the original papers.\n\n\nData\nThey use eleven total datasets. Nine datasets are those used in the original papers on TabNet, DNF-Net, and NODE, drawing three datasets from each paper. Additionally, Shwartz-Ziv & Armon use two Kaggle datasets not used in any of those papers. Sample sizes ranged from 7k to 1M, 10-2000 features, with two being numeric targets, while the other target variables ranged from 2-7 classes. Datasets are described in detail in the paper along with links to the source (all publicly available).\n\n\nModels Explored\nBrief summaries of the DL models are found in the paper.\n\nXGBoost\nTabNet\nNeural Oblivious Decision Ensembles (NODE)\nDNF-Net\n1D-CNN\n\n\n\nQuick Summary\n\nNot counting the ensemble methods…\n\nTabNet did best on all of its own data sets, but was not the best model on any other.\nNODE each did best on 2 of its own 3 data sets, but not on any other.\nDNF-Net best on one of its own 3 data sets, but not on any other.\nXGBoost was best on the remaining 5 datasets.\n\n\n\nCounting the ensemble methods…\n\nTabNet did best on 2 of its own 3 data sets, but was not the best model on any other.\nDNF-Net and NODE each did best on one of its own 3 data sets, but not on any other.\nXGBoost was best on one dataset.\n\nOf those, XGB was notably better on ‘unseen’ data, and comparable to the best performing ensemble. A simple ensemble was also very performant. From the paper:\n\nThe ensemble of all the models was the best model with 2.32% average relative increase, XGBoost was the second best with 3.4%, 1D-CNN had 7.5%, TabNet had 10.5%, DNF-Net had 11.8% and NODE had 14.2% (see Tables 2 and 3 in the appendix for full results).\n\nAs a side note, XGBoost + DL was best, but that defeats the purpose in my opinion. Presumably any notably more complicated setting will be potentially better with enough complexity, but unless there is an obvious way on how to add such complexity, it’s mostly an academic exercise. However, as the authors note, if search is automated, maybe the complexity of combining the models is less of an issue.\n\n\n\nOther stuff\n\nKudos\nThe authors cite the No Free Lunch theorem in the second paragraph, something that appears to be lost on many (most?) of these types of papers touting small increases in performance for some given modeling approach.\n\n\nIssues\nThere are always things like training process/settings that are difficult to fully replicate. By the time authors publish any paper, unless exact records are kept, the iterations (including discussions that rule out various paths) are largely lost to time. This isn’t a knock on this paper, just something to keep in mind.\n\n\nOpinion\nI liked this one in general. They start by giving the competing models their best chance with their own settings and data, which was processed and trained in the same way. Even then, those models still either didn’t perform best, and/or performed relatively poorly on any other dataset."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data",
    "title": "This is definitely not all you need",
    "section": "Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data",
    "text": "Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data\n\nPaper Info\n\nWho: Kadra et al.\nWhere: U of Freiburg, Leibniz U (Germany)\nWhen: 2021-06-06 V1\nArxiv Link\n\n\n\nFrom the Abstract\n\nTabular datasets are the last “unconquered castle” for deep learning… In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters.\n\n\nWe empirically assess the impact of these regularization cocktails for MLPs on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.\n\n\nWe emphasize that some of these publications claim to outperform Gradient Boosted Decision Trees (GDBT) [1, 37], and other papers explicitly stress that their neural networks do not outperform GBDT on tabular datasets [38, 22]. In contrast, we do not propose a new kind of neural architecture, but a novel paradigm for learning a combination of regularization methods.**\n\n\n\nOverview\nThis data is more about exploring regularization techniques (e.g. data augmentation, model averaging via dropout) rather than suggesting any particular model is superior. Even in the second paragraph they state their results do not suggest a performance gain over boosting methods. Their focus is on potentially improving DL for tabular data through regularization with two hypotheses:\n\nRegularization cocktails outperform state-of-the-art deep learning architectures on tabular datasets.\nRegularization cocktails outperform Gradient-Boosted Decision Trees, as the most commonly used traditional ML method for tabular data.\n\n\n\nData\nForty total datasets ranging from as little as ~400 observations to over 400k, and between 4 and 2000 features. All were categorical targets, with about half binary. All available at openml.org with target ID provided.\n\n\nModels Explored\nComparison models:\n\nTabNet: (with author’s proposed defaults)\nNODE: (with author’s proposed defaults)\nAutogluon: Tabular: can use other techniques but restricted to ensembles of neural nets for this demo\nASK-GBDT: GB via Auto-sklearn (Note this tool comes from one of the authors )\nXGBoost: Original implementation\nMLP: Multilayer Perceptron - 9 layers with 512 hidden units each.\nMLP+D: MLP with Dropout\nMLP+C: MLP with regularization cocktail\n\n\n\nQuick Summary\n\nTo begin, their regularization cocktail approach is the clear winner on these datasets, having one outright on over 40% of them (based on table 2).\nStandard XGB performed best (or tied for best) 8 of the 40 data sets, while it or ASK-GBDT were best for 12 datasets combined.\nSimple MLP was best once, while MLP with dropout was best 5 times, while the cocktail method was best in general, across 19 datasets.\nThe ‘fancy’ DL models were the worst performers across the board. TabNet never performed best, and NODE only did once, but the latter also repeatedly failed due to memory issues or run-time limitations (this memory issue was mentioned in the previous paper also).\nHead-to-head, the cocktail beat the standard XGB 26 out of 38 times with three ties. So it wins 65% of the time against XGB, 70% against ASK-GBDT, but 60% against either (i.e. some XGB approach).\n\n\n\nOther Stuff\n\nKudos\n\nRecognize that tabular data is understudied in mainstream DL literature\nThey used a lot of datasets\nThey look at the simplest DL models for comparison\n\n\n\nIssues\n\nI wonder why there was not a single numeric outcome among so many datasets. Furthermore, some of the data are image classification (e.g. Fashion-MNIST), so I’m not sure why they’re included. I wouldn’t use a ‘tabular’ technique when standard computer vision approaches already work so well.\nI’m not familiar with the augmentation techniques they mention, which were devised for image classification, but there have been some used for tabular data for a couple decades at this point that were not mentioned, including simple upsampling, or imputation methods (e.g. SMOTE). That’s not a beef with the article at all, I’ve long wondered why people haven’t been using data augmentation for tabular data given it’s success elsewhere (including for tabular data!).\nThey use a standard t-tests of ranks, but if we’re going to use this sort of approach, we’d maybe want to adjust for all the tests done, and probably for all pairwise comparisons (they show such a table for the regularization methods). Depending on the approach and cutoff, the XGB vs. Cocktail difference may not be significant.\nAlso, I couldn’t duplicate these p-values with R’s default settings for Wilcoxon signed rank tests, and there does in fact seem to be inconsistency between the detailed results and Wilcoxon summaries. For example, in the regularization tests of Table 9, Cocktail vs. WD and DO shows two ties in the first four data sets, yet only 1 tie is reported in the comparison chart for both (Figure 4). For the models, Table 2 show 3 ties of XGB & the Cocktail, with 1 for ASK-G and Cocktail, but 2 and 0 are reported for their Wilcoxon tests. It’s not clear what they did for NODE with all the NAs. I do not believe these discrepancies, nor adjusting for multiple comparisons, will change the results (I re-did those myself).\n\n\n\nOpinion\nIf we ignore the regularization focus and just look at the model comparisons, I’m not overly convinced we have a straightforward victory for cocktail vs. GB as implied in the conclusion. Results appear to be in favor of their proposed method, but not enough to be a near-guarantee in a particular setting, so we’re back to square one of just using the easier/faster/better tool. I’m also not sure who was questioning the use of regularization for neural networks or modeling in general, so the comparison to any model without some form of regularization isn’t as interesting to me. What is interesting to me is that we have another round of evidence that the fancier DL models like TabNet do not perform that well relative to GB or simpler DL architectures."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#revisiting-deep-learning-models-for-tabular-data",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#revisiting-deep-learning-models-for-tabular-data",
    "title": "This is definitely not all you need",
    "section": "Revisiting Deep Learning Models for Tabular Data",
    "text": "Revisiting Deep Learning Models for Tabular Data\n\nPaper Info\n\nWho: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko\nWhere: Yandex (Russia)\nWhen: 2021-06-22\nArxiv Link\nSource code\n\n\n\nFrom the Abstract\n\nThe necessity of deep learning for tabular data is still an unanswered question addressed by a large number of research efforts. The recent literature on tabular DL proposes several deep architectures reported to be superior to traditional “shallow” models like Gradient Boosted Decision Trees. However, since existing works often use different benchmarks and tuning protocols, it is unclear if the proposed models universally outperform GBDT. Moreover, the models are often not compared to each other, therefore, it is challenging to identify the best deep model for practitioners.\n\n\nIn this work, we start from a thorough review of the main families of DL models recently developed for tabular data. We carefully tune and evaluate them on a wide range of datasets and reveal two significant findings. First, we show that the choice between GBDT and DL models highly depends on data and there is still no universally superior solution. Second, we demonstrate that a simple ResNet-like architecture is a surprisingly effective baseline, which outperforms most of the sophisticated models from the DL literature. Finally, we design a simple adaptation of the Transformer architecture for tabular data that becomes a new strong DL baseline and reduces the gap between GBDT and DL models on datasets where GBDT dominates.\n\n\n\nOverview\nThis paper compares different models on a variety of datasets. They are interested in the GB vs. DL debate, but like the previous paper, also interested in how well a simpler DL architecture might perform, and what steps might help the more complicated ones do better.\n\n\nData\nThey have 11 datasets with a mix of binary, multiclass and numeric targets. Sizes range from 20K to 1M+. There appears to be some overlap with the first paper (e.g. Higgs, Cover type).\n\n\nModels Explored\n\n‘Baselines’\n\nXGBoost\nCatBoost\nMLP\nResNet\n\n\n\nDL Comparisons\n\nSNN\nNODE\nTabNet\nGrowNet\nDCN V2\nAutoInt\n\nIn addition, they look at ensembles of these models, but this is not of interest to me for this post.\n\n\n\nQuick Summary\nNote that these refer to the ‘single model’ results, not the results for ensembles.\n\nSome form of boosting performed best on 4 of the 11 datasets.\nResNet was best on four classification tasks, but not once for numeric targets.\nAt this point you won’t be surprised at what doesn’t perform as well- TabNet, NODE, and similar. TabNet, DCN, and GrowNet were never the best performer, and the other three were best one time a piece.\nMLP did not perform best on any data, however the authors note that it ‘is often on par or even better than some of the recently proposed DL models’.\nThey also looked at models with a ‘simple’ transformer architecture. Their results suggest better performance than the other DL models, and similar performance to ResNet.\n\n\n\nOther Stuff\n\nKudos\n\nSharing the source code!\nRecognizing that results at this point are complex at best given the lack of standard datasets\n\n\n\nIssues\n\nThey note a distinction between heterogeneous vs. other types of data. They call data heterogeneous if the predictors are of mixed data types (e.g. categorical, numeric, count), while something like pixel data would be homogeneous because all the columns are essentially the same type. The latter isn’t as interesting to me for this sort enterprise, and I think the former is what most are thinking about for ‘tabular’ data, otherwise we’d just call it what it is (e.g. image or text data), and modeling/estimation is generally quite a bit easier when all the data is the same type. I do think it’s important that they point out that GB is better with heterogeneous data, and I think if you only look at such data, you’d likely see GB methods still outperforming or at worst on par with the best DL methods.\n\n\n\nOpinion\nThese results seem consistent with others at this point. Complex DL isn’t helping, and simpler architectures, even standard MLP show good performance. In the end, we still don’t have any clear winner over GB methods."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#overall-assessment",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#overall-assessment",
    "title": "This is definitely not all you need",
    "section": "Overall Assessment",
    "text": "Overall Assessment\nThese papers put together are helpful in painting a picture of where we are at present with deep learning for tabular data, especially with mixed data types. In this setting, it seems that more complicated DL models do not seem to have any obvious gain over simpler architectures, which themselves do not consistently beat boosting methods. It may also be the case that for data of mixed data types/sources, boosting is still the standard to beat.\nEven though these articles are geared toward comparisons to GB/XGBoost, in several settings I’ve applied them, I typically do not necessarily have appreciably greater success compared to a default setting random forest (e.g. from the ranger package in R), or sufficiently flexible statistical model. Unfortunately this comparison is lacking from the papers, and would have been nice to have, especially for smaller data settings where such models are still very viable. I think a viable fast model, preferably one without any tuning required (or which simply is taken off the shelf) should be the baseline.\nIn that light, for tabular data I think one should maybe start with a baseline of a penalized regression with appropriate interactions (e.g. ridge/lasso), or a more flexible penalized approach (GAMM) as a baseline, the latter especially, as it can at least automatically incorporate nonlinear relationships, and tools like mgcv or gpboost in R can do so with very large data (1 million +) in a matter of seconds. In settings of relatively higher dimensions, interactions and nonlinearities should be prevalent enough such that basis function, tree, and DL models should be superior. Whether they are practically so is the key concern even in those settings. With smaller, noisier data of less dimension, I suspect the tuning/time effort with present day DL models for tabular data will likely not be worth it. This may change very soon however, so such an assumption should be regularly checked.\n \nlast updated: 2024-12-27\nNeural Net image source from UC Business Analytics R Programming Guide"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html",
    "title": "Practical Bayes Part II",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#outline-for-better-bayesian-analysis",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#outline-for-better-bayesian-analysis",
    "title": "Practical Bayes Part II",
    "section": "Outline for Better Bayesian Analysis",
    "text": "Outline for Better Bayesian Analysis\nWe’ll cover the following steps in more detail, but here is a general outline.\n\nFirst generate ‘fake data’ to assess viability of our priors\nWith adequate priors, start with a simple, but plausible model\nFor simple models you likely do not need many iterations, and for debugging/troubleshooting, starting with few iterations can possibly give you a sense of whether there will be problems1. If you are doing standard GLM or simpler versions of common extensions, even the defaults are likely overkill. For example, a basic linear regression should converge almost immediately.\nIf you have problems at this point, see Part I\n\n\n\nExplore and Visualize the Model Results\n\nVisualize covariate relationships\nAssess model effectiveness\n\nUse posterior predictive checks\nOther avenues\n\n\nPrediction and Model Comparisons\n\nGet basic predictions for observations of interest\nExplore a more viable model\n\nAdd interactions\nAdd nonlinear relations\nAccount for other structure (e.g. random effects)\n\nCompare and/or average models\nUse cross-validation to better assess performance\n\n\nWe’ll now demonstrate these steps."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#check-priors",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#check-priors",
    "title": "Practical Bayes Part II",
    "section": "Check Priors",
    "text": "Check Priors\nThough we did some work to select our prior distributions beforehand, we might still be concerned about how influential our priors were. So how can we check whether our priors were informative? The following uses the bayestestR package to do a simple check of whether the posterior standard deviation is greater than 10% of the prior standard deviation2. Having an informative prior isn’t really a problem in my opinion, unless it’s more informative than you wanted. For example, shrinkage of a coefficient towards zero will generally help avoid overfitting.\n\nprior_summary(model_baseline)\n\n                  prior     class      coef group resp dpar nlpar lb ub\n           normal(0, 1)         b                                      \n           normal(0, 1)         b       b11                            \n           normal(0, 1)         b       b21                            \n           normal(0, 1)         b        x1                            \n           normal(0, 1)         b        x2                            \n           normal(0, 1)         b        x3                            \n student_t(3, 2.9, 2.5) Intercept                                      \n    student_t(10, 1, 1)        sd                                  0   \n    student_t(10, 1, 1)        sd           group                  0   \n    student_t(10, 1, 1)        sd Intercept group                  0   \n    student_t(10, 1, 1)     sigma                                  0   \n       source\n         user\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n         user\n (vectorized)\n (vectorized)\n         user\n\nbayestestR::check_prior(model_baseline)\n\n    Parameter Prior_Quality\n1 b_Intercept uninformative\n2       b_b11 uninformative\n3       b_b21   informative\n4        b_x1 uninformative\n5        b_x2 uninformative\n6        b_x3 uninformative\n\n\nThese results suggest that we might be more informative, but for the intercept, which we largely aren’t too worried about, and for the factor that is highly unbalanced (b2), but which has no obvious solution. I personally would be fine with this result, especially since we took initial care in choosing these priors. If you really wanted to, you could change the priors that were informative."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#explore-and-visualize-results",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#explore-and-visualize-results",
    "title": "Practical Bayes Part II",
    "section": "Explore and Visualize Results",
    "text": "Explore and Visualize Results\nNow that we are feeling pretty good about the results we have, we can explore the model further. We can plot covariate effects easily with brms. The conditional_effects function is what we want here. I show results for one effect below. Without interactions or other things going, on they aren’t very interesting, but it’s a useful tool nonetheless. We’ll come back to this later.\n\nconditional_effects(model_baseline, 'b2')\n\n\n\n\n\n\n\n\nWe can also use the hypothesis function to test for specific types of effects. By default they provide a one-sided probability and uncertainty interval. For starters, we can just duplicate what we saw in the previous summary for the b2 effect. The only benefit is to easily obtain the one-sided p-value (e.g. that b2 is less than zero) and the corresponding evidence ratio, which is just p/(1-p).\n\nhypothesis(model_baseline, 'b21 &lt; 0')\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1  (b21) &lt; 0    -0.14      0.18    -0.45     0.16       3.46      0.78     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nBut we can really try anything, which is the power of this function. As an example, the following tests whether the combined effect of our categorical covariates is greater than twice the value of the x1 effect3.\n\nhypothesis(model_baseline, 'abs(b11) + abs(b21) &gt; 2*x1')\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (abs(b11)+abs(b21... &gt; 0     0.92      0.17     0.67     1.22        Inf\n  Post.Prob Star\n1         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nOne should get used to whatever tools are available for further understanding covariate effects or other parameters. This will likely lead to some of the more interesting discussion of your findings, or at least, notably more interesting than a standard regression table."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-effectiveness",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-effectiveness",
    "title": "Practical Bayes Part II",
    "section": "Model Effectiveness",
    "text": "Model Effectiveness\nIt is one thing to look at specific effects. but a natural question to ask is how useful our model actually is as a whole. This then suggests we need to know how to define such utility. Such an assessment definitely cannot be made with something like ‘statistical significance’. Science of any kind is nothing without prediction, so we we can start there.\n\nPosterior predictive checks\nPosterior predictive checks are a key component of Bayesian analysis. The prior checks we did before are just a special case of this. Here we instead use the posterior distributions of parameters to generate the data, and compare this model-implied/synthetic data to what we actually observe. Doing so can give insight to where the model succeeds and fails.\n\npp_check(model_baseline, ndraws = 100)\n\n\n\n\n\n\n\npp_check(model_baseline, ndraws = 10, type ='error_scatter_avg', alpha = .1)\n\n\n\n\n\n\n\n\nIn this case, we see good alignment between model and data, and no obvious pattern to the types of errors we are getting. It is often the case that we see that the model does not capture the most extreme values well, but that’s not terribly surprising. With simulated data, our situation is more pristine to begin with, but you generally can’t expect such a clean result in practice.\nAs an example, consider predictions with and without random effects. Including the cluster-specific effects for prediction appear to do better with the capturing the tails.\n\n\n\n\n\n\n\n\n\nWe can use the same approach to look at specific statistical measures of interest. For example, the following suggests our model is pretty good at capturing the minimum value, but typically underestimates the maximum value, which we noted earlier, is not especially unexpected in practice, particularly with smaller sample data.\n\npp_check(model_baseline, ndraws = 100, type ='stat', stat='median')\n\n\n\n\n\n\n\npp_check(model_baseline, ndraws = 100, type ='stat', stat = 'max')\n\n\n\n\n\n\n\n\nWe can define any function to use for our posterior predictive check. The following shows how to examine the 10th and 90th quantiles. Minimum and maximum values are unlikely to be captured very well due to their inherent variability, so looking at less extreme quantiles (e.g. 10th or 90th percentile) might be a better way to assess whether the model captures the tails of a distribution.\n\nq10 = function(y) quantile(y, 0.1)\nq90 = function(y) quantile(y, 0.9)\n\npp_check(model_baseline, ndraws = 100, type ='stat', stat = 'q90')\n\npp_check(model_baseline, ndraws = 100, type ='stat_2d', stat = c('q10', 'q90'))\n\n\n\nBayes R-squared\nIn this modeling scenario, we can examine the amount of variance accounted for in the target variable by the covariates. I don’t really recommend this beyond linear models that assume a normal distribution for the target, but people like to report it. Conceptually, it is simply a (squared) correlation of fitted values with the observed target values, so can be seen as descriptive statistic. Since we are Bayesians, we also get a ready-made interval for it, as it is based on the posterior predictive distribution. But to stress the complexity in trying to assess this, in this mixed model we can obtain the result with the random effect included (conditional) or without (unconditional). Both are reasonable ways to express the statistic, but the one including the group effect naturally will be superior, assuming the group-level variance is notable in the first place.\n\nbayes_R2(model_baseline)                   # random effects included\n\n    Estimate  Est.Error      Q2.5     Q97.5\nR2 0.4812032 0.01781973 0.4445528 0.5148587\n\nbayes_R2(model_baseline, re_formula = NA)  # random effects not included\n\n    Estimate  Est.Error       Q2.5    Q97.5\nR2 0.1161024 0.01498238 0.08729654 0.144502\n\n# performance::r2_bayes(model_baseline)    # performance package provides both\n\nTo show the limitation of R2, I rerun the model using a restrictive prior on the intercept. Intercepts for the resulting models are different but the other fixed effects are basically the same. The R2 suggests equal performance of both models.\n\n\n\n\n\nmodel\nR2\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nbaseline\n0.116\n0.015\n0.087\n0.145\n\n\nmodified\n0.116\n0.014\n0.089\n0.143\n\n\n\n\n\n\n\nHowever, a posterior predictive check shows clearly the failure of the modified model to capture the data.\n\n\n\n\n\n\n\n\n\nA variant of R2, the ‘LOO’ R2, is also available via the loo_R2 function. LOO stands for leave-one-out, as in leave-one-out cross-validation. It’s based on the residuals from the leave one out predictions. You can think of it as a better way to obtain an adjusted R2 in this setting. The results suggests that the LOO R2 actually picks up the difference in models, and would be lower for the modified model, even if we included the random effects.\nFor more on Bayesian R2, see the resources section"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#basic-prediction",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#basic-prediction",
    "title": "Practical Bayes Part II",
    "section": "Basic prediction",
    "text": "Basic prediction\nWith models in hand, let’s look at our basic predictive capabilities. We can get fitted values which include ‘confidence’ intervals, or predictions, which include ‘prediction’ intervals that include the uncertainty for a new observation. We can specify these as follows. First we create a small data set to make some predictions on. It will include both values for of the binary covariates, and the means of the numeric covariates (0).\n\nprediction_data = crossing(\n  b1 = 0:1,\n  b2 = 0:1,\n  x1 = 0,\n  x2 = 0,\n  x3 = 0\n)\n\n# fitted values\nhead(fitted(model_baseline))  \n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 3.517482 0.3382849 2.855822 4.167321\n[2,] 3.796906 0.3377263 3.118250 4.453510\n[3,] 3.866082 0.3346731 3.179920 4.511195\n[4,] 4.357828 0.3507970 3.688756 5.026747\n[5,] 3.185495 0.3381892 2.527887 3.825222\n[6,] 3.910027 0.3347265 3.244999 4.558492\n\n# new predictions\ndata.frame(\n  prediction_data,\n  predict(model_baseline, newdata = prediction_data, re_formula = NA)\n)\n\n  b1 b2 x1 x2 x3 Estimate Est.Error      Q2.5    Q97.5\n1  0  0  0  0  0 2.696397  1.069881 0.5093926 4.709590\n2  0  1  0  0  0 2.557696  1.013811 0.5566073 4.603278\n3  1  0  0  0  0 3.450119  1.082413 1.3938133 5.471762\n4  1  1  0  0  0 3.324694  1.008474 1.3336782 5.300122\n\n\nIn general, we’d always like to visualize the predictions. We can do so as we did before with the conditional_effects function, which would also allow us to set specific covariate values. For the third plot of the nonlinear effect below, I modify the basic conditional effects plot that brms provides for a slightly cleaner visualization.\n\nconditional_effects(model_baseline, effects = 'x2', conditions = prediction_data[1,])\n\n\n\n\n\n\n\nconditional_effects(model_interact, effects = 'x1:b2')\n\n\n\n\n\n\n\ninit = conditional_effects(model_interact_nonlin, effects = 'x3', spaghetti = T)\n\n\n\n\n\n\n\n\n\n\nExpanding your story through prediction is essential to helping your audience understand the model on a practical level. You would do well to spend time looking at specific data scenarios, especially in the case of nonlinear models (e.g. GLM) and models with interactions."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-comparison",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-comparison",
    "title": "Practical Bayes Part II",
    "section": "Model Comparison",
    "text": "Model Comparison\nIn typical situations it is good to have competing models, and having additional models allows us to see if improvements can be made in one way or another, both to our models, and potentially to our way of thinking about them. In a general sense, we will go about things very similarly in the Bayesian context that we would elsewhere. However, we’ll also more easily apply other approaches that are not so commonly used (even if they can be).\n\nChoosing a model\nIn traditional contexts, we can use a specific approach to pit competing models against one another, selecting the ‘best’ model based on a particular metric, for example, AIC, cross-validation error, etc. With ‘error metrics’, the model with the lowest value is the winner. In this case, nothing is new in the Bayesian world. Here, we can use estimates like WAIC and LOOIC for model comparison, much like you would AIC to compare models in traditional frameworks. The values themselves don’t tell us much, but in comparing models, lower means less predictive error for these ‘information criteria’ metrics, which is what we want4, and since we’re Bayesian, we will even have estimates of uncertainty for these values as well. We also have cross-validation approaches (which IC metrics approximate), which we will demonstrate later.\nWith our new models added to the mix, we can now make some comparisons using loo_compare. First, we’ll add LOOIC estimates to our models, which are not estimated by default.\n\nmodel_baseline = add_criterion(model_baseline,  'loo')\nmodel_interact = add_criterion(model_interact, 'loo')\nmodel_interact_nonlin = add_criterion(model_interact_nonlin, 'loo')\n\nTo start, we’ll show the LOOIC result for the baseline model. We have the total expected log probability (elpd_loo) for the leave-one-out observations. We also get stuff like p_loo, which is the effective number of parameters. For those familiar with penalized maximum likelihood, these are familiar analogues. However we also get a summary regarding Pareto k values, which we’ll talk about soon.\n\n# example\nloo(model_baseline)\n\n\nComputed from 1000 by 1000 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1500.6 21.9\np_loo        88.0  3.8\nlooic      3001.2 43.9\n------\nMCSE of elpd_loo is 0.4.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.3]).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nLet’s now compare the baseline model to the others models using loo_compare. It shows the ‘best’ (lowest-valued) model first, followed by the others. We get the difference of each elpd vs. the lowest, also get a standard error for this difference, which you could use to help assess how different the values are statistically. Just by this standard, the model that is based on the underlying data generating mechanism is the clear winner, as we would expect.\n\nloo_compare(\n  model_baseline, \n  model_interact,\n  model_interact_nonlin\n)\n\n                      elpd_diff se_diff\nmodel_interact_nonlin   0.0       0.0  \nmodel_interact        -44.5       9.4  \nmodel_baseline        -46.3       9.7  \n\n\nNow let’s compare several metrics available to us. In this particular setting, all are generally in agreement in the rank order of the models, though there appears to be no meaningful difference between the baseline and interaction models.\n\n\n\n\n\nmodel\nR2\nloo_R2\nWAIC\nLOOIC\nELPD\nweight\n\n\n\n\nbaseline\n0.48\n0.43\n2999.15\n3001.18\n-1500.59\n7.680717e-06\n\n\ninteract\n0.49\n0.43\n2995.28\n2997.42\n-1498.71\n4.067051e-06\n\n\ninteract_nonlin\n0.53\n0.48\n2905.87\n2908.49\n-1454.25\n9.999883e-01\n\n\n\n\n\n\n\nFor our ultimate model comparison we want to stick to using the IC values. As far as choosing between WAIC vs. LOOIC, the latter has better diagnostics for noting whether there are potential problems in using it. In practice, they will almost always agree with one another. As we noted previously, LOOIC reflects the ELPD, and this value is used in constructing the model weights shown in the last column5. The model weights can then be used in making final predictions (i.e. model averaging), or just providing a different way for your audience to gauge which model might be preferred.\n\n\nProblems at the loo\nAfter the model issues discussed in Part I, the next most common point of confusion I see people have is with model comparison, and using LOO in particular. Part of the reason is that this is an area of ongoing research and development, and most of the tools and documentation are notably technical. Another reason is that these are not perfect tools. They can fail to show notable problems for models that are definitely misspecified, and flag models that are essentially okay. Sometimes they flag models that other indicators may suggest are better models relatively speaking, which actually isn’t a contradiction, but which may indicate an overfit situation.\nSo in general, no tool is perfect, but in the real world we have to get things, so let’s address a couple issues.\n\nNot so different models\nLet’s start with the case where models do not appear to perform very differently. If two models aren’t very different from one another, the usual response is to go with the simpler model. For example, if we were only comparing the baseline model vs. the interaction model, there really isn’t much difference in terms of LOOIC/ELPD. However, we will have to consider things a little differently in the Bayesian context. Consider the following two thoughts.\n\nThe general issue is that with unregularized estimation such as least squares or maximum likelihood, adding parameters to a model (or making a model more complex) leads to overfitting. With regularized estimation such as multilevel modeling, Bayesian inference, lasso, deep learning, etc., the regularization adds complexity but in a way that reduces the problem of overfitting. So traditional notions of model complexity and tradeoffs are overturned. ~ Andrew Gelman\n\n\nSometimes a simple model will outperform a more complex model… Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well. ~ Radford Neal\n\nThe take-home message here is that simpler is not always better. And to be frank, using penalized (a.k.a. regularized) approaches (e.g. lasso, ridge, mixed models) should probably be our default model in the non-Bayesian context, and it turns out that such approaches actually approximate a Bayesian one with specific priors. In the end, you may have to think about things a little more carefully, and given that you are using methods that can help avoid overfitting, you may instead lean on a more complex model with otherwise similar performing models. And that would be closer to how nature works anyway, which is always more complex than our brains can easily understand.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPareto values\nLet’s look again at the basic result from using the loo function.\n\nloo(model_interact)\n\n\nComputed from 1000 by 1000 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1498.7 22.0\np_loo        89.5  3.9\nlooic      2997.4 44.0\n------\nMCSE of elpd_loo is 0.4.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.2]).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nWe haven’t yet discussed Pareto values, but it is not uncommon to get a result with some values that are not ‘good’ or ‘ok’. If you happen to see Pareto values in the ‘bad’ or ‘very bad’ group, what does it mean? You can read the definition provided here, but it may not help many due to the background knowledge needed to parse it. However, you can just understand it as an (leave-one-out) extreme value diagnostic, and if it is a problem, it mostly means your LOOIC may not be good for comparing models.\nAs in the standard model setting, ‘outliers’ indicate model incompetence, or in other words, the model is unable to understand such observations. Unless you have reason to suspect something inherently wrong in the data (e.g. an incorrect value/typo), an outlier is a sign that your model is not able to capture the data fully. It definitely is not a reason to remove the observation!\nIf you have Pareto values &gt; .7, you may recalculate LOOIC with the options provided by the loo function or use the reloo function, getting a better estimate that could then be used in, for example, model stacking for prediction. If you don’t discover many outliers, it probably won’t make much difference in your final estimates and conclusions, and so probably isn’t worth the trouble pursing much further. The output for Pareto values doesn’t even save the row identifying information that would make it easy to find which observations are the problem, but you can do something like the following if you need to.\n\npareto = loo(model_interact_nonlin)\n\nproblems = pareto$pointwise %&gt;% \n  data.frame() %&gt;% \n  rowid_to_column() %&gt;% \n  filter(influence_pareto_k &gt; .5) %&gt;% \n  pull(rowid)\n\nmodel_interact_nonlin$data %&gt;% \n  mutate(rank = rank(y)) %&gt;% \n  slice(problems)\n\n          y b1 b2        x1        x2         x3 group rank\n1  8.198540  1  1 0.3636720 0.0652158  1.3760312    68 1000\n2 -1.123859  1  0 0.8573758 0.6268734 -0.3354954    97    3\n\n\nAs we might have expected, the observations with the more extreme target values are likely to be problems (rank closer to 1 or 1000), but for some of these, there is nothing to suggest why they might be difficult, and it’s even harder to speculate in typical modeling situations with more predictors and complexity. Furthermore, outside of additional model complexity, which might then hamper interpretation, there is often little we can do about this, or at least, what we can do is generally not obvious in applied settings."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-averaging",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-averaging",
    "title": "Practical Bayes Part II",
    "section": "Model Averaging",
    "text": "Model Averaging\nWith the previous statistics for model comparison we can obtain relative model weights, using the model_weights function. This essentially spreads the total probability of the models across all those being compared. These weights in turn allow us to obtain (weighted) average predictions. The key idea being that we do not select a ‘best’ model, but rather combine their results for predictive purposes6.\nWe can start by comparing the first two models. Adding the interactions helped, and comparing the weights suggests that the interaction model would be contributing most to the averaged predictions.\n\n\nMethod: stacking\n------\n               weight\nmodel_baseline 0.234 \nmodel_interact 0.766 \n\n\nIf we compare the baseline to our most complex model, almost the entirety of the weight is placed on the latter.\n\nloo_model_weights(model_baseline, model_interact_nonlin)\n\nMethod: stacking\n------\n                      weight\nmodel_baseline        0.000 \nmodel_interact_nonlin 1.000 \n\n\nNow we compare all three, with roughly the same conclusion.\n\n\n\n\n\nmodel_baseline\nmodel_interact\nmodel_interact_nonlin\n\n\n\n\n1e-05\n0\n0.99999\n\n\n\n\n\n\n\nNow what about those average predictions? Let’s create a data frame that sets the continuous covariates at their means, and at each level of the categorical covariates. For our purposes here, we will also ignore group effects7. We then will make average predictions for those observations using pp_average.\n\nprediction_data = crossing(\n  b1 = 0:1,\n  b2 = 0:1,\n  x1 = 0,\n  x2 = 0,\n  x3 = 0\n)\n\naverage_predictions = pp_average(\n  model_baseline,\n  model_interact,\n  model_interact_nonlin,\n  newdata = prediction_data,\n  re_formula = NA\n)\n\n\n\n\n\n\nb1\nb2\nx1\nx2\nx3\nEstimate\nEst.Error\nQ2.5\nQ97.5\nBaseline Estimate\nModel Nonlin Est.\n\n\n\n\n0\n0\n0\n0\n0\n2.73\n1.03\n0.67\n4.70\n2.69\n2.73\n\n\n0\n1\n0\n0\n0\n2.29\n0.98\n0.31\n4.21\n2.54\n2.30\n\n\n1\n0\n0\n0\n0\n2.95\n1.03\n1.00\n4.98\n3.48\n2.93\n\n\n1\n1\n0\n0\n0\n3.09\n1.02\n1.07\n5.04\n3.34\n3.08\n\n\n\n\n\n\n\nAs expected, we can see that the averaged predictions are essentially the same as what we would get from the model with all the weight. In other scenarios, you may be dealing with a more nuanced result."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#cross-validation",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#cross-validation",
    "title": "Practical Bayes Part II",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nIn machine learning contexts, cross-validation is the default approach to considerations of model performance. We can do so easily within the Bayesian context as well. I go ahead and do so for a single model, as well as all three models, so we can see how our previous performance metrics might change. In general, prediction on a validation set will be expected to be worse than on the training data8, but it is the better estimate of prediction error.\n\nlibrary(future)\n\nplan(multisession)\n\nmodel_interact_nonlin_cv = kfold(\n  model_interact_nonlin,\n  K = 5,\n  chains = 1,\n  save_fits = TRUE\n)\n\nmodel_all_cv = kfold(\n  model_baseline,\n  model_interact,\n  model_interact_nonlin,\n  K = 5,\n  chains = 1,\n  save_fits = TRUE\n)\n\nplan(sequential)\n\nWith a single cross-validation model in place, we can then make predictions with it to get the test error or other metrics of interest. As we expect, the training error, i.e. that on the original/full data is better than the test error, but the latter is the better estimate of our model error, and thus a better metric for comparing models.\n\ntest_predictions = kfold_predict(model_interact_nonlin_cv)\n\n\ntrain_error = yardstick::rmse_vec(truth    = model_interact_nonlin$data$y,\n                                  estimate = fitted(model_interact_nonlin)[, 1])\n\ntest_error  = yardstick::rmse_vec(truth    = test_predictions$y,\n                                  estimate = colMeans(test_predictions$yrep))\n\n\n\n\n\n\ntrain_error\ntest_error\n\n\n\n\n0.933\n1.051\n\n\n\n\n\n\n\nNow let’s revisit our LOOIC comparison, only now it is based on LOOIC via the cross-validation process. We would come to the same conclusions, but we can see that the differences, while still substantial, are not as great. In addition, other standard metrics can help validate the Bayesian-specific metrics, as RMSE does here.\n\n## model_all_cv$diffs\n\n\n\n\nmodel\nelpd_diff\nse_diff\nelpd_kfold\nrmse\n\n\n\n\nmodel_interact_nonlin\n0.00\n0.00\n-1472.57\n1.06\n\n\nmodel_baseline\n-33.62\n11.33\n-1506.19\n1.09\n\n\nmodel_interact\n-41.59\n11.05\n-1514.16\n1.10\n\n\n\n\n\n\n\n\nVariable Selection\nIf desired, we can use cross-validation to help with feature selection. We’ve already discussed why this really shouldn’t be a concern, namely because there rarely is a reason to throw out variables regardless of how minimally important they might be. Furthermore, interactions among variables are the norm, not the exception. So while a variable might not do well on its own, it can be extremely important in how it interacts with another feature.\nIn any case, one can use the projpred package to get a sense of this, and also why it can be problematic. For starters, we cannot test our nonlinear model due to its complexity9. But we can also see that we would not choose the true underlying model using this approach. In addition, for expediency I had to turn off the random effects, otherwise this would take more time than I wanted to spend for this demo (the group effect would have been the first selected). In short, be prepared for issues that might accompany the complexities in your model10.\n\nlibrary(projpred)\n\nmodel_feature_select_cv = update(model_interact, .~. - (1|group), cores = 4)\nref_model = get_refmodel(model_feature_select_cv)  # reference model structure\n\noptions(mc.cores = parallel::detectCores())\nvar_select = cv_varsel(ref_model)   # will take a very long time\n\nWith results in place we can summarize and visualize our results, similar to how we have done before. This is from summary(var_select). You can see how often features are selected across all cv runs via the observation level folds, and the expected loo results.\n\n\n\n\n\nsize\nranking\ncv_proportions\nelpd\nelpd.se\nelpd.diff\nelpd.diff.se\n\n\n\n\n0\n(Intercept)\nNA\n-1779.6\n21.5\n-63.5\n11.3\n\n\n1\nb1\n1.0\n-1736.4\n21.9\n-20.3\n7.1\n\n\n2\nx3\n1.0\n-1714.5\n21.3\n1.6\n2.3\n\n\n3\nb2\n0.8\n-1723.1\n21.7\n-7.0\n2.1\n\n\n4\nx2\n0.7\n-1725.1\n21.7\n-8.9\n1.9\n\n\n5\nx1\n0.8\n-1720.1\n21.7\n-4.0\n1.7\n\n\n6\nb2:x1\n0.8\n-1721.4\n21.7\n-5.3\n1.3\n\n\n7\nb1:b2\n0.8\n-1715.9\n21.6\n0.2\n0.1\n\n\n\n\n\n\n\nThe plot of elpd (higher better) and rmse (lower better) suggest a possible cutoff of submodel size 2, which would include b1 and x3.\n\n# plot predictive performance on training data\nplot(var_select, stats = c('elpd', 'rmse'))\n\n\n\n\n\n\n\n\nHere is the final rank ordering with density plots for the two best features + one other for comparison. Unexpectedly, the ‘top’ feature is almost centered on zero. Go figure.\n\nranking(var_select)$fulldata\n\n[1] \"b1\"    \"x3\"    \"b2\"    \"x2\"    \"x1\"    \"b2:x1\" \"b1:b2\"\n\nmcmc_areas(\n    as.matrix(ref_model$fit),\n    pars = c('b_b11', 'b_x3', 'b_x1')\n  ) +\n  coord_cartesian(xlim = c(-2, 2))"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#the-rabbit-hole-of-model-comparsion",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#the-rabbit-hole-of-model-comparsion",
    "title": "Practical Bayes Part II",
    "section": "The rabbit hole of model comparsion",
    "text": "The rabbit hole of model comparsion\nIf you start to look more into this, there are numerous technical articles, whole websites, and various discussions regarding how to go about it. I’m guessing many do not want to try and parse highly technical information, only to still feel confused about what to actually do. Many suggestions amount to ‘your model is probably misspecified’, but without additional thoughts on how to proceed. Some of the data issues that lead to problems are just the reality of data doing what data does. There are also suggestions that posterior predictive checks (PPCs) can be used to detect the problem. But a difficulty here is that these don’t detect anything by themselves without very specific directed action, nor do they typically have a standard metric to report, so the practical utility does have its limits. In addition, it’s not clear to me that issues or problems regarding specific statistics for model comparison (e.g. LOOIC estimation) should be a basis for altering a model, unless there is an obvious path for doing so. And let’s face it, if there was, you’d probably already be taking it.\nFor those that do want to go down the rabbit hole, I have numerous links in the resources section."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#solutions-for-model-comparison",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#solutions-for-model-comparison",
    "title": "Practical Bayes Part II",
    "section": "Solutions for Model Comparison",
    "text": "Solutions for Model Comparison\nWhen doing model comparison, the following summarizes some basic steps you can take.\n\nDon’t assume you’ll have any certainty about some model being ‘best’.\nUse the metrics noted above, e.g. LOOIC, when making comparisons (not R2).\nAvoid the problem and fit the model that includes everything of interest, assuming you have a decent data size to do so. It is likely you can still learn some things about the model by comparing it to others.\nVariable selection is typically just a model comparison problem restated differently, and in a lot of cases I’ve come across, a misguided endeavor. If something is even minimally important, there is no reason to throw it out, as you’d just have worse predictions doing so. With complex models, you can’t assess one variable without consideration of others, so trying to say that one is more important than the others doesn’t really make sense.\nIf some application performance measure is obvious and available to assess, pick a model that does best in that setting.\nIf trying to select among many competing models, e.g. feature selection, you should consider why you are in this situation. If you don’t have much data, then the usual model selection criteria may lead you notably astray. If you have a lot of data, consider why you need to select a subset of predictors and not use all available. If you are somewhere in between, note that you’ll likely spend a lot more time here and still not be confident in the results. However, there are approaches, such as those in the projpred package, that might be useful, but likely will only work for simpler models."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#footnotes",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#footnotes",
    "title": "Practical Bayes Part II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you were coding in Stan directly, you can run a single iteration to see if your code compiles at all.↩︎\nDaniel Lakeland proposes (as a comment in the discussion of the 10% approach outlined) an alternative approach is whether the posterior estimate falls within the 95% highest density interval of the prior. This is available via the method argument in the demonstrated function (method = 'lakeland').↩︎\nAs in the text of the output, this is the same as testing whether abs(b1) + abs(b2) - 2*x1 &gt; 0. In this case the resulting value is greater than zero with high probability.↩︎\nSimilar to AIC, LOOIC is ~ -2*expected log posterior density (ELPD), similar to how we use -2*log likelihood (a.k.a. deviance) in standard approaches for AIC. We don’t add a penalty for parameters here, and I think this is because the regularization is already built in to the modeling process, and the number of parameters might be more difficult to define in the Bayesian context with priors.↩︎\nTechnically we can use WAIC to produce weights like we do with AIC, e.g. exp(waic) / sum(exp(all_waics)), but this isn’t recommended. The stacking approach allows similar models to share their weight, while more unique models will mostly keep their weight as additional models are added.↩︎\nSome might be familiar with Bayesian model averaging. Conceptually we aren’t changing much, but BMA assumes that one of our models is the true model, while the stacking approach underlying these weights does not. It is also different from conventional stacking in machine learning in that we are trying to average posterior predictive distributions, rather than point estimates.↩︎\nIn other words, for prediction we set re_formula = NA.↩︎\nAt the time of this writing, the underlying use of the furrr package defaults to not using a seed in it’s parallelization process, and then warns you that a seed has not been set for each repeated use of a cluster. Passing a seed through the seed argument won’t actually do anything presently here, so one will hope that furrr will change their default behavior. It’s a nuisance that can be ignored though.↩︎\nPerhaps this might be possible in a future release, but there are other complications that might make it problematic still.↩︎\nI revisited this late 2024, and it took hours to run the cv_varsel under these defaults, and I found the resulting objects even less intuitive to work with. They mention in the vignette there are many ways to speed it up, but accuracy will be compromised. In short, this is not something you could iterate over. On the plus side, the results were consistent with what I had seen before, for better or worse.↩︎"
  },
  {
    "objectID": "posts/2020-11-30-models-by-example/index.html",
    "href": "posts/2020-11-30-models-by-example/index.html",
    "title": "Models by Example",
    "section": "",
    "text": "New Book\nI’ve completed a new bookdown document, Models by Example, that converts most of the code from my Miscellaneous R repo. I initially just wanted to update the code, but decided to use a more formal approach to make it cleaner and more accessible. It’s mostly complete, though may be added to on rare occasion, and further cleaned as I find annoying bits here and there. Each topic contains ‘by-hand’ demonstration, such that you can see conceptually how a model is estimated, or technique employed. This can help those that want to dive a little deeper to get a peek behind the curtain of the functions and packages they use, hopefully empowering them to go further with such models.\nTopics covered include the following, and I plan to post a sample chapter soon.\n\nModels\n\nLinear Regression\nLogistic Regression\nOne-factor Mixed Model\nTwo-factor Mixed Model\nMixed Model via ML\nProbit & Bivariate Probit\nHeckman Selection\nMarginal Structural Model\nTobit\nCox Survival\nHurdle Model\nZero-Inflated Model\nNaive Bayes\nMultinomial\nOrdinal\nMarkov Model\nHidden Markov Model\nQuantile Regression\nCubic Spline Model\nGaussian Processes\nNeural Net\nExtreme Learning Machine\nReproducing Kernel Hilbert Space Regression\nConfirmatory Factor Analysis\n\n\n\nBayesian\n\nBasics\nBayesian t-test\nBayesian Linear Regression\nBayesian Beta Regression\nBayesian Mixed Model\nBayesian Multilevel Mediation\nBayesian IRT\nBayesian CFA\nBayesian Nonparametric Models\nBayesian Stochastic Volatility Model\nBayesian Multinomial Models\nVariational Bayes Regression\nTopic Model\n\n\n\nEstimation\n\nMaximum Likelihood\nPenalized Maximum Likelihood\nL1 (lasso) regularization\nL2 (ridge) regularization\nNewton and IRLS\nNelder Mead\nExpectation-Maximization\nGradient Descent\nStochastic Gradient Descent\nMetropolis Hastings\nHamiltonian Monte Carlo\n\n\n\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{clark2020,\n  author = {Clark, Michael},\n  title = {Models by {Example}},\n  date = {2020-11-30},\n  url = {https://m-clark.github.io/posts/2020-11-30-models-by-example/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nClark, Michael. 2020. “Models by Example.” November 30,\n2020. https://m-clark.github.io/posts/2020-11-30-models-by-example/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models Demystified",
    "section": "",
    "text": "Some News for the New Year\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nLong time no see…\n\n\n\n\n\n\nmiscellaneous\n\n\n\nNew modeling book under way! \n\n\n\n\n\nMay 20, 2024\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nStuff Going On\n\n\n\n\n\n\nmiscellaneous\n\n\n\nPenalty kicks, class imbalance, tabular deep learning, industry and academia \n\n\n\n\n\nMar 10, 2023\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Linear Models\n\n\n\n\n\n\ndeep learning\n\n\nboosting\n\n\nGLM\n\n\nregression\n\n\nmachine learning\n\n\n\nA demonstration using pytorch \n\n\n\n\n\nOct 10, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Time\n\n\n\n\n\n\nmixed models\n\n\nGAM\n\n\nboosting\n\n\ntime series\n\n\ndeep learning\n\n\n\nDemonstrating some times series approaches\n\n\n\n\n\nAug 10, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming Odds & Ends\n\n\n\n\n\n\nprogramming\n\n\n\nExplorations in faster data processing and other problems. \n\n\n\n\n\nJul 25, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning for Tabular Data\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nA continuing exploration \n\n\n\n\n\nMay 1, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Descent\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nRethinking what we thought we knew. \n\n\n\n\n\nNov 13, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nThis is definitely not all you need\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nA summary of findings regarding deep learning for tabular data. \n\n\n\n\n\nJul 19, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Bayes Part I\n\n\n\n\n\n\nbayesian\n\n\n\nDealing with common model problems. \n\n\n\n\n\nFeb 28, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Bayes Part II\n\n\n\n\n\n\nbayesian\n\n\n\nTaking a better approach and avoiding issues. \n\n\n\n\n\nFeb 28, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nModels by Example\n\n\n\n\n\n\nregression\n\n\nmachine learning\n\n\n\nRoll your own to understand more. \n\n\n\n\n\nNov 30, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nMicro-macro models\n\n\n\n\n\n\nmixed models\n\n\nSEM\n\n\nregression\n\n\nfactor analysis\n\n\n\nAn analysis in the wrong direction? Predicting group level targets with lower level covariates. \n\n\n\n\n\nAug 31, 2020\n\n\nMichael Clark\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Models {Demystified}},\n  url = {https://m-clark.github.io/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Models Demystified.” n.d. https://m-clark.github.io/."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "A lot of the following are things I do for fun or personal interest and development. Here you will find a summary of stuff that I’ve done in the past, but for my latest efforts, check me out on GitHub. Almost all of my work nowadays is for clients and internal use that is not publicly available."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Code",
    "section": "R Packages",
    "text": "R Packages\nI haven’t had time to do much with these anymore, but they are still available GitHub and still mostly functional.\n mixedup\n A package for extracting results from mixed models from several packages that are easy to use and viable for presentation.  \n confusionMatrix\n Given predictions and a target variable, this package provides a wealth of summary statistics that can be calculated from a single confusion matrix, and return tidy results with as few dependencies as possible.  \n 198R\n R with its collar flipped, or the movie Drive if it was all about R programming, writing R code on a beach in Miami as the sun sets, R wearing sunglasses at night, R asking you to take it home tonight because it doesn’t want to let you go until you see the light, Countach &gt; Testarrosa, but Delorean &gt; all except R, R if Automan had lasted longer than 1 season, driving down Mulholland Dr. at night thinking about R code, R playing a cello at the end of a dock on a lake before taking a ride in a badass helicopter, R with its hair all done up with Aquanet… You get the idea.  \n visibly\n This is a collection of functions that I use related to visualization, e.g. the palette generating function (create_palette) and clean visualization themes for ggplot and plotly. In addition, there are visualizations specific to mixed and additive models.  \n 538 football club rankings\n This package grabs the table located at 538, and additionally does some summary by league and country.  \n gammit\n The package provides a set of functions to aid using mgcv (possibly solely) for mixed models. Mostly superseded by mixedup.  \n tidyext\n This package is a collection of functions that do the things I commonly need to do with data while doing other processing within the dataverse. Most of the functionality is now standard in tidyverse, so this is essentially deprecated.  \n lazerhawk\n While the name is more or less explanatory, to clarify, this is a package of miscellaneous functions that were mostly useful to me. Now deprecated.  \nIn addition to these, though they are not publicly available, I’ve created even more involved packages for specific project work."
  },
  {
    "objectID": "code.html#code-snippets",
    "href": "code.html#code-snippets",
    "title": "Code",
    "section": "Code Snippets",
    "text": "Code Snippets\nThe vast majority of these code snippets are conceptual demonstrations of more complicated models. The audience was generally faculty, researchers, and graduate students in applied fields who, like I did, want to go beyond their basic statistical training. However, I hope it helps anyone who happens to stumble across it. I don’t really update this page anymore, as I’ve cleaned and moved much of these over to Model Estimation by Example, so I would look for something you see here in the corresponding chapter of that document. In general, you can find all of my code at GitHub.\n\n\nModel Fitting\nstandard linear regression, standard logistic regression, penalized regression, lasso regression, ridge regression, newton and IRLS, nelder-mead (Python) (R), gradient descent (stochastic), bivariate probit, heckman selection, tobit, naive bayes, multinomial regression, ordinal regression, quantile regression, hurdle poisson, hurdle negbin, zero-inflated poisson, zero-inflated negbin, Cox survival, confirmatory factor analysis, Markov model, hidden Markov model (R) (Python), stochastic volatility, extreme learning machine, Chinese restaurant process, Indian buffet process, One-line models (an exercise), …\n\nMixed models\none factor random effects (R) (Julia) (Matlab), two factor random effects (R) (Julia) (Matlab), mixed model via ML, mixed model, mixed model with correlated random effects, See the documents section for more…\n\n\nBayesian\nBEST t-test, linear regression (Compare with BUGS version, JAGS), mixed model, mixed model with correlated random effects, beta regression, mixed model with beta response (Stan) (JAGS), mixture model, topic model, multinomial models, multilevel mediation, variational bayes regression, gaussian process, horseshoe prior, item response theory, …\n\n\nEM\nEM mixture univariate, EM mixture multivariate, EM probit, EM pca, EM probabilistic pca, EM state space model\n\n\nWiggly\n\nGaussian processses\nGaussian Process noisy, Gaussian Process noise-free, reproducing kernel hilbert space regression, Bayesian Gaussian process, …\n\n\nAdditive models\ncubic spline, …\n\n\n\n\nProgramming Shenanigans\nThis is old stuff I was doing while learning programming languages. Fun at the time, but mostly useless.\nFizzBuzz test (R) (julia) (Python), Reverse a string recursively (R) (Python), Recursive Word Wrap (R) (Python), calculate compound interest recursively, get US Congress roll call data, Scrape xkcd (R) (Python), Shakespearean Insulter, spurious correlation with ratios, R matrix speedups, …"
  },
  {
    "objectID": "code.html#shiny-apps",
    "href": "code.html#shiny-apps",
    "title": "Code",
    "section": "Shiny Apps",
    "text": "Shiny Apps\nFun at the time, these were some my forays into the Shiny world.\n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n Historical Football Data\nMy annual dive into the frustration of Shiny has produced an app to explore historical football/soccer data for various European leagues (Premier, La Liga, Serie A etc.) and MLS. One can create tables for a given country/league and year selected, with some leagues having multiple tiers available, and stretching back many decades. Beyond that, one can get a specific team’s historical finishing position, league games for a specific season, all-time tables, and all-time head-to-head results (within a league).  \n Last Statements of the Texas Executed\nA demonstration of both text analysis and literate programming/document generation with a dynamic and interactive research document. The texts regard the last statements of offenders in Texas. Sadly no longer functional, as the shiny environment appears to not have been preserved. It was actually a very nifty demonstration for its time.  \n A History of Tornados\nBecause I had too much time on my hands and wanted to try out the dashboard feature of R Markdown. Maps tornado activity from 1950-2015. (Archived)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What I do\nI am currently a Senior Machine Learning Scientist for OneSix, where I engage with client projects across multiple industries, helping them get the most from their data to maximize customer satisfaction, increase profitability, and explore new data-enabled territory. In my role, I strive to use the best tools to translate complex results into actionable insights.\nThroughout my career in data science, I have navigated a vast landscape of modeling, visualizing, and understanding data. I have conducted causal inference for marketing campaigns, classified biomedical images to detect pathology, analyzed text to uncover political sentiment, and explored baboon survival rates based on their social status. My experience spans dozens of industries and academic disciplines, helping clients and researchers unlock the full potential of their data.\nAdditionally, I have a strong background in teaching, writing, and conducting workshops on these topics, empowering others to gain expertise and become self-sufficient. Recently, I authored a book - Models Demystified - which offers a comprehensive overview of the statistical and machine learning landscape, along with related topics. It will be available from CRC Press in 2025.\nI am passionate about doing quality work that answers the questions at hand. What drew me to the world of data science and keeps my interest is that it frees me to engage in many different domains, and it provides a great many tools with which to discover more about the things we humans are most interested in. My CV can be found here.\n\n\nPersonal\nI was born and raised in Texas, but have lived a good chunk of my life in the Midwest. I currently live in Ann Arbor with my wife, our daughter, and our dog Sulu. We tend to keep things simple, and like to go on walks and hikes in the neighborhood and surrounding areas of A2, take the occasional trip to some place new, and make big deals out of the little things.\n\n\nAcademic Background\nWhile my interest is in data science generally, I started off majoring in psychology and philosophy as an undergraduate, and eventually obtained a Ph.D. in Experimental Psychology. During graduate school, I became interested in statistics for practical reasons, eventually choosing it as a concentration, and I also started consulting at that time. That turned out to be a good fit for me, and I’ve been exploring and analyzing data ever since.\n\n\n\n\n\n\nMichael Clark\n\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "Content",
    "section": "",
    "text": "Here you’ll find documents of varying technical degree covering things of interest to me, or which I think will be interesting to those I engage with. Generally you’ll find a mix of demonstrations on statistical and machine learning topics, programming, and data processing and visualization. Most focus on application in R as that’s what I used to primarily program with, but you’ll find plenty of Python demonstrations as well."
  },
  {
    "objectID": "documents.html#book",
    "href": "documents.html#book",
    "title": "Content",
    "section": "Book",
    "text": "Book\n Models Demystified\nThis book should be out on CRC Press in 2025. It is a comprehensive overview of the statistical and machine learning landscape, along with related topics. It is designed to be accessible to those with a basic understanding of statistics, but also to provide a deeper dive into the concepts for those with more experience. It covers an array of useful models from simple linear regression to deep learning. The book is designed to be a reference for those who want to understand the models and techniques they are using, and to provide a guide for those who want to learn new techniques."
  },
  {
    "objectID": "documents.html#long-form",
    "href": "documents.html#long-form",
    "title": "Content",
    "section": "Long Form",
    "text": "Long Form\n Mixed Models with R\nThis document focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R. It covers topics such as loss functions, cross-validation, regularization, and bias-variance trade-off, techniques such as penalized regression, random forests, and neural nets, and more.  \n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves, IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc."
  },
  {
    "objectID": "documents.html#blog-posts",
    "href": "documents.html#blog-posts",
    "title": "Content",
    "section": "Blog Posts",
    "text": "Blog Posts\n\nDeep Linear Models\nExploring Time\nProgramming Odds and Ends\nDeep Learning for Tabular Data II\nThe Double Descent Phenomenon\nDeep Learning for Tabular Data\nPractical Bayesian Analysis (I, II)\nMicro-macro Models\nPredictions with an Offset\nFactor Analysis and Related Methods\nConvergence Problems in Mixed Models\nCategorical Random Effects\nMixed Models for Big Data\nFractional Regression\nGroup Comparisons in SEM\n\nEmpirical Bayes\nShrinkage in Mixed Models\n\nMediation Models"
  },
  {
    "objectID": "documents.html#statistical",
    "href": "documents.html#statistical",
    "title": "Content",
    "section": "Statistical",
    "text": "Statistical\n\nModels By Example\n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n\n\nModeling in R\n Data Modeling in R\nThis document demonstrates a wide array of statistical and other models in R. Generic code is provided for standard regression, mixed, additive, survival, and latent variable models, principal components, factor analysis, SEM, cluster analysis, time series, spatial models, zero-altered models, text analysis, Bayesian analysis, machine learning and more. \nThe document is designed for newcomers to R, whether in a statistical sense, or just a programming one. It also should appeal to those working in other packages who are curious how to do the same sorts of things in R.\n\n\nBayesian\n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n MCMC algorithms\nList of MCMC algorithms with brief descriptions.  \n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n\n\nMixed Models\n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Mixed Models Overview\nAn overview that introduces mixed models for those with varying technical/statistical backgrounds.  \n Mixed Models Introduction\nA non-technical document to introduce mixed models for those who have used ANOVA.  \n Clustered Data Situations\nA comparison of standard models, cluster robust standard errors, fixed effect models, mixed models (random effects models), generalized estimating equations (GEE), and latent growth curve models for dealing with clustered data (e.g. longitudinal, hierarchical etc.).  \n Mixed Model Estimation\nDemonstration of mixed models via maximum likelihood and link to additive models. \n Mixed and Growth Curve Models\nA comparison of the mixed model vs. latent variable approach for longitudinal data (growth curve models), with simulation of performance in situations of small sample sizes. \n\n\nLatent Variables/SEM\n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The initial workshop was given to an audience of varying background and statistical skill, but the document should be useful to anyone interested in the techniques covered. It is completely R-based, with special emphasis on the lavaan package. It will continue to be a work in progress, particularly the sections after the SEM chapter. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves. Topics I hope to provide overviews of in the future include other latent variable techniques/extensions such as IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc.  \n Factor Analysis and Related Methods\nThis document gives a brief overview of many matrix factorization, dimension reduction, and latent variable techniques. Here is a list:  \n\nPrincipal Components Analysis - Factor Analysis - Probabilistic Components Analysis - Non-negative Matrix Factorization - Latent Dirichlet Allocation - Structural Equation Modeling - Item Response Theory - Independent Components Analysis - Multidimensional Scaling - t-Distributed Stochastic Neighbor Embedding (t-sne) - Recommender Systems - Hidden Markov Models - Random Effects Models - Bayesian Approaches - Mixture Models - k-means Cluster Analysis - Hierarchical Cluster Analysis - Latent Class Analysis\n\n Latent Variables, Sum Scores, Single Items\nIt is very common to use sum scores of several variables as a single entity to be used in subsequent analysis (e.g. a regression model). Some may even more use a single variable even though multiple indicators are available. Assuming the multiple measures indicate a latent construct, such typical practice would be problematic relative to using estimated factor scores, either constructed as part of a two-stage process or as part of a structural equation model. This document covers simulations in which comparisons in performance are made between latent variable and sum score or single item approaches.  \n Lord’s Paradox\nSummary of Pearl’s 2014 and 2013 technical reports on some modeling situations such as Lord’s Paradox and Simpson’s Paradox that lead to surprising results that are initially at odds with our intuition. Looks particularly at the issue of change scores vs. controlling for baseline.  \n\n\nOther Statistical\n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R.  \n Reliability\nAn unfinished document that ties together some ideas regarding the statistical and conceptual notion of reliability..  \n Fractional Regression\nA quick primer regarding data between zero and one, including zero and one.  \n Categorical Regression Models\nAn overview of regression models for binary, multinomial, and ordinal outcomes, with connections among various types of models.  \n Topic Modeling Demo\nA demonstration of Latent Dirichlet Allocation for topic modeling in R.  \n Comparing Measures of Dependency\nA summary of articles that look at various measures of dependency Pearson’s r, Spearman’s rho, and Hoeffding’s D, and newer ones such as Distance Correlation and Maximal Information Coefficient."
  },
  {
    "objectID": "documents.html#programming",
    "href": "documents.html#programming",
    "title": "Content",
    "section": "Programming",
    "text": "Programming\nCheck the workshops section also for programming-related content.\nPractical Data Science (more details about this document below). The intention was to cover five key topics: basic information processing, programming, modeling, visualization, and publication/presentation.\nExploratory Data Analysis Tools An overview of various packages useful for quick exploration of data.\n FastR\nA notebook on how to make R faster before or irrespective of the machinery used. Topics include avoiding loops, vectorization, faster I/O etc.  \n Engaging the Web with R\nDocument regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach."
  },
  {
    "objectID": "documents.html#workshops",
    "href": "documents.html#workshops",
    "title": "Content",
    "section": "Workshops",
    "text": "Workshops\nI used to give workshops regularly when I worked in academia. Although they generally won’t age well, I have kept the content here for any that might be interested."
  },
  {
    "objectID": "documents.html#miscellaneous",
    "href": "documents.html#miscellaneous",
    "title": "Content",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n R for Social Science\nThis was put together in a couple of days under duress, and is put here in case someone can find it useful (and thus make the time spent on it not completely wasted)."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html",
    "title": "Micro-macro models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#introduction",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#introduction",
    "title": "Micro-macro models",
    "section": "Introduction",
    "text": "Introduction\nEvery once in a while, it comes up that someone has clustered data, with covariates that vary at different levels, and where mixed models or similar would normally be implemented, but in which the target variable only varies at the cluster level (or ‘group’ level- I will use the terms interchangeably). Though the outcome is at the cluster level, the individual may still want to use information from lower-level/within-cluster variables. Such situations are generically referred to as micro-macro models, to distinguish between the standard setting where the target varies at the lower level (which does not require a special name). An example might be using team member traits to predict team level scores. While conceptually one wants to use all available information in a model, normally we just run a model at the cluster (team) level using summaries of variables that would otherwise vary within the cluster, for example, using mean scores or proportions. Not only is it natural, it makes conceptual sense, and as such it is the default approach. Alternatives include using the within cluster variables as predictors, but this wouldn’t be applicable except in balanced settings where they would represent the same thing for each group, and even in the balanced settings collinearity might be a notable issue. So how would we deal with this?\n\nPrequsites\nFor the following you should have familiarity with mixed/multilevel models, and it would help to have an understanding of factor analysis and structural equation modeling."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#predicting-group-level-outcomes",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#predicting-group-level-outcomes",
    "title": "Micro-macro models",
    "section": "Predicting Group-Level Outcomes",
    "text": "Predicting Group-Level Outcomes\nCroon and van Veldhoven Croon and Veldhoven (2007) (CV) present a group-level regression model (e.g. a basic linear model) as follows.\n\\[y_g = \\beta_0 + \\xi_g\\beta_1 + Z_g\\beta_2 + \\epsilon_g\\]\nIn this depiction, \\(y_g\\) is the group level target variable, the \\(Z_g\\) represent the typical observed group-level covariates and corresponding coefficients (\\(\\beta_2\\)). If this were the entirety of the model, there would be no ‘levels’ to consider and we could use a standard model, say OLS regression. In the case we are interested in, some variables vary within these clusters, while others do not. Again, normally we might do a mixed model, but remember, \\(y_g\\) only varies at the group level, so that won’t really work.\nIn this setting then, \\(\\xi_g\\) represents an aggregated effect of the lower level variables. In standard practice it would just be the calculated mean, proportion, or some other metric with values for each cluster. In the CV depiction however, it is a latent (or perhaps several) latent variables and their corresponding effects \\(\\beta_1\\).\nIf we assume a single \\(\\xi_g\\) variable, the model for the underlying within-cluster variables is the standard latent variable model, a.k.a factor analysis. With an observed multivariate \\(x\\), e.g. repeated observations of some measure for an individual or, as before, team member scores, we have the latent linear model as follows:\n\\[\\textbf{x}_{ig} = \\xi_g\\lambda + v_{ig}\\]\nwhere \\(x_{ig}\\) are the (possibly repeated) observations \\(i\\) for a group/individual \\(g\\), \\(\\lambda\\) are the factor loadings and variances are constant. We can now see the full model as a structural equation model as follows for a situation with five observations per group."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-aggregate-approaches",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-aggregate-approaches",
    "title": "Micro-macro models",
    "section": "Issues with Aggregate Approaches",
    "text": "Issues with Aggregate Approaches\nCV suggest that simple aggregation, e.g. using a group mean, will result in problems, specifically biased estimates. They simulate data that varies the number of groups/clusters, the number of observations within groups, the intraclass correlation of observations within a group. In most of the cases they explore, the bias for the aggregate mean effect is notable, and there is sometimes small bias for the group level covariates, if they are collinear with the aggregate covariate. We will duplicate this approach later.\nAn approach to adjusting the group mean is offered by CV, with the structural model implied. These adjusted group means, or in their parlance, best linear unbiased predictors (BLUPs), result in a bias-free result. The notion of a BLUP will be familiar to those who use mixed models, as that is what the random effects are for a standard linear mixed model. As such, later on we’ll take a look at using a mixed model as a possible solution. In any case, once the adjusted means are calculated, you can then run your standard regression with the bias mostly eliminated."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-adjustment",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-adjustment",
    "title": "Micro-macro models",
    "section": "Issues with Adjustment",
    "text": "Issues with Adjustment\nIt turns out the the weighting calculation proffered by CV is somewhat complicated, not easily implemented, and rarely used. Foster-Johnson & Kromrey Foster-Johnson and Kromrey (2018) (FJK) looked further into its utility, as well as other possible solutions that might be easier to implement. As far as type I error rate goes, FJK demonstrated that using the CV adjusted group means offers no advantage over unadjusted, and even showed less statistical power. They suggested that a standard correction for heteroscedasticity (White’s) might be enough. In applying corrected standard errors for both unadjusted and adjusted group means, FJK found there to be additional power for both approaches, but if anything still favored the standard group mean. What’s more, while the bias remained, there was actually notable variability in the adjusted mean results. FJK’s final recommendation was to use the usual group means with robust standard errors, easily implemented in any statistical package.\nI will add that the adjustment still uses an underlying factor model of equal loadings and variances across the observations. For notably reliable scales this might be reasonable, but it isn’t a necessity. In repeated measures settings for example, we might see decreased variability across time, or practice effects, which might make the assumption more tenuous."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#my-perspective",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#my-perspective",
    "title": "Micro-macro models",
    "section": "My Perspective",
    "text": "My Perspective\nMy first glance at the issue raised by CV immediately called to mind the standard measurement model typically employed for factor analysis, i.e. a latent linear model. So my interpretation was that we are simply talking about a well known fact in measurement: that reliability of the measure is key in using a mean or sum score, and decreased reliability attenuates the correlation among the variables in question. I even did a simulation demonstrating the problem a while back. So in this case, I’m interested in the issue from a reliability perspective.\nIt turns out that factor models and mixed models share a lot in common. Those familiar with growth curve models know that they are equivalent to mixed models, but the comparison is a more general one of random effects methods. To demonstrate the equivalence, I’ll use a cleaned up version of the Big 5 data in the psych package. Specifically, we’ll use the five items that belong to the Agreeableness measure.\nFirst we make the data in both wide and long. The former makes it amenable to factor analysis, while the latter is what we need for a mixed model.\n\n# data prep for long and wide format\n\nagree_df = noiris::big_five %&gt;% \n  select(A1:A5) %&gt;% \n  drop_na()\n\nagree_long = agree_df %&gt;% \n  mutate(id = factor(row_number())) %&gt;% \n  pivot_longer(-id, names_to = 'variable', values_to = 'value')\n\nThe standard factor model will have to be constrained to have equal loadings and item variances. In addition, we’ll estimate the intercepts, but otherwise this is your basic factor analysis.\n\n # or use growth() to save some of the model tedium\ncfa_model_agree = \"\n  agree =~ a*A1 + a*A2 + a*A3 + a*A4 + a*A5\n  \n  A1 ~~ var*A1\n  A2 ~~ var*A2\n  A3 ~~ var*A3\n  A4 ~~ var*A4\n  A5 ~~ var*A5\n\"\n\nlibrary(lavaan)\n\ncfa_fit_agree = cfa(cfa_model_agree, data = agree_df, meanstructure = T) \n\nsummary(cfa_fit_agree)\n\nlavaan 0.6.17 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n  Number of equality constraints                     4\n\n  Number of observations                          2709\n\nModel Test User Model:\n                                                      \n  Test statistic                               744.709\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  agree =~                                            \n    A1         (a)    1.000                           \n    A2         (a)    1.000                           \n    A3         (a)    1.000                           \n    A4         (a)    1.000                           \n    A5         (a)    1.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .A1               -2.412    0.026  -94.340    0.000\n   .A2                4.797    0.026  187.611    0.000\n   .A3                4.599    0.026  179.859    0.000\n   .A4                4.682    0.026  183.107    0.000\n   .A5                4.551    0.026  177.982    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .A1       (var)    1.201    0.016   73.607    0.000\n   .A2       (var)    1.201    0.016   73.607    0.000\n   .A3       (var)    1.201    0.016   73.607    0.000\n   .A4       (var)    1.201    0.016   73.607    0.000\n   .A5       (var)    1.201    0.016   73.607    0.000\n    agree             0.571    0.022   25.621    0.000\n\n\nWhen we run the mixed model, we get the same variance and intercept estimates.\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(mixedup) # for post-processing\n\nmixed_fit = lmer(value ~ -1 + variable + (1 |id), data = agree_long,  REML = FALSE)\nsummarise_model(mixed_fit, digits = 3)\n\nComputing profile confidence intervals ...\n\n\n\nVariance Components:\n\n\n    Group    Effect Variance    SD SD_2.5 SD_97.5 Var_prop\n       id Intercept    0.571 0.755  0.727   0.785    0.322\n Residual        NA    1.201 1.096  1.081   1.111    0.678\n\n\n\nFixed Effects:\n\n\n       Term  Value    SE       t P_value Lower_2.5 Upper_97.5\n variableA1 -2.412 0.026 -94.340   0.000    -2.462     -2.362\n variableA2  4.797 0.026 187.611   0.000     4.747      4.847\n variableA3  4.599 0.026 179.859   0.000     4.549      4.649\n variableA4  4.682 0.026 183.107   0.000     4.632      4.732\n variableA5  4.551 0.026 177.982   0.000     4.501      4.601\n\n\nWe can also see that the estimated factor scores agree with the estimated random effects.\n\n\n\n\n\nindex\nEstimated.Factor.Scores\nEstimated.Random.Effects\n\n\n\n\n1\n-0.453\n-0.453\n\n\n2\n-0.312\n-0.312\n\n\n3\n-0.594\n-0.594\n\n\n4\n-0.031\n-0.031\n\n\n5\n-0.453\n-0.453\n\n\n6\n-0.031\n-0.031\n\n\n2704\n-0.453\n-0.453\n\n\n2705\n-1.720\n-1.720\n\n\n2706\n-0.312\n-0.312\n\n\n2707\n-0.453\n-0.453\n\n\n2708\n-1.297\n-1.297\n\n\n2709\n-1.157\n-1.157\n\n\n\n\n\n\n\nUsually when the term BLUP comes up it is in reference to the random effects estimated from a linear mixed model. As such, I thought it might be interesting to see how a mixed or factor model might be used to deal with the bias. I also thought it was a bit odd that neither CV nor FJK actually conduct the implied SEM (but see the paper co-authored by the lavaan package author Devlieger, Mayer, and Rosseel (2016)), so I wanted to look at that too."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#model-setup",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#model-setup",
    "title": "Micro-macro models",
    "section": "Model Setup",
    "text": "Model Setup\nFor our demonstration, I will create some data as CV did and run a variety of models to see what we get. My focus is on bias, not coverage or power, as I think FJK covered those aspects plenty. The models in particular are:\n\nStandard linear model: a basic group level analysis using unadjusted means.\nRandom effects: a group level model using estimated factor scores using lavaan, or the BLUPs from lme4, or those with heterogeneous variance via glmmTMB1. These involved a two-step approach, with the factor/mixed model followed by the standard linear model.\nStructural equation model: A full, single-step SEM via lavaan. This model has the ability to account for the correlation of the Z and latent variable. It is exactly as CV depict in their Figure 1 and Figure @ref(fig:sem-plot) above.\nAdjusted means: Use CV’s approach"
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#data-setup",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#data-setup",
    "title": "Micro-macro models",
    "section": "Data Setup",
    "text": "Data Setup\nI made a function2 to create data with the values shown in CV (p. 52) for a single aggregate \\(X\\) and single group-level covariate \\(Z\\). Using their notation, the model that generates the data is the following:\n\\[y_g = .3 + .3Z_g + .3\\xi_g + \\epsilon_g\\] \\[x_{ig} = \\xi + \\nu_g\\]\nAs there, \\(\\sigma^2_\\epsilon\\) is .35. While they look at a variety of situations, I’ll just consider a single scenario for our purposes, where the correlation of the \\(Z\\) and \\(\\xi\\) was .3, the intraclass correlation of the observed \\(x_{ig}\\) was .1 (i.e. \\(\\sigma^2_\\nu\\) = 9), the number of groups was 100 and the number of observations per group was balanced at 10 (row 16 of their table 1). I simulated 1000 such data sets so that we could examine the mean value of the estimated coefficients. I first started by analyzing the result with a factor analysis, and if there are any problems such as negative variances or lack of convergence, the data is regenerated, as that will also help with any issues the mixed model would have. So the final 1000 data sets don’t have convergence issues or other problems that might make the results a little wonky."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#results",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#results",
    "title": "Micro-macro models",
    "section": "Results",
    "text": "Results\nHere are the results. We can first take a peek at the estimated scores from the two-step approaches. The CV adjustment appears closely matched to the true score at first, but we see it’s range is very wild, which is what FJK found also. Interestingly, the BLUPs from the mixed models have less variance than the true scores. The factor score is in keeping with the BLUPs, but appears also to have notable extremes, but far less than the CV adjustment. We’ll talk about why these extremes may arise later.\n\n\n\nEstimated scores\n\n\nVariable\nN\nMean\nSD\nMin\nQ1\nMedian\nQ3\nMax\n% Missing\n\n\n\n\nTrue\n1e+05\n0.01\n1.00\n-4.31\n-0.67\n0\n0.68\n4.44\n0\n\n\nCV Adj\n1e+05\n0.01\n1.07\n-22.97\n-0.65\n0\n0.66\n25.64\n0\n\n\nUnadjusted\n1e+05\n0.00\n1.39\n-6.32\n-0.93\n0\n0.94\n6.96\n0\n\n\nBLUP_mixed\n1e+05\n0.00\n0.74\n-4.20\n-0.48\n0\n0.48\n3.98\n0\n\n\nBLUP_mixed_hetvar\n1e+05\n0.00\n0.74\n-4.18\n-0.48\n0\n0.47\n3.97\n0\n\n\nFactor Score\n1e+05\n0.00\n0.87\n-7.29\n-0.47\n0\n0.46\n6.54\n0\n\n\n\n\n\n\n\nNow let’s look at the bias in the estimates.\n\n\n\nPercent bias\n\n\nModel\nIntercept\nZ\nX\n\n\n\n\nUnadjusted\n0.149\n14.494\n-49.877\n\n\nBLUP_mixed\n0.051\n14.494\n-1.828\n\n\nBLUP_mixed_hetvar\n0.053\n14.647\n-1.673\n\n\nFactor Score\n0.063\n16.502\n11.968\n\n\nCV Adj\n-0.663\n-2.368\n7.992\n\n\nSEM\n-0.017\n-1.777\n4.744\n\n\nTrue\n-0.567\n-0.380\n0.513\n\n\n\n\n\n\n\nThe results suggest a couple things. First, the results of CV were duplicated for the unadjusted setting, where the group level covariate has a slight bias upward, but the aggregate is severely downwardly biased3. We can also see that a two-step approach using BLUPs from a mixed model (with or without heterogeneous variances), or factor scores, either eliminate or notably reduce the bias for the aggregate score, but still have issue with the group level covariate. This is because of the correlation between the group level and lower level covariates, which if zero, would result in no bias, and has long been a known issue with mixed models. The factor scores had some very wild results at times, even after overcoming basic inadmissible results. In the end, we see that the calculated adjustment and SEM both essentially eliminate the bias by practical standards. It is worth noting that the bias for either the factor analysis or SEM would be completely eliminated if the model adds a regression of the latent variable onto the group level covariate \\(Z\\).\nNote that in practice, a two-step approach, such as using the mixed model BLUPs or factor scores, comes with the same issue of using an estimate rather than observed score that we have using the mean. Even if there is no bias, the estimated uncertainty would be optimistic as it doesn’t take into account the estimation process. This uncertainty decreases with the number of observations per group (or number of items from the factor analytic perspective), but would technically need to be dealt with, e.g. using ‘factor score regression’ Devlieger, Mayer, and Rosseel (2016) or more simply, just doing the SEM."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#reliability",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#reliability",
    "title": "Micro-macro models",
    "section": "Reliability",
    "text": "Reliability\nInterestingly, if we look at the reliability of the measure, we shouldn’t be surprised at the results. Reliability may be thought of as the amount of variance in an observed score that is true score variance Revelle and Condon (2019). Since the underlying construct is assumed unidimensional, we can examine something like coefficient \\(\\alpha\\), which gives a sense of how reliable the mean or total score would be. Doing so reveals a fairly poor measure for 10 observations per group under the CV settings. The mean coefficient \\(\\alpha\\) is 0.52, the max of which is 0.74, which, from a measurement model perspective, would be unacceptable4. This is all to say that we have rediscovered attenuation in correlation due to (lack of) reliability, something addressed by Spearman over a century ago5.\nIn actual repeated measures, or with constructed scales, it’s probably unlikely we would have this poor of a measure. Indeed, if we think a mean is appropriate in the first place, we are probably assuming that the scores are something that can be meaningfully combined in the first place, because if a latent construct doesn’t actually explain the observations well, then what is the point of estimating it?\nIn our current context, we can create a more reliable measure by decreasing the variance value for \\(\\sigma^2_\\nu\\) which is the residual variance for the observed items at the lower level. Decreasing it from 9 to 1, puts the observed scores in a notably better place (\\(\\alpha\\) = 0.91), and if we actually have a reliable measure (or even just increase the number of observations per group, as noted by CV), the results show hardly any bias for the group level effect and a near negligible one for the mean effect.\n\n\n\nPercent bias\n\n\nModel\nIntercept\nZ\nX\n\n\n\n\nUnadjusted\n-0.011\n2.698\n-9.651"
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#summary",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#summary",
    "title": "Micro-macro models",
    "section": "Summary",
    "text": "Summary\nIn the end we relearn a valuable, but very old lesson. The take home story here, at least to me, is to have a reliable measure and/or get more observations per group if you can, which would be the same advice for any clustered data situation. If you do have a reliable measure, such as a proportion of simple counts, or a known scale with good properties, using the mean should not give you too much pause. As a precaution, you might go ahead and use White’s correction as suggested by FJK. If you have enough data and the model isn’t overly complicated, consider doing the SEM.\nlast updated: 2024-12-29"
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#footnotes",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#footnotes",
    "title": "Micro-macro models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI wasn’t sure in the mixed model whether to include the item and or group level Z as fixed effects. Results did not change much, so I went with a mixed model with no fixed effects to make them closer to the scale of the mean scores.↩︎\nAll code is contained within the R markdown file that produced this post.↩︎\nFor those who may not have access to the article, the values for percentage bias in CV were as follows: for the unadjusted model, the bias for the coefficients under these conditions was 0.6, 15.3, -50.4, and for the adjusted model, -1.1, -1.3, 5.0.↩︎\nA typical cutoff for coefficient \\(\\alpha\\) for a good measure is .8. We can actually use a ‘G-theory’ approach and calculate this by hand \\(\\frac{1}{1+9/10}\\), where 1 is the variance CV fixed for the true score, and 9 is residual variance. \\(\\frac{1}{1+9}\\) is the \\(\\rho_x\\), i.e. intraclass correlation, that they have in Table 1. In the better scenario \\(\\rho_x\\) = \\(\\frac{1}{1+4}\\) = .2 and the reliability is \\(\\frac{1}{1+4/10}\\) = .71, which is notably better, though still substandard. Even then we can see from their table dramatic decreases in bias from that improvement in reliability.↩︎\nThe lack of reliability is likely the culprit behind the wider range in the estimated factor scores as well.↩︎"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html",
    "title": "Practical Bayes Part I",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#rhat-effective-sample-size",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#rhat-effective-sample-size",
    "title": "Practical Bayes Part I",
    "section": "Rhat & Effective Sample Size",
    "text": "Rhat & Effective Sample Size\nWe will start with a simple standard regression model that we know is not adequate. We will use default priors2 and run very few iterations.\n\n# no priors, no complexity, all default settings, few iterations\nlibrary(brms)\n\nmodel_start_100 = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 100,\n  verbose = F,\n  seed = 123\n)\n\nWarning: The largest R-hat is 1.1, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\nsummary(model_start_100)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ b1 + b2 + x1 + x2 + x3 \n   Data: main_df (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 100; warmup = 50; thin = 1;\n         total post-warmup draws = 200\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.72      0.15     2.44     3.00 1.10       36      144\nb11           0.83      0.06     0.71     0.94 1.01      140      115\nb21          -0.12      0.15    -0.41     0.15 1.10       35       82\nx1            0.03      0.03    -0.04     0.09 1.00      310      239\nx2           -0.04      0.03    -0.10     0.02 1.00      355      237\nx3            0.28      0.03     0.22     0.35 1.04      349      132\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.10      0.02     1.06     1.15 1.00      220      145\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nAs is common in other R models, for factors, the effect is the stated category vs. the reference. Since we only have binary variables, b11 is show the level 1 vs. level 0 (reference) for the first binary covariate.\nThe warnings about Rhat and effective sample size (ESS) are likely to pop up if you use default iterations for more complex models. They mostly regard the efficiency of the sampling process, and whether you have enough samples to have stable parameter estimates. Ideally Rhat is close to 1.0, and ESS is at least a notable percentage of the total posterior samples (e.g. 50%).\nThe fix for these warnings is usually simple, just let the model run for more iterations beyond your warmup. The default is 2000 iterations, with warmup half of that. Warmup iterations are not used in calculation of parameter estimates, so you can just increase the number of iterations relative to it, or increase both by only increasing the iter argument.\nIn the following, we plot the estimated values across each iteration for each chain, called a trace plot, as well as the density plot of the values from the entire chain. From this we could see that things might be problematic (e.g. we’d want more symmetric density plots for the regression coefficients), but only if you are used to looking at these things, so it will take some practice.\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'combo')\n\n\n\n\n\n\n\n\nTo get some intuition for what you would expect, just plot a series from a normal distribution.\n\nggplot2::qplot(x = 1:1000, y = rnorm(1000), geom = 'line')\n\n\n\n\n\n\n\nggplot2::qplot(x = rnorm(1000), geom = 'density')\n\n\n\n\n\n\n\n\nOther plots allow us to look at the same sorts of things from a different perspective, or break out results by each chain.\n\nmcmc_plot(model_start_100, type = 'areas')\n\n\n\n\n\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'violin')\n\n\n\n\n\n\n\n\nWhile things seem okay, if we single out a particular chain, we might think otherwise. In this case, the Intercept and b2 coefficients may be problematic, given they do not seem to vary as much as the others (chain 2 for example, but also for other chains).\n\nmcmc_plot(model_start_100, highlight = 2, type = 'trace_highlight')\n\n\n\n\n\n\n\n\nSome suggest to look at rank plots instead of traditional trace plots. Really, all we’ve changed is looking for something ‘fuzzy’, to looking for something ‘approximately uniform’, so my opinion is that it’s not much of an improvement visually or intuitively. In general, histograms, which are variants of bar charts, are rarely an improvement for any visualization. If you do use it, you can use an overlay approach to see if the ranks are mixing, but this looks a lot like what I’d be looking for from a trace plot.\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'rank_hist')\n\n\n\n\n\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'rank_overlay')"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#some-details",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#some-details",
    "title": "Practical Bayes Part I",
    "section": "Some details",
    "text": "Some details\nA little more detail will possibly provide additional understanding, and also some guidance, when looking at your own results.\n\nRhat\nThe \\(\\hat{R}\\) (or Rhat) statistic measures the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains. If all chains are at equilibrium, these will be the same and \\(\\hat{R}\\) will be 1.0. If the chains have not converged to a common distribution, the \\(\\hat{R}\\) statistic will be greater than one.\nWhat we want: values near 1.0 (&lt; 1 okay) and less than 1.05\n\nmcmc_plot(model_start_100, type = 'rhat')\n\n\n\n\n\n\n\n\n\n\nESS\nEffective sample size is an estimate of the effective number of independent draws from the posterior distribution of the parameter of interest. Because the draws within a chain are not independent if there is autocorrelation, the effective sample size will typically be smaller than the total number of iterations, but the calculation of this statistic is bit more art than science, and can even be greater than the number of posterior draws. Note also that the plot is based on slightly different values than reported by the brms summary function.\n\nBulk ESS: ESS for the mean/median (n_eff in rstanarm). Tells us whether the parameter estimates are stable.\nTail ESS: ESS for the 5% and 95% quantiles. Tells us whether the interval estimates for the parameters are stable. Tail-ESS can help diagnose problems due to different scales of the chains and slow mixing in the tails.\n\nWhat we want: ESS &gt; 10% percent of total posterior samples for sure, but &gt; 50% is best. At least 100 is desired for decent estimates of autocorrelation. For Bulk-ESS we want &gt; 100 times the number of chains.\n\nmcmc_plot(model_start_100, type = 'neff')  # &lt; .1 problem\n\n\n\n\n\n\n\n\n\n\nTrace plot\nShows the estimated parameter values at each iteration. In general you would like a random bouncing around an average value.\n\nmcmc_plot(model_start_100, type = 'trace')\n\n\n\n\n\n\n\n\nWhat we want: Something like random normal draws over a series. Trace plots in general should look ‘grassy’, or like a ‘fuzzy caterpillar’, which might not be very descriptive, but deviations are usually striking and obvious in my experience. If you see chains that look like they are getting stuck around certain estimates, or separating from one another, this would indicate a serious problem. If the chains are not converging with one another, you were probably already getting warnings and messages.\n\n\nDensity plot\nShows the density of the posterior draws for the parameters.\n\nmcmc_plot(model_start_100, type = 'dens')\n\n\n\n\n\n\n\n\nWhat we want: For the density plots of regression coefficients, these should be roughly normal looking. For variance parameters you may see skewness, especially if the estimate is relatively near zero with smaller data sets. In general, we would not want to see long tails or bimodality for the typical parameters of interest with models you’d be doing with rstanarm and brms.\n\n\nRank plot\nFrom bayesplot help file:\n\nWhereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.\n\n\nmcmc_plot(model_start_100, type = 'rank_hist')\n\n\n\n\n\n\n\n\nWhat we want: Uniform distribution, a good mixing of lines for the overlay version.\n\n\nACF plot\nThe acf, or autocorrelation function plot, is exactly the same thing you’d visualize for any time series. It is a plot of a series of correlations of a parameter with specific lags of itself. Autocorrelation does not bias estimates, but increased autocorrelation may suggest a more inefficient/slower exploration of the parameter space. At lag zero, the series estimates are perfectly correlated with themselves, so that’s where the plot usually starts.\nWhat we want: Quick drop off, but not really that important. By the time you find it’s an issue, your model has already run.\n\nmcmc_plot(model_start_100, type = 'acf')\n\n\n\n\n\n\n\n\n\n\nEfficiency plots\nI have seen these often recommended, but I’m not aware of a package does them, though it seems they may make their way to shinystan at some point. Aki Vehtari has supplied a walk-through and some code (see the resources section below), but there isn’t really documentation for the functions, and they likely won’t work outside of rstan objects, or at least I had limited success with applying them to brms objects. Furthermore, these are getting into waters that are beyond what I’d expect applied users to be wading through.\n\n\nSolution for Rhat/ESS warnings\nTo summarize, the general solution to Rhat and ESS warnings is simply to do more iterations (relative to the warmup). To keep posterior samples and model objects from becoming unwieldy in size3, consider thinning also. Thinning saves only a select amount of the available posterior samples. For example, setting thin = 10 means only every tenth sample will be saved. This will also reduce autocorrelation, as the draws retained after thinning are not as correlated with one another as successive draws would be. However, if you thin too much, you may not have enough for effective sample size.\n\n# default with 4 chains, 1000 warmup 2000 total = 4*(2000 - 1000) = 4000 post-warmup draws\nbrm(model)\n\n# 4*(4000 - 2000) = 8000 posterior draws\nbrm(model, warmup = 2000, iter = 4000)\n\n# 4*(4000 - 2000)/8 = 1000 posterior draws\nbrm(model, warmup = 2000, iter = 4000, thin = 8)"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#bfmi-low",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#bfmi-low",
    "title": "Practical Bayes Part I",
    "section": "BFMI low",
    "text": "BFMI low\nYou may see a warning that says some number of chains had an ‘estimated Bayesian Fraction of Missing Information (BFMI) that was too low’. This implies that the adaptation phase of the Markov Chains did not turn out well, and those chains likely did not explore the posterior distribution efficiently. For more details on this diagnostic, you can see Betancourt’s article, but this will almost surely be too technical for many applied and even more advanced users.\nIn this case, the problem here is often remedied by just adding more iterations. I typically keep to 1000 posterior samples, which makes for nicer visualizations of distributions without creating relatively large model objects. However, you may need more to get a satisfactory result.\n\nmodel_start = update(\n  model_start_100,\n  warmup = 2000,\n  iter = 2250,      # 1000 posterior draws\n  cores = 4,\n  seed = 123\n) \n\nIn this case, we no longer have any warnings, and even one of our more problematic coefficients looks fine now.\n\nsummary(model_start)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ b1 + b2 + x1 + x2 + x3 \n   Data: main_df (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2250; warmup = 2000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.72      0.20     2.32     3.10 1.00     1583      938\nb11           0.83      0.07     0.69     0.97 1.00     1711      895\nb21          -0.12      0.20    -0.50     0.29 1.01     1509      864\nx1            0.03      0.03    -0.04     0.10 1.00     1504      947\nx2           -0.03      0.04    -0.11     0.05 1.00     1279      803\nx3            0.28      0.04     0.21     0.35 1.00     1136      810\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.11      0.02     1.06     1.15 1.00     1357      735\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(model_start, par = 'b2')\n\n\n\n\n\n\n\n\n\nSolution for low BFMI\nIf there aren’t other serious problems, add more iterations to deal with low BFMI. In some cases, switching from a heavy-tailed prior (e.g. student t) to something else (e.g. normal) would be helpful, but some other approaches typically would involve having to write Stan code directly to reparameterize the model. Otherwise, you may need to approach it similarly to the problem of divergent transitions."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#tree-depth",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#tree-depth",
    "title": "Practical Bayes Part I",
    "section": "Tree Depth",
    "text": "Tree Depth\nTree depth is a more technical warning that has to do with the details of Hamiltonian Monte Carlo. Practically speaking:\n\nLack of convergence and hitting the maximum number of leapfrog steps (equivalently maximum tree depth) are indicative of improper posteriors ~ Stan User Guide\n\nSometimes you’ll get a warning about hitting maximum tree depth, and without getting overly technical, the fix is easy enough. Just set the maximum higher.\n\nSolution for max tree depth\nUse the control argument to increase the value beyond the default of 10.\n\nmodel_update_treedepth = update(\n  model,\n  control = list(max_tree_depth = 15)\n)"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#divergent-transitions",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#divergent-transitions",
    "title": "Practical Bayes Part I",
    "section": "Divergent Transitions",
    "text": "Divergent Transitions\nDivergent transitions are a technical issue that indicates something may be notably wrong with the data or model (technical details). They indicate that the sampling process has ‘gone off the rails’, and that the divergent iteration’s results, and anything based on them (i.e. subsequent draws, parameter estimates), can’t be trusted. Unlike the other problems we’ve discussed, this is more difficult to navigate.\nWhy might this happen?\n\ninsufficient data for the model’s complexity\npoor model\nhigh collinearity\nimproper or otherwise problematic priors\nseparability (logistic regression)\nany number of other things\n\nAs an example, I’ll make an overly complex model with only a small random sample of the data, improper priors, and use very few warmups/iterations.\n\nmodel_problem = brm(\n  bf(\n    y ~ b1*b2*x1 + x2*b2 + x3*b2 + (1 + x1 + b2|group),\n    sigma ~ x1 + b1*b2\n  ),\n  data   = main_df %&gt;% slice_sample(prop = .1),\n  family = student,\n  cores  = 4,\n  warmup = 5,\n  iter   = 1005,\n  thin   = 4,\n  seed   = 123\n)\n\nWarning: There were 206 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: The largest R-hat is 2.21, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\nSo what do we do in this case? Well let’s start with visual inspection.\n\nVisualization: Pairs plot\nA diagnostic tool that is typically suggested to look at with divergent transitions is the pairs plot. It is just a scatterplot matrix of the parameters estimates (and log posterior value), but it suffers from a few issues. The plot is slow to render even with few parameters, and simply too unwieldy to use for many typical modeling situations. If you somehow knew in advance which parameters were causing issues, you could narrow it down by only looking at those parameters. But if you knew which parameters were the problem, you wouldn’t need the pairs plot.\nAnother issue is that it isn’t what you think it is at first glance. The upper diagonal is not just the flipped coordinates of the lower diagonal like every other scatterplot matrix you’ve seen. The chains are split such that half are used for the above diagonal plots, and the other for the lower, with the split being based on the amount of numerical error (above or below the median). I suspect this may not help applied users interpret things, but the gist is, that if your red points show up only on the upper diagonal, changing the adapt_delta part of the control argument may help (see below), otherwise it likely won’t4.\nLet’s take a look at the pairs plot anyway. I’ll use hex bins instead of standard points because the point plots have no transparency by default. In addition, we’ll use a density plot on the diagonal, instead of the histogram.\n\nmcmc_plot(\n  model_problem,\n  pars = c('b_b11', 'b_b21', 'b_x1'),\n  type = 'pairs',\n  diag_fun = 'dens',\n  off_diag_fun = 'hex',\n  fixed = TRUE\n)\n\n\n\n\n\n\n\n\nWith problematic cases, what you might see on the off-diagonal plots is some sort of ‘funneling’, which would indicate where the sampler is getting stuck in the parameter space. However, this visual notion isn’t defined well, as it may be happening without being obvious, displaying just a bump, or just some weird patterns as above. But you’ll also regularly see correlated parameters, but it’s unclear whether these might necessarily be a problem in a given situation.\nFor the initial model we ran, the pairs plot for all parameters takes several seconds to produce, and even with the hex option, it is still difficult to parse without closer inspection. It shows the intercept and b2 parameters to be notably correlated, possibly indirectly due to the poor priors.\n\nmcmc_plot(\n  model_start_100,\n  type = 'pairs',\n  off_diag_fun = 'hex',\n  diag_fun = 'dens'\n)\n\n\n\n\n\n\n\n\nWhat we want: Roughly little correlation among parameters, mostly symmetric densities for typical regression parameters.\n\n\nVisualization: Parallel Coordinates Plot\nIt is also suggested to look at parallel coordinates plots, but unfortunately there are issues with these plots as well. The order of the variable/parameter axis is arbitrary, and yet the order can definitely influence your perception of any patterns. Also, unless everything is on similar scales, they simply aren’t going to be very useful, but even if you scale your data in some fashion, the estimates given divergent transitions may be notably beyond a reasonable scale.\nAs in our pairs plot, we’d be looking for a pattern among the divergences, specifically a concentration for a parameter where the lines seemingly converge to a point. If this isn’t the case, the divergences are probably false positives5. I had to add some ggplot options to help this to be more legible, and you will likely have to as well. In the following, you might think the b_sigma_x1 coefficient for dispersion is a problem, which might suggest we need to rethink the prior for it. In reality it’s likely just that it’s being estimated to be near zero, as it should be, especially since non-divergent transitions are also bouncing around that value. For the most part we don’t see much pattern here.\n\nmcmc_parcoord(\n  model_problem,\n  pars = vars(matches('^b')),\n  size = .25, \n  alpha = .01,\n  np = nuts_params(model_problem),  # without this div trans won't be highlighted\n  np_style = parcoord_style_np(\n    div_color = \"#ff5500\",\n    div_size = 1,\n    div_alpha = .1\n  )\n) +\n  guides(x = guide_axis(n.dodge = 2)) +\n  theme(\n    axis.text.x = element_text(size = 6),\n    panel.grid.major.x = element_line(color = '#00AAFF80', size = .1)\n  )\n\n\n\n\n\n\n\n\nWhat we want: Roughly no obvious pattern. If the divergence lines are not showing any particular concentration, it could be that these are false positives.\n\n\nSolution for divergent transitions\nUnfortunately the solution to divergent transitions is usually not straightforward. The typical starting point for solving the problem of divergent transitions is to use the control argument to increase adapt_delta, for example, from .80 to .996, and let your model have more warmup/total iterations, which is the primary issue here. In the cases I see for myself and clients, increasing adapt_delta rarely helps, but it doesn’t hurt to try. I often will just start with it increased for more complex models, just to save messing with it later.\n\nmodel = brm(..., control = list(adapt_delta = .99))\n\nAside from that you will have to look more deeply, including issues with priors, model specification, and more. I find this problem often comes from poor data (e.g. not scaled, possible separation in logistic models, etc.), combined with a complex model (e.g. complicated random effects structure), and beyond that, the priors may need to be amended. You should at least not have uniform priors for any parameter, and as we’ll see in Part II, you can use a simulation approach to help choose better priors. Often these data and prior checks are a better approach to solving the problem than doing something after the fact. You may also need to simplify the model to better deal with your data nuances. As an example, for mixed models, it may be that some variance components are not necessary.\nSome solutions offered on the forums assume you are coding in Stan directly, such as reparameterizing your model, using specific types of priors, etc. If you are writing Stan code rather than using a modeling package, you definitely need to double check it, as typos or other mistakes can certainly result in a problematic model. However, this post is for those using modeling packages, so I will not offer such remedies, and they are usually not obvious anyway."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#other-messages",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#other-messages",
    "title": "Practical Bayes Part I",
    "section": "Other messages",
    "text": "Other messages\nCertain builds of rstan for some types of settings (e.g. specific operating systems) will often have warnings or other messages. Sometimes it looks like a bunch of gobbledygook, which is typically something happening at the C++ level. If your model runs and produces output despite these messages, you can typically ignore them in most cases. Even then, you should look it up on the forums just to be sure.\n\nParser warnings\nParser warnings are either a deprecation warning, or another more serious kind (Jacobian). The latter will not happen if you’re using higher level interfaces (e.g. brms), rather than programming in Stan directly. The other kind, deprecation warnings, are not something you can do anything about, but the developer of the package will likely need to make minor changes to the code to avoid them in the future. I’ve never seen parser warnings from using rstanarm or brms.\n\n\nCompilation warnings\nCompiler warnings happen regularly and indicate something going on at the compiler level, typically that something in Stan is being compiled but not used. You can ignore these.\n\n\nPackage warnings\nLike any good package, when things go unexpectedly, or just to be informative, modeling packages like rstanarm and brms will provide you messages or warnings. These do not have to do with the Stan part of things. For example, brms will warn you that it will drop cases with missing values.\n\nRows containing NAs were excluded from the model.\n\nSome issues can be more subtle. For example, you may get a message that the model is compiling but then nothing happens. This might be because of a typo in a distribution name for your priors, or some similar goof7.\n\n\nSolutions for other messages\nIf you are using a package to interface with Stan and not having an issue with the model (i.e. it runs, converges), these messages can largely be ignored, unless it is a warning from the package itself, which typically should be investigated."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#model-comparison-problems",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#model-comparison-problems",
    "title": "Practical Bayes Part I",
    "section": "Model Comparison Problems",
    "text": "Model Comparison Problems\nFor a discussion of loo and related issues, see Part II."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#but-its-slow",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#but-its-slow",
    "title": "Practical Bayes Part I",
    "section": "But it’s slow!",
    "text": "But it’s slow!\nAnother problem people seem to be concerned with is the speed of the analysis. If you have a lot of data, or more to the point, a lot of parameters, your model can be very slow. For standard models with rstanarm and brms, there may be no real benefit doing it Bayesian style if you have millions of data points and simpler models. If your model is only taking a couple minutes, then you really have nothing to complain about- watch some YouTube or something while it runs. If your model takes on the order of hours, work with less data or simpler models until you have your modeling code, plots, etc. squared away. At that point you can run your primary analysis and wait it out. A slow model may also be indicative of a poorly specified/understood model, so you may have to think hard about how you are approaching the problem.\n\nSolutions for a slow model\n\nFor any model, Bayesian or otherwise, doing things like standardizing, logging or other variable transformations will put parameters in more reasonable domains, resulting in a more manageable estimation process. For example, if you standardize predictors, then the coefficients are on similar scales and a Normal(0, 1) prior can typically be applied.\nUsing more informative priors can avoid exploring areas of the posterior that aren’t going to lead to plausible results. We will show how to do this in Part II.\nUse less iterations if there are no other issues.\nIf possible, work with less data or simpler models until ready for the full model.\nIf possible, work with a different version of the model that can still answer the question of interest. For example, if an ordinal model is causing a lot of issues, and you’re not interested in category specific probabilities, just treat the target variable as numeric8.\nStop fishing/p-hacking/torturing your data so that you don’t have to run dozens of models. You’re likely not to care if the models takes hours if you only run them once or twice."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#shinystan",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#shinystan",
    "title": "Practical Bayes Part I",
    "section": "shinystan",
    "text": "shinystan\nThe shinystan package makes exploring model diagnostics actually fun! Using launch_shinystan on a model opens a browser window, providing interactive visualizations to help you see what’s going on with it. You can look at many of the previous plots, plus a few others we haven’t shown.\nUnfortunately the documentation in the browser doesn’t really tell you what to look for in these plots. The glossary contains information that is likely overly technical for applied users, and if there is a problem, there’s not really a whole lot to go on. In addition, some plots are difficult to use (e.g. trying to assess whether overlapping histograms are similar), or are probably only useful if you have very obvious problem (e.g. lots of divergent transitions). As an example, consider the tree depth plots. What would be good here?\n\nThe documentation tells you the value should be somewhere between 0 and whatever max_treedepth is set at. If they are ‘large’, the documentation states the problem could be due to different things, but the solutions are to either reparameterize of the model (probably not possible unless using Stan code directly), or just increase the value. It doesn’t seem to tell you what those plots are supposed to look like, and unfortunately that severely limits their utility. The divergence and energy plots are similarly under-explained. Many refer users to Betancourt’s wonderful articles on the details, but these are far too technical for those not already steeped in the approach9.\nAll that said, luckily there is a nice walkthrough if you do have a hankering to go down that path, and provides more details on the statistics and what you should be looking for in the visualizations."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#general",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#general",
    "title": "Practical Bayes Part I",
    "section": "General",
    "text": "General\n\nAki Vehtari’s website has demos and explanation for things like model comparison, Rhat/Neff, and more.\nbayesplot Vignette for Diagnostic Plots\nJeffrey Arnold’s Bayesian Notes has nice examples of many models and good summaries otherwise.\n\nBayesian Basics, an applied overview of Bayesian analysis with Stan as the backdrop."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#priors",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#priors",
    "title": "Practical Bayes Part I",
    "section": "Priors",
    "text": "Priors\nPrior Choice Recommendations by the Stan group.\n\nrstanarm vignette"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#rhatess",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#rhatess",
    "title": "Practical Bayes Part I",
    "section": "Rhat/ESS",
    "text": "Rhat/ESS\n\nStan Reference on Effective Sample Size\nVehtari et al. on Rank plots for details.\nVehtari’s appendix for the above (probably more accessible)\nVehtari’s code for efficiency plots, Code for functions"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#divergence",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#divergence",
    "title": "Practical Bayes Part I",
    "section": "Divergence",
    "text": "Divergence\n\nStan Reference on Divergent Transitions\nDivergent Transitions A Primer\nCase study: Divergences and Bias"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#other-warnings",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#other-warnings",
    "title": "Practical Bayes Part I",
    "section": "Other warnings",
    "text": "Other warnings\n\nBrief Guide to Stan’s Warnings"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#visual-diagnostics",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#visual-diagnostics",
    "title": "Practical Bayes Part I",
    "section": "Visual diagnostics",
    "text": "Visual diagnostics\n\nVisual MCMC diagnostics"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#misc",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#misc",
    "title": "Practical Bayes Part I",
    "section": "Misc",
    "text": "Misc\n\nGelman’s advice for slow models\nbrms vignettes\nUse CmdStan to save memory\nStan case studies"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#footnotes",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#footnotes",
    "title": "Practical Bayes Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI will spare the details of my opinions, which are almost entirely negative.↩︎\nFor this model, they are uniform/improper priors for the regression coefficients and half-t for the residual variance. You can always use prior_summary on a brms model to see this.↩︎\nWith more posterior samples comes slower visualizations and possibly other computations.↩︎\nAnother issue is that you could change how the chains are split and it could potentially dramatically change the how the pattern of divergent transitions looks.↩︎\nThis seems counter to the common suggestion on forums and GitHub issues that even 1 divergent transition renders results suspect. I’ve never seen results meaningfully change for something with just a couple divergent transitions to one that has none, and often when there are that few, even rerunning the model will result in no divergences.↩︎\nIn my experience, there isn’t a need to guess some value between .80 and .99 as the time differences are typically not great, say between .9, .95, and .99, unless your model already takes a very long time. Also, if it doesn’t work at .99, it won’t at .9999 either.↩︎\nI’m definitely not speaking from experience here or anything! Nomral distributions do exist, I’m sure of it! 😬↩︎\nIt is often suggested in the Stan world to reparameterize models. However, this advice doesn’t really apply in the case of using rstanarm or brms (i.e. where you aren’t writing Stan code directly), and it assumes a level of statistical expertise many would not have, or even if they do, the route to respecifying the model may not be obvious.↩︎\nBetancourt, whose work I admire greatly, typically makes my head spin.↩︎"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html",
    "href": "posts/2021-05-time-series/index.html",
    "title": "Exploring Time",
    "section": "",
    "text": "NB: This post was revisited when updated the website, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#intro",
    "href": "posts/2021-05-time-series/index.html#intro",
    "title": "Exploring Time",
    "section": "Intro",
    "text": "Intro\n\nThis post was mostly complete around May 2021, but for various reasons not actually posted until August of 2022. I haven’t changed much aside from adding a section on boosting, and have used the results I conjured up previously (for the most part). However, many package updates since then may mean that parts of the code may not work as well, especially for the torch code. I would also recommend modeltime as starting point for implementing a variety of model approaches for time series data with R. It was still pretty new when this was first written, but has many new features and capabilities, and could do some version of the models shown here.\n\nIt is extremely common to have data that exists over a period of time. For example, we might have yearly sports statistics, daily manufacturing records, server logs that might be occurring many times per second, and similar. There are many approaches we could use to model the data in these scenarios. When there are few time points and they are clustered within other units, like repeated observations of exercise data for many individuals, we often use tools like mixed models for example, and even with many observations in a series, we can still use tools like that. But sometimes there may be no natural clustering, or we might want to use other approaches to handle additional complexity.\nThis post is inspired by a co-worker’s efforts in using PyTorch to analyze Chicago Transit data. Cody Dirks wrote a post where he used a Python module developed by our group at Strong Analytics to analyze the ridership across all the ‘L’. This post can be seen as a demonstration of some simpler models which might also be viable for a given situation such as this, allowing for quick dives, or even as ends in themselves."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#outline",
    "href": "posts/2021-05-time-series/index.html#outline",
    "title": "Exploring Time",
    "section": "Outline",
    "text": "Outline\nThe models we’ll go through are the following:\n\nError models and random effects\nGAM\nMore elaborate time series with seasonal and other effects\nBoosting and Deep learning\n\nIn what follows I will show some more detailed code in the beginning, but won’t show it later for conciseness, focusing mostly just on the basic model code. You can always find the code for these posts on my GitHub."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#quick-summary",
    "href": "posts/2021-05-time-series/index.html#quick-summary",
    "title": "Exploring Time",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nClassical econometrics approaches like ARIMA models may take notable effort to match the flexibility of other approaches one might take with time series. It’s also difficult to believe a specific lag/ma number will hold up with any data change.\nGAMs extend mixed models, and should probably be preferred if a probabilistic approach is desired. Prophet-style approaches would likely take notable effort and still likely lack performance, without adding interpretability.\nFor black box methods, boosting can do very well without much feature engineering, but possibly take a bit more for parameter tuning. Deep learning methods may be your best bet given data size and other data modeling needs."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#data-description",
    "href": "posts/2021-05-time-series/index.html#data-description",
    "title": "Exploring Time",
    "section": "Data Description",
    "text": "Data Description\nAs noted in Cody’s post, over 750,000 people use the Chicago Transit Authority’s ‘L’ system to get around the city. There are 8 interconnected rail lines named after colors- the Red, Blue, Green, Brown, Pink, Orange, Purple, and Yellow, 145 entry/exit stations, and over 2,300 combined trips by its railcars every day1.\nThe city of Chicago provides ridership data that can be accessed publicly.\n\nridership\nstation info\n\nIn Cody’s exploration, he added pertinent information regarding weather, sporting events, and more. You can access the processed data.\nFor our demonstrations we have daily ridership from 2012-2018, and we will use a variety of methods to model this. We will use a normalized ride count (mean of 0, standard deviation of 1) as our target variable.\n\nImport & Setup\nTo get things started we’ll use the tidyverse for some additional data processing, and lubridate for any date processing, for example, converting to weekdays.\n\n# Data Processing\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n# Misc\n\nSTART_DT = '2008-06-01'\nEND_DT   = '2018-12-31'\nSPLIT_DT = '2017-06-01'\n\n\nMain data\nI start with data having already been processed, but as mentioned the source is publicly available. I use data.table to read it in more quickly, but it’s default date class can cause issues with other packages, so I deal with that. I also extract the year, month, weekday, etc.\n\ndf = data.table::fread('../../data/time-series/processed_df.csv')\n\ndf_start = df %&gt;% \n  as_tibble() %&gt;% \n  select(-contains('_attributes'), -(tsun:wt22)) %&gt;% \n  mutate(\n    date      = as_date(date), # remove IDATE class\n    rides_log = log(rides),\n    year      = year(date),\n    year_fac  = factor(year),\n    month     = month(date, label = TRUE),\n    day       = factor(wday(date, label = TRUE), ordered = FALSE),\n    year_day  = lubridate::yday(date),\n    line      = factor(line),\n    snow_scaled = scale(snow)[, 1],\n    colors = as.character(line),\n    colors = ifelse(colors == 'purple_express', 'purple4', colors),\n    red_line_modernization = \n      ifelse(\n        between(date, as_date('2013-05-19'), as_date('2013-10-20')), \n        1, \n        0\n      )\n  ) %&gt;% \n  arrange(date, line)\n\n\n\n\nTraining and Validation\nWe split our data into training and validation sets, such that everything before 2017-06-01 is used for training, while everything after will be used for testing model performance2.\n\ndf_train = df_start %&gt;% \n  filter(date &lt; SPLIT_DT, !is.na(rides))\n\ndf_validate = df_start %&gt;% \n  filter(date &gt;= SPLIT_DT, !is.na(rides))\n\nred_line_train = df_train %&gt;% \n  filter(line == 'red')\n\nred_line_validate = df_validate %&gt;% \n  filter(line == 'red')\n\n\n\nOther\nHolidays are available via the prophet package, which we’ll be demonstrating a model with later. The data we’re using already has a ‘holiday vs. not’ variable for simplicity, though it comes from a different source. The prophet version has both the actual date and the observed date counted as a holiday, and I prefer to use both.\n\nholidays = prophet::generated_holidays %&gt;% \n  filter(country == 'US') %&gt;% \n  mutate(ds = as.numeric(as_date(ds))) %&gt;%\n  droplevels()\n\nWe’ll take a quick look at the red line similar to Cody’s post, so we can feel we have the data processed as we should.\n\n\n\n\n\n\n\n\n\nWith the data ready to go, we are ready for modeling, so let’s get started!"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#classical-time-series",
    "href": "posts/2021-05-time-series/index.html#classical-time-series",
    "title": "Exploring Time",
    "section": "Classical Time Series",
    "text": "Classical Time Series\n\nIntro\nClassical times series from an econometrics perspective often considers a error model that accounts for the correlation a current observation has with past observations. A traditional example is the so-called autoregressive, or AR, model, which lets a current observation be predicted by past observations up to a certain point. For example, would could start by just using the last observation to predict the current one. Next we extend this to predict the current based on the previous two observations, and so on. How many lags we use is part of the model exploration.\n\\[y_t = \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + \\varepsilon_t\\]\nWe can extend this to include not just past observations but also past residuals, called a moving average. So formally, our ARMA (p, q) model now looks like this for an observation \\(y\\) at time \\(t\\):\n\\[y_t = \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots +\\theta_q \\varepsilon_{t-q})\\]\nWe can also use differencing, for example subtracting the previous time value from the current observation value for all values, to come to the final ARIMA (p, d, q) model. See Hyndman and Athanasopoulos (2021) for more details.\n\n\nModel\nEven base R comes with basic time series models such as this. However, as mentioned, we typically don’t know what to set the values of an ARIMA(p, d, q) to. A quick way to explore this is via the forecast package, which will search over the various hyperparameters and select one based on AIC. Note that fable, a package we will be using later, will also allow such an approach, and if you’d like to go ahead and start using it I show some commented code below.\n\nmodel_arima = forecast::auto.arima(\n  red_line_train$rides_scaled\n)\n\n# model_arima = red_line_train %&gt;%\n#   select(date, rides_scaled) %&gt;%\n#   tsibble::as_tsibble() %&gt;%\n#   fabletools::model(fable::ARIMA(\n#     rides_scaled ~ 0 + PDQ(0,0,0),\n#     stepwise = FALSE,\n#     approximation = FALSE\n#   ))\n# fabletools::report(model_arima)\n\n\n\nExplore\nIn this case we have a selected AR of 3 and MA of 4 for the centered value. But looking at the predictions, we can see this is an almost useless result for any number of days out, and does little better than guessing.\n\nbroom::tidy(model_arima)\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n0.238\n0.048\n\n\nar2\n-0.285\n0.035\n\n\nar3\n0.354\n0.043\n\n\nma1\n-0.725\n0.046\n\n\nma2\n-0.189\n0.046\n\n\nma3\n-0.575\n0.029\n\n\nma4\n0.552\n0.025\n\n\n\n\n\n\n\n\n# plot(acf(residuals(model_arima))) # weekly autocorrelation still exists\n\nred_line_validate %&gt;% \n  slice(1:30)  %&gt;%\n  mutate(pred = predict(model_arima, n.ahead = 30)$pred) %&gt;%\n  # mutate(pred = forecast(model_arima, h = 30)$.mean) %&gt;%\n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'darkred')\n\n\n\n\n\n\n\n\nWe’ll use yardstick to help us evaluate performance for this and subsequent models. In this case however, the visualization told us enough- a basic ARIMA isn’t going cut it.\n\nlibrary(yardstick)\n\n# this function will be used for all subsequent models!\nmetric_score = metric_set(rmse, mae, rsq) \n\n# validation\ndata.frame(\n  pred = predict(model_arima, newdata = red_line_validate, n.ahead = 30)$pred,\n  observed = red_line_validate$rides_scaled[1:30]\n) %&gt;%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.632\n\n\nmae\nstandard\n0.572\n\n\nrsq\nstandard\n0.132\n\n\n\n\n\n\n\nOne nice thing about the forecast package is that it can include additional features via the xreg argument, which is exactly what we need- additional information. Now our model looks something like this, where \\(X\\) is our model matrix of features and \\(\\beta\\) their corresponding regression weights.\n\\[y_t = X_t\\beta + \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots +\\theta_q \\varepsilon_{t-q})\\]\nAdding these is not exactly straightforward, since it requires a matrix rather than a data frame, but this is not too big a deal once you are used to creating model matrices.\n\nmm = model.matrix(\n  ~ . - 1, \n  data = red_line_train %&gt;% \n    select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization)\n)\n\nmodel_arima_xreg = forecast::auto.arima(\n  red_line_train$rides_scaled,\n  max.p = 10,\n  max.q = 10,\n  xreg  = mm\n)\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n-0.444\n0.018\n\n\nar2\n-0.430\n0.018\n\n\nar3\n-0.370\n0.019\n\n\nar4\n-0.325\n0.019\n\n\nar5\n-0.312\n0.019\n\n\nar6\n-0.356\n0.019\n\n\nar7\n0.307\n0.018\n\n\nar8\n-0.051\n0.017\n\n\nis_weekend\n-1.154\n0.023\n\n\nis_holiday\n-1.045\n0.021\n\n\nis_cubs_game\n0.208\n0.015\n\n\nis_sox_game\n0.072\n0.015\n\n\ntmax_scaled\n0.085\n0.011\n\n\nprcp_scaled\n-0.031\n0.004\n\n\nred_line_modernization\n-0.550\n0.131\n\n\n\n\n\n\n\nThis is looking much better! We can also see how notably different the ARMA structure is relative to the previous model. We also see that adding weekend and holiday effects result in a huge drop in ridership as expected, while baseball games and good weather will lead to an increase.\nIn the following code, we create a model matrix similar to the training data that we can feed into the predict function. The forecast package also offers a glance method if desired.\n\nnd = red_line_validate %&gt;%\n  select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization) %&gt;% \n  model.matrix( ~ . - 1, data = .)\n\npreds = predict(model_arima_xreg, newxreg = nd, n.ahead = nrow(red_line_validate))$pred\n\n\np_arima_red = red_line_validate %&gt;% \n  mutate(pred = preds) %&gt;% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'red') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'ARIMA')\n\np_arima_red\n\n\n\n\n\n\n\n\n\n\nAnd here we can see performance is notably improved (restrict to first 30 obs for a direct comparison to the previous).\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.371\n\n\nmae\nstandard\n0.282\n\n\nrsq\nstandard\n0.747"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#mixed-model-with-ar-structure",
    "href": "posts/2021-05-time-series/index.html#mixed-model-with-ar-structure",
    "title": "Exploring Time",
    "section": "Mixed model with AR Structure",
    "text": "Mixed model with AR Structure\n\nIntro\nMore generally, we can think of that original AR error as a random effect, such that after the linear predictor is constructed, we add a random effect based on the correlation structure desired, in this case, autoregressive. In the mixed model setting, it is actually quite common to use an AR residual structure within a cluster or group, and here we can do so as well, as the data is naturally grouped by line.\nTo make this a bit more clear, we can state the AR effect more formally as follows for a single line at time \\(t\\):\n\\[z_t  \\sim N(0, \\Sigma_{ar})\\] \\[\\Sigma_{ar} = cov(z(s), z(t)) = \\sigma^2\\exp(-\\theta|t-s|)\\]\nWhere t,s are different time points, e.g. within a line.\nIf we were to simulate it for 4 time points, with autocovariance value of .5, we could do so as follows3.\n\nn_clusters   = 1\nn_timepoints = 4\nmu  = 0\nvar = 1  # not actually used if the value is 1\nS = .5^as.matrix(dist(1:n_timepoints))\n\nS\n\nz = MASS::mvrnorm(mu = rep(mu, n_timepoints), Sigma = S)\n\nz\n\nAnd here is our typical model with a single random effect, e.g. for line:\n\\[ y_{tl} \\sim X\\beta + z^{line}_{l} + e_{tl}\\] \\[\\textrm{z}_{l} \\sim N(0, \\sigma_l^2)\\] \\[\\epsilon \\sim N(0, \\sigma_e^2)\\]\nThe X may be at either line or observation level, and potentially the \\(\\beta\\) could vary by line.\nPutting it all together, we’re just adding the AR random effect to the standard mixed model for a single line.\n\\[ y_{tl} \\sim X\\beta + z^{ar}_t +z^{line}_{l} + e_{tl}\\]\n\n\nData Prep\nSo let’s try this! First some minor data prep to add holidays.\n\ndf_train_mixed = df_train %&gt;% \n  mutate(date = as.numeric(date)) %&gt;% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %&gt;% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\ndf_validate_mixed = df_validate %&gt;% \n  mutate(date = as.numeric(date)) %&gt;% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %&gt;% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\n\n\nModel\nFor the model, we can now easily think of it as we do other standard modeling scenarios. Along with standard features, we’ll add random effects for line, day, day x line interaction, etc. Finally we also add an AR random effect. For each line, we have an autoregressive structure for days, such that days right next to each other are correlated, and this correlation tapers off as days are further apart. This is not our only option, but seems straightforward to me.\nDepending on what you include in the model, you may have convergence issues, so feel free to reduce the complexity if needed. For example, most of the day effect is captured by weekend vs. not, and a by line year trend wasn’t really necessary. In addition, the way the AR random effect variance is estimated as noted above, this essentially captures the line intercept variance.\n\nmodel_mixed = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  tmax_scaled + \n  prcp_scaled + \n  snow_scaled +\n  # year_day +\n  ar1(0 + day|line) +     # the 0 + is a nuance of tmb's approach\n  (1|holiday) +           # as RE with all holidays instead of just holiday vs. not\n  (1|year) +     \n  (1 | red_line_modernization:line) +  # the project shifted ridership from red to other lines\n  # (1|day) #+ \n  # (1|line) +\n  (1|day:line) #+\n  # (1 + year_day|line)\n\nlibrary(glmmTMB)\n\nfit_mixed = glmmTMB(model_mixed, data = df_train_mixed)\n\n\n\nExplore\nThe mixed model approach is nice because it is highly interpretable. We get both standard regression coefficients, and variance components to help us understand how the rest of the variance breaks down. For example, I would interpret the following that that line and weekend are the biggest contributors to the variability seen, and that we have high autocorrelation, as expected.\n\nlibrary(mixedup)\n\nsummarise_model(fit_mixed, digits = 4)\n\nextract_cor_structure(fit_mixed, which_cor = 'ar1')\n\nWe can visually inspect how well it matches the data. In the following the colored lines are the predictions, while the observed is gray. It looks like performance tapers for more recent time periods, and holiday effects are not as prevalent for some lines (e.g. yellow). The latter could be helped by adding a holiday:line random effect.\n\nlibrary(glmmTMB)\n\np_mixed = df_validate_mixed %&gt;% \n  droplevels() %&gt;% \n  mutate(pred = predict(fit_mixed, newdata = ., allow.new.levels=TRUE)) %&gt;%\n  mutate(date = as_date(date)) %&gt;% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .1) +\n  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +\n  facet_grid(rows = vars(line), scales = 'free_y') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'Mixed')\n\np_mixed\n\n\n\n\n\n\n\n\nAs before we can measure performance via yardstick. This model does appears to do very well.\n\n# validation\ndata.frame(\n  pred = predict(fit_mixed, newdata = df_validate_mixed, allow.new.levels = TRUE),\n  observed = df_validate_mixed$rides_scaled\n) %&gt;%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.189\n\n\nmae\nstandard\n0.110\n\n\nrsq\nstandard\n0.965\n\n\n\n\n\n\n\nFor more on autocorrelation structure in the mixed model setting, see my mixed model document here4."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#generalized-additive-models",
    "href": "posts/2021-05-time-series/index.html#generalized-additive-models",
    "title": "Exploring Time",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\n\nIntro\nWe can generalize mixed models even further to incorporate nonlinear components, which may include cyclic or other effects. Such models are typically referred to as generalized additive models (GAMs). AR processes themselves can be seen as a special case of gaussian processes, which can potentially be approximated via GAMs. As GAMs can accommodate spatial, temporal, nonlinear, and other effects, they are sometimes more generally referred to as structured additive regression models, or STARs.\n\n\nData Prep\nThe data prep for the GAM is the same as with the mixed model, so we’ll just use that data.\n\ndf_train_gam = df_train_mixed\n\ndf_validate_gam = df_validate_mixed\n\n\n\nModel\nWith data in place we are ready to conduct the model. We have numerous options for how we’d like to take this. However, as an example, I tried various smooths, but didn’t really see much difference, which is actually a good thing. For any further improvements we’d likely have to tweak the core model itself. I also use bam for a quicker result, but this isn’t really necessary, as it didn’t even take a minute to run with standard gam. As with the mixed model, we will use holiday as a random effect, but we add the holiday by line interaction since we saw that need. In addition, our year-day by line interaction should help some with the tailing off of more recent predictions.\n\nlibrary(mgcv)\n\n# for year, use year (numeric) or use year_fac, but for latter, it will not be\n# able to predict any year not in the training data unless you use\n# drop.unused.levels.\nmodel_gam = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  s(tmax_scaled) + \n  s(prcp_scaled) + \n  s(snow_scaled) +\n  s(red_line_modernization, line, bs = 're') +\n  s(holiday,  bs = 're') +\n  s(holiday, line,  bs = 're') +\n  s(year_fac, bs = 're') +      \n  s(day,  bs = 're') + \n  s(line, bs = 're') + \n  s(line, day, bs = 're') + \n  s(year_day, by = line, bs = c('ds', 'fs'))\n\n\n# will take a while!\n# fit_gam = gam(\n#   model_gam, \n#   data     = df_train_gam,\n#   drop.unused.levels = FALSE, \n#   method   = \"REML\"\n# )\n\n# fast even without parallel\nfit_gam = bam(\n  model_gam, \n  data     = df_train_gam,\n  drop.unused.levels = FALSE, \n  method   = \"fREML\",\n  discrete = TRUE     # will fit the model in a second rather than a couple seconds\n  # nthreads = 8,     # this option assumes a cluster is available. not necessary for this.\n)\n\n\n\nExplore\nAs with glmmTMB, I use a custom function to summarize the model, or extract different components from it. From the initial glance we can see things that we expect (e.g. line and weekend effects are large).\n\nmixedup::summarise_model(fit_gam)\n\n\nVariance Components:\n\n\n                       Group                 Effect Variance   SD SD_2.5      SD_97.5 Var_prop\n                 tmax_scaled              Intercept     0.01 0.09   0.04 2.100000e-01     0.01\n                 prcp_scaled              Intercept     0.00 0.01   0.00 2.000000e-02     0.00\n                 snow_scaled              Intercept     0.00 0.01   0.00 2.000000e-02     0.00\n                        line red_line_modernization     0.09 0.31   0.19 4.900000e-01     0.13\n                     holiday              Intercept     0.05 0.22   0.14 3.300000e-01     0.06\n                     holiday                   line     0.04 0.21   0.18 2.400000e-01     0.06\n                    year_fac              Intercept     0.00 0.06   0.04 1.000000e-01     0.01\n                         day              Intercept     0.00 0.00   0.00 4.162009e+69     0.00\n                        line              Intercept     0.49 0.70   0.42 1.150000e+00     0.65\n                        line                    day     0.05 0.22   0.18 2.700000e-01     0.06\n           year_day:lineblue              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n          year_day:linebrown              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n          year_day:linegreen              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:lineorange              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n           year_day:linepink              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:linepurple              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n year_day:linepurple_express              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n            year_day:linered              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:lineyellow              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n                    Residual                     NA     0.02 0.13   0.13 1.300000e-01     0.02\n\n\n\nFixed Effects:\n\n\n         Term Value   SE     t P_value Lower_2.5 Upper_97.5\n    Intercept -0.28 0.24 -1.14    0.25     -0.76       0.20\n   is_weekend -0.54 0.06 -8.27    0.00     -0.66      -0.41\n is_cubs_game  0.04 0.00 13.81    0.00      0.03       0.04\n  is_sox_game  0.01 0.00  3.81    0.00      0.00       0.02\n\n\nNow we can visualize test predictions broken about by line. The greater flexibility of the GAM for fixed and other effects allows it to follow the trends more easily than the standard linear mixed model approach.\n\n\n\n\n\n\n\n\n\nWe can also see improvement over our standard mixed model approach, and our best performance yet.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.140\n\n\nmae\nstandard\n0.087\n\n\nrsq\nstandard\n0.981"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#prophet",
    "href": "posts/2021-05-time-series/index.html#prophet",
    "title": "Exploring Time",
    "section": "Prophet",
    "text": "Prophet\n\nIntro\nProphet is an approach from Facebook that uses Stan to estimate a time series model taking various trends, seasonality, and other factors under consideration. By default, it only uses Stan for optimization (e.g. via ‘BFGS’), but you can switch to fully Bayesian if desired, and take advantage of all that the Bayesian approach has to offer.\n\n\nData Prep\nThe prophet package in R takes some getting used to. We have to have specific names for our variables, and unfortunately have to do extra work to incorporate categorical features. We can use recipes (like yardstick, part of the tidymodels ’verse) to set up the data (e.g. one-hot encoding).\n\nlibrary(prophet)\nlibrary(recipes)\n\ndf_train_prophet = df_train %&gt;% \n  arrange(date, line) %&gt;% \n  rename(y  = rides_scaled,\n         ds = date)\n\nrec = recipes::recipe(~., df_train_prophet)\n\ndf_train_prophet = rec %&gt;% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %&gt;% \n  prep(training = df_train_prophet) %&gt;% \n  bake(new_data = df_train_prophet) %&gt;% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\ndf_validate_prophet = df_validate %&gt;% \n  arrange(date, line)%&gt;%\n  rename(ds = date, y = rides_scaled)\n\nrec = recipe(~., df_validate_prophet)\n\ndf_validate_prophet = rec %&gt;% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %&gt;% \n  prep(training = df_train_prophet) %&gt;% \n  bake(new_data = df_validate_prophet) %&gt;% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\n\n\nModel\nWith data in place, we are ready to build the model. Note that later we will compare results to fable.prophet, which will mask some of the functions here, or vice versa depending on which you load first. I would suggest only doing the prophet model, or only doing the fable model, rather than trying to do both at the same time, to avoid any mix-up.\nAfter setting up the model, you have to add additional features in separate steps. Prophet has a nice way to incorporate holidays though. When you run this model, you may have to wait for a minute or so.\n\n# use prophet::prophet in case you have fable.prophet loaded also\nmodel_prophet = prophet::prophet(\n  holidays = generated_holidays %&gt;% filter(country == 'US'),\n  yearly.seasonality = FALSE,\n  seasonality.mode = \"multiplicative\",\n  changepoint.prior.scale = .5\n)\n\nline_names = c(\n  'blue',\n  'brown',\n  'green',\n  'orange',\n  'pink',\n  'purple',\n  'purple_express',\n  'red',\n  'yellow'\n)\n\npredictors = c(\n  'is_weekend',\n  'is_cubs_game',\n  'is_sox_game',\n  # 'is_holiday',\n  'tmax_scaled',\n  'prcp_scaled',\n  'snow_scaled',\n  line_names\n)\n\nfor (i in predictors) {\n  model_prophet = add_regressor(model_prophet, i, standardize = FALSE, mode = 'additive')\n}\n\nmodel_prophet = add_country_holidays(model_prophet, country_name = 'US')\n\nfit_prophet = fit.prophet(model_prophet, df = df_train_prophet)\n\nforecast = predict(fit_prophet, df_validate_prophet)\n\n\n\nExplore\nWe now visualize predictions as we did with the GAM. But one of the nice things with prophet is that you can plot the various parts of the model results via the plot method or prophet_plot_components (not shown). Unfortunately, our baseline effort seems to undersmooth our more popular lines (blue, red), and overreacts to some of the others (purple, yellow). This is because it’s not really changing the predictions according to line.\n\n\n\n\n\n\n\n\n\nWe can also assess performance as before, but note that prophet has it’s own cross-validation capabilities which would be better to utilize if this was your primary tool. Between the previous visualization and these metrics, our first stab doesn’t appear to do as well as the GAM, so you might like to go back and tweak things.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.280\n\n\nmae\nstandard\n0.198\n\n\nrsq\nstandard\n0.925"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#fable",
    "href": "posts/2021-05-time-series/index.html#fable",
    "title": "Exploring Time",
    "section": "Fable",
    "text": "Fable\n\nIntro\nI came across fable.prophet as a possibly easier way to engage prophet. It is an extension of fable and related packages, which are very useful for time series processing and analysis. Note that it is 0.1.0 version development, and hasn’t had much done with it in the past year, so your mileage may vary with regard to utility by the time you read this5. But with it we can specify the model in more of an R fashion, and we now don’t have as much data pre-processing.\n\n\nData Prep\nOne key difference using fable.prophet is that it works with tsibble objects, and thus must have unique date observations. We can do this by setting line as the key6.\n\nlibrary(fable.prophet)\n\ndf_train_fable = df_train_prophet %&gt;% \n  as_tsibble(index = ds, key = line)\n\ndf_validate_fable = df_validate_prophet %&gt;% \n  as_tsibble(index = ds, key = line)\n\nholidays_fable = holidays %&gt;% \n  filter(country == 'US') %&gt;% \n  mutate(ds = as_date(ds)) %&gt;% \n  as_tsibble()\n\n\n\nModel\nBeyond this we use functions within our formula to set the various components. With line as the key, fable is actually running separate prophet models for each line, and we can do so in parallel if desired.\n\nmodel_prophet = fable.prophet::prophet(\n  y ~ \n    growth('linear', changepoint_prior_scale = 0.5) +\n    season(\"week\", type = \"multiplicative\") +\n    holiday(holidays_fable) +\n    xreg(\n      is_weekend,\n      is_cubs_game,\n      is_sox_game,\n      # is_holiday,\n      tmax_scaled,\n      prcp_scaled,\n      snow_scaled\n    ) \n)\n\n# library(future)\n# plan(multisession)\n\n# furrr is used under the hood, and though it wants a seed, it doesn't\n# automatically use one so will give warnings. I don't think it can be passed\n# via the model function, so expect to see ignorable warnings (suppressed here).\n\nfit_fable = model(df_train_fable, mdl = model_prophet)\n\nforecast_fable = fit_fable %&gt;% \n  forecast(df_validate_fable) \n\n# plan(sequential)\n\n\n\nExplore\nWith fable.prophet visualization, we have the more automatic plots, but again we’ll stick with the basic validation plot we’ve been doing.\n\ncomponents(fit_fable)\ncomponents(fit_fable) %&gt;%\n  autoplot()\nforecast_fable %&gt;%\n  autoplot(level = 95, color = '#ff5500')\n\nThis model does well, and better than basic prophet, but we can see its limitations, for example, with the yellow line, and more recent ridership in general.\n\n\n\n\n\n\n\n\n\nAnd we check performance as before. The fable model is doing almost as well as our GAM approach did.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.154\n\n\nmae\nstandard\n0.092\n\n\nrsq\nstandard\n0.979\n\n\n\n\n\n\n\nOne nice thing about the fable approach is its internal performance metrics, which are easily obtained. It will give us results for each line7, validation data results shown. We see that we have more error for the popular lines as before, but in terms of percentage error, the other lines are showing more difficulty. You can find out more about the additional metrics available here.\n\naccuracy(fit_fable)\naccuracy(forecast_fable, df_validate_fable)\n\n\n\n\n\n\n.model\nline\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nACF1\n\n\n\n\nmdl\nblue\nTest\n0.083\n0.251\n0.180\n-27.007\n62.199\n0.641\n\n\nmdl\nbrown\nTest\n0.001\n0.126\n0.084\n-31.034\n88.711\n0.621\n\n\nmdl\ngreen\nTest\n0.088\n0.118\n0.100\n-105.627\n112.000\n0.660\n\n\nmdl\norange\nTest\n0.035\n0.084\n0.063\n-43.228\n60.244\n0.638\n\n\nmdl\npink\nTest\n0.058\n0.089\n0.072\n-18.386\n20.452\n0.675\n\n\nmdl\npurple\nTest\n0.006\n0.012\n0.009\n-0.633\n0.982\n0.503\n\n\nmdl\npurple_express\nTest\n0.049\n0.107\n0.083\n-41.459\n218.861\n0.752\n\n\nmdl\nred\nTest\n0.045\n0.297\n0.211\n-3.003\n14.880\n0.598\n\n\nmdl\nyellow\nTest\n-0.027\n0.030\n0.027\n2.892\n2.912\n0.726\n\n\n\n\n\n\n\nThe fable results suggests what we already knew from our GAM and mixed model approach, that interactions of the series with line are important. We weren’t easily able to do this with the default prophet (it would likely require adding time x line interaction regresssors), so it’s nice that we have the option here."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#gbm",
    "href": "posts/2021-05-time-series/index.html#gbm",
    "title": "Exploring Time",
    "section": "GBM",
    "text": "GBM\n\nIntro\nThis part is actually new from when I first wrote up this post over a year ago. I basically wanted to test if a boosting approach would work decently out of the box without doing anything special regarding the structure of the data. I don’t add it to the summary visualizations, but provide the standard results here.\n\n\nData Prep\nI’ll use lightgbm which I’ve been increasingly using of late. It requires a matrix object for input, and so some additional processing as well.\n\nlibrary(lightgbm)\n\n# lightgbm only accepts a matrix input\ndf_train_lgb_init = df_train %&gt;% \n  select(\n    rides_scaled,\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %&gt;% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  )\n\nX_train = as.matrix(df_train_lgb_init %&gt;% select(-rides_scaled))\n\nX_test  = df_validate %&gt;% \n  select(\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %&gt;% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  ) %&gt;% \n  as.matrix()\n\n\ndf_train_lgb = lgb.Dataset(\n  X_train,\n  label = df_train_lgb_init$rides_scaled,\n  categorical_feature = c(\n    'is_weekday',\n    'is_holiday',\n    'is_cubs_game',\n    'is_sox_game',\n    'line',\n    'year',\n    'month'\n  )\n)\n\ndf_test_lgb  = lgb.Dataset.create.valid(\n  df_train_lgb, \n  X_test,\n  label = df_validate$rides_scaled\n)\n\n\n\nModel\nTypically we would perform some sort of search over the (many) parameters available to tweak with lightgbm, like the number of trees, learning rate, regularizer parameters and more. I ignore that, but I did fiddle with the learning rate and bumped up the nrounds (trees), but that’s it.\n\nparams = list(\n  objective       = \"regression\"\n  , metric        = \"l2\"\n  , min_data      = 10L\n  , learning_rate = .01\n)\n\nfit_gbm = lgb.train(\n  params    = params\n  , data    = df_train_lgb\n  , nrounds = 2000L\n)\n\n\n\nExplore\nSome may be surprised at how well this does, but regular users of boosting probably are not. We didn’t have to do much and it’s already the best performing model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.130\n\n\nmae\nstandard\n0.076\n\n\nrsq\nstandard\n0.984"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#torch",
    "href": "posts/2021-05-time-series/index.html#torch",
    "title": "Exploring Time",
    "section": "Torch",
    "text": "Torch\nAt this point we have a collection of models that are still relatively interpretable, and mostly within our standard regression model framework. It’s good to see them able to perform very well without too much complexity. However, we still have other methods available that would be more computationally demanding, are more opaque in operations, but which would potentially provide the most accurate forecasts. For this we turn to using PyTorch, which is now available via the torch package in R8.\nIn using torch, we’re going to follow the demo series at the RStudio AI blog 9. It shows in four parts how to use a recurrent neural network. In their example, they use a data set for a single series with (summarized) daily values, similar to our daily counts here. We will use the final model demonstrated in the series, a soi disant seq2seq model that includes an attention mechanism. More detail can be found here. The conceptual gist of the model can be described as taking a set of time points to predict another set of future time points, and doing so for all points in the series.\nTo be clear, they only use a single series, no other information (e.g. additional regressors). So we will do the same, coming full circle to what we started out with, just looking at daily ridership- a single time series for the red line.\n\nData\nAs usual we’ll need some data prep, both for initial training-test split creation, but also specifically for usage with Torch.\n\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(torch)\n\n\ndf_train_torch = df_train %&gt;%\n  filter(line == 'red', year &lt; 2017) %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ndf_validate_torch = df_validate %&gt;%\n  filter(line == 'red', year &gt;= 2017) %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ndf_test_torch = df_validate %&gt;%\n  filter(line == 'red', date &gt; '2017-12-24') %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ntrain_mean = mean(df_train_torch)\ntrain_sd   = sd(df_train_torch)\n\n\n\nTorch data\nFor our data, we will use a week behind lag to predict the following week. This seems appropriate for this problem, but for any particular time series problem we’d want to probably think hard about this and/or test different settings.\n\nn_timesteps = 7    # we use a week instead of 14 days in original blog\nn_forecast  = 7    # look ahead one week\n\n\ncta_dataset &lt;- dataset(\n  name = \"cta_dataset\",\n\n  initialize = function(x, n_timesteps, sample_frac = 1) {\n\n    self$n_timesteps &lt;- n_timesteps\n    self$x &lt;- torch_tensor((x - train_mean) / train_sd)\n\n    n &lt;- length(self$x) - self$n_timesteps - 1\n\n    self$starts &lt;- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n\n  },\n\n  .getitem = function(i) {\n\n    start &lt;- self$starts[i]\n    end &lt;- start + self$n_timesteps - 1\n    lag &lt;- 1\n\n    list(\n      x = self$x[start:end],\n      y = self$x[(start+lag):(end+lag)]$squeeze(2)\n    )\n\n  },\n\n  .length = function() {\n    length(self$starts)\n  }\n)\n\nbatch_size = 32\n\ntrain_ds = cta_dataset(df_train_torch, n_timesteps)\ntrain_dl = dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds = cta_dataset(df_validate_torch, n_timesteps)\nvalid_dl = dataloader(valid_ds, batch_size = batch_size)\n\ntest_ds  = cta_dataset(df_test_torch, n_timesteps)\ntest_dl  = dataloader(test_ds, batch_size = 1)\n\n\n\nModel\nI leave it to the blog for details, but briefly, there are four components to the model:\n\nEncoder: takes input, and produces outputs and states via RNN\nDecoder: takes the last predicted value as input and current context to make a new prediction\nSeq2Seq: essentially encodes once, and calls the decoder in a loop\nAttention: allows output from the encoder at a specific time point to provide ‘context’ for the decoder\n\n\nnet =\n  seq2seq_module(\n    \"gru\",\n    input_size     = 1,\n    hidden_size    = 32,\n    attention_type = \"multiplicative\",\n    attention_size = 8,\n    n_forecast     = n_forecast\n  )\n\nb = dataloader_make_iter(train_dl) %&gt;% dataloader_next()\n\nnet(b$x, b$y, teacher_forcing_ratio = 1)\n\n\n\nTraining\nWith data in place, we’re ready to train the model. For the most part, not much is going on here that would be different from other deep learning situations, e.g. choosing an optimizer, number of epochs, etc. We’ll use mean squared error as our loss, and I create an object to store the validation loss over the epochs of training. I played around with it a bit, and you’re probably not going to see much improvement after letting it go for 100 epochs.\n\noptimizer = optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs = 100\n\ntrain_batch &lt;- function(b, teacher_forcing_ratio) {\n\n  optimizer$zero_grad()\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio)\n  target &lt;- b$y\n\n  loss &lt;- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n  loss$backward()\n  optimizer$step()\n\n  loss$item()\n\n}\n\nvalid_batch &lt;- function(b, teacher_forcing_ratio = 0) {\n\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio)\n  target &lt;- b$y\n\n  loss &lt;- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n\n  loss$item()\n\n}\n\n\nall_valid_loss = c()\n\nfor (epoch in 1:num_epochs) {\n\n  net$train()\n  train_loss &lt;- c()\n\n  coro::loop(for (b in train_dl) {\n    loss &lt;- train_batch(b, teacher_forcing_ratio = 0.0)\n    train_loss &lt;- c(train_loss, loss)\n  })\n\n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n\n  net$eval()\n  valid_loss &lt;- c()\n\n  coro::loop(for (b in valid_dl) {\n    loss &lt;- valid_batch(b)\n    valid_loss &lt;- c(valid_loss, loss)\n  })\n  \n  all_valid_loss = c(all_valid_loss, mean(valid_loss))\n\n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\n\nEvaluations\n\nnet$eval()\n\ntest_preds = vector(mode = \"list\", length = length(test_dl))\n\ni = 1\n\ncoro::loop(for (b in test_dl) {\n\n  if (i %% 100 == 0)\n    print(i)\n\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio = 0)\n  preds &lt;- as.numeric(output)\n\n  test_preds[[i]] &lt;- preds\n  i &lt;&lt;- i + 1\n})\n\nFor this visualization, we do things a little different. In our current setup, we have 7 timesteps predicting 7 day windows. We started our test set at the beginning of December so that the first prediction is January first, and then proceeds accordingly.\n\n# same as test\ndf_eval_torch = df_validate %&gt;%\n  filter(line == 'red', date &gt; '2017-12-01') %&gt;%\n  select(rides_scaled, date) %&gt;%\n  as_tsibble()\n\ntest_preds_plot = vector(mode = \"list\", length = length(test_preds))\n\nfor (i in 1:(length(test_preds_plot)-  n_forecast)) {\n  test_preds_plot[[i]] =\n    data.frame(\n      pred = c(\n        rep(NA, n_timesteps + (i - 1)),\n        test_preds[[i]] * train_sd + train_mean,\n        rep(NA, nrow(df_eval_torch) - (i - 1) - n_timesteps - n_forecast)\n      )\n    )\n}\n\ndf_eval_torch_plot0 =\n  bind_cols(df_eval_torch, bind_cols(test_preds_plot))\n\nA visualization of the predictions makes this more clear. Each 7 day segment is making predictions for the next 7 days. The following predictions are for the last two months, with each column a set of 7 predictions for that time point.\n\n\n\n\n\n\n\n\n\nSo for our red line plot, we’ll just use the average prediction at each date to make it comparable to the other plots. In general it looks to be doing okay, even armed with no contextual information. Certainly better than the base ARIMA plot.\n\n\n\n\n\n\n\n\n\nHowever, we can see that there is much information lost just adhering to the series alone.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.804\n\n\nmae\nstandard\n0.584\n\n\nrsq\nstandard\n0.120"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#all",
    "href": "posts/2021-05-time-series/index.html#all",
    "title": "Exploring Time",
    "section": "All",
    "text": "All"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#summary",
    "href": "posts/2021-05-time-series/index.html#summary",
    "title": "Exploring Time",
    "section": "Summary",
    "text": "Summary\n\nARIMA: no real reason to still be doing such a simplified model\nMixed Model: may be just what you need, but may lack in other settings\nGAM: great, more viable than some might suspect, easy implementation\nProphet/Fable: Prophet needs notable work out of the box, though Fable saves you some of that work, and did great in this situation via by-group models\nGBM: can it really be this easy?\nTorch: pretty good even with minimal information\n\nTo get some information on what Torch would do at the next level, i.e. adding additional features and other considerations, see Cody’s post."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#footnotes",
    "href": "posts/2021-05-time-series/index.html#footnotes",
    "title": "Exploring Time",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is also the Purple express line, which is very irregular compared to the others.↩︎\nTechnically we should scale the test set using the mean/sd of the training set, and though with very large data this should not matter, for time series it’s a particular concern as data can ‘shift’ over time.↩︎\nThis follows Bolker’s demo.↩︎\nI always appreciated the depiction of this topic in West, Welch, and Galecki (2022) quite a bit.↩︎\nA year plus later after that statement, it still hasn’t gone beyond 0.1.0, so I don’t think this will continue to be useful for very long. Unfortunate, but honestly, it’s not clear prophet&lt;/span itself can do much better than many other tools.↩︎\nfable.prophet may have a bug enabling the holidays functionality with parallel, so you can just use the original holiday column if you do so (single core doesn’t take too long).↩︎\nWe can also do this with our previous method with a split-by-apply approach. You would obtain the same results, so this serves as a nice supplement to our ‘overall’ metrics.↩︎\nFor the basics of using PyTorch via R, including installation, see the RStudio post.↩︎\nThe blog code actually has several issues, but the github repo should work fine and is what is followed for this demo.↩︎"
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html",
    "href": "posts/2021-10-30-double-descent/index.html",
    "title": "Double Descent",
    "section": "",
    "text": "A co-worker passed along a recent article (Dar, Muthukumar, and Baraniuk 2021) on the topic of double descent in machine learning. I figured I’d summarize some key points I came across while perusing it and some referenced articles. In addition, I’ll provide an accessible example demonstrating the phenomenon."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#what-is-double-descent",
    "href": "posts/2021-10-30-double-descent/index.html#what-is-double-descent",
    "title": "Double Descent",
    "section": "What is double descent?",
    "text": "What is double descent?\n\nBias-variance trade-off\nTo understand double descent you have to revisit the concept of the bias-variance trade-off. Without going into too much detail, the main idea with it is that having an overly complex model leads to overfitting the training data, which results in worse prediction on new data, at least relative to what simpler models would have done. The classic figure looks like the following, where blue is the training error and the red is the test error. Thin lines represent one path of complexity (e.g. across a random sample of the data), while the thicker lines are the average at a particular point of model complexity.\n\nIf we don’t have a sufficiently complex model, both training and test error will be poor, the case of underfitting. Our model is a poor approximation of the true underlying function, and predicts poorly on data both seen and unseen. When we have too much model complexity relative to the size of our data (e.g. more covariates, nonlinear effects, interactions, etc.), we pass into the overfit situation. Essentially, while our model function would result in a decrease in error with the data it’s trained on (lower bias as it better approximates the true underlying function), with too much complexity, you’d also eventually have notable changes in prediction (high variance) with any slight deviation in the underlying training data. We can even get to the point where we fit the training data perfectly, but it will be overly susceptible to the noise in the data, and not do well with unseen observations.\nTo combat this, we usually attempt to find a balance between overly simple and overly complex models. This would be the point where test error is among its lowest point for a desirable level of complexity (e.g. around 20-25 df in the figure above), before it begins to rise again. This may be accomplished more explicitly, for example, picking a model through cross-validation, or more implicitly, for example, through regularization (Belkin et al. (2019)). For more detail on the bias-variance trade-off, you can look at the exposition in the main article noted above, my document here, or any number of places, as it is an extremely well-known idea in machine learning.\n\n\nDouble Descent\nThe funny thing is, it turns out that the above actually only applies to a specific scenario, one which we will call underparameterized models. We can simplify this notion by just thinking of the case where the number of our parameters to estimate is less than or equal to the number of observations we have to work with. Nowadays though, it’s not uncommon to have what we’d call overparameterized models, such as random forests and neural networks, sometimes with even billions of parameters, far exceeding the data size. In this scenario, when we revisit the trade-off, something unusual happens!\n\n\n\nFigure from Dar, Muthukumar, and Baraniuk (2021)\n\n\nSuch models may have near zero training error, yet do well on unseen data. As we increase complexity, we see something like a second bias-variance trade-off beyond the point where the data is perfectly fit (interpolated). This point is where model complexity (e.g. in terms of number of parameters) p equals the number of observations N, and this is where the realm of the overparameterized models begins. Now test error begins to drop again with increasing complexity."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#an-example",
    "href": "posts/2021-10-30-double-descent/index.html#an-example",
    "title": "Double Descent",
    "section": "An example",
    "text": "An example\nI thought it would be amusing to try this with the classic mtcars data set available in base R. With this data, our goal will be to predict fuel consumption in miles per gallon (mpg). First we will split the data into training and test components. We create a model where our number of parameters (p), in this case standard regression coefficients, will equal the number of observations (N). Some of the more technically savvy will know that if the number of features and/or parameters to estimate p equals the number of observations N, a standard linear regression model will fit the data perfectly1, demonstrated below.\n\n\nIf not familiar, the mtcars object is a data frame that comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\nnc = ncol(mtcars) \nnr = nc\nfit_perfect = lm(mpg ~ ., mtcars[1:nr, ])\n# summary(fit_perfect) # not shown, all inferential estimates are NaN\n\n\n\n\n\n\n\n\n\n\nNow let’s look at the test error, our prediction on the unseen data we didn’t use in fitting the model. When we do, we see the usual bias-variance trade-off. Our generalizability capabilities have plummeted, as we have overfit the training data and were unable to accommodate unseen observations. We are even predicting negative mpg in some cases!\n\n\n\n\n\n\n\n\n\n\np ≤ N\nLet’s extend the demonstration more fully. We now create models of increasing complexity, starting with an intercept only model (i.e. just using the mean for prediction), to one where all other columns (10) in the data are predictors. Here I repeatedly sampled mtcars of size \\(N = 10\\) for training, the remainder for test, and also shuffled the columns each time, doing so for a total of 250 times2. Here is the result- the classic bias variance trade-off curve. The larger dot shows the test error minimum, at about 3 covariates (plus intercept). The vertical line denotes our point of interpolation.\n\n\n\nDouble Descent in the underparameterized setting.\n\n\n\n\np &gt; N\nSo with one of the simpler data sets around we were able to demonstrate the bias-variance trade-off clearly. But now let’s try overparameterized models! We don’t need anything fancy or complicated to do this, so for our purposes, I’m just going to add cubic spline basis expansions for the wt, disp, and hp features3. This will definitely be enough to put us in a situation where we have more parameters than data, i.e. p &gt; N, but doesn’t make things too abstract4.\nThe basic linear model approach we might typically use fails to estimate the additional parameters in this situation, so we need a different estimator. Some are familiar with penalized regression techniques such as lasso and ridge regression, and we could use those here. However, I’ll use ridgeless regression, as depicted in Hastie et al. (2019), and which, like ridge regression, is a straightforward variant of the usual least squares regression5. I estimate the coefficients/weights on the training data, and make predictions for the training and test set, calculating their respective errors. Here is an example of the primary function used.\n\nfit_ridgeless = function(X_train, y, X_test, y_test){\n  # get the coefficient estimates\n  b = pseudo_inv(crossprod(X_train)) %*% crossprod(X_train, y)\n  \n  # get training/test predictions\n  predictions_train = X_train %*% b\n  predictions_test  = X_test %*% b\n  \n  # get training/test error\n  rmse_train = sqrt(mean((y - predictions_train[,1])^2))\n  rmse_test  = sqrt(mean((y_test - predictions_test[,1])^2))\n  \n  # return result\n  list(\n    b = b,\n    predictions_train = predictions_train,\n    predictions_test  = predictions_test,\n    rmse_train = rmse_train,\n    rmse_test  = rmse_test\n  )\n}\n\nWe can test the function as follows with as little as 10 observations, where p (all predictor coefficients plus intercept = 11 parameters) is greater than N (10). This demonstrates that the ridgeless approach can provide an estimate for all the parameters (unlike the standard lm function), and we also see very low training error, but relatively high test error (in terms of the root mean square error.)\n\nn = 10\n\nX = as.matrix(cbind(1, mtcars[, -1]))\ny = mtcars$mpg # mpg is the first column\n\nX_train = X[1:n, ]\ny_train = mtcars$mpg[1:n]\nX_test  = X[-(1:n),]\ny_test  = y[-(1:n)]\n\nresult = fit_ridgeless(X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0.84\n\n\n−1.69\n\n\n0.08\n\n\n−0.08\n\n\n2.76\n\n\n−1.29\n\n\n0.24\n\n\n2.32\n\n\n3.26\n\n\n2.26\n\n\n0.66\n\n\n\n\n\n\n\n\n\n\n\n\n\nrmse_train\nrmse_test\n\n\n\n\n0.05\n5.79\n\n\n\n\n\n\n\nIf we do this for more complex models (max linear features, plus each additional set of features associated with a cubic spline basis expansions), we obtain the following. Now we see the second descent in test error takes form!\n\n\n\nDouble Descent in the overparameterized setting.\n\n\nPutting our results together gives us the double descent curve.\n\n\n\nDouble Descent in the overparameterized setting.\n\n\n\nNote that this all holds for the most part with classification problems, including multiclass (or multivariate/class targets).\n\nWe not only see the double descent pattern, but we can also note that the global test error minimum occurs with the model with the most parameters. The gray dot is the lowest test error with the underparameterized settings, while the dark red is the global test error minimum."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#why-does-this-happen",
    "href": "posts/2021-10-30-double-descent/index.html#why-does-this-happen",
    "title": "Double Descent",
    "section": "Why does this happen?",
    "text": "Why does this happen?\nUnderstanding the double descent phenomenon is an area of active research, and there are some technical issues we won’t cover here. However, we can note a couple things more broadly. When we’re in the underparameterized situation, we ultimately begin to force features that have no association with the target to fit the data anyway. Once you move beyond the point of where these features are useful, test error begins to rise again, until the point of interpolation where test error is even worse than guessing (or just guessing in the classification case).\nBeyond the interpolation point, all models we potentially employ using this estimation technique will have the capacity to fit the training data perfectly, i.e. zero bias. This allows us to fit the remaining noise in the data with the additional features employed by the more complex models. There is no guarantee that among the models you fit that the lowest test error will be found relative to the underparameterized setting. However, the lowest test error to be found is ‘out there’ somewhere6. So adding complexity will potentially allow you to find improved test error.\nAnother way to put it is that we have a single class of models to consider, and under and overparameterized are special cases of that more general class. Any one of these might result in the lowest test error. The overparameterized models, which may contain complex nonlinearities and interactions, are likely to be more compatible with the data than the simpler models7. So odds are good that at least one of them will have a smaller test error as well. In any case, restricting ourselves to the underparameterized setting is definitely no guarantee that we will find the most performant model.\nOne caveat is that the model we used is an example of ‘implicit’ regularization, one in which there is no hyper-parameter to set (or discover through cross-validation), like with ridge and lasso. With other techniques (e.g. optimally chosen ridge regression estimator) we may still be able to achieve optimal test error without complete interpolation, and show a reduced peak.\nDar, Muthukumar, and Baraniuk (2021) note that in the overparameterized setting, we can distinguish the signal part of the error term that reduces as a function of N/p, where the noise part of the error term is a function of p/N. In addition, there is a portion of test error related to model misspecification, which will always decrease with overparameterization. In addition, one must consider both feature correlations as well as correlations among observations. Having more complex covariance structure doesn’t negate the double descent phenomenon, but they suggest that, for example, cases where there is low effective dimension within these additional features will more readily display the double descent.\nAnother issue is that in any given situation it is difficult to know where in the realm of available models we exist presently. So additional complexity, or even additional data, may in fact hurt performance (Nakkiran et al. 2019)."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#conclusion",
    "href": "posts/2021-10-30-double-descent/index.html#conclusion",
    "title": "Double Descent",
    "section": "Conclusion",
    "text": "Conclusion\nThe double descent phenomenon is a quite surprising scenario, especially for those who have only heard of the classical bias-variance trade off. There is still much to learn regarding it, but such research is off and running. For practical purposes, it is worth keeping it in mind to aid us in model selection and thinking about our modeling strategies in general."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#footnotes",
    "href": "posts/2021-10-30-double-descent/index.html#footnotes",
    "title": "Double Descent",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR2 = 1 in the standard linear model setting.↩︎\nNote that the intercept term is added after data shuffling so when p = 1 it is the intercept only model, i.e. guessing the mean.↩︎\nI used mgcv to so this, then added them in whole for each term to the previously shuffled model matrix. These columns are not shuffled. By default these will add 10 columns each to the model matrix.↩︎\nFor more on generalized additive models, see my document.↩︎\nRidgeless regression has the same form as the ‘normal’ equations for least squares, but instead of \\(\\beta \\sim (X^TX)^{-1} \\cdot X^Ty\\), we have \\(\\beta \\sim (X^TX)^{+} \\cdot X^Ty\\) where the first part is the pseudo-inverse of \\(X\\). It is similar to equations for ridge regression (see my demo here) and can be seen as an approximation to it as the ridge penalty tends toward zero.↩︎\nFox Mulder told me so.↩︎\nBecause nature is just funny that way.↩︎"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html",
    "href": "posts/2022-07-25-programming/index.html",
    "title": "Programming Odds & Ends",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#introduction",
    "href": "posts/2022-07-25-programming/index.html#introduction",
    "title": "Programming Odds & Ends",
    "section": "Introduction",
    "text": "Introduction\nOftentimes I’m looking to gain speed/memory advantages, or maybe just exploring how to do the same thing differently in case it becomes useful later on. I thought I’d start posting them, but then I don’t get around to it. Here are a few that have come up over the past year (or two 😞), and even as I wrote this, more examples kept coming up, so I may come back to add more in the future."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#quick-summary",
    "href": "posts/2022-07-25-programming/index.html#quick-summary",
    "title": "Programming Odds & Ends",
    "section": "Quick summary",
    "text": "Quick summary\nData processing efficiency really depends on context. Faster doesn’t mean memory efficient, and what may be the best in a standard setting can be notably worse in others. Some approaches won’t show their value until the data is very large, or there are many groups to deal with, while others will get notably worse. Also, you may not want an additional package dependency beyond what you’re using, and may need a base R approach. The good news is you’ll always have options!\nOne caveat: I’m not saying that the following timed approaches are necessarily the best/fastest, I mostly stuck to ones I’d try first. You may find even better for your situation! A great resource to keep in mind is the fastverse, which is a collection of packages with speed and efficiency in mind, and includes a couple that are demonstrated here."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#setup-and-orientation",
    "href": "posts/2022-07-25-programming/index.html#setup-and-orientation",
    "title": "Programming Odds & Ends",
    "section": "Setup and orientation",
    "text": "Setup and orientation\nRequired packages.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(dtplyr)\nlibrary(tidyfast)\nlibrary(data.table)\nlibrary(collapse)\nlibrary(vctrs)\nlibrary(bench)   # for timings\n\nAlso, in the following bench marks I turn off checking if the results are equivalent (i.e. check = FALSE) because even if the resulting data is the same, the objects may be different classes, or some objects may even be of the same class, but have different attributes. You are welcome to double check that you would get the same thing as I did. Also, you might want to look at the autoplot of the bench mark summaries, as many results have some variability that isn’t captured by just looking at median/best times."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#fill-in-missing-values",
    "href": "posts/2022-07-25-programming/index.html#fill-in-missing-values",
    "title": "Programming Odds & Ends",
    "section": "Fill in missing values",
    "text": "Fill in missing values\nWe’ll start with the problem of filling in missing values by group. I’ve created a realistic example where the missingness is seen randomly across multiple columns, and differently across groups. I’ve chosen to compare tidyr, tidyfast, the underlying approach of tidyr via vctrs, and data.table.\nNote that only data.table is not last observation carried forward by default (‘down’ in tidyr parlance), so that argument is made explicit. All of these objects will have different attributes or classes. tidyfast for some reason renames the grouping variable to ‘by’. If you wrap all of these in data.frame, that will remove the attributes and give them all the same class, so you can verify they return the same result.\n\nset.seed(1234)\n\nN = 1e5\nNg = 5000\n\ncreate_missing &lt;- function(x) {\n  x[sample(1:length(x), 5)] = NA\n  x\n}\n\n\ndf_missing = tibble(grp = rep(1:Ng, e = N / Ng)) %&gt;%\n  arrange(grp) %&gt;%\n  group_by(grp) %&gt;%\n  mutate(\n    x = 1:n(),\n    y = rpois(n(), 5),\n    z = rnorm(n(), 5),\n    across(x:z, create_missing)\n  ) %&gt;% \n  ungroup()\n\ndf_missing %&gt;% head(5)\n\ndt_missing = as.data.table(df_missing) %&gt;% setkey(grp)\ntf_missing = copy(dt_missing)\n\nbm_fill &lt;-\n  bench::mark(\n    %!in%\n    tidyr    = fill(group_by(df_missing, grp), x:z),  \n    tidyfast = dt_fill(tf_missing, x, y, z, id = grp),\n    vctrs    = df_missing %&gt;% group_by(grp) %&gt;% mutate(across(x:z, vec_fill_missing)),\n    dt = dt_missing[\n      ,\n      .(x = nafill(x, type = 'locf'),\n        y = nafill(y, type = 'locf'),\n        z = nafill(z, type = 'locf')),\n      by = grp\n    ],\n    check = FALSE,\n    iterations = 10\n  )\n\nThis is a great example of where there is a notable speed/memory trade-off. Very surprising how much memory data.table uses1, while not giving much speed advantage relative to the tidyr. Perhaps there is something I’m missing (😏)? Also note that we can get an even ‘tidier’ advantage by using vctrs directly, rather than wrapping it via tidyr, and seeing how easy it is to use, it’s probably the best option.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ntidyfast\n18.3ms\n24.5ms\n39.87MB\n1\n6.06\n\n\nvctrs\n27ms\n28.5ms\n6.58MB\n1.16\n1\n\n\ndt\n111.7ms\n117.8ms\n237.08MB\n4.81\n36.04\n\n\ntidyr\n132.3ms\n147.2ms\n6.59MB\n6.01\n1"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#antijoins",
    "href": "posts/2022-07-25-programming/index.html#antijoins",
    "title": "Programming Odds & Ends",
    "section": "Antijoins",
    "text": "Antijoins\nSometimes you just don’t want that! In the following we have a situation where we want to filter values based on the negation of some condition. Think of a case where certain person IDs are not viable for consideration for analysis. Many times, a natural approach would be to use something like a filter where instead of using vals %in% values_desired, we just negate that with a bang (!) operator. However, another approach is to create a data frame of the undesired values and use an anti_join. When using joins in general, you get a relative advantage by explicitly noting the variables you’re joining on, so I compare that as well for demonstration. Finally, in this particular example we could use data.table’s built-in character match, chin.\n\nset.seed(123)\n\ndf1 = tibble(\n  id = sample(letters, 10000, replace = TRUE)\n)\n\ndf2 = tibble(\n  id = sample(letters[1:10], 10000, replace = TRUE)\n)\n\ndf1_lazy = lazy_dt(df1)\ndf2_lazy = lazy_dt(df2)\n\ndf1_dt = data.table(df1)\ndf2_dt = data.table(df2)\n\nsuppressMessages({\n  bm_antijoin = bench::mark(\n    in_     = filter(df1, !id %in% letters[1:10]),\n    in_dtp  = collect(filter(df1_lazy, !id %in% letters[1:10])),     # not usable until collected/as_tibbled\n    chin    = filter(df1, !id %chin% letters[1:10]),                 # chin for char vector only, from data.table\n    chin_dt = df1_dt[!df1_dt$id %chin% letters[1:10],],              \n    coll    = fsubset(df1, id %!in% letters[1:10]),                  # can work with dt or tidyverse\n    aj      = anti_join(df1, df2, by = 'id'),\n    aj_noby = anti_join(df1, df2),\n\n    iterations = 100,\n    check      = FALSE\n  )\n})\n\nIn this case, the fully data.table approach is best in speed and memory, but collapse is not close behind2. In addition, if you are in the tidyverse, the anti_join function is a very good option. Hopefully the lesson about explicitly setting the by argument is made clear.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nchin_dt\n152µs\n160µs\n206KB\n1\n1\n\n\ncoll\n167µs\n185µs\n268KB\n1.16\n1.3\n\n\naj\n529µs\n559µs\n293KB\n3.5\n1.42\n\n\nchin\n626µs\n662µs\n270KB\n4.15\n1.31\n\n\nin_\n716µs\n804µs\n387KB\n5.04\n1.88\n\n\nin_dtp\n923µs\n988µs\n479KB\n6.2\n2.33\n\n\naj_noby\n9.95ms\n10.962ms\n323KB\n68.72\n1.57"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#lagleaddifferences",
    "href": "posts/2022-07-25-programming/index.html#lagleaddifferences",
    "title": "Programming Odds & Ends",
    "section": "Lag/lead/differences",
    "text": "Lag/lead/differences\nHere we are interested in getting the difference in the current value of some feature from it’s last (or next) value, typically called a lag (lead). Note that it doesn’t have to be the last value, but that is most common. In the tidyverse we have lag/lead functions, or with data.table, we have the generic shift function that can do both. In the following I look at using that function in the fully data.table situation or within a tibble.\n\nset.seed(1234)\n\nN = 1e5\nNg = 5000\n\ndf  = tibble(\n  x = rpois(N, 10),\n  grp = rep(1:Ng, e = N / Ng)\n)\n\ndt = as.data.table(df)\n\nbm_lag = bench::mark(\n  dplyr_lag  = mutate(df, x_diff = x - lag(x)),\n  dplyr_lead = mutate(df, x_diff = x - lead(x)),\n  dt_lag     = dt[, x_diff := x - shift(x)],\n  dt_lead    = dt[, x_diff := x - shift(x, n = -1)],\n  dt_dp_lag  = mutate(df, x_diff = x - shift(x)),\n  dt_dp_lead = mutate(df, x_diff = x - shift(x, n = -1)),\n  coll_lag   = ftransform(df, x_diff = x - flag(x)),\n  coll_lead  = ftransform(df, x_diff = x - flag(x, n = -1)),\n  iterations = 100,\n  check      = FALSE\n)\n\nIn this case, collapse is best, with data.table not far behind, but using the shift function within the tidy approach is a very solid gain. Oddly, lag and lead seem somewhat different in terms of speed and memory.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_lag\n226µs\n260µs\n783.84KB\n1\n1\n\n\ncoll_lead\n250µs\n309µs\n783.84KB\n1.18\n1\n\n\ndt_lag\n316µs\n354µs\n813.87KB\n1.36\n1.04\n\n\ndt_lead\n319µs\n355µs\n813.87KB\n1.36\n1.04\n\n\ndt_dp_lag\n792µs\n818µs\n782.91KB\n3.14\n1\n\n\ndt_dp_lead\n797µs\n892µs\n782.91KB\n3.42\n1\n\n\ndplyr_lead\n994µs\n1.115ms\n1.53MB\n4.28\n2\n\n\ndplyr_lag\n1.044ms\n1.2ms\n1.15MB\n4.61\n1.5\n\n\n\n\n\n\n\nWhat about a grouped scenario? To keep it simple we’ll just look at using lagged values.\n\nbm_lag_grp = bench::mark(\n  dt_lag    = dt[, x_diff := x - shift(x), by = grp],\n  dt_dp_lag = mutate(group_by(df, grp), x_diff = x - shift(x)),\n  dplyr_lag = mutate(group_by(df, grp), x_diff = x - lag(x)),\n  coll_lag  = fmutate(fgroup_by(df, grp), x_diff = x - flag(x)),\n  \n  iterations = 10,\n  check      = FALSE\n)\n\nIn the grouped situation, using a collapse isn’t best for memory, but the speed gain is ridiculous!!\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_lag\n484µs\n548µs\n1.59MB\n1\n3.52\n\n\ndt_lag\n25.692ms\n28.067ms\n462.32KB\n51.2\n1\n\n\ndt_dp_lag\n30.843ms\n32.969ms\n2.61MB\n60.15\n5.77\n\n\ndplyr_lag\n77.156ms\n78.771ms\n5.19MB\n143.7\n11.49"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#firstlast",
    "href": "posts/2022-07-25-programming/index.html#firstlast",
    "title": "Programming Odds & Ends",
    "section": "First/Last",
    "text": "First/Last\nIn this demo, we want to take the first (last) value of each group. Surprisingly, for the same functionality, it turns out that the number of groups matter when doing groupwise operations. For the following I’ll even use a base R approach (though within dplyr’s mutate) to demonstrate some differences.\n\nset.seed(1234)\n\nN = 100000\n\ndf = tibble(\n  x  = rpois(N, 10),\n  id = sample(1:100, N, replace = TRUE)\n)\n\ndt = as.data.table(df)\n\nbm_first = bench::mark(\n  base_first  = summarize(group_by(df, id), x = x[1]),\n  base_last   = summarize(group_by(df, id), x = x[length(x)]),\n  dplyr_first = summarize(group_by(df, id), x = dplyr::first(x)),\n  dplyr_last  = summarize(group_by(df, id), x = dplyr::last(x)),\n  dt_first    = dt[, .(x = data.table::first(x)), by = id],\n  dt_last     = dt[, .(x = data.table::last(x)),  by = id],\n  coll_first  = ffirst(fgroup_by(df, id)),\n  coll_last   = flast(fgroup_by(df, id)),\n  \n  iterations  = 100,\n  check       = FALSE\n)\n\nThe first result is actually not too surprising, in that the fully dt approaches are fast and memory efficient, though collapse is notably faster. Somewhat interesting is that the base last is a bit faster than dplyr’s last (technically nth) approach.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_last\n307µs\n341µs\n783.53KB\n1\n1.72\n\n\ncoll_first\n299µs\n345µs\n783.53KB\n1.01\n1.72\n\n\ndt_last\n698µs\n876µs\n454.62KB\n2.57\n1\n\n\ndt_first\n689µs\n895µs\n454.62KB\n2.62\n1\n\n\nbase_last\n1.944ms\n2.036ms\n2.06MB\n5.97\n4.64\n\n\ndplyr_first\n2.026ms\n2.127ms\n2.06MB\n6.24\n4.64\n\n\nbase_first\n1.984ms\n2.165ms\n2.06MB\n6.35\n4.64\n\n\ndplyr_last\n2.111ms\n2.553ms\n2.06MB\n7.49\n4.64\n\n\n\n\n\n\n\nIn the following, the only thing that changes is the number of groups.\n\nset.seed(1234)\n\nN = 100000\n\ndf = tibble(\n  x = rpois(N, 10),\n  id = sample(1:(N/10), N, replace = TRUE) # &lt;--- change is here\n)\n\ndt = as.data.table(df)\n\nbm_first_more_groups = bench::mark(\n  base_first  = summarize(group_by(df, id), x = x[1]),\n  base_last   = summarize(group_by(df, id), x = x[length(x)]),\n  dplyr_first = summarize(group_by(df, id), x = dplyr::first(x)),\n  dplyr_last  = summarize(group_by(df, id), x = dplyr::last(x)),\n  dt_first    = dt[, .(x = data.table::first(x)), by = id],\n  dt_last     = dt[, .(x = data.table::last(x)),  by = id],\n  coll_first  = ffirst(group_by(df, id)),\n  coll_last   = flast(group_by(df, id)),\n  iterations  = 100,\n  check       = FALSE\n)\n\nNow what the heck is going on here? The base R approach is way faster than even data.table, while not using any more memory than what dplyr is doing (because of the group-by-summarize). More to the point is that collapse is notably faster than the other options, but still a bit heavy memory-wise relative to data.table.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_last\n2.813ms\n3.006ms\n2.79MB\n1\n3.8\n\n\ncoll_first\n2.838ms\n3.128ms\n2.79MB\n1.04\n3.8\n\n\nbase_first\n10.596ms\n11.4ms\n3.06MB\n3.79\n4.17\n\n\nbase_last\n12.551ms\n13.151ms\n3.06MB\n4.37\n4.17\n\n\ndt_last\n17.588ms\n18.318ms\n752.15KB\n6.09\n1\n\n\ndt_first\n17.671ms\n18.379ms\n752.15KB\n6.11\n1\n\n\ndplyr_first\n20.066ms\n20.943ms\n3.06MB\n6.97\n4.17\n\n\ndplyr_last\n20.916ms\n21.357ms\n3.16MB\n7.1\n4.31"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#coalesceifelse",
    "href": "posts/2022-07-25-programming/index.html#coalesceifelse",
    "title": "Programming Odds & Ends",
    "section": "Coalesce/ifelse",
    "text": "Coalesce/ifelse\nIt’s very often we want to change a single value based on some condition, often starting with ifelse. This is similar to our previous fill situation for missing values, but applies a constant as opposed to last/next value. Coalesce is similar to tidyr’s fill, and is often used in cases where we might otherwise use an ifelse style approach . In the following, we want to change NA values to zero, and there are many ways we might go about it.\n\nset.seed(1234)\nx = rnorm(1000)\nx[x &gt; 2] = NA\n\n\nbm_coalesce = bench::mark(\n  base      = {x[is.na(x)] &lt;- 0; x},\n  ifelse    = ifelse(is.na(x), 0, x),\n  if_else   = if_else(is.na(x), 0, x),\n  vctrs     = vec_assign(x, is.na(x), 0),\n  tidyr     = replace_na(x, 0),\n  fifelse   = fifelse(is.na(x), 0, x),\n  coalesce  = coalesce(x, 0),\n  fcoalesce = fcoalesce(x, 0),\n  nafill    = nafill(x, fill = 0),\n  coll      = replace_NA(x)   # default is 0\n)\n\nThe key result here to me is just how much memory the dplyr if_else approach is using, as well as how fast and memory efficient the base R approach is even with a second step. While providing type safety, if_else is both slow and a memory hog, so probably anything else is better. tidyr itself would be a good option here, and while it makes up for the memory issue, it’s relatively slow compared to other approaches, including the function it’s a wrapper for (vec_assign), which is also demonstrated. Interestingly, fcoalesce and fifelse would both be better options than data.table’s other approach that is explicitly for this task.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nfcoalesce\n1µs\n1µs\n7.86KB\n1\n1\n\n\ncoll\n1µs\n1µs\n7.86KB\n1.03\n1\n\n\nbase\n2µs\n3µs\n7.91KB\n2.17\n1.01\n\n\nfifelse\n3µs\n4µs\n11.81KB\n2.6\n1.5\n\n\nvctrs\n4µs\n4µs\n11.81KB\n3.09\n1.5\n\n\nnafill\n5µs\n6µs\n23.95KB\n4.14\n3.05\n\n\ntidyr\n9µs\n10µs\n11.81KB\n7.09\n1.5\n\n\ncoalesce\n15µs\n17µs\n20.19KB\n11.71\n2.57\n\n\nifelse\n15µs\n17µs\n47.31KB\n12.11\n6.02\n\n\nif_else\n21µs\n25µs\n71.06KB\n17.54\n9.04"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#conditional-slide",
    "href": "posts/2022-07-25-programming/index.html#conditional-slide",
    "title": "Programming Odds & Ends",
    "section": "Conditional Slide",
    "text": "Conditional Slide\nI recently had a problem where I wanted to do a apply a certain function that required taking the difference between the current and last value as we did in the lag demo. The problem was that ‘last’ depended on a specific condition being met. The basic idea is that we want to take x - lag(x) but where the condition is FALSE, we need to basically ignore that value for consideration as the last value, and only use the previous value for which the condition is TRUE. In the following, for the first two values where the condition is met, this is straightforward (6 minus 10). But for the fourth row, 4 should subtract 6, rather than 5, because the condition is FALSE.\n\nset.seed(1234)\n\ndf = tibble(\n  x = sample(1:10),\n  cond = c(TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE),\n  group = rep(c('a', 'b'), e = 5)\n)\n\ndf \n\n# A tibble: 10 × 3\n       x cond  group\n   &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;\n 1    10 TRUE  a    \n 2     6 TRUE  a    \n 3     5 FALSE a    \n 4     4 TRUE  a    \n 5     1 FALSE a    \n 6     8 TRUE  b    \n 7     2 FALSE b    \n 8     7 FALSE b    \n 9     9 TRUE  b    \n10     3 FALSE b    \n\n\nWhile somewhat simple in concept, it doesn’t really work with simple lags, as the answer would be wrong, or sliding functions, because the window is adaptive. I wrote the following function to deal with this. By default, it basically takes our vector under consideration, x, makes it NA where the condition doesn’t hold, then fills in the NA values with the last value using the vec_fill_missing (or a supplied constant/single value). However there is flexibility beyond that type of fill. In addition, the function applied is generic, and could be applied to the newly created variable (.x), or use both the original (x) and the newly created variable.\n\nconditional_slide &lt;-\n  function(x,\n           condition,\n           fun,\n           direction  = c(\"down\"),\n           fill_value = NA,\n           na_value   = NA,\n           ...) {\n    \n    if (!direction %in% c(\"constant\", \"down\", \"up\", \"downup\", \"updown\"))\n      rlang::abort('direction must be one of \"constant\", \"down\", \"up\", \"downup\", \"updown\"')\n    \n    if (length(x) != length(condition))\n      rlang::abort('condition and x must be the same length')\n    \n    # can't use dplyr/dt ifelse since we won't know class type of fill_value\n    conditional_val &lt;- ifelse(direction == 'constant', fill_value, NA)    \n    .x &lt;- ifelse(condition, x, conditional_val)\n    \n    if (direction != 'constant')\n      .x &lt;- vctrs::vec_fill_missing(.x, direction = direction)\n    \n    class(.x) &lt;- class(x)\n    \n    result &lt;- fun(x, .x, ...)\n    \n    if (!is.na(na_value))\n      result[is.na(result)] &lt;- na_value\n    \n    result\n  }\n\nThe first example applies the function, x - lag(x), to our dataset, and which in my case, I also wanted to apply within groups, which caused further problems for some of the available functions I thought would otherwise be applicable. I also show it for another type of problem, taking the cumulative sum, as well as just conditionally changing the values to zero.\n\ndf %&gt;%\n group_by(group) %&gt;%\n mutate(\n   # demo first difference\n   simple_diff = x - dplyr::lag(x),\n   cond_diff = conditional_slide(x, cond, fun = \\(x, .x) x - lag(.x), na_value = 0),\n   \n   # demo cumulative sum\n   simple_cumsum = cumsum(x),\n   cond_cumsum   = conditional_slide(\n     x,\n     cond,\n     fun = \\(x, .x) cumsum(.x),\n     direction = 'constant',\n     fill = 0\n   ),\n   \n   # demo fill last\n   simple_fill_last = vec_fill_missing(x),\n   cond_fill_last = conditional_slide(\n     x,\n     cond,\n     fun = \\(x, .x) .x,\n     direction = 'down'\n   )\n )\n\n# A tibble: 10 × 9\n# Groups:   group [2]\n       x cond  group simple_diff cond_diff simple_cumsum cond_cumsum\n   &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;       &lt;int&gt;     &lt;dbl&gt;         &lt;int&gt;       &lt;int&gt;\n 1    10 TRUE  a              NA         0            10          10\n 2     6 TRUE  a              -4        -4            16          16\n 3     5 FALSE a              -1        -1            21          16\n 4     4 TRUE  a              -1        -2            25          20\n 5     1 FALSE a              -3        -3            26          20\n 6     8 TRUE  b              NA         0             8           8\n 7     2 FALSE b              -6        -6            10           8\n 8     7 FALSE b               5        -1            17           8\n 9     9 TRUE  b               2         1            26          17\n10     3 FALSE b              -6        -6            29          17\n# ℹ 2 more variables: simple_fill_last &lt;int&gt;, cond_fill_last &lt;int&gt;\n\n\nThis is one of those things that comes up from time to time where trying to apply a standard tool likely won’t cut it. You may find similar situations where you need to modify what’s available and create some functionality tailored to your needs."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#take-the-first-true",
    "href": "posts/2022-07-25-programming/index.html#take-the-first-true",
    "title": "Programming Odds & Ends",
    "section": "Take the first TRUE",
    "text": "Take the first TRUE\nSometimes we want the first instance of a condition. For example, we might want the position or value of the first number &gt; than some value. We’ve already investigated using dplyr or data.table’s first, and I won’t do so again here except to say they are both notably slower and worse on memory here. We have a few approaches we might take in base R. Using which would be common, but there is also which.max, that, when applied to logical vectors, gives the position of the first TRUE (which.min gives the position of the first FALSE). In addition, there is the Position function, which I didn’t even know about until messing with this problem.\n\nset.seed(123)\n\nx = sort(rnorm(10000))\n\nmarker = 2\n\nbm_first_true_1 = bench::mark(\n  which     = which(x &gt; marker)[1],\n  which_max = which.max(x &gt; marker),\n  pos       = Position(\\(x) x &gt; marker, x)\n)\n\n\n# make it slightly more challenging\nx = sort(rnorm(1e6))\n\nmarker = 4 \n\nbm_first_true_2 = bench::mark(\n  which     = which(x &gt; marker)[1],\n  which_max = which.max(x &gt; marker),\n  pos       = Position(\\(x) x &gt; marker, x)\n)\n\nInterestingly Position provides the best memory performance, but is prohibitively slower. which.max is probably your best bet.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nwhich\n15µs\n20µs\n79.14KB\n1\n14.19\n\n\nwhich_max\n18µs\n20µs\n39.11KB\n1.01\n7.01\n\n\npos\n1.986ms\n2.158ms\n5.58KB\n109.67\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nwhich\n1.44ms\n1.69ms\n7.63MB\n1\n1400.58\n\n\nwhich_max\n1.79ms\n2ms\n3.81MB\n1.18\n700.29\n\n\npos\n213.55ms\n219.13ms\n5.58KB\n129.4\n1\n\n\n\n\n\n\n\nBut not so fast? The following makes the first case come very quickly, where Position blows the other options out of the water! I guess if you knew this was going to be the case you could take serious advantage.\n\nset.seed(123)\n\nx = sort(rnorm(100000), decreasing = TRUE)\n\nx[1:30] = 4\n\n\nbm_first_true_3 = bench::mark(\n  which     = which(x &lt; marker)[1],\n  which_max = which.max(x &lt; marker),\n  pos       = Position(\\(x) x &lt; marker, x) \n)\n\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\npos_rev\n10µs\n10µs\n5.58KB\n1\n1\n\n\nwhich_max_rev\n110µs\n130µs\n390.67KB\n13.04\n70.04\n\n\nwhich_rev\n160µs\n220µs\n1.14MB\n22.55\n210.09"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#group-by-filteringslicing",
    "href": "posts/2022-07-25-programming/index.html#group-by-filteringslicing",
    "title": "Programming Odds & Ends",
    "section": "Group by filtering/slicing",
    "text": "Group by filtering/slicing\nThe previous situation was the basis for this next demo where we utilize which.max. Here we want to filter in one scenario, such that if all values are zero, we drop them, and in the second, we want to only retain certain values based on a condition. In this latter case, the condition is that at least one non-zero has occurred, in which case we want to keep all of those values from that point on (even if they are zero).\nTo make things more clear, for the example data that follows, we want to drop group 1 entirely, the initial part of group 2, and retain all of group 3.\n\nlibrary(tidyverse)\n\nset.seed(12345)\n\ndf = tibble(\n  group = rep(1:3, e = 10),\n  value = c(\n    rep(0, 10),\n    c(rep(0, 3), sample(0:5, 7, replace = TRUE)), \n    sample(0:10, 10, replace = TRUE)\n  )\n)\n\n\ndf %&gt;% \n  group_by(group) %&gt;% \n  filter(!all(value == 0)) %&gt;% \n  slice(which.max(value &gt; 0):n())\n\n# A tibble: 17 × 2\n# Groups:   group [2]\n   group value\n   &lt;int&gt; &lt;dbl&gt;\n 1     2     5\n 2     2     2\n 3     2     1\n 4     2     3\n 5     2     1\n 6     2     4\n 7     2     2\n 8     3     7\n 9     3     1\n10     3     5\n11     3    10\n12     3     5\n13     3     6\n14     3     9\n15     3     0\n16     3     7\n17     3     6\n\n\nIn the above scenario, we take two steps to illustrate our desired outcome conceptually. Ideally though, we’d like one step, because it is just a general filtering. You might think maybe to change which.max to which and just slice, but this would remove all zeros, when we want to retain zeros after the point where at least some values are greater than zero. Using row_number was a way I thought to get around things.\n\ndf %&gt;% \n  group_by(group) %&gt;% \n  filter(!all(value == 0) & row_number() &gt;= which.max(value &gt; 0))\n\n\nbm_filter_slice = bench::mark(\n  orig = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0)) %&gt;% \n    slice(which.max(value &gt; 0):n()),\n  \n  new = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0) & row_number() &gt;= which.max(value &gt; 0))\n)\n\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\norig\n2ms\n2.2ms\n10.25KB\n1\n1.11\n\n\nnew\n6.1ms\n7.1ms\n9.21KB\n3.28\n1\n\n\n\n\n\n\n\nWell we got it to one operation, but now it takes longer and has no memory advantage. Are we on the wrong track? Let’s try with a realistically sized data set with a lot of groups.\n\nset.seed(1234)\n\nN = 100000\ng = 1:(N/4)\n\ndf = tibble(\n  group = rep(g, e = 4),\n  value = sample(0:5, size = N, replace = TRUE)\n)\n\nbm_filter_slice2 = bench::mark(\n  orig = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0)) %&gt;% \n    slice(which.max(value &gt; 0):n()),\n  \n  new = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0) & row_number() &gt;= which.max(value &gt; 0))\n)\n\nNow we have the reverse scenario. The single filter is notably faster and more memory efficient.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nnew\n208.6ms\n209.9ms\n12.9MB\n1\n1\n\n\norig\n949.8ms\n949.8ms\n28.3MB\n4.53\n2.19"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#tidy-timings",
    "href": "posts/2022-07-25-programming/index.html#tidy-timings",
    "title": "Programming Odds & Ends",
    "section": "Tidy timings",
    "text": "Tidy timings\n\nOverview\n\nThis tidy timings section comes from a notably old exploration I rediscovered (I think it was originally Jan 2020), but it looks like tidyfast still has some functionality beyond dtplyr, and it doesn’t hurt to revisit. I added a result for collapse. My original timings were on a nicely suped up pc, but the following are on a year and a half old macbook with an M1 processer, and were almost 2x faster.\n\nHere I take a look at some timings for data processing tasks. My reason for doing so is that dtplyr has recently arisen from the dead, and tidyfast has come on the scene, so I wanted a quick reference for myself and others to see how things stack up against data.table.\nSo we have the following:\n\nBase R: Just kidding. If you’re using base R approaches for this aggregate you will always be slower. Functions like aggregate, tapply and similar could be used in these demos, but I leave that as an exercise to the reader. I’ve done them, and it isn’t pretty.\ndplyr: standard data wrangling workhorse package\ntidyr: has some specific functionality not included in dplyr\ndata.table: another commonly used data processing package that purports to be faster and more memory efficient (usually but not always)\ntidyfast: can only do a few things, but does them quickly.\ncollapse: many replacements for base R functions.\n\n\n\nStandard grouped operation\nThe following demonstrates some timings from this post on stackoverflow. I reproduced it on my own machine based on 50 million observations. The grouped operations that are applied are just a sum and length on a vector. As this takes several seconds to do even once, I only do it one time.\n\nset.seed(123)\nn = 5e7\nk = 5e5\nx = runif(n)\ngrp = sample(k, n, TRUE)\n\ntiming_group_by_big = list()\n\n\n# dplyr\ntiming_group_by_big[[\"dplyr\"]] = system.time({\n    df = tibble(x, grp)\n    r.dplyr = summarise(group_by(df, grp), sum(x), n())\n})\n\n# dtplyr\ntiming_group_by_big[[\"dtplyr\"]] = system.time({\n    df = lazy_dt(tibble(x, grp))\n    r.dtplyr = df %&gt;% group_by(grp) %&gt;% summarise(sum(x), n()) %&gt;% collect()\n})\n\n# tidyfast\ntiming_group_by_big[[\"tidyfast\"]] = system.time({\n    dt = setnames(setDT(list(x, grp)), c(\"x\",\"grp\"))\n    r.tidyfast = dt_count(dt, grp)\n})\n\n# data.table\ntiming_group_by_big[[\"data.table\"]] = system.time({\n    dt = setnames(setDT(list(x, grp)), c(\"x\",\"grp\"))\n    r.data.table = dt[, .(sum(x), .N), grp]\n})\n\n# collapse\ntiming_group_by_big[[\"collapse\"]] = system.time({\n     df = tibble(x, grp)\n    r.data.table = fsummarise(fgroup_by(df, grp), x = fsum(x),  n = fnobs(x))\n})\n\ntiming_group_by_big = timing_group_by_big %&gt;% \n  do.call(rbind, .) %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column('package')\n\n\n\n\n\n\n\n\n\npackage\nelapsed\n\n\n\n\ndplyr\n7.03\n\n\ndtplyr\n1.30\n\n\ncollapse\n1.02\n\n\ndata.table\n0.67\n\n\ntidyfast\n0.47\n\n\n\n\n\n\n\nWe can see that all options are notable improvements on dplyr. tidyfast is a little optimistic, as it can count but does not appear to do a summary operation like means or sums.\n\n\nCount\nTo make things more evenly matched, we’ll just do a simple grouped count. In the following, I add a different option for dplyr if all we want are group sizes. In addition, you have to ‘collect’ the data for a dtplyr object, otherwise the resulting object is not actually a usable tibble, and we don’t want to count the timing until it actually performs the operation. You can do this with the collect function or as_tibble.\n\ndata(flights, package = 'nycflights13')\nhead(flights)\n\nflights_dtp = lazy_dt(flights)\n\nflights_dt = data.table(flights)\n\nbm_count_flights = bench::mark(\n  dplyr_base = count(flights, arr_time),\n  dtplyr     = collect(count(flights_dt, arr_time)),\n  tidyfast   = dt_count(flights_dt, arr_time),\n  data.table = flights_dt[, .(n = .N), by = arr_time],\n  iterations = 100,\n  check = FALSE\n)\n\nHere are the results. It’s important to note the memory as well as the time. The faster functions here are taking a bit more memory to do it. If dealing with very large data this could be more important if operations timings aren’t too different.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ndata.table\n2ms\n2.4ms\n9.07MB\n1\n1.5\n\n\ntidyfast\n2ms\n3.3ms\n9.05MB\n1.38\n1.49\n\n\ndplyr_gs\n3.7ms\n4ms\n6.06MB\n1.65\n1\n\n\ndtplyr\n3.5ms\n4.2ms\n9.06MB\n1.73\n1.5\n\n\ndplyr_base\n9.3ms\n10.5ms\n6.12MB\n4.31\n1.01\n\n\n\n\n\n\n\nJust for giggles I did the same in Python with a pandas DataFrame, and depending on how you go about it you could be notably slower than all these methods, or less than half the standard dplyr approach. Unfortunately I can’t reproduce it here3, but I did run it on the same machine using a df.groupby().size() approach to create the same type of data frame. Things get worse as you move to something not as simple, like summarizing with a custom function, even if that custom function is still simple arithmetic.\nA lot of folks that use Python primarily still think R is slow, but that is mostly just a sign that they don’t know how to effectively program with R for data science. I know folks who use Python more, but also use tidyverse, and I use R more but also use pandas quite a bit. It’s not really a debate - tidyverse is easier, less verbose, and generally faster relative to pandas, especially for more complicated operations. If you start using tools like data.table, then there is really no comparison for speed and efficiency. You can run the following for comparison.\n\nimport pandas as pd\n\n\n# flights = r.flights\nflights = pd.read_parquet('../../data/flights.parquet')\n\nflights.groupby(\"arr_time\", as_index=False).size()\n\n      arr_time  size\n0          1.0   201\n1          2.0   164\n2          3.0   174\n3          4.0   173\n4          5.0   206\n...        ...   ...\n1406    2356.0   202\n1407    2357.0   207\n1408    2358.0   189\n1409    2359.0   222\n1410    2400.0   150\n\n[1411 rows x 2 columns]\n\ndef test(): \n  flights.groupby(\"arr_time\", as_index=False).arr_time.count()\n \ntest()\n\n\nimport timeit\n\ntimeit.timeit() # see documentation\n\n0.004075458040460944\n\ntest_result = timeit.timeit(stmt=\"test()\", setup=\"from __main__ import test\", number = 100)\n\n# default result is in seconds for the total number of 100 runs\ntest_result/100*1000  # ms per run \n\n3.751741250162013"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#summary",
    "href": "posts/2022-07-25-programming/index.html#summary",
    "title": "Programming Odds & Ends",
    "section": "Summary",
    "text": "Summary\nProgramming is a challenge, and programming in a computationally efficient manner is even harder. Depending on your situation, you may need to switch tools or just write your own to come up with the best solution."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#footnotes",
    "href": "posts/2022-07-25-programming/index.html#footnotes",
    "title": "Programming Odds & Ends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ndata.table modifies in place, so it technically it doesn’t have anything to fill after the first run. As a comparison, I created new columns as the filled in values, and this made almost no speed/memory difference. I also tried copy(dt_missing)[...], which had a minor speed hit. I also tried using setkey first but that made no difference. Note also that data.table has setnafill, but this apparently has no grouping argument, so is not demonstrated.↩︎\nAs of this writing, I’m new to the collapse package, and so might be missing other uses that might be more efficient.↩︎\nThis is because reticulate still has issues with M1 out of the box, and even then getting it to work can be a pain.↩︎"
  },
  {
    "objectID": "posts/2023-03-misc/index.html",
    "href": "posts/2023-03-misc/index.html",
    "title": "Stuff Going On",
    "section": "",
    "text": "It’s been a bit so thought I’d force myself to post a couple things I’ve played around with, or that aren’t ready yet for a full post, or won’t be one."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "href": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "title": "Stuff Going On",
    "section": "Football players still don’t know penalty kick basics",
    "text": "Football players still don’t know penalty kick basics\nDid a quick and dirty Bayesian analysis to get posterior probabilities for location, controlling for various factors. As a side note, I won the office world cup challenge with a fancy model of which I will never reveal the details, but may or may not have included lots of guessing and luck."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#tabular-data-post",
    "href": "posts/2023-03-misc/index.html#tabular-data-post",
    "title": "Stuff Going On",
    "section": "Tabular data post",
    "text": "Tabular data post\nI finally did my first post at the Strong blog! It’s a high-level overview of tabular data and deep learning that summarizes some of my previous posts here and here."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#class-imbalance",
    "href": "posts/2023-03-misc/index.html#class-imbalance",
    "title": "Stuff Going On",
    "section": "Class Imbalance",
    "text": "Class Imbalance\nFor my next post at the Strong blog, Elizabeth Monroe and I are working on a similarly high-level overview of issues with class imbalance we’ve been coming across. I will probably provide even more details and simulation results in a post on this site eventually, but here is a preview plot showing (mis)calibration plots at varying degrees of imbalance and different sample sizes."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#two-years-at-strong",
    "href": "posts/2023-03-misc/index.html#two-years-at-strong",
    "title": "Stuff Going On",
    "section": "Two years at Strong",
    "text": "Two years at Strong\nHard to believe for me anyway, but I’ve been out of academia for two years now, after previously spending my professional lifetime there. Aside from some of the obvious differences, one of the more satisfying changes for me has been that the skills I’ve acquired are utilized on a daily basis, and something I need to continuously develop for the job. At Strong our clients want good results in a timely fashion, and though the results might be notably complex, they still need to be sufficiently interpretable as well as reproducible/production-ready. I also have come across more desire for causal explanations from clients, which might be surprising to what is typically assumed for academia vs. industry. Clients obviously require buy-in for what we do, but they ultimately defer to us for the expertise we provide.\nStrong Analytics was a great move for me, because they clearly value the strong academic background of its employees, but are practically minded, and focus on skills that allow one to be nimble enough to get the clients what they need. Just like I was in academia, I am surrounded by a diverse group of smart folks I respect a lot, and am happy to solve some tough problems with. I feel I’ve learned how to get things done in a more efficient manner, and do a better job of explaining what I’ve done to wider audience.\nAmong some things I miss with academia, one was working with faculty and grad students who were just starting with an idea, and continuing a relationship with them until ultimately getting to publication or a successful dissertation defense after a very long journey. Another was giving workshops regularly where you could help people with their initial baby steps into the large world of data science. In general, it was easy to feel personally invested in the individuals you were working with, and their successes felt like your own.\nHowever, in academia it was often a struggle to get buy-in for more complicated methods or new techniques, because the stakes were typically lower and people knew the minimum required to get them published, defended or whatever, and mostly just wanted help getting to that point. There’s nothing wrong with that necessarily, that’s just the practical reality, and a reflection of what’s valued in academia. Despite that, I can say I definitely had some good partnerships with people involved in challenging research that was very rewarding, and those projects made it generally very satisfying to work in academia.\nUltimately though, I’m happy to have made the jump. It’s a bit weird to me how much drama there is on this topic on twitter and elsewhere. It’s really not that big of a deal which route you go, and in the grand scheme of things, almost no one will care if you work in academia or industry but you. There are pros and cons to both, and people should just pick what will make them happier."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#coming-up",
    "href": "posts/2023-03-misc/index.html#coming-up",
    "title": "Stuff Going On",
    "section": "Coming up",
    "text": "Coming up\nWhenever I can get around to it, I’ll try and post on those class imbalance simulations mentioned above, conformal prediction, and some other fun stuff. Stay tuned!"
  },
  {
    "objectID": "posts/2025-news/index.html",
    "href": "posts/2025-news/index.html",
    "title": "Some News for the New Year",
    "section": "",
    "text": "Among many things that have happened recently, I’ve become a father to a beautiful baby girl, and written a book to come out this year. Fun stuff!\nElsewhere, my employer Strong Analytics merged with OneSix, which has gone well so far. It’s been great to expand our capabilities and personnel, and I’m excited to see what we can do in the future.\nNot so fun - I’m also attempting to migrate my distill site to quarto and using VS Code, which has been, at best, very difficult. I love using Quarto, and highly recommend it for most things (including if you never created a website before), but this particular aspect has not been straightforward. So, while my website will ultimately look slightly better, and hopefully be easier to maintain, the old content will be lacking for a while while I try to get things back in order. Stay tuned."
  },
  {
    "objectID": "posts/2025-news/index.html#update",
    "href": "posts/2025-news/index.html#update",
    "title": "Some News for the New Year",
    "section": "",
    "text": "Among many things that have happened recently, I’ve become a father to a beautiful baby girl, and written a book to come out this year. Fun stuff!\nElsewhere, my employer Strong Analytics merged with OneSix, which has gone well so far. It’s been great to expand our capabilities and personnel, and I’m excited to see what we can do in the future.\nNot so fun - I’m also attempting to migrate my distill site to quarto and using VS Code, which has been, at best, very difficult. I love using Quarto, and highly recommend it for most things (including if you never created a website before), but this particular aspect has not been straightforward. So, while my website will ultimately look slightly better, and hopefully be easier to maintain, the old content will be lacking for a while while I try to get things back in order. Stay tuned."
  },
  {
    "objectID": "posts/2025-news/index.html#goals",
    "href": "posts/2025-news/index.html#goals",
    "title": "Some News for the New Year",
    "section": "Goals",
    "text": "Goals\nFor this website in 2025, there are a couple posts I hope to do:\n\nA post on class imbalance Elizabeth and I intended for out work blog but which never was published.\nA post I had on conformal prediction that was likewise intended for the work blog.\nSomething new that is of interest\n\nI can also foresee more review based stuff, or just having my code be independent of the post. It’s only every few years that I update my site significantly, but I definitely get tired of trying to maintain this stuff when many things conspire against doing so. You can use specific environments, but then they will still likely unusable in the future if the package functionality or even the versions are no longer supported. I also don’t want a post two years from now to be beholden to a package’s current functionality. Caching would solve a lot of it, but doesn’t seem to respected when other aspects of the computing environment change."
  },
  {
    "objectID": "posts/2025-news/index.html#migration-issues",
    "href": "posts/2025-news/index.html#migration-issues",
    "title": "Some News for the New Year",
    "section": "Migration Issues",
    "text": "Migration Issues\nAfter a lot of effort, it looks like I finally got quarto to use an appropriate python environment (and I’ve given up on trying to get ‘post-specific’ environments to work at present). Then came the general publishing problems…\nIn the end I may lose a lot of the previous code content, since quarto doesn’t appear to respect the old web cached objects I had associated with prior posts (which included now defunct or notably modified packages). It also has to use a different directory output, which means I have update links along with rerunning old posts.\nIssues I came across in case it’s useful to others:\n\nhttps://github.com/quarto-dev/quarto-cli/issues/10276\nhttps://github.com/quarto-dev/quarto-cli/issues/5220\nDeployment error (had to ‘rerun all’ from github itself)\nDefault radian pointing to wrong python environment which would then automatically load that environment and ignore any other env setting.\nhttps://github.com/quarto-dev/quarto-cli/issues/9929 (I think this was because I was in the gh-pages branch and not the main branch)\n\nWhat my ultimate solution was:\nFor Python:\n\nThe only env I could get things to recognize was a conda env in a default location for conda envs. My preference for uv created env, and secondarily, standard py env would not be recognized.\n\nWould not recognize any env in project directories\n\nIn .Rprofile (not .Renviron, not _environment, which were not resepected) put Sys.setenv(RETICULATE_PYTHON = \"~/anaconda3/envs/m-clark-github-io/bin/python\") followed by library(reticulate).\nI feel like relatively very little is tested with Python for websites, and even less for mixing with R.\n\nFor R:\n\nQuarto does not adhere to the project directory for posts. So the post must be relative to the post file, rather than the project file, which hampers interactivity and/or requires using something like here::here() to get the project directory (which assumes RStudio, though should still work, and still won’t apply to Python).\n\nFor publishing:\n\nThe biggest issue was the inability to use the top-level directory as the output_dir as I had before.\nI also now have to change every post file from its previous name to ‘index.qmd’ within the date-named directory in order for previous links to work. I could add an alias to every one of the files and let them redirect, but I prefer the cleaner address, and it’s easier to rename the files collectively than to add aliases to every post.\nI had to discover that you can’t be in the gh-pages branch (which I’m still fuzzy as to the need of). It’s mentioned in the doc, but not stressed or highlighted at all.\n\nOnce I was able to get quarto to render the pages in the first place, it published pretty easily via quarto publish gh-pages. I then had an issue where when publishing it gave some 404 error. I noticed it had updated the gh-pages branch after I pushed a recent change to main, so I pushed that branch as well, then tried to publish and it worked. Honestly they need more documentation as to exactly what the workflow is, especially for a blog (and with actual code, rather than a 2 year old demo website with a title suggesting there is but doesn’t actually have code).\nSo it seems the workflow is something like this:\n\nMake change on main\nPush to main\nPublish\nCross fingers"
  }
]