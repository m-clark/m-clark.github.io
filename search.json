[
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "I used to give workshops regularly when I worked in academia, and I have kept the content here. Some were not so much workshops as talks without any expectation of hands-on exercises or similar, so may not be as useful without the in-person context. Some of these, especially programming specific ones, are likely too dated to be useful anymore. The modeling focused ones may still have relevant content however."
  },
  {
    "objectID": "workshops.html#last-efforts",
    "href": "workshops.html#last-efforts",
    "title": "Workshops",
    "section": "Last efforts",
    "text": "Last efforts\nThese were among the last workshops I gave:\n\nDistill for R Markdown\nExploratory Data Analysis Tools\nMixed Models with R\nMore Mixed Models\nPatchwork and gganimate\nLibrary Learning Analytics Workshop\nGetting More from RStudio\nLatent Variable Models\nGeneralized Additive Models\nMixed Models\n\n\nTexts\nThese are the texts that serve as the basis for the workshops.\n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Structural Equation Modeling\nThis document regards a recent workshop given on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The document should be useful to anyone interested in the techniques covered, though it is R-based, with special emphasis on the lavaan package. \n Easy Bayes with rstanarm and brms\nThis workshop provides an overview of the rstanarm and brms packages. Basic modeling syntax is provided, as well as diagnostic checking, model comparison (posterior predictive checks , WAIC/LOO ), and how to get more from the models (marginal effects , posterior probabilities posterior probabilities, etc.). \n Factor Analysis and Related Methods\nThis workshop will expose participants to a variety of related techniques that might fall under the heading of ‘factor analysis’, latent variable modeling, dimension reduction and similar, such as principal components analysis, factor analysis, and measurement models, with possible exposure to and demonstration of latent Dirichlet allocation, mixture models, item response theory, and others. Brief overviews with examples of the more common techniques will be provided. \n Introduction to R Markdown\nThis workshop will introduce participants to the basics of R Markdown. After an introduction to concepts related to reproducible programming and research, demonstrations of standard markdown as well as overviews of different formats will be provided, including exercises. This document has been superseded by Practical Data Science, and will no longer be updated. \n Text Analysis with R\nThis document covers a wide range of topics, including how to process text generally, and demonstrations of sentiment analysis, parts-of-speech tagging, and topic modeling. Exercises are provided for some topics. It has practically no relevance in the modern large language model era."
  },
  {
    "objectID": "workshops.html#been-awhile",
    "href": "workshops.html#been-awhile",
    "title": "Workshops",
    "section": "Been awhile…",
    "text": "Been awhile…\nThese haven’t been given recently and are increasingly out date, but some content may be useful.\n My God, it’s full of STARs! Using astrology to get more from your data.\nTalk on structured additive regression models, and generalized additive models in particular. \n Become a Bayesian in 10 Minutes\nThis document regards a talk aimed at giving an introduction Bayesian modeling in R via the Stan programming language. It doesn’t assume too much statistically or any prior Bayesian experience. For those with such experience, they can quickly work with the code or packages discussed. I post them here because they exist and provide a quick overview, but you’d get more from the more extensive document. \n Engaging the Web with R\nDocument regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach.  \n Ceci n’est pas une %&gt;%\nExploring your data with R. A workshop that introduces some newer modes of data wrangling within R, with an eye toward visualization. Focus on dplyr and magrittr packages. No longer available as the javascript the slides were based on kept producing vulnerabilities for my website. Nowadays, using pipes is standard anyway. \n Getting More from RStudio\nAn afternoon talk on how to use RStudio for more than just coding."
  },
  {
    "objectID": "posts/2024-05-20/index.html",
    "href": "posts/2024-05-20/index.html",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  },
  {
    "objectID": "posts/2024-05-20/index.html#book-in-progess",
    "href": "posts/2024-05-20/index.html#book-in-progess",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html",
    "href": "posts/2022-09-deep-linear-models/index.html",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: I attempted to update my website in 2025 which required rerunning these in a new env. Hopefully the output still makes sense.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#introduction",
    "href": "posts/2022-09-deep-linear-models/index.html#introduction",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: I attempted to update my website in 2025 which required rerunning these in a new env. Hopefully the output still makes sense.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "title": "Deep Linear Models",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s get the primary packages loaded first.\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport torch\n\nNext, we’ll use the well-known titanic dataset, and to start things off, we’ll need to get a sense of what we’re dealing with. The basic idea is that we’d like to predict survival based on key features like sex, age, ticket class and more.\n\n# non-kaggle-requiring url here: https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/data/dl-linear-regression/titanic/train.csv\ndf_titanic_train = pd.read_csv('../../data/dl-linear-regression/titanic/train.csv')\n# df_titanic_train\n\n\ndf_titanic_train.describe()\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "href": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "title": "Deep Linear Models",
    "section": "Initial Data Processing",
    "text": "Initial Data Processing\nThe data is not ready for modeling as is, so we’ll do some additional processing to get it ready. We’ll check out the missing values and replace them with modes3.\n\ndf_titanic_train.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\nmodes = df_titanic_train.mode().iloc[0]\n\ndf_titanic_train.fillna(modes, inplace = True)\n\ndf_titanic_train.describe(include = (np.number))\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]\n\n\nWith features, sometimes it is worthwhile to log transform data for potentially more efficient optimization search. Since we have zeros, we add 1 before taking the log.\n\ndf_titanic_train['Fare'].hist()\n\n\nNow the transformed data looks a little more manageable. More to the point, we won’t potentially have huge coefficients relative to other covariates because of the range of the data.\n\ndf_titanic_train['LogFare'] = np.log1p(df_titanic_train['Fare'])\n\n# df_titanic_train['LogFare'].hist()\n\n\nThe Pclass (passenger class) feature is actually categorical.\n\npclasses = sorted(df_titanic_train.Pclass.unique())\npclasses\n\n[np.int64(1), np.int64(2), np.int64(3)]\n\n\nHere are the other categorical features.\n\ndf_titanic_train.describe(include = [object])\n\n                           Name   Sex  Ticket    Cabin Embarked\ncount                       891   891     891      891      891\nunique                      891     2     681      147        3\ntop     Braund, Mr. Owen Harris  male  347082  B96 B98        S\nfreq                          1   577       7      691      646\n\n\nIn order to use categorical variables, they need to be changed to numbers4, so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use embeddings5, particularly for things that have lots of unique categories.\n\ndf_titanic_train = pd.get_dummies(df_titanic_train, columns = [\"Sex\", \"Pclass\", \"Embarked\"])\n\nLet’s take a look at our data now.\n\ndf_titanic_train.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\ndf_titanic_train.head()\n\n   PassengerId  Survived  ... Embarked_Q  Embarked_S\n0            1         0  ...      False        True\n1            2         1  ...      False       False\n2            3         1  ...      False        True\n3            4         1  ...      False        True\n4            5         0  ...      False        True\n\n[5 rows x 18 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "title": "Deep Linear Models",
    "section": "Getting Started with pytorch",
    "text": "Getting Started with pytorch\n\nSetup\nNow we are ready to prep things for specific use with pytorch. I will not use the same terminology as in Jeremy’s original post, so for us, target = ‘dependent variable’ and X is our feature matrix6. Both of these will be pytorch tensors, which for our purposes is just another word for an array of arbitrary size.\n\nfrom torch import tensor\ndevice = torch.device('cpu')\ntarget = tensor(df_titanic_train.Survived)\n\n\ndummies = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nall_features = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummies \n\nX = df_titanic_train[all_features].apply(pd.to_numeric).astype(float)\nX = tensor(X.values, dtype = torch.float)\n\nX.shape\n\ntorch.Size([891, 12])"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "href": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "title": "Deep Linear Models",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nWe have our feature matrix and target variable prepped. The next step is to map the features to the target by means of predicted values. In linear regression, we typically call the weights that produce the predictions coefficients, but in standard deep/machine learning terminology, they are usually called weights, or more generally, parameters. Here, we generate some random values between -.5 and .5 to get started7:.\n\ntorch.manual_seed(442)\n\n&lt;torch._C.Generator object at 0x156153330&gt;\n\nn_coeff = X.shape[1]\ncoeffs = torch.rand(n_coeff) - 0.5  # default would produce values from 0 to 1\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625])\n\n\nThe original post did a form of min-max scaling to the features, basically putting everything on a potentially [0, 1] scale. Here we’ll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.\n\n# vals,indices = X.max(dim=0)\n# X = X / vals\nX_means = X.mean(dim = 0, keepdim = True)\nX_sds   = X.std(dim = 0)\n\nX_sc = (X - X_means) / X_sds\n\n# X_sc.mean(dim = 0)  # all means = 0 \n# X_sc.std(dim = 0)   # all sd = 1\n\nAs noted in the original post and worth iterating here for our statistical modeling crowd, we don’t estimate an intercept for this model and keep all the dummy coded features. The following takes our coefficients, multiplies them by their respective feature, and sums them.\n\npreds = (X_sc * coeffs).sum(axis = 1)\npreds[:10]\n\ntensor([ 0.6000, -1.9341,  0.2080,  0.1723, -0.0032,  0.3088, -0.5066,  1.6219,\n         0.6990, -1.2584])\n\n\nWe can calculate our loss, the difference in our predictions versus the target values, in many ways. Here we get the mean squared error.\n\nloss = torch.square(preds - target).mean()\nloss\n\ntensor(1.3960)\n\n\nNow we’ll create functions that do the previous steps, and finally, give it a test run! In the original fastai formulation, they use mean absolute error for the loss, which actually is just the L1loss that is available in torch. For a change of pace, we’ll keep our mean squared error, which is sometimes called L2 loss (this will create different results from the original notebook). I create the option within the function for you to do either. Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.\n\ndef calc_preds(X, weights):\n    return((X * weights).sum(axis = 1))\n\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n    \n    # torch.abs(calc_preds(X, coeffs)-target).mean()  # original\n\n    if which == 'l2':\n      loss = torch.nn.MSELoss()\n    else: \n      loss = torch.nn.L1Loss()\n      \n    L = loss(preds, target.float())\n      \n    return(L)\n\ncalc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which = 'l1')\n\n(tensor(1.3960), tensor(0.8891))\n\n\n\nDoing a Gradient Descent Step\nWe can continue our journey onward to actually estimating the weights rather than specifying them directly, since we definitely don’t want to just keep guessing! This is an iterative process where we still start with an initial (random) guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps epochs, and getting our next guess requires calculating what’s called a gradient. Here are some resources for more detail:\n\nHow Does a Neural Net Really Work?: great intro by Jeremy Howard\nSome by-hand code using gradient descent for linear regression, R, Python: By yours truly\n\nIn any case, this is basic functionality within pytorch, and it will keep track of each step taken.\n\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\n\nloss = calc_loss(X_sc, coeffs, target)\nloss\n\ntensor(1.3960, grad_fn=&lt;MseLossBackward0&gt;)\n\n\nWe use backward to calculate the gradients and inspect them.\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-0.9311,  0.6245,  0.4957, -0.7423,  0.6008, -0.6008, -0.9158,  0.0938,\n         0.7127, -1.7183,  0.1715,  1.3974])\n\n\nEach time backward is called, the gradients are added to the previous values. We can see here that they’ve now doubled.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-1.8621,  1.2491,  0.9914, -1.4847,  1.2015, -1.2015, -1.8317,  0.1877,\n         1.4254, -3.4366,  0.3431,  2.7947])\n\n\nWhat we want instead is to set them back to zero after they are used for our estimation step. The following does this.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)     # sub subtracts in place\n    coeffs.grad.zero_()                # zeros out in place\n    print(calc_loss(X, coeffs, target))\n\ntensor([-0.1836, -0.0488,  0.0922, -0.0035, -0.4435, -0.1345,  0.7624,  0.2854,\n         0.0661,  0.0763,  0.1588, -0.0567], requires_grad=True)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\ntensor(37.9424)\n\n\n\n\nTraining the Linear Model\nWe typically would split our data into training and test. We can do so here, or keep this data as training and import test.csv for the test set. The latter is actually used for the Kaggle submission, but that’s not a goal here. We’ll use scikit-learn for the splitting.\n\nfrom sklearn.model_selection import train_test_split\n\n# test size .2 in keeping with fastai RandomSplitter default\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n  X_sc, \n  target.float(), \n  test_size = 0.2, \n  random_state = 808\n)\n  \n\nlen(train_x), len(valid_x) # might be one off of the original notebook\n\n(712, 179)\n\n\nAs before, we’ll create functions to help automate our steps:\n\none to initialize the weights\na function to update weights\none to do a full epoch (using weights to calculate loss, updating weights)\none to train the entire model (run multiple times/epochs)\n\nAs mentioned, the approach here is to create functions that are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each verbose value epoch (e.g. verbose = 10 means you’ll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (possibly not too big a deal depending on your IDE).\n\ndef init_weights(n_wts): \n    return (torch.rand(n_wts) - 0.5).requires_grad_()\n\ndef update_weights(weights, lr):\n    weights.sub_(weights.grad * lr)\n    weights.grad.zero_()\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\ndef train_model(X, target, epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1])\n    \n    for i in range(epochs): \n        one_epoch(X, coeffs, target, lr = lr, i = i, verbose = verbose)\n    return coeffs\n\nTry out the functions if you like (not shown).\n\ncalc_loss(X_sc, init_weights(X_sc.shape[1]), target).backward()\n\n\none_epoch(train_x, init_weights(train_x.shape[1]), train_y, .01)\n\nNow train the model for multiple epochs. The loss drops very quickly before becoming more steady.\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, verbose = 5, lr = .2)\n\n 1.375618\n  0.296216\n  0.284019\n  0.281221\n  0.280271\n  0.279923\n  0.279794\n  0.279746\n  0.279728\n  0.279721\n \n\n\nLet’s create a function to show our estimated parameters/weights/coefficients in a pretty fashion.\n\ndef show_coeffs(estimates): \n  coef_dict = dict(zip(all_features, estimates.requires_grad_(False).numpy()))\n  return pd.DataFrame(coef_dict, index = ['value']).T\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.090825\nSibSp      -0.054449\nParch      -0.016111\nLogFare     0.046320\nSex_male   -0.406538\nSex_female -0.171426\nPclass_1    0.408707\nPclass_2    0.335766\nPclass_3    0.329800\nEmbarked_C  0.057091\nEmbarked_Q  0.032813\nEmbarked_S  0.039464\n\n\n\n\nMeasuring Accuracy\nIt’s one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data. The following function will convert our estimates to a binary value like our target, and compares them to the target. Depending on how you did your training setup, it might be pretty bad or at least better than guessing.\n\ndef acc(X, weights, target): \n    return (target.bool() == (calc_preds(X, weights) &gt; 0.5)).float().mean()\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7051), tensor(0.6425))\n\n\n\n\nUsing sigmoid\nNothing about the previous setup actually produces a result on the probability scale, so basing a cutoff of .5 is meaningless. you can inspect them and might see values are above 1 or below zero, which we generally don’t want8. However we do have a solution. The sigmoid function9 allows us to transform our predictions to values between 0 and 1, i.e. probabilities in this context, and in particular, the probability of survival. Then our acc function will be more appropriate, where any probability &gt; .5 will be given a value of 1 (or True technically), while others will be 0/False.\n\ndef calc_preds(X, weights):\n    return torch.sigmoid((X*weights).sum(axis = 1))\n\nWe also will do more iterations, and fiddle with the learning rate (a.k.a. step size)\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  epochs = 500,\n  lr = 1,\n  verbose = 100\n)\n\n 0.314158\n  0.154329\n  0.154237\n  0.154232\n  0.154232\n \n\n\nNot too shabby!\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7823), tensor(0.7989))\n\n\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.516476\nSibSp      -0.423656\nParch      -0.179623\nLogFare     0.396468\nSex_male   -0.927410\nSex_female  0.349448\nPclass_1    0.713895\nPclass_2    0.320935\nPclass_3    0.078919\nEmbarked_C  0.107378\nEmbarked_Q  0.082943\nEmbarked_S -0.036137\n\n\nIn implementing the sigmoid, let’s go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster)10. To do this, the coefficients will need to be a column vector, so we change our init_coeffs function slightly11.\n\ndef calc_preds(X, weights): \n    return torch.sigmoid(X@weights)\n\ndef init_coeffs(n_wts): \n    return (torch.rand(n_wts, 1) * 0.1).requires_grad_()\n\nNow our functions are more like the mathematical notation we’d usually see for linear regression.\n\\[\\hat{y} = X\\beta\\]\n\n\nCompare to Linear/Logistic Regression\nBefore getting too excited, let’s compare our results to basic linear and logistic regression. The linear regression is more like our model before using the sigmoid transformation, while the logistic is more like when we used it. Depending on your settings, the logistic regression is probably doing better at this point.\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\n\n\nreg = linear_model.LinearRegression()\nreg.fit(train_x, train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\nacc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)\n\n(tensor(0.7989), tensor(0.7821))\n\n\n\nreg = linear_model.LogisticRegression()\nreg.fit(train_x, train_y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n0.7821229050279329\n\n\nIt looks like our coefficient estimates are similar to the logistic regression ones.\n\nshow_coeffs(coeffs_est).assign(logreg = np.round(reg.coef_.T, 4))\n\n               value  logreg\nAge        -0.516476 -0.4799\nSibSp      -0.423656 -0.4191\nParch      -0.179623 -0.1265\nLogFare     0.396468  0.3441\nSex_male   -0.927410 -0.6262\nSex_female  0.349448  0.6262\nPclass_1    0.713895  0.3941\nPclass_2    0.320935  0.0675\nPclass_3    0.078919 -0.3945\nEmbarked_C  0.107378  0.0546\nEmbarked_Q  0.082943  0.0655\nEmbarked_S -0.036137 -0.0890"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "href": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "title": "Deep Linear Models",
    "section": "A Neural Network",
    "text": "A Neural Network\n\nAt this point we’ve basically reproduced a general linear model. A neural network, on the other hand, has from one to many hidden layers of varying types in between input and output. Let’s say we have a single layer with two nodes. For a fully connected or dense network, we’d need weights to map our features to each node of the hidden layer (n_wts * n_hidden parameters total), and then another set of weights to map the hidden nodes to our next layer, which is our output, basically creating the predicted values. You can think of it as a second hidden layer with a single output node. With additional hidden nodes we add more complexity, but also flexibility, to the model. But this may come at a price, e.g. more difficulty with training due to the additional parameters that have to be estimated.\nSo basically we need matrices of weights, and the following function allows us to create those. We also add a bias/intercept/constant for the hidden-to-output processing. In the first layer, we divide the weights by n_hidden to create sums at the next layer that are of similar magnitude as the inputs. In general though, there are many ways to initialize weights.\n\ndef init_weights(n_wts, n_hidden = 20):\n    layer1 = (torch.rand(n_wts, n_hidden) - 0.5) / n_hidden # n_wts x n_hidden matrix of weights\n    layer2 = torch.rand(n_hidden, 1) - 0.3                  # n_hidden weights\n    const  = torch.rand(1)[0]                               # constant\n    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\nNow we revise our calc_preds function to incorporate all the weights. Initially, we extract the different sets of weights that are estimated by the model. For the original inputs, we multiply them by the layer 1 weights and sum. Then we apply a transformation to them to induce nonlinearity. Typical approaches are the sigmoid function we used before, hyperbolic tangent, and, probably the most common, the relu. The original notebook used relu, while I use a more recent one called Mish, which is a variant of relu. The hidden layer nodes then get multiplied by their respective weights and summed with the constant added. We then use our sigmoid function to get the probability scale as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(X, weights):\n    l1, l2, const = weights\n    res = F.mish(X@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res).flatten()\n\nWith additional sets of weights, we use an update loop.\n\ndef update_weights(weights, lr):\n    for layer in weights:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, lr = 1, verbose = 10)\n\n 0.325837\n  0.155810\n  0.141485\n  0.137652\n  0.136034\n \n\n\nAt this point we’re doing a little bit better in general, and even better than standard logistic regression on the test set!\n\nacc(train_x, coeffs_est, train_y), \\\nacc(valid_x, coeffs_est, valid_y), \\\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n(tensor(0.8160), tensor(0.8045), 0.7821229050279329)"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "href": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "title": "Deep Linear Models",
    "section": "Deep Learning",
    "text": "Deep Learning\nWe previously used a single hidden layer, but we want to go deeper! That’s the whole point of deep learning right? The following modifies our previous functions to allow for an arbitrary number of layers. You’ll note there are some hacks to get the weights in a good way for each layer12, but you normally wouldn’t have to do that on your own, since most tools will provide sensible modifications.\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\n# change loss to binary\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n\n    loss = torch.nn.BCELoss()\n\n    L = loss(preds, target.float())\n\n    return(L)\n\n\ndef init_weights(n_wts, hiddens):  \n    sizes = [n_wts] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]\n    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers, consts\n\n\ndef calc_preds(X, weights):\n    layers, consts = weights\n    n = len(layers)\n    res = X\n    \n    for i, l in enumerate(layers):\n        res = res@l + consts[i]\n    \n    if i != n-1: \n      res = F.mish(res)\n      \n    \n    return torch.sigmoid(res).flatten()\n\ndef update_weights(weights, lr):\n    layers, consts = weights\n    for layer in layers + consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1], hiddens)\n    \n    for i in range(epochs): \n        if verbose != 0:\n            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)\n    \n    return coeffs\n\nWith everything set up, let’s do some deep linear modeling! You can play around with the number of hidden layers, number of nodes and other settings. Feel free to explore!\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  hiddens = [500, 250, 100],\n  epochs  = 500,\n  lr      = 1e-4,\n  verbose = 10\n)\n\n 5.123790\n  0.666971\n  0.653124\n  0.640325\n  0.628476\n  0.617496\n  0.607313\n  0.597861\n  0.589081\n  0.580918\n  0.573322\n  0.566249\n  0.559658\n  0.553510\n  0.547772\n  0.542413\n  0.537403\n  0.532715\n  0.528326\n  0.524212\n  0.520354\n  0.516733\n  0.513330\n  0.510130\n  0.507118\n  0.504281\n  0.501605\n  0.499080\n  0.496695\n  0.494439\n  0.492305\n  0.490283\n  0.488366\n  0.486547\n  0.484820\n  0.483178\n  0.481616\n  0.480129\n  0.478712\n  0.477361\n  0.476072\n  0.474840\n  0.473663\n  0.472538\n  0.471461\n  0.470429\n  0.469440\n  0.468493\n  0.467583\n  0.466710\n \n\n\nHooray! Our best model yet (at least tied).\n\npd.DataFrame({\n    'acc_train': acc(train_x, coeffs_est, train_y).flatten(), \n    'acc_test': acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm': accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int))\n}, index = ['value'])\n\n       acc_train  acc_test  acc_test_glm\nvalue    0.77809  0.804469      0.782123"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "href": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "title": "Deep Linear Models",
    "section": "The Elephant in the Room",
    "text": "The Elephant in the Room\nAs noted in my previous posts [1, 2], probably your biggest challenge in implementing a deep learning model for tabular data, one with mixed data types and other complexities, is beating an off the shelf boosting model. Here is a quick demo with lightgbm.\n\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(\n  # n_estimators = 500,  # the sorts of parameters you can play with (many more!)\n  # max_depth    = 4,\n  # reg_alpha    = .1\n)\n\nmodel.fit(train_x, train_y)\n\nLGBMClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifieriFittedLGBMClassifier() \n\nmodel.score(valid_x, valid_y.numpy())\n\n0.8491620111731844\n\n\n\n# sklearn example\n# from sklearn.ensemble import HistGradientBoostingClassifier\n# \n# res = HistGradientBoostingClassifier().fit(train_x.numpy(), train_y.numpy())\n# \n# res.score(valid_x.numpy(), valid_y.numpy())\n\nNo tuning at all, and we’re already doing significantly better. Granted, if you use a packaged DL model for tabular data like the one in fastai, you should be doing better than our little demo. Even then though, you may still find the boosting results tough to beat.\n\ndf_accs = pd.DataFrame({ \n    'acc_test_dl':   acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm':  accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int)),\n    'acc_test_lgbm': model.score(valid_x, valid_y.numpy())\n}, index = ['value']).round(4)\n\ndf_accs\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue       0.8045        0.7821         0.8492\n\n\n\ndf_perc_improvement = 100 * (df_accs / df_accs.iloc[0,1] - 1)  # % improvement\ndf_perc_improvement\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue     2.864075           0.0       8.579466"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#summary",
    "href": "posts/2022-09-deep-linear-models/index.html#summary",
    "title": "Deep Linear Models",
    "section": "Summary",
    "text": "Summary\nThis was a lot of work to do slightly better than a logistic regression! However, there is a lot going on with a typical DL model that would likely prove even better. But it also serves as a reminder to have a suitable baseline, and as we saw with the lightgbm model, it can take little effort to do very well without deep learning. Hopefully though, the peek behind the scenes to do some ‘deep’ linear modeling can make it more accessible for you."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "href": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "title": "Deep Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI won’t actually use fastai, since they aren’t up to supporting M1/2 Macs very well. I think it was only used for the train/test data split anyway. I would rant a bit about this, but a lot of fastai is geared toward non-local computing, and the fault is really with Apple and NVidia as near as I can tell.↩︎\nI’m also not going to go into broadcasting, submitting to Kaggle, and other things that I don’t think are necessary for our purposes here.↩︎\nJust as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here, and sometimes that’s enough.↩︎\nEven though every modeling tool requires this, strangely very few in the Python world offer options for automatic handling of such things, but it’s getting better.↩︎\nWe actually aren’t too far removed from this in our model coming up, the main difference is that we don’t treat the categorical feature part of the model separately.↩︎\nI’ll not perpetuate calling features/predictor variables that are clearly not independent as independent. That nomenclature really only works for randomized experiments, and that is definitely not the case here.↩︎\nYou could use torch.randn to get standard normal values, and often times we’ll even start with just zeros if we really are just doing a standard linear model.↩︎\nUnless you are an economist, in which case you call it a linear probability model and ignore the ridiculous predictions because you have very fine standard errors.↩︎\nA lot of R folks seem unaware that the base R plogis function accomplishes this.↩︎\nThe @ operator is essentially the dot product, so x@y is np.dot(x, y)↩︎\nThe fastai demo also changes the target to a column vector, but this doesn’t seem necessary.↩︎\nAnd they probably aren’t as good for the changes I’ve made.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models Demystified",
    "section": "",
    "text": "Some News for the New Year\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nLong time no see…\n\n\n\n\n\n\nmiscellaneous\n\n\n\nNew modeling book under way! \n\n\n\n\n\nMay 20, 2024\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nStuff Going On\n\n\n\n\n\n\nmiscellaneous\n\n\n\nPenalty kicks, class imbalance, tabular deep learning, industry and academia \n\n\n\n\n\nMar 10, 2023\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Linear Models\n\n\n\n\n\n\ndeep learning\n\n\nboosting\n\n\nGLM\n\n\nregression\n\n\nmachine learning\n\n\n\nA demonstration using pytorch \n\n\n\n\n\nOct 10, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning for Tabular Data\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nA continuing exploration \n\n\n\n\n\nMay 1, 2022\n\n\nMichael Clark\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Models {Demystified}},\n  url = {https://m-clark.github.io/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Models Demystified.” n.d. https://m-clark.github.io/."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "A lot of the following are things I do for fun or personal interest and development. Here you will find a summary of stuff that I’ve done in the past, but for my latest efforts, check me out on GitHub. Almost all of my work nowadays is for clients and internal use that is not publicly available."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Code",
    "section": "R Packages",
    "text": "R Packages\nI haven’t had time to do much with these anymore, but they are still available GitHub and still mostly functional.\n mixedup\n A package for extracting results from mixed models from several packages that are easy to use and viable for presentation.  \n confusionMatrix\n Given predictions and a target variable, this package provides a wealth of summary statistics that can be calculated from a single confusion matrix, and return tidy results with as few dependencies as possible.  \n 198R\n R with its collar flipped, or the movie Drive if it was all about R programming, writing R code on a beach in Miami as the sun sets, R wearing sunglasses at night, R asking you to take it home tonight because it doesn’t want to let you go until you see the light, Countach &gt; Testarrosa, but Delorean &gt; all except R, R if Automan had lasted longer than 1 season, driving down Mulholland Dr. at night thinking about R code, R playing a cello at the end of a dock on a lake before taking a ride in a badass helicopter, R with its hair all done up with Aquanet… You get the idea.  \n visibly\n This is a collection of functions that I use related to visualization, e.g. the palette generating function (create_palette) and clean visualization themes for ggplot and plotly. In addition, there are visualizations specific to mixed and additive models.  \n 538 football club rankings\n This package grabs the table located at 538, and additionally does some summary by league and country.  \n gammit\n The package provides a set of functions to aid using mgcv (possibly solely) for mixed models. Mostly superseded by mixedup.  \n tidyext\n This package is a collection of functions that do the things I commonly need to do with data while doing other processing within the dataverse. Most of the functionality is now standard in tidyverse, so this is essentially deprecated.  \n lazerhawk\n While the name is more or less explanatory, to clarify, this is a package of miscellaneous functions that were mostly useful to me. Now deprecated.  \nIn addition to these, though they are not publicly available, I’ve created even more involved packages for specific project work."
  },
  {
    "objectID": "code.html#code-snippets",
    "href": "code.html#code-snippets",
    "title": "Code",
    "section": "Code Snippets",
    "text": "Code Snippets\nThe vast majority of these code snippets are conceptual demonstrations of more complicated models. The audience was generally faculty, researchers, and graduate students in applied fields who, like I did, want to go beyond their basic statistical training. However, I hope it helps anyone who happens to stumble across it. I don’t really update this page anymore, as I’ve cleaned and moved much of these over to Model Estimation by Example, so I would look for something you see here in the corresponding chapter of that document. In general, you can find all of my code at GitHub.\n\n\nModel Fitting\nstandard linear regression, standard logistic regression, penalized regression, lasso regression, ridge regression, newton and IRLS, nelder-mead (Python) (R), gradient descent (stochastic), bivariate probit, heckman selection, tobit, naive bayes, multinomial regression, ordinal regression, quantile regression, hurdle poisson, hurdle negbin, zero-inflated poisson, zero-inflated negbin, Cox survival, confirmatory factor analysis, Markov model, hidden Markov model (R) (Python), stochastic volatility, extreme learning machine, Chinese restaurant process, Indian buffet process, One-line models (an exercise), …\n\nMixed models\none factor random effects (R) (Julia) (Matlab), two factor random effects (R) (Julia) (Matlab), mixed model via ML, mixed model, mixed model with correlated random effects, See the documents section for more…\n\n\nBayesian\nBEST t-test, linear regression (Compare with BUGS version, JAGS), mixed model, mixed model with correlated random effects, beta regression, mixed model with beta response (Stan) (JAGS), mixture model, topic model, multinomial models, multilevel mediation, variational bayes regression, gaussian process, horseshoe prior, item response theory, …\n\n\nEM\nEM mixture univariate, EM mixture multivariate, EM probit, EM pca, EM probabilistic pca, EM state space model\n\n\nWiggly\n\nGaussian processses\nGaussian Process noisy, Gaussian Process noise-free, reproducing kernel hilbert space regression, Bayesian Gaussian process, …\n\n\nAdditive models\ncubic spline, …\n\n\n\n\nProgramming Shenanigans\nThis is old stuff I was doing while learning programming languages. Fun at the time, but mostly useless.\nFizzBuzz test (R) (julia) (Python), Reverse a string recursively (R) (Python), Recursive Word Wrap (R) (Python), calculate compound interest recursively, get US Congress roll call data, Scrape xkcd (R) (Python), Shakespearean Insulter, spurious correlation with ratios, R matrix speedups, …"
  },
  {
    "objectID": "code.html#shiny-apps",
    "href": "code.html#shiny-apps",
    "title": "Code",
    "section": "Shiny Apps",
    "text": "Shiny Apps\nFun at the time, these were some my forays into the Shiny world.\n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n Historical Football Data\nMy annual dive into the frustration of Shiny has produced an app to explore historical football/soccer data for various European leagues (Premier, La Liga, Serie A etc.) and MLS. One can create tables for a given country/league and year selected, with some leagues having multiple tiers available, and stretching back many decades. Beyond that, one can get a specific team’s historical finishing position, league games for a specific season, all-time tables, and all-time head-to-head results (within a league).  \n Last Statements of the Texas Executed\nA demonstration of both text analysis and literate programming/document generation with a dynamic and interactive research document. The texts regard the last statements of offenders in Texas. Sadly no longer functional, as the shiny environment appears to not have been preserved. It was actually a very nifty demonstration for its time.  \n A History of Tornados\nBecause I had too much time on my hands and wanted to try out the dashboard feature of R Markdown. Maps tornado activity from 1950-2015. (Archived)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What I do\nI am currently a Senior Machine Learning Scientist for OneSix, where I engage with client projects across multiple industries, helping them get the most from their data to maximize customer satisfaction, increase profitability, and explore new data-enabled territory. In my role, I strive to use the best tools to translate complex results into actionable insights.\nThroughout my career in data science, I have navigated a vast landscape of modeling, visualizing, and understanding data. I have conducted causal inference for marketing campaigns, classified biomedical images to detect pathology, analyzed text to uncover political sentiment, and explored baboon survival rates based on their social status. My experience spans dozens of industries and academic disciplines, helping clients and researchers unlock the full potential of their data.\nAdditionally, I have a strong background in teaching, writing, and conducting workshops on these topics, empowering others to gain expertise and become self-sufficient. Recently, I authored a book - Models Demystified - which offers a comprehensive overview of the statistical and machine learning landscape, along with related topics. It will be available from CRC Press in 2025.\nI am passionate about doing quality work that answers the questions at hand. What drew me to the world of data science and keeps my interest is that it frees me to engage in many different domains, and it provides a great many tools with which to discover more about the things we humans are most interested in. My CV can be found here.\n\n\nPersonal\nI was born and raised in Texas, but have lived a good chunk of my life in the Midwest. I currently live in Ann Arbor with my wife, our daughter, and our dog Sulu. We tend to keep things simple, and like to go on walks and hikes in the neighborhood and surrounding areas of A2, take the occasional trip to some place new, and make big deals out of the little things.\n\n\nAcademic Background\nWhile my interest is in data science generally, I started off majoring in psychology and philosophy as an undergraduate, and eventually obtained a Ph.D. in Experimental Psychology. During graduate school, I became interested in statistics for practical reasons, eventually choosing it as a concentration, and I also started consulting at that time. That turned out to be a good fit for me, and I’ve been exploring and analyzing data ever since.\n\n\n\n\n\n\nMichael Clark\n\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "Content",
    "section": "",
    "text": "Here you’ll find documents of varying technical degree covering things of interest to me, or which I think will be interesting to those I engage with. Generally you’ll find a mix of demonstrations on statistical and machine learning topics, programming, and data processing and visualization. Most focus on application in R as that’s what I used to primarily program with, but you’ll find plenty of Python demonstrations as well."
  },
  {
    "objectID": "documents.html#book",
    "href": "documents.html#book",
    "title": "Content",
    "section": "Book",
    "text": "Book\n Models Demystified\nThis book should be out on CRC Press in 2025. It is a comprehensive overview of the statistical and machine learning landscape, along with related topics. It is designed to be accessible to those with a basic understanding of statistics, but also to provide a deeper dive into the concepts for those with more experience. It covers an array of useful models from simple linear regression to deep learning. The book is designed to be a reference for those who want to understand the models and techniques they are using, and to provide a guide for those who want to learn new techniques."
  },
  {
    "objectID": "documents.html#long-form",
    "href": "documents.html#long-form",
    "title": "Content",
    "section": "Long Form",
    "text": "Long Form\n Mixed Models with R\nThis document focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R. It covers topics such as loss functions, cross-validation, regularization, and bias-variance trade-off, techniques such as penalized regression, random forests, and neural nets, and more.  \n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves, IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc."
  },
  {
    "objectID": "documents.html#blog-posts",
    "href": "documents.html#blog-posts",
    "title": "Content",
    "section": "Blog Posts",
    "text": "Blog Posts\n\nDeep Linear Models\nExploring Time\nProgramming Odds and Ends\nDeep Learning for Tabular Data II\nThe Double Descent Phenomenon\nDeep Learning for Tabular Data\nPractical Bayesian Analysis (I, II)\nMicro-macro Models\nPredictions with an Offset\nFactor Analysis and Related Methods\nConvergence Problems in Mixed Models\nCategorical Random Effects\nMixed Models for Big Data\nFractional Regression\nGroup Comparisons in SEM\n\nEmpirical Bayes\nShrinkage in Mixed Models\n\nMediation Models"
  },
  {
    "objectID": "documents.html#statistical",
    "href": "documents.html#statistical",
    "title": "Content",
    "section": "Statistical",
    "text": "Statistical\n\nModels By Example\n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n\n\nModeling in R\n Data Modeling in R\nThis document demonstrates a wide array of statistical and other models in R. Generic code is provided for standard regression, mixed, additive, survival, and latent variable models, principal components, factor analysis, SEM, cluster analysis, time series, spatial models, zero-altered models, text analysis, Bayesian analysis, machine learning and more. \nThe document is designed for newcomers to R, whether in a statistical sense, or just a programming one. It also should appeal to those working in other packages who are curious how to do the same sorts of things in R.\n\n\nBayesian\n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n MCMC algorithms\nList of MCMC algorithms with brief descriptions.  \n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n\n\nMixed Models\n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Mixed Models Overview\nAn overview that introduces mixed models for those with varying technical/statistical backgrounds.  \n Mixed Models Introduction\nA non-technical document to introduce mixed models for those who have used ANOVA.  \n Clustered Data Situations\nA comparison of standard models, cluster robust standard errors, fixed effect models, mixed models (random effects models), generalized estimating equations (GEE), and latent growth curve models for dealing with clustered data (e.g. longitudinal, hierarchical etc.).  \n Mixed Model Estimation\nDemonstration of mixed models via maximum likelihood and link to additive models. \n Mixed and Growth Curve Models\nA comparison of the mixed model vs. latent variable approach for longitudinal data (growth curve models), with simulation of performance in situations of small sample sizes. \n\n\nLatent Variables/SEM\n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The initial workshop was given to an audience of varying background and statistical skill, but the document should be useful to anyone interested in the techniques covered. It is completely R-based, with special emphasis on the lavaan package. It will continue to be a work in progress, particularly the sections after the SEM chapter. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves. Topics I hope to provide overviews of in the future include other latent variable techniques/extensions such as IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc.  \n Factor Analysis and Related Methods\nThis document gives a brief overview of many matrix factorization, dimension reduction, and latent variable techniques. Here is a list:  \n\nPrincipal Components Analysis - Factor Analysis - Probabilistic Components Analysis - Non-negative Matrix Factorization - Latent Dirichlet Allocation - Structural Equation Modeling - Item Response Theory - Independent Components Analysis - Multidimensional Scaling - t-Distributed Stochastic Neighbor Embedding (t-sne) - Recommender Systems - Hidden Markov Models - Random Effects Models - Bayesian Approaches - Mixture Models - k-means Cluster Analysis - Hierarchical Cluster Analysis - Latent Class Analysis\n\n Latent Variables, Sum Scores, Single Items\nIt is very common to use sum scores of several variables as a single entity to be used in subsequent analysis (e.g. a regression model). Some may even more use a single variable even though multiple indicators are available. Assuming the multiple measures indicate a latent construct, such typical practice would be problematic relative to using estimated factor scores, either constructed as part of a two-stage process or as part of a structural equation model. This document covers simulations in which comparisons in performance are made between latent variable and sum score or single item approaches.  \n Lord’s Paradox\nSummary of Pearl’s 2014 and 2013 technical reports on some modeling situations such as Lord’s Paradox and Simpson’s Paradox that lead to surprising results that are initially at odds with our intuition. Looks particularly at the issue of change scores vs. controlling for baseline.  \n\n\nOther Statistical\n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R.  \n Reliability\nAn unfinished document that ties together some ideas regarding the statistical and conceptual notion of reliability..  \n Fractional Regression\nA quick primer regarding data between zero and one, including zero and one.  \n Categorical Regression Models\nAn overview of regression models for binary, multinomial, and ordinal outcomes, with connections among various types of models.  \n Topic Modeling Demo\nA demonstration of Latent Dirichlet Allocation for topic modeling in R.  \n Comparing Measures of Dependency\nA summary of articles that look at various measures of dependency Pearson’s r, Spearman’s rho, and Hoeffding’s D, and newer ones such as Distance Correlation and Maximal Information Coefficient."
  },
  {
    "objectID": "documents.html#programming",
    "href": "documents.html#programming",
    "title": "Content",
    "section": "Programming",
    "text": "Programming\nCheck the workshops section also for programming-related content.\nPractical Data Science (more details about this document below). The intention was to cover five key topics: basic information processing, programming, modeling, visualization, and publication/presentation.\nExploratory Data Analysis Tools An overview of various packages useful for quick exploration of data.\n FastR\nA notebook on how to make R faster before or irrespective of the machinery used. Topics include avoiding loops, vectorization, faster I/O etc.  \n Engaging the Web with R\nDocument regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach."
  },
  {
    "objectID": "documents.html#workshops",
    "href": "documents.html#workshops",
    "title": "Content",
    "section": "Workshops",
    "text": "Workshops\nI used to give workshops regularly when I worked in academia. Although they generally won’t age well, I have kept the content here for any that might be interested."
  },
  {
    "objectID": "documents.html#miscellaneous",
    "href": "documents.html#miscellaneous",
    "title": "Content",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n R for Social Science\nThis was put together in a couple of days under duress, and is put here in case someone can find it useful (and thus make the time spent on it not completely wasted)."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "title": "Deep Learning for Tabular Data",
    "section": "TLDR: the meta-analysis",
    "text": "TLDR: the meta-analysis\nI collected most of the results from the summarized articles here and those covered in the previous post to see if we come to any general conclusions about which methods are best or work best in certain settings. In the following tables, I excluded those I knew to be image data, as well as datasets where I thought results were indistinguishable across all models tested (e.g. less than 1% difference in accuracy). This left comparisons for 92 datasets across six articles. However, it’s important to note that these were not independent datasets or studies. For example, Gorishniy et al. are the source of two papers and essentially the same testing situations, and other datasets were common across papers (e.g. Higgs Boson). In the rare situations there was a tie, I gave the nod to boosting methods as a. the whole point is to do better than those, b. they are the easier model to implement, and c. they are not always given the same advantages in these studies (e.g. pre-processing).\n\nFeature Type\nThe following shows results by feature type.\n\nHeterogeneous: at least 10% of categorical or numeric data with the rest of the other\nMinimal combo: means any feature inclusion of a different type. In the second table I collapse to ‘any heterogeneous’.\nBoost: Any boosting method (most of the time it’s XGBoost but could include lightGBM or other variant)\nMLP: multilayer perceptron or some variant\nDL_complex: A DL method more complex than MLP and which is typically the focus of the paper\n\nThe results suggest that current DL approaches’ strength is mostly with purely numeric data, and for heterogeneous data, simpler MLP or Boosting will generally prevail. I initially thought that boosting would do even better with heterogeneous data, and I still suspect that with more heterogeneous data and on more equal footing, results would tilt even more.\n\n\n\n\nTable 1: Feature Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nHeterogeneous\nMin. Combo\n\n\n\n\nBoost\n2\n10\n14\n6\n\n\nMLP\n2\n4\n9\n11\n\n\nDL_complex\n0\n22\n7\n5\n\n\n\n\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nAny Combo\n\n\n\n\nBoost\n2\n10\n20\n\n\nMLP\n2\n4\n20\n\n\nDL_complex\n0\n22\n12\n\n\n\n\n\n\n\n\n\n\n\n\nSample/Feature Set Size\nThe following suggests that complex DL methods are going to require a lot of data to perform better. This isn’t that surprising but the difference here is quite dramatic. Interestingly, MLP methods worked well for fewer features. N total in this case means total size reported (not just training).\n\n\n\n\nTable 2: Sample Size\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nN features\nN total\n\n\n\n\nBoost\n209\n133,309\n\n\nDL_complex\n207\n530,976\n\n\nMLP\n114\n114,164\n\n\n\n\n\n\n\n\n\n\n\n\nTarget Type\nIn the following we compare binary (bin), multiclass (mc), and numeric (num) target results1, but there’s no strong conclusion for this. The main thing to glean from this is that these papers do not test numeric targets nearly enough. Across dozens of disciplines and countless datasets that I’ve come across in various settings, if anything, this ratio should be reversed.\n\n\n\n\nTable 3: Target Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nbin\nmc\nnum\n\n\n\n\nBoost\n17\n10\n5\n\n\nDL_complex\n17\n11\n6\n\n\nMLP\n10\n14\n2\n\n\n\n\n\n\n\n\n\n\n\n\nCombinations\nIn the following I look at any heterogeneous, smaller data (N &lt; 200,000). A complex DL model will likely not do great in this setting.\n\n\n\n\nTable 4: Combinations\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nn\n\n\n\n\nBoost\n19\n\n\nDL_complex\n8\n\n\nMLP\n19\n\n\n\n\n\n\n\n\n\n\nNow, on to the details of some of the recent results that were included."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "On Embeddings for Numerical Features in Tabular Deep Learning",
    "text": "On Embeddings for Numerical Features in Tabular Deep Learning\n\nAuthors: Gorishniy, Rubachev, & Babenko\nYear: 2022\nArxiv Link\n\n\nOverview\nYura Gorishniy, Rubachev, and Babenko (2022) pit several architectures against one another, such as standard multilayer perceptron (MLP), ResNet, and their own transformer approach (see Yuri Gorishniy et al. (2021)). Their previous work, which was summarized in my earlier post, was focused on the architecture, while here they focus on embedding approaches. The primary idea is to take the value of some feature and expand it to some embedding space, then use the embedding in lieu of the raw feature. It can essentially be seen as a pre-processing task.\nOne approach they use is piecewise linear encoding (PLE), which they at one point describe as ‘a continuous alternative to the one-hot encoding’2. Another embedding they use is basically a fourier transform.\n\n\nData\n\n12 public datasets mostly from previous works on tabular DL and Kaggle competitions.\nSizes were from ~10K to &gt;1M.\nTarget variables were binary, multiclass, or numeric.\nThe number of features ranged from 8 to 200.\n\n9 of 12 data sets had only numeric features, two had a single categorical feature, and unfortunately, only one of these might be called truly heterogeneous, i.e., with a notable mix of categorical and numeric features3.\n\n\n\nModels Explored\n\nCatBoost\nXGBoost\nMLP, MLP*\nResNet, ResNet*\nTransformer*\n\n* Using proposed embeddings\n\n\nQuick Summary\n\nA mix of results with no clear/obvious winners (results are less distinguishable if one keeps to the actual precision of the performance metrics, and even less so if talking about statistical differences in performance).\n\nSeveral datasets showed no practical difference across any model (e.g. all accuracy results within ~.01 of each other).\n\nEmbedding-based approaches generally tend to improve over their non-embedding counter parts (e.g. MLP + embedding &gt; MLP), this was possibly the clearest result of the paper.\nI’m not sure we could say the same for ResNet, where results were similar with or without embedding\nXGBoost was best on the one truly heterogeneous dataset.\n\n\n\nIn general this was an interesting paper, and I liked the simple embedding approaches used. It was nice to see that they may be useful in some contexts. The fourier transform is something that analysts (including our team at Strong) have used in boosting, so I’m a bit curious why they don’t do Boosting + embeddings for comparison for that or both embedding types. These embeddings can be seen as a pre-processing step, so nothing would keep someone from using them for any model.\nAnother interesting aspect was how little difference there was in model performance. It seemed half the datasets showed extremely small differences between any model type."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "title": "Deep Learning for Tabular Data",
    "section": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training",
    "text": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training\n\nAuthors: Somepalli, Goldblum, Schwarzschild, Bayan-Bruss, & Goldstein\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper applies BERT-style attention over rows and columns, along with embedding/data augmentation. They distinguish the standard attention over features, with intersample attention of rows. In addition, they use CutMix for data augmentation (originally devised for images), which basically combines pairs of observations to create a new observation4. Their model is called SAINT, the Self-Attention and Intersample Attention Transformer.\n\n\nData\n\n16 data sets\nAll classification, 2 multiclass\n6 are heterogeneous, 2 notably so\nSizes 200 to almost 500K\n\n\n\nModels Explored\n\nLogistic Regression (!)\nRandom Forest\nBoosting\n\nCatBoost\nXGBoost\nLightGBM\n\nMLP\nTabNet\nVIME\nTabTransformer\nSAINT\n\n\n\nQuick Summary\n\nIt seems the SAINT does quite well on some of the data, and average AUROC across all datasets is higher than XGB.\nMain table shows only 9 datasets though, which they call ‘representative’ but it’s not clear what that means when you only have 16 to start. One dataset showed near perfect classification for all models so will not be considered. Of the 15 total remaining:\n\nSAINT wins 10 (including 3 heterogeneous)\nBoosting wins 5 (including 2 heterogeneous)\n\nSAINT benefits from data augmentation. This could have been applied to any of the other models, but doesn’t appear to have been done.\nAt least they also used some form of logistic regression as a baseline, though I couldn’t find details on its implementation (e.g. regularization, including interactions). I don’t think this sort of simple baseline is utilized enough.\n\nThis is an interesting result, but somewhat dampened by lack of including numeric targets and more heterogeneous data. The authors include small data settings which is great, and are careful to not generalize despite some good results, which I can appreciate.\nI really like the fact they also compare a simple logistic regression to these models, because if you’re not able to perform notably better relative to the simplest model one could do, then why would we care? The fact that logistic regression is at times competitive and even beats boosting/SAINT methods occasionally gives me pause though. Perhaps some of these data are not sufficiently complex to be useful in distinguishing these methods? It is realistic though. While it’s best not to assume as such, sometimes a linear model is appropriate given the features and target at hand."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
    "text": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning\n\nAuthors: Kossen, Band, Lyle, Gomez, Rainforth, & Gal\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper introduces Non-Parametric Transformers, which focus on holistic processing of multiple inputs, and attempts to consider an entire dataset as input as opposed to a single row. Their model attempts to learn relations between data points to aid prediction. They use a mask to identify prediction points from the non-masked data, i.e. the entire \\(X_{\\textrm{not masked}}\\text{ }\\) data used to predict \\(X_{\\textrm{masked}}\\text{ }\\). The X matrix actually includes the target (also masked vs. not). At prediction, the model is able to make use of the correlations of inputs of training to ultimately make a prediction.\n\n\nData\n\n10 datasets from UCI, 2 are image (CIFAR MNIST)\n4 binary, 2 multiclass, 4 numeric targets\n\n\n\nModels Explored\n\nNPT\nBoosting\n\nGB\nXGB\nCatBoost\nLightGBM\n\nRandom Forest\nTabNet\nKnn\n\n\n\nQuick Summary\n\nGood performance of these models, but not too different from best boosting model for any type of data.\n\nNPT best on binary classification, but similar to CatBoost\nSame as XGB and similar to MLP on multiclass\nBoosting slightly better on numeric targets, but NPT similar\n\nAs seen several times now, TabNet continues to underperform\nk-nn regression worst (not surprising)\n\nWhen I first read the abstract where they say “We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input.”, I immediately was like ‘What about this, that, and those?’. The key phrase was ‘deep learning’, because the authors note later that this has a very long history in the statistical modeling realm. I was glad to see in their background of the research that they explicitly noted the models that came to my mind, like gaussian processes, kernel regression, etc. Beyond that, many are familiar with techniques like knn-regression and predictive mean matching, so it’s definitely not new to consider more than a single data point for prediction. I thought it was good of them to add k-nn regression to the model mix, even though it was not going to do well compared to the other approaches.\nThough the author’s acknowledge a clear thread/history here, I’m not sure this result is the fundamental shift they claim, versus a further extension/expansion into the DL domain. Even techniques that may work on a single input at a time may ultimately be taking advantage of correlations among the inputs (e.g. spatial correlations in images). Also, automatic learning of feature interactions is standard even in basic regularized regression settings, but here their focus is on observation interactions (but see k-nn regression)."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "title": "Deep Learning for Tabular Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn the two reviews on DL for tabular data that I’ve done, it appears there is more work in store for DL methods applied to tabular data. While it’d be nice to have any technique that would substantially improve prediction for such settings, I do have a suspicion results are likely rosier than they are, since that is just about the case for any newly touted technique, and at least in some cases, I don’t think we’re even making apple to apple comparisons.\nThat said, I do feel like some ground has been made for DL applications for tabular data, in that architectures can now more consistently performing as well as boosting methods in certain settings, especially if we include MLP. In the end though, results don’t appear strong enough to warrant a switch from boosting for truly heterogeneous data, or even tabular data in general. I feel like someday we’ll maybe have a breakthrough, but in the meantime, we can just agree that messy data is hard stuff to model, and the best tool is whichever one works for your specific situation."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "title": "Deep Learning for Tabular Data",
    "section": "Guidelines for future research",
    "text": "Guidelines for future research\nI was thinking about what would be a convincing result, the type of setting and setup where if a DL technique was consistently performing statistically better than boosting methods, I’d be impressed. So I’ve made a list of things I’d like to see more of, and which would make for a better story if the DL method were to beat out other techniques.\n\nAlways use heterogeneous data. For giggles let’s say 20%+ of the minority feature type.\nFeatures should at least be minimally correlated, if not notably so.\nImage data results are not interesting (why would we use boosting on this in practice?).\nNumeric targets should at least be as much of focus as categorical targets.\nInclude ‘small’ datasets.\nInclude very structured data (e.g. clustered with repeated observations, geographical points, time series).\nUse a flexible generalized additive or similar penalized regression with interactions as a baseline statistical model.\nMaybe add survival targets to the mix.\nIf using a pre-processing step that is done outside of modeling, this likely should be applied to non-DL methods for better comparison, especially, if we’re only considering predictive accuracy and don’t care too much about interpretation.\nNote your model variants before analyzing any data. Tweaking/torturing model architecture after results don’t pan out is akin to p-hacking in the statistical realm, and likewise wastes both researcher and reader’s time.\nRegarding results…\n\nDon’t claim differences that you don’t have precision to do so, or at least back them up with an actual statistical test.\nIf margin of error in the metrics is overlapping, while statistically they could be different, practically they probably aren’t to most readers. Don’t make a big deal about it.\nIt is unlikely anyone will be interested in three decimal place differences for rmse/acc type metrics, and statistically, results often don’t even support two decimal precision.\nReport how you are obtaining uncertainty in any error estimates.\nIf straightforward, try to give an estimate of total tuning/run times.\n\nWith the datasets\n\nName datasets exactly how they are named at the source you obtained them from, provide direct links\nProvide a breakdown for both feature and target types\nProvide clear delineation of total/training/validation/test sizes"
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "title": "Deep Learning for Tabular Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t refer to numeric targets as ‘regression’ because that’s silly for so many reasons. 😄↩︎\nA quick look suggests it’s not too dissimilar from a b-spline.↩︎\nI’ll let you go ahead and make your own prediction about which method was best on that data set.↩︎\nIt’s not clear to me how well this CutUp approach would actually preserve feature correlations. My gut tells me the feature correlations of this approach would be reduced relative to the observed, since the variability of the new observations is likely reduced. This ultimately may not matter for predictive purposes or their ultimate use in embeddings. However, I wonder if something like SMOTE, random (bootstrap) sampling, other DL methods like autoencoders, or similar approaches might do the same or better.↩︎"
  },
  {
    "objectID": "posts/2023-03-misc/index.html",
    "href": "posts/2023-03-misc/index.html",
    "title": "Stuff Going On",
    "section": "",
    "text": "It’s been a bit so thought I’d force myself to post a couple things I’ve played around with, or that aren’t ready yet for a full post, or won’t be one."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "href": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "title": "Stuff Going On",
    "section": "Football players still don’t know penalty kick basics",
    "text": "Football players still don’t know penalty kick basics\nDid a quick and dirty Bayesian analysis to get posterior probabilities for location, controlling for various factors. As a side note, I won the office world cup challenge with a fancy model of which I will never reveal the details, but may or may not have included lots of guessing and luck."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#tabular-data-post",
    "href": "posts/2023-03-misc/index.html#tabular-data-post",
    "title": "Stuff Going On",
    "section": "Tabular data post",
    "text": "Tabular data post\nI finally did my first post at the Strong blog! It’s a high-level overview of tabular data and deep learning that summarizes some of my previous posts here and here."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#class-imbalance",
    "href": "posts/2023-03-misc/index.html#class-imbalance",
    "title": "Stuff Going On",
    "section": "Class Imbalance",
    "text": "Class Imbalance\nFor my next post at the Strong blog, Elizabeth Monroe and I are working on a similarly high-level overview of issues with class imbalance we’ve been coming across. I will probably provide even more details and simulation results in a post on this site eventually, but here is a preview plot showing (mis)calibration plots at varying degrees of imbalance and different sample sizes."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#two-years-at-strong",
    "href": "posts/2023-03-misc/index.html#two-years-at-strong",
    "title": "Stuff Going On",
    "section": "Two years at Strong",
    "text": "Two years at Strong\nHard to believe for me anyway, but I’ve been out of academia for two years now, after previously spending my professional lifetime there. Aside from some of the obvious differences, one of the more satisfying changes for me has been that the skills I’ve acquired are utilized on a daily basis, and something I need to continuously develop for the job. At Strong our clients want good results in a timely fashion, and though the results might be notably complex, they still need to be sufficiently interpretable as well as reproducible/production-ready. I also have come across more desire for causal explanations from clients, which might be surprising to what is typically assumed for academia vs. industry. Clients obviously require buy-in for what we do, but they ultimately defer to us for the expertise we provide.\nStrong Analytics was a great move for me, because they clearly value the strong academic background of its employees, but are practically minded, and focus on skills that allow one to be nimble enough to get the clients what they need. Just like I was in academia, I am surrounded by a diverse group of smart folks I respect a lot, and am happy to solve some tough problems with. I feel I’ve learned how to get things done in a more efficient manner, and do a better job of explaining what I’ve done to wider audience.\nAmong some things I miss with academia, one was working with faculty and grad students who were just starting with an idea, and continuing a relationship with them until ultimately getting to publication or a successful dissertation defense after a very long journey. Another was giving workshops regularly where you could help people with their initial baby steps into the large world of data science. In general, it was easy to feel personally invested in the individuals you were working with, and their successes felt like your own.\nHowever, in academia it was often a struggle to get buy-in for more complicated methods or new techniques, because the stakes were typically lower and people knew the minimum required to get them published, defended or whatever, and mostly just wanted help getting to that point. There’s nothing wrong with that necessarily, that’s just the practical reality, and a reflection of what’s valued in academia. Despite that, I can say I definitely had some good partnerships with people involved in challenging research that was very rewarding, and those projects made it generally very satisfying to work in academia.\nUltimately though, I’m happy to have made the jump. It’s a bit weird to me how much drama there is on this topic on twitter and elsewhere. It’s really not that big of a deal which route you go, and in the grand scheme of things, almost no one will care if you work in academia or industry but you. There are pros and cons to both, and people should just pick what will make them happier."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#coming-up",
    "href": "posts/2023-03-misc/index.html#coming-up",
    "title": "Stuff Going On",
    "section": "Coming up",
    "text": "Coming up\nWhenever I can get around to it, I’ll try and post on those class imbalance simulations mentioned above, conformal prediction, and some other fun stuff. Stay tuned!"
  },
  {
    "objectID": "posts/2025-news/index.html",
    "href": "posts/2025-news/index.html",
    "title": "Some News for the New Year",
    "section": "",
    "text": "Among many things that have happened recently, I’ve had a daughter, and written a book to come out this year. Fun stuff!\nNot so fun - I’m also attempting to migrate my distill site to quarto and using VS Code, which has been, at best, a dumpster fire. After a lot of effort, it looks like I finally got quarto to know how to use the appropriate python environment (and I’ve given up on trying to get post-specific environments to work). Then came the publishing problems…\nIn the end I will likely lose a lot of the code content since quarto doesn’t appear to respect the old web cached objects I had associated with prior posts (which included now defunct or notably modified packages), it also has to use a different directory output, which means I have update links along with rerunning old posts.\nSo, while my website will ultimately look slightly better, the old content will be lacking for a while while I try to redo them. Stay tuned.\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{clark2025,\n  author = {Clark, Michael},\n  title = {Some {News} for the {New} {Year}},\n  date = {2025-01-01},\n  url = {https://m-clark.github.io/posts/2025-news/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nClark, Michael. 2025. “Some News for the New Year.” January\n1, 2025. https://m-clark.github.io/posts/2025-news/."
  }
]