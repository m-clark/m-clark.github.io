[
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "I used to give workshops regularly when I worked in academia, and I have kept the content here. Some were not so much workshops as talks without any expectation of hands-on exercises or similar, so may not be as useful without the in-person context. Some of these, especially programming specific ones, are likely too dated to be useful anymore. The modeling focused ones may still have relevant content however."
  },
  {
    "objectID": "workshops.html#last-efforts",
    "href": "workshops.html#last-efforts",
    "title": "Workshops",
    "section": "Last efforts",
    "text": "Last efforts\nThese were among the last workshops I gave:\n\nDistill for R Markdown\nExploratory Data Analysis Tools\nMixed Models with R\nMore Mixed Models\nPatchwork and gganimate\nLibrary Learning Analytics Workshop\nGetting More from RStudio\nLatent Variable Models\nGeneralized Additive Models\nMixed Models\n\n\nTexts\nThese are the texts that serve as the basis for the workshops.\n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Structural Equation Modeling\nThis document regards a recent workshop given on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The document should be useful to anyone interested in the techniques covered, though it is R-based, with special emphasis on the lavaan package. \n Easy Bayes with rstanarm and brms\nThis workshop provides an overview of the rstanarm and brms packages. Basic modeling syntax is provided, as well as diagnostic checking, model comparison (posterior predictive checks , WAIC/LOO ), and how to get more from the models (marginal effects , posterior probabilities posterior probabilities, etc.). \n Factor Analysis and Related Methods\nThis workshop will expose participants to a variety of related techniques that might fall under the heading of ‘factor analysis’, latent variable modeling, dimension reduction and similar, such as principal components analysis, factor analysis, and measurement models, with possible exposure to and demonstration of latent Dirichlet allocation, mixture models, item response theory, and others. Brief overviews with examples of the more common techniques will be provided. \n Introduction to R Markdown\nThis workshop will introduce participants to the basics of R Markdown. After an introduction to concepts related to reproducible programming and research, demonstrations of standard markdown as well as overviews of different formats will be provided, including exercises. This document has been superseded by Practical Data Science, and will no longer be updated. \n Text Analysis with R\nThis document covers a wide range of topics, including how to process text generally, and demonstrations of sentiment analysis, parts-of-speech tagging, and topic modeling. Exercises are provided for some topics. It has practically no relevance in the modern large language model era."
  },
  {
    "objectID": "workshops.html#been-awhile",
    "href": "workshops.html#been-awhile",
    "title": "Workshops",
    "section": "Been awhile…",
    "text": "Been awhile…\nThese haven’t been given recently and are increasingly out date, but some content may be useful.\n My God, it’s full of STARs! Using astrology to get more from your data.\nTalk on structured additive regression models, and generalized additive models in particular. \n Become a Bayesian in 10 Minutes\nThis document regards a talk aimed at giving an introduction Bayesian modeling in R via the Stan programming language. It doesn’t assume too much statistically or any prior Bayesian experience. For those with such experience, they can quickly work with the code or packages discussed. I post them here because they exist and provide a quick overview, but you’d get more from the more extensive document. \n Engaging the Web with R\nDocument regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach.  \n Ceci n’est pas une %&gt;%\nExploring your data with R. A workshop that introduces some newer modes of data wrangling within R, with an eye toward visualization. Focus on dplyr and magrittr packages. No longer available as the javascript the slides were based on kept producing vulnerabilities for my website. Nowadays, using pipes is standard anyway. \n Getting More from RStudio\nAn afternoon talk on how to use RStudio for more than just coding."
  },
  {
    "objectID": "posts/2024-05-20/index.html",
    "href": "posts/2024-05-20/index.html",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  },
  {
    "objectID": "posts/2024-05-20/index.html#book-in-progess",
    "href": "posts/2024-05-20/index.html#book-in-progess",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html",
    "href": "posts/2022-09-deep-linear-models/index.html",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#introduction",
    "href": "posts/2022-09-deep-linear-models/index.html#introduction",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "title": "Deep Linear Models",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s get the primary packages loaded first.\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport torch\n\nNext, we’ll use the well-known titanic dataset, and to start things off, we’ll need to get a sense of what we’re dealing with. The basic idea is that we’d like to predict survival based on key features like sex, age, ticket class and more.\n\n# non-kaggle-requiring url here: https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/data/dl-linear-regression/titanic/train.csv\ndf_titanic_train = pd.read_csv('../../data/dl-linear-regression/titanic/train.csv')\n# df_titanic_train\n\n\ndf_titanic_train.describe()\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "href": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "title": "Deep Linear Models",
    "section": "Initial Data Processing",
    "text": "Initial Data Processing\nThe data is not ready for modeling as is, so we’ll do some additional processing to get it ready. We’ll check out the missing values and replace them with modes3.\n\ndf_titanic_train.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\nmodes = df_titanic_train.mode().iloc[0]\n\ndf_titanic_train.fillna(modes, inplace = True)\n\ndf_titanic_train.describe(include = (np.number))\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]\n\n\nWith features, sometimes it is worthwhile to log transform data for potentially more efficient optimization search. Since we have zeros, we add 1 before taking the log.\n\ndf_titanic_train['Fare'].hist()\n\n\nNow the transformed data looks a little more manageable. More to the point, we won’t potentially have huge coefficients relative to other covariates because of the range of the data.\n\ndf_titanic_train['LogFare'] = np.log1p(df_titanic_train['Fare'])\n\n# df_titanic_train['LogFare'].hist()\n\n\nThe Pclass (passenger class) feature is actually categorical.\n\npclasses = sorted(df_titanic_train.Pclass.unique())\npclasses\n\n[np.int64(1), np.int64(2), np.int64(3)]\n\n\nHere are the other categorical features.\n\ndf_titanic_train.describe(include = [object])\n\n                           Name   Sex  Ticket    Cabin Embarked\ncount                       891   891     891      891      891\nunique                      891     2     681      147        3\ntop     Braund, Mr. Owen Harris  male  347082  B96 B98        S\nfreq                          1   577       7      691      646\n\n\nIn order to use categorical variables, they need to be changed to numbers4, so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use embeddings5, particularly for things that have lots of unique categories.\n\ndf_titanic_train = pd.get_dummies(df_titanic_train, columns = [\"Sex\", \"Pclass\", \"Embarked\"])\n\nLet’s take a look at our data now.\n\ndf_titanic_train.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\ndf_titanic_train.head()\n\n   PassengerId  Survived  ... Embarked_Q  Embarked_S\n0            1         0  ...      False        True\n1            2         1  ...      False       False\n2            3         1  ...      False        True\n3            4         1  ...      False        True\n4            5         0  ...      False        True\n\n[5 rows x 18 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "title": "Deep Linear Models",
    "section": "Getting Started with pytorch",
    "text": "Getting Started with pytorch\n\nSetup\nNow we are ready to prep things for specific use with pytorch. I will not use the same terminology as in Jeremy’s original post, so for us, target = ‘dependent variable’ and X is our feature matrix6. Both of these will be pytorch tensors, which for our purposes is just another word for an array of arbitrary size.\n\nfrom torch import tensor\ndevice = torch.device('cpu')\ntarget = tensor(df_titanic_train.Survived)\n\n\ndummies = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nall_features = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummies \n\nX = df_titanic_train[all_features].apply(pd.to_numeric).astype(float)\nX = tensor(X.values, dtype = torch.float)\n\nX.shape\n\ntorch.Size([891, 12])"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "href": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "title": "Deep Linear Models",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nWe have our feature matrix and target variable prepped. The next step is to map the features to the target by means of predicted values. In linear regression, we typically call the weights that produce the predictions coefficients, but in standard deep/machine learning terminology, they are usually called weights, or more generally, parameters. Here, we generate some random values between -.5 and .5 to get started7:.\n\ntorch.manual_seed(442)\n\n&lt;torch._C.Generator object at 0x16d8c3330&gt;\n\nn_coeff = X.shape[1]\ncoeffs = torch.rand(n_coeff) - 0.5  # default would produce values from 0 to 1\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625])\n\n\nThe original post did a form of min-max scaling to the features, basically putting everything on a potentially [0, 1] scale. Here we’ll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.\n\n# vals,indices = X.max(dim=0)\n# X = X / vals\nX_means = X.mean(dim = 0, keepdim = True)\nX_sds   = X.std(dim = 0)\n\nX_sc = (X - X_means) / X_sds\n\n# X_sc.mean(dim = 0)  # all means = 0 \n# X_sc.std(dim = 0)   # all sd = 1\n\nAs noted in the original post and worth iterating here for our statistical modeling crowd, we don’t estimate an intercept for this model and keep all the dummy coded features. The following takes our coefficients, multiplies them by their respective feature, and sums them.\n\npreds = (X_sc * coeffs).sum(axis = 1)\npreds[:10]\n\ntensor([ 0.6000, -1.9341,  0.2080,  0.1723, -0.0032,  0.3088, -0.5066,  1.6219,\n         0.6990, -1.2584])\n\n\nWe can calculate our loss, the difference in our predictions versus the target values, in many ways. Here we get the mean squared error.\n\nloss = torch.square(preds - target).mean()\nloss\n\ntensor(1.3960)\n\n\nNow we’ll create functions that do the previous steps, and finally, give it a test run! In the original fastai formulation, they use mean absolute error for the loss, which actually is just the L1loss that is available in torch. For a change of pace, we’ll keep our mean squared error, which is sometimes called L2 loss (this will create different results from the original notebook). I create the option within the function for you to do either. Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.\n\ndef calc_preds(X, weights):\n    return((X * weights).sum(axis = 1))\n\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n    \n    # torch.abs(calc_preds(X, coeffs)-target).mean()  # original\n\n    if which == 'l2':\n      loss = torch.nn.MSELoss()\n    else: \n      loss = torch.nn.L1Loss()\n      \n    L = loss(preds, target.float())\n      \n    return(L)\n\ncalc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which = 'l1')\n\n(tensor(1.3960), tensor(0.8891))\n\n\n\nDoing a Gradient Descent Step\nWe can continue our journey onward to actually estimating the weights rather than specifying them directly, since we definitely don’t want to just keep guessing! This is an iterative process where we still start with an initial (random) guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps epochs, and getting our next guess requires calculating what’s called a gradient. Here are some resources for more detail:\n\nHow Does a Neural Net Really Work?: great intro by Jeremy Howard\nSome by-hand code using gradient descent for linear regression, R, Python: By yours truly\n\nIn any case, this is basic functionality within pytorch, and it will keep track of each step taken.\n\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\n\nloss = calc_loss(X_sc, coeffs, target)\nloss\n\ntensor(1.3960, grad_fn=&lt;MseLossBackward0&gt;)\n\n\nWe use backward to calculate the gradients and inspect them.\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-0.9311,  0.6245,  0.4957, -0.7423,  0.6008, -0.6008, -0.9158,  0.0938,\n         0.7127, -1.7183,  0.1715,  1.3974])\n\n\nEach time backward is called, the gradients are added to the previous values. We can see here that they’ve now doubled.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-1.8621,  1.2491,  0.9914, -1.4847,  1.2015, -1.2015, -1.8317,  0.1877,\n         1.4254, -3.4366,  0.3431,  2.7947])\n\n\nWhat we want instead is to set them back to zero after they are used for our estimation step. The following does this.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)     # sub subtracts in place\n    coeffs.grad.zero_()                # zeros out in place\n    print(calc_loss(X, coeffs, target))\n\ntensor([-0.1836, -0.0488,  0.0922, -0.0035, -0.4435, -0.1345,  0.7624,  0.2854,\n         0.0661,  0.0763,  0.1588, -0.0567], requires_grad=True)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\ntensor(37.9424)\n\n\n\n\nTraining the Linear Model\nWe typically would split our data into training and test. We can do so here, or keep this data as training and import test.csv for the test set. The latter is actually used for the Kaggle submission, but that’s not a goal here. We’ll use scikit-learn for the splitting.\n\nfrom sklearn.model_selection import train_test_split\n\n# test size .2 in keeping with fastai RandomSplitter default\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n  X_sc, \n  target.float(), \n  test_size = 0.2, \n  random_state = 808\n)\n  \n\nlen(train_x), len(valid_x) # might be one off of the original notebook\n\n(712, 179)\n\n\nAs before, we’ll create functions to help automate our steps:\n\none to initialize the weights\na function to update weights\none to do a full epoch (using weights to calculate loss, updating weights)\none to train the entire model (run multiple times/epochs)\n\nAs mentioned, the approach here is to create functions that are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each verbose value epoch (e.g. verbose = 10 means you’ll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (possibly not too big a deal depending on your IDE).\n\ndef init_weights(n_wts): \n    return (torch.rand(n_wts) - 0.5).requires_grad_()\n\ndef update_weights(weights, lr):\n    weights.sub_(weights.grad * lr)\n    weights.grad.zero_()\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\ndef train_model(X, target, epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1])\n    \n    for i in range(epochs): \n        one_epoch(X, coeffs, target, lr = lr, i = i, verbose = verbose)\n    return coeffs\n\nTry out the functions if you like (not shown).\n\ncalc_loss(X_sc, init_weights(X_sc.shape[1]), target).backward()\n\n\none_epoch(train_x, init_weights(train_x.shape[1]), train_y, .01)\n\nNow train the model for multiple epochs. The loss drops very quickly before becoming more steady.\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, verbose = 5, lr = .2)\n\n 1.375618\n  0.296216\n  0.284019\n  0.281221\n  0.280271\n  0.279923\n  0.279794\n  0.279746\n  0.279728\n  0.279721\n \n\n\nLet’s create a function to show our estimated parameters/weights/coefficients in a pretty fashion.\n\ndef show_coeffs(estimates): \n  coef_dict = dict(zip(all_features, estimates.requires_grad_(False).numpy()))\n  return pd.DataFrame(coef_dict, index = ['value']).T\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.090825\nSibSp      -0.054449\nParch      -0.016111\nLogFare     0.046320\nSex_male   -0.406538\nSex_female -0.171426\nPclass_1    0.408707\nPclass_2    0.335766\nPclass_3    0.329800\nEmbarked_C  0.057091\nEmbarked_Q  0.032813\nEmbarked_S  0.039464\n\n\n\n\nMeasuring Accuracy\nIt’s one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data. The following function will convert our estimates to a binary value like our target, and compares them to the target. Depending on how you did your training setup, it might be pretty bad or at least better than guessing.\n\ndef acc(X, weights, target): \n    return (target.bool() == (calc_preds(X, weights) &gt; 0.5)).float().mean()\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7051), tensor(0.6425))\n\n\n\n\nUsing sigmoid\nNothing about the previous setup actually produces a result on the probability scale, so basing a cutoff of .5 is meaningless. you can inspect them and might see values are above 1 or below zero, which we generally don’t want8. However we do have a solution. The sigmoid function9 allows us to transform our predictions to values between 0 and 1, i.e. probabilities in this context, and in particular, the probability of survival. Then our acc function will be more appropriate, where any probability &gt; .5 will be given a value of 1 (or True technically), while others will be 0/False.\n\ndef calc_preds(X, weights):\n    return torch.sigmoid((X*weights).sum(axis = 1))\n\nWe also will do more iterations, and fiddle with the learning rate (a.k.a. step size)\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  epochs = 500,\n  lr = 1,\n  verbose = 100\n)\n\n 0.314158\n  0.154329\n  0.154237\n  0.154232\n  0.154232\n \n\n\nNot too shabby!\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7823), tensor(0.7989))\n\n\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.516476\nSibSp      -0.423656\nParch      -0.179623\nLogFare     0.396468\nSex_male   -0.927410\nSex_female  0.349448\nPclass_1    0.713895\nPclass_2    0.320935\nPclass_3    0.078919\nEmbarked_C  0.107378\nEmbarked_Q  0.082943\nEmbarked_S -0.036137\n\n\nIn implementing the sigmoid, let’s go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster)10. To do this, the coefficients will need to be a column vector, so we change our init_coeffs function slightly11.\n\ndef calc_preds(X, weights): \n    return torch.sigmoid(X@weights)\n\ndef init_coeffs(n_wts): \n    return (torch.rand(n_wts, 1) * 0.1).requires_grad_()\n\nNow our functions are more like the mathematical notation we’d usually see for linear regression.\n\\[\\hat{y} = X\\beta\\]\n\n\nCompare to Linear/Logistic Regression\nBefore getting too excited, let’s compare our results to basic linear and logistic regression. The linear regression is more like our model before using the sigmoid transformation, while the logistic is more like when we used it. Depending on your settings, the logistic regression is probably doing better at this point.\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\n\n\nreg = linear_model.LinearRegression()\nreg.fit(train_x, train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\nacc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)\n\n(tensor(0.7989), tensor(0.7821))\n\n\n\nreg = linear_model.LogisticRegression()\nreg.fit(train_x, train_y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n0.7821229050279329\n\n\nIt looks like our coefficient estimates are similar to the logistic regression ones.\n\nshow_coeffs(coeffs_est).assign(logreg = np.round(reg.coef_.T, 4))\n\n               value  logreg\nAge        -0.516476 -0.4799\nSibSp      -0.423656 -0.4191\nParch      -0.179623 -0.1265\nLogFare     0.396468  0.3441\nSex_male   -0.927410 -0.6262\nSex_female  0.349448  0.6262\nPclass_1    0.713895  0.3941\nPclass_2    0.320935  0.0675\nPclass_3    0.078919 -0.3945\nEmbarked_C  0.107378  0.0546\nEmbarked_Q  0.082943  0.0655\nEmbarked_S -0.036137 -0.0890"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "href": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "title": "Deep Linear Models",
    "section": "A Neural Network",
    "text": "A Neural Network\n\nAt this point we’ve basically reproduced a general linear model. A neural network, on the other hand, has from one to many hidden layers of varying types in between input and output. Let’s say we have a single layer with two nodes. For a fully connected or dense network, we’d need weights to map our features to each node of the hidden layer (n_wts * n_hidden parameters total), and then another set of weights to map the hidden nodes to our next layer, which is our output, basically creating the predicted values. You can think of it as a second hidden layer with a single output node. With additional hidden nodes we add more complexity, but also flexibility, to the model. But this may come at a price, e.g. more difficulty with training due to the additional parameters that have to be estimated.\nSo basically we need matrices of weights, and the following function allows us to create those. We also add a bias/intercept/constant for the hidden-to-output processing. In the first layer, we divide the weights by n_hidden to create sums at the next layer that are of similar magnitude as the inputs. In general though, there are many ways to initialize weights.\n\ndef init_weights(n_wts, n_hidden = 20):\n    layer1 = (torch.rand(n_wts, n_hidden) - 0.5) / n_hidden # n_wts x n_hidden matrix of weights\n    layer2 = torch.rand(n_hidden, 1) - 0.3                  # n_hidden weights\n    const  = torch.rand(1)[0]                               # constant\n    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\nNow we revise our calc_preds function to incorporate all the weights. Initially, we extract the different sets of weights that are estimated by the model. For the original inputs, we multiply them by the layer 1 weights and sum. Then we apply a transformation to them to induce nonlinearity. Typical approaches are the sigmoid function we used before, hyperbolic tangent, and, probably the most common, the relu. The original notebook used relu, while I use a more recent one called Mish, which is a variant of relu. The hidden layer nodes then get multiplied by their respective weights and summed with the constant added. We then use our sigmoid function to get the probability scale as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(X, weights):\n    l1, l2, const = weights\n    res = F.mish(X@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res).flatten()\n\nWith additional sets of weights, we use an update loop.\n\ndef update_weights(weights, lr):\n    for layer in weights:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, lr = 1, verbose = 10)\n\n 0.325837\n  0.155810\n  0.141485\n  0.137652\n  0.136034\n \n\n\nAt this point we’re doing a little bit better in general, and even better than standard logistic regression on the test set!\n\nacc(train_x, coeffs_est, train_y), \\\nacc(valid_x, coeffs_est, valid_y), \\\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n(tensor(0.8160), tensor(0.8045), 0.7821229050279329)"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "href": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "title": "Deep Linear Models",
    "section": "Deep Learning",
    "text": "Deep Learning\nWe previously used a single hidden layer, but we want to go deeper! That’s the whole point of deep learning right? The following modifies our previous functions to allow for an arbitrary number of layers. You’ll note there are some hacks to get the weights in a good way for each layer12, but you normally wouldn’t have to do that on your own, since most tools will provide sensible modifications.\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\n# change loss to binary\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n\n    loss = torch.nn.BCELoss()\n\n    L = loss(preds, target.float())\n\n    return(L)\n\n\ndef init_weights(n_wts, hiddens):  \n    sizes = [n_wts] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]\n    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers, consts\n\n\ndef calc_preds(X, weights):\n    layers, consts = weights\n    n = len(layers)\n    res = X\n    \n    for i, l in enumerate(layers):\n        res = res@l + consts[i]\n    \n    if i != n-1: \n      res = F.mish(res)\n      \n    \n    return torch.sigmoid(res).flatten()\n\ndef update_weights(weights, lr):\n    layers, consts = weights\n    for layer in layers + consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1], hiddens)\n    \n    for i in range(epochs): \n        if verbose != 0:\n            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)\n    \n    return coeffs\n\nWith everything set up, let’s do some deep linear modeling! You can play around with the number of hidden layers, number of nodes and other settings. Feel free to explore!\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  hiddens = [500, 250, 100],\n  epochs  = 500,\n  lr      = 1e-4,\n  verbose = 10\n)\n\n 5.123790\n  0.666971\n  0.653124\n  0.640325\n  0.628476\n  0.617496\n  0.607313\n  0.597861\n  0.589081\n  0.580918\n  0.573322\n  0.566249\n  0.559658\n  0.553510\n  0.547772\n  0.542413\n  0.537403\n  0.532715\n  0.528326\n  0.524212\n  0.520354\n  0.516733\n  0.513330\n  0.510130\n  0.507118\n  0.504281\n  0.501605\n  0.499080\n  0.496695\n  0.494439\n  0.492305\n  0.490283\n  0.488366\n  0.486547\n  0.484820\n  0.483178\n  0.481616\n  0.480129\n  0.478712\n  0.477361\n  0.476072\n  0.474840\n  0.473663\n  0.472538\n  0.471461\n  0.470429\n  0.469440\n  0.468493\n  0.467583\n  0.466710\n \n\n\nHooray! Our best model yet (at least tied).\n\npd.DataFrame({\n    'acc_train': acc(train_x, coeffs_est, train_y).flatten(), \n    'acc_test': acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm': accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int))\n}, index = ['value'])\n\n       acc_train  acc_test  acc_test_glm\nvalue    0.77809  0.804469      0.782123"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "href": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "title": "Deep Linear Models",
    "section": "The Elephant in the Room",
    "text": "The Elephant in the Room\nAs noted in my previous posts [1, 2], probably your biggest challenge in implementing a deep learning model for tabular data, one with mixed data types and other complexities, is beating an off the shelf boosting model. Here is a quick demo with lightgbm.\n\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(\n  # n_estimators = 500,  # the sorts of parameters you can play with (many more!)\n  # max_depth    = 4,\n  # reg_alpha    = .1\n)\n\nmodel.fit(train_x, train_y)\n\nLGBMClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifieriFittedLGBMClassifier() \n\nmodel.score(valid_x, valid_y.numpy())\n\n0.8491620111731844\n\n\n\n# sklearn example\n# from sklearn.ensemble import HistGradientBoostingClassifier\n# \n# res = HistGradientBoostingClassifier().fit(train_x.numpy(), train_y.numpy())\n# \n# res.score(valid_x.numpy(), valid_y.numpy())\n\nNo tuning at all, and we’re already doing significantly better. Granted, if you use a packaged DL model for tabular data like the one in fastai, you should be doing better than our little demo. Even then though, you may still find the boosting results tough to beat.\n\ndf_accs = pd.DataFrame({ \n    'acc_test_dl':   acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm':  accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int)),\n    'acc_test_lgbm': model.score(valid_x, valid_y.numpy())\n}, index = ['value']).round(4)\n\ndf_accs\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue       0.8045        0.7821         0.8492\n\n\n\ndf_perc_improvement = 100 * (df_accs / df_accs.iloc[0,1] - 1)  # % improvement\ndf_perc_improvement\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue     2.864075           0.0       8.579466"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#summary",
    "href": "posts/2022-09-deep-linear-models/index.html#summary",
    "title": "Deep Linear Models",
    "section": "Summary",
    "text": "Summary\nThis was a lot of work to do slightly better than a logistic regression! However, there is a lot going on with a typical DL model that would likely prove even better. But it also serves as a reminder to have a suitable baseline, and as we saw with the lightgbm model, it can take little effort to do very well without deep learning. Hopefully though, the peek behind the scenes to do some ‘deep’ linear modeling can make it more accessible for you."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "href": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "title": "Deep Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI won’t actually use fastai, since they aren’t up to supporting M1/2 Macs very well. I think it was only used for the train/test data split anyway. I would rant a bit about this, but a lot of fastai is geared toward non-local computing, and the fault is really with Apple and NVidia as near as I can tell.↩︎\nI’m also not going to go into broadcasting, submitting to Kaggle, and other things that I don’t think are necessary for our purposes here.↩︎\nJust as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here, and sometimes that’s enough.↩︎\nEven though every modeling tool requires this, strangely very few in the Python world offer options for automatic handling of such things, but it’s getting better.↩︎\nWe actually aren’t too far removed from this in our model coming up, the main difference is that we don’t treat the categorical feature part of the model separately.↩︎\nI’ll not perpetuate calling features/predictor variables that are clearly not independent as independent. That nomenclature really only works for randomized experiments, and that is definitely not the case here.↩︎\nYou could use torch.randn to get standard normal values, and often times we’ll even start with just zeros if we really are just doing a standard linear model.↩︎\nUnless you are an economist, in which case you call it a linear probability model and ignore the ridiculous predictions because you have very fine standard errors.↩︎\nA lot of R folks seem unaware that the base R plogis function accomplishes this.↩︎\nThe @ operator is essentially the dot product, so x@y is np.dot(x, y)↩︎\nThe fastai demo also changes the target to a column vector, but this doesn’t seem necessary.↩︎\nAnd they probably aren’t as good for the changes I’ve made.↩︎"
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html",
    "href": "posts/2021-10-30-double-descent/index.html",
    "title": "Double Descent",
    "section": "",
    "text": "A co-worker passed along a recent article (Dar, Muthukumar, and Baraniuk 2021) on the topic of double descent in machine learning. I figured I’d summarize some key points I came across while perusing it and some referenced articles. In addition, I’ll provide an accessible example demonstrating the phenomenon."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#what-is-double-descent",
    "href": "posts/2021-10-30-double-descent/index.html#what-is-double-descent",
    "title": "Double Descent",
    "section": "What is double descent?",
    "text": "What is double descent?\n\nBias-variance trade-off\nTo understand double descent you have to revisit the concept of the bias-variance trade-off. Without going into too much detail, the main idea with it is that having an overly complex model leads to overfitting the training data, which results in worse prediction on new data, at least relative to what simpler models would have done. The classic figure looks like the following, where blue is the training error and the red is the test error. Thin lines represent one path of complexity (e.g. across a random sample of the data), while the thicker lines are the average at a particular point of model complexity.\n\nIf we don’t have a sufficiently complex model, both training and test error will be poor, the case of underfitting. Our model is a poor approximation of the true underlying function, and predicts poorly on data both seen and unseen. When we have too much model complexity relative to the size of our data (e.g. more covariates, nonlinear effects, interactions, etc.), we pass into the overfit situation. Essentially, while our model function would result in a decrease in error with the data it’s trained on (lower bias as it better approximates the true underlying function), with too much complexity, you’d also eventually have notable changes in prediction (high variance) with any slight deviation in the underlying training data. We can even get to the point where we fit the training data perfectly, but it will be overly susceptible to the noise in the data, and not do well with unseen observations.\nTo combat this, we usually attempt to find a balance between overly simple and overly complex models. This would be the point where test error is among its lowest point for a desirable level of complexity (e.g. around 20-25 df in the figure above), before it begins to rise again. This may be accomplished more explicitly, for example, picking a model through cross-validation, or more implicitly, for example, through regularization (Belkin et al. (2019)). For more detail on the bias-variance trade-off, you can look at the exposition in the main article noted above, my document here, or any number of places, as it is an extremely well-known idea in machine learning.\n\n\nDouble Descent\nThe funny thing is, it turns out that the above actually only applies to a specific scenario, one which we will call underparameterized models. We can simplify this notion by just thinking of the case where the number of our parameters to estimate is less than or equal to the number of observations we have to work with. Nowadays though, it’s not uncommon to have what we’d call overparameterized models, such as random forests and neural networks, sometimes with even billions of parameters, far exceeding the data size. In this scenario, when we revisit the trade-off, something unusual happens!\n\n\n\nFigure from Dar, Muthukumar, and Baraniuk (2021)\n\n\nSuch models may have near zero training error, yet do well on unseen data. As we increase complexity, we see something like a second bias-variance trade-off beyond the point where the data is perfectly fit (interpolated). This point is where model complexity (e.g. in terms of number of parameters) p equals the number of observations N, and this is where the realm of the overparameterized models begins. Now test error begins to drop again with increasing complexity."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#an-example",
    "href": "posts/2021-10-30-double-descent/index.html#an-example",
    "title": "Double Descent",
    "section": "An example",
    "text": "An example\nI thought it would be amusing to try this with the classic mtcars data set available in base R. With this data, our goal will be to predict fuel consumption in miles per gallon (mpg). First we will split the data into training and test components. We create a model where our number of parameters (p), in this case standard regression coefficients, will equal the number of observations (N). Some of the more technically savvy will know that if the number of features and/or parameters to estimate p equals the number of observations N, a standard linear regression model will fit the data perfectly1, demonstrated below.\n\n\nIf not familiar, the mtcars object is a data frame that comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\nnc = ncol(mtcars) \nnr = nc\nfit_perfect = lm(mpg ~ ., mtcars[1:nr, ])\n# summary(fit_perfect) # not shown, all inferential estimates are NaN\n\n\n\n\n\n\n\n\n\n\nNow let’s look at the test error, our prediction on the unseen data we didn’t use in fitting the model. When we do, we see the usual bias-variance trade-off. Our generalizability capabilities have plummeted, as we have overfit the training data and were unable to accommodate unseen observations. We are even predicting negative mpg in some cases!\n\n\n\n\n\n\n\n\n\n\np ≤ N\nLet’s extend the demonstration more fully. We now create models of increasing complexity, starting with an intercept only model (i.e. just using the mean for prediction), to one where all other columns (10) in the data are predictors. Here I repeatedly sampled mtcars of size \\(N = 10\\) for training, the remainder for test, and also shuffled the columns each time, doing so for a total of 250 times2. Here is the result- the classic bias variance trade-off curve. The larger dot shows the test error minimum, at about 3 covariates (plus intercept). The vertical line denotes our point of interpolation.\n\n\n\nDouble Descent in the underparameterized setting.\n\n\n\n\np &gt; N\nSo with one of the simpler data sets around we were able to demonstrate the bias-variance trade-off clearly. But now let’s try overparameterized models! We don’t need anything fancy or complicated to do this, so for our purposes, I’m just going to add cubic spline basis expansions for the wt, disp, and hp features3. This will definitely be enough to put us in a situation where we have more parameters than data, i.e. p &gt; N, but doesn’t make things too abstract4.\nThe basic linear model approach we might typically use fails to estimate the additional parameters in this situation, so we need a different estimator. Some are familiar with penalized regression techniques such as lasso and ridge regression, and we could use those here. However, I’ll use ridgeless regression, as depicted in Hastie et al. (2019), and which, like ridge regression, is a straightforward variant of the usual least squares regression5. I estimate the coefficients/weights on the training data, and make predictions for the training and test set, calculating their respective errors. Here is an example of the primary function used.\n\nfit_ridgeless = function(X_train, y, X_test, y_test){\n  # get the coefficient estimates\n  b = pseudo_inv(crossprod(X_train)) %*% crossprod(X_train, y)\n  \n  # get training/test predictions\n  predictions_train = X_train %*% b\n  predictions_test  = X_test %*% b\n  \n  # get training/test error\n  rmse_train = sqrt(mean((y - predictions_train[,1])^2))\n  rmse_test  = sqrt(mean((y_test - predictions_test[,1])^2))\n  \n  # return result\n  list(\n    b = b,\n    predictions_train = predictions_train,\n    predictions_test  = predictions_test,\n    rmse_train = rmse_train,\n    rmse_test  = rmse_test\n  )\n}\n\nWe can test the function as follows with as little as 10 observations, where p (all predictor coefficients plus intercept = 11 parameters) is greater than N (10). This demonstrates that the ridgeless approach can provide an estimate for all the parameters (unlike the standard lm function), and we also see very low training error, but relatively high test error (in terms of the root mean square error.)\n\nn = 10\n\nX = as.matrix(cbind(1, mtcars[, -1]))\ny = mtcars$mpg # mpg is the first column\n\nX_train = X[1:n, ]\ny_train = mtcars$mpg[1:n]\nX_test  = X[-(1:n),]\ny_test  = y[-(1:n)]\n\nresult = fit_ridgeless(X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0.84\n\n\n−1.69\n\n\n0.08\n\n\n−0.08\n\n\n2.76\n\n\n−1.29\n\n\n0.24\n\n\n2.32\n\n\n3.26\n\n\n2.26\n\n\n0.66\n\n\n\n\n\n\n\n\n\n\n\n\n\nrmse_train\nrmse_test\n\n\n\n\n0.05\n5.79\n\n\n\n\n\n\n\nIf we do this for more complex models (max linear features, plus each additional set of features associated with a cubic spline basis expansions), we obtain the following. Now we see the second descent in test error takes form!\n\n\n\nDouble Descent in the overparameterized setting.\n\n\nPutting our results together gives us the double descent curve.\n\n\n\nDouble Descent in the overparameterized setting.\n\n\n\nNote that this all holds for the most part with classification problems, including multiclass (or multivariate/class targets).\n\nWe not only see the double descent pattern, but we can also note that the global test error minimum occurs with the model with the most parameters. The gray dot is the lowest test error with the underparameterized settings, while the dark red is the global test error minimum."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#why-does-this-happen",
    "href": "posts/2021-10-30-double-descent/index.html#why-does-this-happen",
    "title": "Double Descent",
    "section": "Why does this happen?",
    "text": "Why does this happen?\nUnderstanding the double descent phenomenon is an area of active research, and there are some technical issues we won’t cover here. However, we can note a couple things more broadly. When we’re in the underparameterized situation, we ultimately begin to force features that have no association with the target to fit the data anyway. Once you move beyond the point of where these features are useful, test error begins to rise again, until the point of interpolation where test error is even worse than guessing (or just guessing in the classification case).\nBeyond the interpolation point, all models we potentially employ using this estimation technique will have the capacity to fit the training data perfectly, i.e. zero bias. This allows us to fit the remaining noise in the data with the additional features employed by the more complex models. There is no guarantee that among the models you fit that the lowest test error will be found relative to the underparameterized setting. However, the lowest test error to be found is ‘out there’ somewhere6. So adding complexity will potentially allow you to find improved test error.\nAnother way to put it is that we have a single class of models to consider, and under and overparameterized are special cases of that more general class. Any one of these might result in the lowest test error. The overparameterized models, which may contain complex nonlinearities and interactions, are likely to be more compatible with the data than the simpler models7. So odds are good that at least one of them will have a smaller test error as well. In any case, restricting ourselves to the underparameterized setting is definitely no guarantee that we will find the most performant model.\nOne caveat is that the model we used is an example of ‘implicit’ regularization, one in which there is no hyper-parameter to set (or discover through cross-validation), like with ridge and lasso. With other techniques (e.g. optimally chosen ridge regression estimator) we may still be able to achieve optimal test error without complete interpolation, and show a reduced peak.\nDar, Muthukumar, and Baraniuk (2021) note that in the overparameterized setting, we can distinguish the signal part of the error term that reduces as a function of N/p, where the noise part of the error term is a function of p/N. In addition, there is a portion of test error related to model misspecification, which will always decrease with overparameterization. In addition, one must consider both feature correlations as well as correlations among observations. Having more complex covariance structure doesn’t negate the double descent phenomenon, but they suggest that, for example, cases where there is low effective dimension within these additional features will more readily display the double descent.\nAnother issue is that in any given situation it is difficult to know where in the realm of available models we exist presently. So additional complexity, or even additional data, may in fact hurt performance (Nakkiran et al. 2019)."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#conclusion",
    "href": "posts/2021-10-30-double-descent/index.html#conclusion",
    "title": "Double Descent",
    "section": "Conclusion",
    "text": "Conclusion\nThe double descent phenomenon is a quite surprising scenario, especially for those who have only heard of the classical bias-variance trade off. There is still much to learn regarding it, but such research is off and running. For practical purposes, it is worth keeping it in mind to aid us in model selection and thinking about our modeling strategies in general."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#footnotes",
    "href": "posts/2021-10-30-double-descent/index.html#footnotes",
    "title": "Double Descent",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR2 = 1 in the standard linear model setting.↩︎\nNote that the intercept term is added after data shuffling so when p = 1 it is the intercept only model, i.e. guessing the mean.↩︎\nI used mgcv to so this, then added them in whole for each term to the previously shuffled model matrix. These columns are not shuffled. By default these will add 10 columns each to the model matrix.↩︎\nFor more on generalized additive models, see my document.↩︎\nRidgeless regression has the same form as the ‘normal’ equations for least squares, but instead of \\(\\beta \\sim (X^TX)^{-1} \\cdot X^Ty\\), we have \\(\\beta \\sim (X^TX)^{+} \\cdot X^Ty\\) where the first part is the pseudo-inverse of \\(X\\). It is similar to equations for ridge regression (see my demo here) and can be seen as an approximation to it as the ridge penalty tends toward zero.↩︎\nFox Mulder told me so.↩︎\nBecause nature is just funny that way.↩︎"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html",
    "href": "posts/2021-05-time-series/index.html",
    "title": "Exploring Time",
    "section": "",
    "text": "NB: This post was revisited when updated the website, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#intro",
    "href": "posts/2021-05-time-series/index.html#intro",
    "title": "Exploring Time",
    "section": "Intro",
    "text": "Intro\n\nThis post was mostly complete around May 2021, but for various reasons not actually posted until August of 2022. I haven’t changed much aside from adding a section on boosting, and have used the results I conjured up previously (for the most part). However, many package updates since then may mean that parts of the code may not work as well, especially for the torch code. I would also recommend modeltime as starting point for implementing a variety of model approaches for time series data with R. It was still pretty new when this was first written, but has many new features and capabilities, and could do some version of the models shown here.\n\nIt is extremely common to have data that exists over a period of time. For example, we might have yearly sports statistics, daily manufacturing records, server logs that might be occurring many times per second, and similar. There are many approaches we could use to model the data in these scenarios. When there are few time points and they are clustered within other units, like repeated observations of exercise data for many individuals, we often use tools like mixed models for example, and even with many observations in a series, we can still use tools like that. But sometimes there may be no natural clustering, or we might want to use other approaches to handle additional complexity.\nThis post is inspired by a co-worker’s efforts in using PyTorch to analyze Chicago Transit data. Cody Dirks wrote a post where he used a Python module developed by our group at Strong Analytics to analyze the ridership across all the ‘L’. This post can be seen as a demonstration of some simpler models which might also be viable for a given situation such as this, allowing for quick dives, or even as ends in themselves."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#outline",
    "href": "posts/2021-05-time-series/index.html#outline",
    "title": "Exploring Time",
    "section": "Outline",
    "text": "Outline\nThe models we’ll go through are the following:\n\nError models and random effects\nGAM\nMore elaborate time series with seasonal and other effects\nBoosting and Deep learning\n\nIn what follows I will show some more detailed code in the beginning, but won’t show it later for conciseness, focusing mostly just on the basic model code. You can always find the code for these posts on my GitHub."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#quick-summary",
    "href": "posts/2021-05-time-series/index.html#quick-summary",
    "title": "Exploring Time",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nClassical econometrics approaches like ARIMA models may take notable effort to match the flexibility of other approaches one might take with time series. It’s also difficult to believe a specific lag/ma number will hold up with any data change.\nGAMs extend mixed models, and should probably be preferred if a probabilistic approach is desired. Prophet-style approaches would likely take notable effort and still likely lack performance, without adding interpretability.\nFor black box methods, boosting can do very well without much feature engineering, but possibly take a bit more for parameter tuning. Deep learning methods may be your best bet given data size and other data modeling needs."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#data-description",
    "href": "posts/2021-05-time-series/index.html#data-description",
    "title": "Exploring Time",
    "section": "Data Description",
    "text": "Data Description\nAs noted in Cody’s post, over 750,000 people use the Chicago Transit Authority’s ‘L’ system to get around the city. There are 8 interconnected rail lines named after colors- the Red, Blue, Green, Brown, Pink, Orange, Purple, and Yellow, 145 entry/exit stations, and over 2,300 combined trips by its railcars every day1.\nThe city of Chicago provides ridership data that can be accessed publicly.\n\nridership\nstation info\n\nIn Cody’s exploration, he added pertinent information regarding weather, sporting events, and more. You can access the processed data.\nFor our demonstrations we have daily ridership from 2012-2018, and we will use a variety of methods to model this. We will use a normalized ride count (mean of 0, standard deviation of 1) as our target variable.\n\nImport & Setup\nTo get things started we’ll use the tidyverse for some additional data processing, and lubridate for any date processing, for example, converting to weekdays.\n\n# Data Processing\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n# Misc\n\nSTART_DT = '2008-06-01'\nEND_DT   = '2018-12-31'\nSPLIT_DT = '2017-06-01'\n\n\nMain data\nI start with data having already been processed, but as mentioned the source is publicly available. I use data.table to read it in more quickly, but it’s default date class can cause issues with other packages, so I deal with that. I also extract the year, month, weekday, etc.\n\ndf = data.table::fread('../../data/time-series/processed_df.csv')\n\ndf_start = df %&gt;% \n  as_tibble() %&gt;% \n  select(-contains('_attributes'), -(tsun:wt22)) %&gt;% \n  mutate(\n    date      = as_date(date), # remove IDATE class\n    rides_log = log(rides),\n    year      = year(date),\n    year_fac  = factor(year),\n    month     = month(date, label = TRUE),\n    day       = factor(wday(date, label = TRUE), ordered = FALSE),\n    year_day  = lubridate::yday(date),\n    line      = factor(line),\n    snow_scaled = scale(snow)[, 1],\n    colors = as.character(line),\n    colors = ifelse(colors == 'purple_express', 'purple4', colors),\n    red_line_modernization = \n      ifelse(\n        between(date, as_date('2013-05-19'), as_date('2013-10-20')), \n        1, \n        0\n      )\n  ) %&gt;% \n  arrange(date, line)\n\n\n\n\nTraining and Validation\nWe split our data into training and validation sets, such that everything before 2017-06-01 is used for training, while everything after will be used for testing model performance2.\n\ndf_train = df_start %&gt;% \n  filter(date &lt; SPLIT_DT, !is.na(rides))\n\ndf_validate = df_start %&gt;% \n  filter(date &gt;= SPLIT_DT, !is.na(rides))\n\nred_line_train = df_train %&gt;% \n  filter(line == 'red')\n\nred_line_validate = df_validate %&gt;% \n  filter(line == 'red')\n\n\n\nOther\nHolidays are available via the prophet package, which we’ll be demonstrating a model with later. The data we’re using already has a ‘holiday vs. not’ variable for simplicity, though it comes from a different source. The prophet version has both the actual date and the observed date counted as a holiday, and I prefer to use both.\n\nholidays = prophet::generated_holidays %&gt;% \n  filter(country == 'US') %&gt;% \n  mutate(ds = as.numeric(as_date(ds))) %&gt;%\n  droplevels()\n\nWe’ll take a quick look at the red line similar to Cody’s post, so we can feel we have the data processed as we should.\n\n\n\n\n\n\n\n\n\nWith the data ready to go, we are ready for modeling, so let’s get started!"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#classical-time-series",
    "href": "posts/2021-05-time-series/index.html#classical-time-series",
    "title": "Exploring Time",
    "section": "Classical Time Series",
    "text": "Classical Time Series\n\nIntro\nClassical times series from an econometrics perspective often considers a error model that accounts for the correlation a current observation has with past observations. A traditional example is the so-called autoregressive, or AR, model, which lets a current observation be predicted by past observations up to a certain point. For example, would could start by just using the last observation to predict the current one. Next we extend this to predict the current based on the previous two observations, and so on. How many lags we use is part of the model exploration.\n\\[y_t = \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + \\varepsilon_t\\]\nWe can extend this to include not just past observations but also past residuals, called a moving average. So formally, our ARMA (p, q) model now looks like this for an observation \\(y\\) at time \\(t\\):\n\\[y_t = \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots +\\theta_q \\varepsilon_{t-q})\\]\nWe can also use differencing, for example subtracting the previous time value from the current observation value for all values, to come to the final ARIMA (p, d, q) model. See Hyndman and Athanasopoulos (2021) for more details.\n\n\nModel\nEven base R comes with basic time series models such as this. However, as mentioned, we typically don’t know what to set the values of an ARIMA(p, d, q) to. A quick way to explore this is via the forecast package, which will search over the various hyperparameters and select one based on AIC. Note that fable, a package we will be using later, will also allow such an approach, and if you’d like to go ahead and start using it I show some commented code below.\n\nmodel_arima = forecast::auto.arima(\n  red_line_train$rides_scaled\n)\n\n# model_arima = red_line_train %&gt;%\n#   select(date, rides_scaled) %&gt;%\n#   tsibble::as_tsibble() %&gt;%\n#   fabletools::model(fable::ARIMA(\n#     rides_scaled ~ 0 + PDQ(0,0,0),\n#     stepwise = FALSE,\n#     approximation = FALSE\n#   ))\n# fabletools::report(model_arima)\n\n\n\nExplore\nIn this case we have a selected AR of 3 and MA of 4 for the centered value. But looking at the predictions, we can see this is an almost useless result for any number of days out, and does little better than guessing.\n\nbroom::tidy(model_arima)\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n0.238\n0.048\n\n\nar2\n-0.285\n0.035\n\n\nar3\n0.354\n0.043\n\n\nma1\n-0.725\n0.046\n\n\nma2\n-0.189\n0.046\n\n\nma3\n-0.575\n0.029\n\n\nma4\n0.552\n0.025\n\n\n\n\n\n\n\n\n# plot(acf(residuals(model_arima))) # weekly autocorrelation still exists\n\nred_line_validate %&gt;% \n  slice(1:30)  %&gt;%\n  mutate(pred = predict(model_arima, n.ahead = 30)$pred) %&gt;%\n  # mutate(pred = forecast(model_arima, h = 30)$.mean) %&gt;%\n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'darkred')\n\n\n\n\n\n\n\n\nWe’ll use yardstick to help us evaluate performance for this and subsequent models. In this case however, the visualization told us enough- a basic ARIMA isn’t going cut it.\n\nlibrary(yardstick)\n\n# this function will be used for all subsequent models!\nmetric_score = metric_set(rmse, mae, rsq) \n\n# validation\ndata.frame(\n  pred = predict(model_arima, newdata = red_line_validate, n.ahead = 30)$pred,\n  observed = red_line_validate$rides_scaled[1:30]\n) %&gt;%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.632\n\n\nmae\nstandard\n0.572\n\n\nrsq\nstandard\n0.132\n\n\n\n\n\n\n\nOne nice thing about the forecast package is that it can include additional features via the xreg argument, which is exactly what we need- additional information. Now our model looks something like this, where \\(X\\) is our model matrix of features and \\(\\beta\\) their corresponding regression weights.\n\\[y_t = X_t\\beta + \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots +\\theta_q \\varepsilon_{t-q})\\]\nAdding these is not exactly straightforward, since it requires a matrix rather than a data frame, but this is not too big a deal once you are used to creating model matrices.\n\nmm = model.matrix(\n  ~ . - 1, \n  data = red_line_train %&gt;% \n    select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization)\n)\n\nmodel_arima_xreg = forecast::auto.arima(\n  red_line_train$rides_scaled,\n  max.p = 10,\n  max.q = 10,\n  xreg  = mm\n)\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n-0.444\n0.018\n\n\nar2\n-0.430\n0.018\n\n\nar3\n-0.370\n0.019\n\n\nar4\n-0.325\n0.019\n\n\nar5\n-0.312\n0.019\n\n\nar6\n-0.356\n0.019\n\n\nar7\n0.307\n0.018\n\n\nar8\n-0.051\n0.017\n\n\nis_weekend\n-1.154\n0.023\n\n\nis_holiday\n-1.045\n0.021\n\n\nis_cubs_game\n0.208\n0.015\n\n\nis_sox_game\n0.072\n0.015\n\n\ntmax_scaled\n0.085\n0.011\n\n\nprcp_scaled\n-0.031\n0.004\n\n\nred_line_modernization\n-0.550\n0.131\n\n\n\n\n\n\n\nThis is looking much better! We can also see how notably different the ARMA structure is relative to the previous model. We also see that adding weekend and holiday effects result in a huge drop in ridership as expected, while baseball games and good weather will lead to an increase.\nIn the following code, we create a model matrix similar to the training data that we can feed into the predict function. The forecast package also offers a glance method if desired.\n\nnd = red_line_validate %&gt;%\n  select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization) %&gt;% \n  model.matrix( ~ . - 1, data = .)\n\npreds = predict(model_arima_xreg, newxreg = nd, n.ahead = nrow(red_line_validate))$pred\n\n\np_arima_red = red_line_validate %&gt;% \n  mutate(pred = preds) %&gt;% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'red') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'ARIMA')\n\np_arima_red\n\n\n\n\n\n\n\n\n\n\nAnd here we can see performance is notably improved (restrict to first 30 obs for a direct comparison to the previous).\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.371\n\n\nmae\nstandard\n0.282\n\n\nrsq\nstandard\n0.747"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#mixed-model-with-ar-structure",
    "href": "posts/2021-05-time-series/index.html#mixed-model-with-ar-structure",
    "title": "Exploring Time",
    "section": "Mixed model with AR Structure",
    "text": "Mixed model with AR Structure\n\nIntro\nMore generally, we can think of that original AR error as a random effect, such that after the linear predictor is constructed, we add a random effect based on the correlation structure desired, in this case, autoregressive. In the mixed model setting, it is actually quite common to use an AR residual structure within a cluster or group, and here we can do so as well, as the data is naturally grouped by line.\nTo make this a bit more clear, we can state the AR effect more formally as follows for a single line at time \\(t\\):\n\\[z_t  \\sim N(0, \\Sigma_{ar})\\] \\[\\Sigma_{ar} = cov(z(s), z(t)) = \\sigma^2\\exp(-\\theta|t-s|)\\]\nWhere t,s are different time points, e.g. within a line.\nIf we were to simulate it for 4 time points, with autocovariance value of .5, we could do so as follows3.\n\nn_clusters   = 1\nn_timepoints = 4\nmu  = 0\nvar = 1  # not actually used if the value is 1\nS = .5^as.matrix(dist(1:n_timepoints))\n\nS\n\nz = MASS::mvrnorm(mu = rep(mu, n_timepoints), Sigma = S)\n\nz\n\nAnd here is our typical model with a single random effect, e.g. for line:\n\\[ y_{tl} \\sim X\\beta + z^{line}_{l} + e_{tl}\\] \\[\\textrm{z}_{l} \\sim N(0, \\sigma_l^2)\\] \\[\\epsilon \\sim N(0, \\sigma_e^2)\\]\nThe X may be at either line or observation level, and potentially the \\(\\beta\\) could vary by line.\nPutting it all together, we’re just adding the AR random effect to the standard mixed model for a single line.\n\\[ y_{tl} \\sim X\\beta + z^{ar}_t +z^{line}_{l} + e_{tl}\\]\n\n\nData Prep\nSo let’s try this! First some minor data prep to add holidays.\n\ndf_train_mixed = df_train %&gt;% \n  mutate(date = as.numeric(date)) %&gt;% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %&gt;% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\ndf_validate_mixed = df_validate %&gt;% \n  mutate(date = as.numeric(date)) %&gt;% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %&gt;% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\n\n\nModel\nFor the model, we can now easily think of it as we do other standard modeling scenarios. Along with standard features, we’ll add random effects for line, day, day x line interaction, etc. Finally we also add an AR random effect. For each line, we have an autoregressive structure for days, such that days right next to each other are correlated, and this correlation tapers off as days are further apart. This is not our only option, but seems straightforward to me.\nDepending on what you include in the model, you may have convergence issues, so feel free to reduce the complexity if needed. For example, most of the day effect is captured by weekend vs. not, and a by line year trend wasn’t really necessary. In addition, the way the AR random effect variance is estimated as noted above, this essentially captures the line intercept variance.\n\nmodel_mixed = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  tmax_scaled + \n  prcp_scaled + \n  snow_scaled +\n  # year_day +\n  ar1(0 + day|line) +     # the 0 + is a nuance of tmb's approach\n  (1|holiday) +           # as RE with all holidays instead of just holiday vs. not\n  (1|year) +     \n  (1 | red_line_modernization:line) +  # the project shifted ridership from red to other lines\n  # (1|day) #+ \n  # (1|line) +\n  (1|day:line) #+\n  # (1 + year_day|line)\n\nlibrary(glmmTMB)\n\nfit_mixed = glmmTMB(model_mixed, data = df_train_mixed)\n\n\n\nExplore\nThe mixed model approach is nice because it is highly interpretable. We get both standard regression coefficients, and variance components to help us understand how the rest of the variance breaks down. For example, I would interpret the following that that line and weekend are the biggest contributors to the variability seen, and that we have high autocorrelation, as expected.\n\nlibrary(mixedup)\n\nsummarise_model(fit_mixed, digits = 4)\n\nextract_cor_structure(fit_mixed, which_cor = 'ar1')\n\nWe can visually inspect how well it matches the data. In the following the colored lines are the predictions, while the observed is gray. It looks like performance tapers for more recent time periods, and holiday effects are not as prevalent for some lines (e.g. yellow). The latter could be helped by adding a holiday:line random effect.\n\nlibrary(glmmTMB)\n\np_mixed = df_validate_mixed %&gt;% \n  droplevels() %&gt;% \n  mutate(pred = predict(fit_mixed, newdata = ., allow.new.levels=TRUE)) %&gt;%\n  mutate(date = as_date(date)) %&gt;% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .1) +\n  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +\n  facet_grid(rows = vars(line), scales = 'free_y') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'Mixed')\n\np_mixed\n\n\n\n\n\n\n\n\nAs before we can measure performance via yardstick. This model does appears to do very well.\n\n# validation\ndata.frame(\n  pred = predict(fit_mixed, newdata = df_validate_mixed, allow.new.levels = TRUE),\n  observed = df_validate_mixed$rides_scaled\n) %&gt;%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.189\n\n\nmae\nstandard\n0.110\n\n\nrsq\nstandard\n0.965\n\n\n\n\n\n\n\nFor more on autocorrelation structure in the mixed model setting, see my mixed model document here4."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#generalized-additive-models",
    "href": "posts/2021-05-time-series/index.html#generalized-additive-models",
    "title": "Exploring Time",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\n\nIntro\nWe can generalize mixed models even further to incorporate nonlinear components, which may include cyclic or other effects. Such models are typically referred to as generalized additive models (GAMs). AR processes themselves can be seen as a special case of gaussian processes, which can potentially be approximated via GAMs. As GAMs can accommodate spatial, temporal, nonlinear, and other effects, they are sometimes more generally referred to as structured additive regression models, or STARs.\n\n\nData Prep\nThe data prep for the GAM is the same as with the mixed model, so we’ll just use that data.\n\ndf_train_gam = df_train_mixed\n\ndf_validate_gam = df_validate_mixed\n\n\n\nModel\nWith data in place we are ready to conduct the model. We have numerous options for how we’d like to take this. However, as an example, I tried various smooths, but didn’t really see much difference, which is actually a good thing. For any further improvements we’d likely have to tweak the core model itself. I also use bam for a quicker result, but this isn’t really necessary, as it didn’t even take a minute to run with standard gam. As with the mixed model, we will use holiday as a random effect, but we add the holiday by line interaction since we saw that need. In addition, our year-day by line interaction should help some with the tailing off of more recent predictions.\n\nlibrary(mgcv)\n\n# for year, use year (numeric) or use year_fac, but for latter, it will not be\n# able to predict any year not in the training data unless you use\n# drop.unused.levels.\nmodel_gam = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  s(tmax_scaled) + \n  s(prcp_scaled) + \n  s(snow_scaled) +\n  s(red_line_modernization, line, bs = 're') +\n  s(holiday,  bs = 're') +\n  s(holiday, line,  bs = 're') +\n  s(year_fac, bs = 're') +      \n  s(day,  bs = 're') + \n  s(line, bs = 're') + \n  s(line, day, bs = 're') + \n  s(year_day, by = line, bs = c('ds', 'fs'))\n\n\n# will take a while!\n# fit_gam = gam(\n#   model_gam, \n#   data     = df_train_gam,\n#   drop.unused.levels = FALSE, \n#   method   = \"REML\"\n# )\n\n# fast even without parallel\nfit_gam = bam(\n  model_gam, \n  data     = df_train_gam,\n  drop.unused.levels = FALSE, \n  method   = \"fREML\",\n  discrete = TRUE     # will fit the model in a second rather than a couple seconds\n  # nthreads = 8,     # this option assumes a cluster is available. not necessary for this.\n)\n\n\n\nExplore\nAs with glmmTMB, I use a custom function to summarize the model, or extract different components from it. From the initial glance we can see things that we expect (e.g. line and weekend effects are large).\n\nmixedup::summarise_model(fit_gam)\n\n\nVariance Components:\n\n\n                       Group                 Effect Variance   SD SD_2.5      SD_97.5 Var_prop\n                 tmax_scaled              Intercept     0.01 0.09   0.04 2.100000e-01     0.01\n                 prcp_scaled              Intercept     0.00 0.01   0.00 2.000000e-02     0.00\n                 snow_scaled              Intercept     0.00 0.01   0.00 2.000000e-02     0.00\n                        line red_line_modernization     0.09 0.31   0.19 4.900000e-01     0.13\n                     holiday              Intercept     0.05 0.22   0.14 3.300000e-01     0.06\n                     holiday                   line     0.04 0.21   0.18 2.400000e-01     0.06\n                    year_fac              Intercept     0.00 0.06   0.04 1.000000e-01     0.01\n                         day              Intercept     0.00 0.00   0.00 4.162009e+69     0.00\n                        line              Intercept     0.49 0.70   0.42 1.150000e+00     0.65\n                        line                    day     0.05 0.22   0.18 2.700000e-01     0.06\n           year_day:lineblue              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n          year_day:linebrown              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n          year_day:linegreen              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:lineorange              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n           year_day:linepink              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:linepurple              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n year_day:linepurple_express              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n            year_day:linered              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:lineyellow              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n                    Residual                     NA     0.02 0.13   0.13 1.300000e-01     0.02\n\n\n\nFixed Effects:\n\n\n         Term Value   SE     t P_value Lower_2.5 Upper_97.5\n    Intercept -0.28 0.24 -1.14    0.25     -0.76       0.20\n   is_weekend -0.54 0.06 -8.27    0.00     -0.66      -0.41\n is_cubs_game  0.04 0.00 13.81    0.00      0.03       0.04\n  is_sox_game  0.01 0.00  3.81    0.00      0.00       0.02\n\n\nNow we can visualize test predictions broken about by line. The greater flexibility of the GAM for fixed and other effects allows it to follow the trends more easily than the standard linear mixed model approach.\n\n\n\n\n\n\n\n\n\nWe can also see improvement over our standard mixed model approach, and our best performance yet.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.140\n\n\nmae\nstandard\n0.087\n\n\nrsq\nstandard\n0.981"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#prophet",
    "href": "posts/2021-05-time-series/index.html#prophet",
    "title": "Exploring Time",
    "section": "Prophet",
    "text": "Prophet\n\nIntro\nProphet is an approach from Facebook that uses Stan to estimate a time series model taking various trends, seasonality, and other factors under consideration. By default, it only uses Stan for optimization (e.g. via ‘BFGS’), but you can switch to fully Bayesian if desired, and take advantage of all that the Bayesian approach has to offer.\n\n\nData Prep\nThe prophet package in R takes some getting used to. We have to have specific names for our variables, and unfortunately have to do extra work to incorporate categorical features. We can use recipes (like yardstick, part of the tidymodels ’verse) to set up the data (e.g. one-hot encoding).\n\nlibrary(prophet)\nlibrary(recipes)\n\ndf_train_prophet = df_train %&gt;% \n  arrange(date, line) %&gt;% \n  rename(y  = rides_scaled,\n         ds = date)\n\nrec = recipes::recipe(~., df_train_prophet)\n\ndf_train_prophet = rec %&gt;% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %&gt;% \n  prep(training = df_train_prophet) %&gt;% \n  bake(new_data = df_train_prophet) %&gt;% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\ndf_validate_prophet = df_validate %&gt;% \n  arrange(date, line)%&gt;%\n  rename(ds = date, y = rides_scaled)\n\nrec = recipe(~., df_validate_prophet)\n\ndf_validate_prophet = rec %&gt;% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %&gt;% \n  prep(training = df_train_prophet) %&gt;% \n  bake(new_data = df_validate_prophet) %&gt;% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\n\n\nModel\nWith data in place, we are ready to build the model. Note that later we will compare results to fable.prophet, which will mask some of the functions here, or vice versa depending on which you load first. I would suggest only doing the prophet model, or only doing the fable model, rather than trying to do both at the same time, to avoid any mix-up.\nAfter setting up the model, you have to add additional features in separate steps. Prophet has a nice way to incorporate holidays though. When you run this model, you may have to wait for a minute or so.\n\n# use prophet::prophet in case you have fable.prophet loaded also\nmodel_prophet = prophet::prophet(\n  holidays = generated_holidays %&gt;% filter(country == 'US'),\n  yearly.seasonality = FALSE,\n  seasonality.mode = \"multiplicative\",\n  changepoint.prior.scale = .5\n)\n\nline_names = c(\n  'blue',\n  'brown',\n  'green',\n  'orange',\n  'pink',\n  'purple',\n  'purple_express',\n  'red',\n  'yellow'\n)\n\npredictors = c(\n  'is_weekend',\n  'is_cubs_game',\n  'is_sox_game',\n  # 'is_holiday',\n  'tmax_scaled',\n  'prcp_scaled',\n  'snow_scaled',\n  line_names\n)\n\nfor (i in predictors) {\n  model_prophet = add_regressor(model_prophet, i, standardize = FALSE, mode = 'additive')\n}\n\nmodel_prophet = add_country_holidays(model_prophet, country_name = 'US')\n\nfit_prophet = fit.prophet(model_prophet, df = df_train_prophet)\n\nforecast = predict(fit_prophet, df_validate_prophet)\n\n\n\nExplore\nWe now visualize predictions as we did with the GAM. But one of the nice things with prophet is that you can plot the various parts of the model results via the plot method or prophet_plot_components (not shown). Unfortunately, our baseline effort seems to undersmooth our more popular lines (blue, red), and overreacts to some of the others (purple, yellow). This is because it’s not really changing the predictions according to line.\n\n\n\n\n\n\n\n\n\nWe can also assess performance as before, but note that prophet has it’s own cross-validation capabilities which would be better to utilize if this was your primary tool. Between the previous visualization and these metrics, our first stab doesn’t appear to do as well as the GAM, so you might like to go back and tweak things.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.280\n\n\nmae\nstandard\n0.198\n\n\nrsq\nstandard\n0.925"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#fable",
    "href": "posts/2021-05-time-series/index.html#fable",
    "title": "Exploring Time",
    "section": "Fable",
    "text": "Fable\n\nIntro\nI came across fable.prophet as a possibly easier way to engage prophet. It is an extension of fable and related packages, which are very useful for time series processing and analysis. Note that it is 0.1.0 version development, and hasn’t had much done with it in the past year, so your mileage may vary with regard to utility by the time you read this5. But with it we can specify the model in more of an R fashion, and we now don’t have as much data pre-processing.\n\n\nData Prep\nOne key difference using fable.prophet is that it works with tsibble objects, and thus must have unique date observations. We can do this by setting line as the key6.\n\nlibrary(fable.prophet)\n\ndf_train_fable = df_train_prophet %&gt;% \n  as_tsibble(index = ds, key = line)\n\ndf_validate_fable = df_validate_prophet %&gt;% \n  as_tsibble(index = ds, key = line)\n\nholidays_fable = holidays %&gt;% \n  filter(country == 'US') %&gt;% \n  mutate(ds = as_date(ds)) %&gt;% \n  as_tsibble()\n\n\n\nModel\nBeyond this we use functions within our formula to set the various components. With line as the key, fable is actually running separate prophet models for each line, and we can do so in parallel if desired.\n\nmodel_prophet = fable.prophet::prophet(\n  y ~ \n    growth('linear', changepoint_prior_scale = 0.5) +\n    season(\"week\", type = \"multiplicative\") +\n    holiday(holidays_fable) +\n    xreg(\n      is_weekend,\n      is_cubs_game,\n      is_sox_game,\n      # is_holiday,\n      tmax_scaled,\n      prcp_scaled,\n      snow_scaled\n    ) \n)\n\n# library(future)\n# plan(multisession)\n\n# furrr is used under the hood, and though it wants a seed, it doesn't\n# automatically use one so will give warnings. I don't think it can be passed\n# via the model function, so expect to see ignorable warnings (suppressed here).\n\nfit_fable = model(df_train_fable, mdl = model_prophet)\n\nforecast_fable = fit_fable %&gt;% \n  forecast(df_validate_fable) \n\n# plan(sequential)\n\n\n\nExplore\nWith fable.prophet visualization, we have the more automatic plots, but again we’ll stick with the basic validation plot we’ve been doing.\n\ncomponents(fit_fable)\ncomponents(fit_fable) %&gt;%\n  autoplot()\nforecast_fable %&gt;%\n  autoplot(level = 95, color = '#ff5500')\n\nThis model does well, and better than basic prophet, but we can see its limitations, for example, with the yellow line, and more recent ridership in general.\n\n\n\n\n\n\n\n\n\nAnd we check performance as before. The fable model is doing almost as well as our GAM approach did.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.154\n\n\nmae\nstandard\n0.092\n\n\nrsq\nstandard\n0.979\n\n\n\n\n\n\n\nOne nice thing about the fable approach is its internal performance metrics, which are easily obtained. It will give us results for each line7, validation data results shown. We see that we have more error for the popular lines as before, but in terms of percentage error, the other lines are showing more difficulty. You can find out more about the additional metrics available here.\n\naccuracy(fit_fable)\naccuracy(forecast_fable, df_validate_fable)\n\n\n\n\n\n\n.model\nline\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nACF1\n\n\n\n\nmdl\nblue\nTest\n0.083\n0.251\n0.180\n-27.007\n62.199\n0.641\n\n\nmdl\nbrown\nTest\n0.001\n0.126\n0.084\n-31.034\n88.711\n0.621\n\n\nmdl\ngreen\nTest\n0.088\n0.118\n0.100\n-105.627\n112.000\n0.660\n\n\nmdl\norange\nTest\n0.035\n0.084\n0.063\n-43.228\n60.244\n0.638\n\n\nmdl\npink\nTest\n0.058\n0.089\n0.072\n-18.386\n20.452\n0.675\n\n\nmdl\npurple\nTest\n0.006\n0.012\n0.009\n-0.633\n0.982\n0.503\n\n\nmdl\npurple_express\nTest\n0.049\n0.107\n0.083\n-41.459\n218.861\n0.752\n\n\nmdl\nred\nTest\n0.045\n0.297\n0.211\n-3.003\n14.880\n0.598\n\n\nmdl\nyellow\nTest\n-0.027\n0.030\n0.027\n2.892\n2.912\n0.726\n\n\n\n\n\n\n\nThe fable results suggests what we already knew from our GAM and mixed model approach, that interactions of the series with line are important. We weren’t easily able to do this with the default prophet (it would likely require adding time x line interaction regresssors), so it’s nice that we have the option here."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#gbm",
    "href": "posts/2021-05-time-series/index.html#gbm",
    "title": "Exploring Time",
    "section": "GBM",
    "text": "GBM\n\nIntro\nThis part is actually new from when I first wrote up this post over a year ago. I basically wanted to test if a boosting approach would work decently out of the box without doing anything special regarding the structure of the data. I don’t add it to the summary visualizations, but provide the standard results here.\n\n\nData Prep\nI’ll use lightgbm which I’ve been increasingly using of late. It requires a matrix object for input, and so some additional processing as well.\n\nlibrary(lightgbm)\n\n# lightgbm only accepts a matrix input\ndf_train_lgb_init = df_train %&gt;% \n  select(\n    rides_scaled,\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %&gt;% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  )\n\nX_train = as.matrix(df_train_lgb_init %&gt;% select(-rides_scaled))\n\nX_test  = df_validate %&gt;% \n  select(\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %&gt;% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  ) %&gt;% \n  as.matrix()\n\n\ndf_train_lgb = lgb.Dataset(\n  X_train,\n  label = df_train_lgb_init$rides_scaled,\n  categorical_feature = c(\n    'is_weekday',\n    'is_holiday',\n    'is_cubs_game',\n    'is_sox_game',\n    'line',\n    'year',\n    'month'\n  )\n)\n\ndf_test_lgb  = lgb.Dataset.create.valid(\n  df_train_lgb, \n  X_test,\n  label = df_validate$rides_scaled\n)\n\n\n\nModel\nTypically we would perform some sort of search over the (many) parameters available to tweak with lightgbm, like the number of trees, learning rate, regularizer parameters and more. I ignore that, but I did fiddle with the learning rate and bumped up the nrounds (trees), but that’s it.\n\nparams = list(\n  objective       = \"regression\"\n  , metric        = \"l2\"\n  , min_data      = 10L\n  , learning_rate = .01\n)\n\nfit_gbm = lgb.train(\n  params    = params\n  , data    = df_train_lgb\n  , nrounds = 2000L\n)\n\n\n\nExplore\nSome may be surprised at how well this does, but regular users of boosting probably are not. We didn’t have to do much and it’s already the best performing model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.130\n\n\nmae\nstandard\n0.076\n\n\nrsq\nstandard\n0.984"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#torch",
    "href": "posts/2021-05-time-series/index.html#torch",
    "title": "Exploring Time",
    "section": "Torch",
    "text": "Torch\nAt this point we have a collection of models that are still relatively interpretable, and mostly within our standard regression model framework. It’s good to see them able to perform very well without too much complexity. However, we still have other methods available that would be more computationally demanding, are more opaque in operations, but which would potentially provide the most accurate forecasts. For this we turn to using PyTorch, which is now available via the torch package in R8.\nIn using torch, we’re going to follow the demo series at the RStudio AI blog 9. It shows in four parts how to use a recurrent neural network. In their example, they use a data set for a single series with (summarized) daily values, similar to our daily counts here. We will use the final model demonstrated in the series, a soi disant seq2seq model that includes an attention mechanism. More detail can be found here. The conceptual gist of the model can be described as taking a set of time points to predict another set of future time points, and doing so for all points in the series.\nTo be clear, they only use a single series, no other information (e.g. additional regressors). So we will do the same, coming full circle to what we started out with, just looking at daily ridership- a single time series for the red line.\n\nData\nAs usual we’ll need some data prep, both for initial training-test split creation, but also specifically for usage with Torch.\n\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(torch)\n\n\ndf_train_torch = df_train %&gt;%\n  filter(line == 'red', year &lt; 2017) %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ndf_validate_torch = df_validate %&gt;%\n  filter(line == 'red', year &gt;= 2017) %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ndf_test_torch = df_validate %&gt;%\n  filter(line == 'red', date &gt; '2017-12-24') %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ntrain_mean = mean(df_train_torch)\ntrain_sd   = sd(df_train_torch)\n\n\n\nTorch data\nFor our data, we will use a week behind lag to predict the following week. This seems appropriate for this problem, but for any particular time series problem we’d want to probably think hard about this and/or test different settings.\n\nn_timesteps = 7    # we use a week instead of 14 days in original blog\nn_forecast  = 7    # look ahead one week\n\n\ncta_dataset &lt;- dataset(\n  name = \"cta_dataset\",\n\n  initialize = function(x, n_timesteps, sample_frac = 1) {\n\n    self$n_timesteps &lt;- n_timesteps\n    self$x &lt;- torch_tensor((x - train_mean) / train_sd)\n\n    n &lt;- length(self$x) - self$n_timesteps - 1\n\n    self$starts &lt;- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n\n  },\n\n  .getitem = function(i) {\n\n    start &lt;- self$starts[i]\n    end &lt;- start + self$n_timesteps - 1\n    lag &lt;- 1\n\n    list(\n      x = self$x[start:end],\n      y = self$x[(start+lag):(end+lag)]$squeeze(2)\n    )\n\n  },\n\n  .length = function() {\n    length(self$starts)\n  }\n)\n\nbatch_size = 32\n\ntrain_ds = cta_dataset(df_train_torch, n_timesteps)\ntrain_dl = dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds = cta_dataset(df_validate_torch, n_timesteps)\nvalid_dl = dataloader(valid_ds, batch_size = batch_size)\n\ntest_ds  = cta_dataset(df_test_torch, n_timesteps)\ntest_dl  = dataloader(test_ds, batch_size = 1)\n\n\n\nModel\nI leave it to the blog for details, but briefly, there are four components to the model:\n\nEncoder: takes input, and produces outputs and states via RNN\nDecoder: takes the last predicted value as input and current context to make a new prediction\nSeq2Seq: essentially encodes once, and calls the decoder in a loop\nAttention: allows output from the encoder at a specific time point to provide ‘context’ for the decoder\n\n\nnet =\n  seq2seq_module(\n    \"gru\",\n    input_size     = 1,\n    hidden_size    = 32,\n    attention_type = \"multiplicative\",\n    attention_size = 8,\n    n_forecast     = n_forecast\n  )\n\nb = dataloader_make_iter(train_dl) %&gt;% dataloader_next()\n\nnet(b$x, b$y, teacher_forcing_ratio = 1)\n\n\n\nTraining\nWith data in place, we’re ready to train the model. For the most part, not much is going on here that would be different from other deep learning situations, e.g. choosing an optimizer, number of epochs, etc. We’ll use mean squared error as our loss, and I create an object to store the validation loss over the epochs of training. I played around with it a bit, and you’re probably not going to see much improvement after letting it go for 100 epochs.\n\noptimizer = optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs = 100\n\ntrain_batch &lt;- function(b, teacher_forcing_ratio) {\n\n  optimizer$zero_grad()\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio)\n  target &lt;- b$y\n\n  loss &lt;- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n  loss$backward()\n  optimizer$step()\n\n  loss$item()\n\n}\n\nvalid_batch &lt;- function(b, teacher_forcing_ratio = 0) {\n\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio)\n  target &lt;- b$y\n\n  loss &lt;- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n\n  loss$item()\n\n}\n\n\nall_valid_loss = c()\n\nfor (epoch in 1:num_epochs) {\n\n  net$train()\n  train_loss &lt;- c()\n\n  coro::loop(for (b in train_dl) {\n    loss &lt;- train_batch(b, teacher_forcing_ratio = 0.0)\n    train_loss &lt;- c(train_loss, loss)\n  })\n\n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n\n  net$eval()\n  valid_loss &lt;- c()\n\n  coro::loop(for (b in valid_dl) {\n    loss &lt;- valid_batch(b)\n    valid_loss &lt;- c(valid_loss, loss)\n  })\n  \n  all_valid_loss = c(all_valid_loss, mean(valid_loss))\n\n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\n\nEvaluations\n\nnet$eval()\n\ntest_preds = vector(mode = \"list\", length = length(test_dl))\n\ni = 1\n\ncoro::loop(for (b in test_dl) {\n\n  if (i %% 100 == 0)\n    print(i)\n\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio = 0)\n  preds &lt;- as.numeric(output)\n\n  test_preds[[i]] &lt;- preds\n  i &lt;&lt;- i + 1\n})\n\nFor this visualization, we do things a little different. In our current setup, we have 7 timesteps predicting 7 day windows. We started our test set at the beginning of December so that the first prediction is January first, and then proceeds accordingly.\n\n# same as test\ndf_eval_torch = df_validate %&gt;%\n  filter(line == 'red', date &gt; '2017-12-01') %&gt;%\n  select(rides_scaled, date) %&gt;%\n  as_tsibble()\n\ntest_preds_plot = vector(mode = \"list\", length = length(test_preds))\n\nfor (i in 1:(length(test_preds_plot)-  n_forecast)) {\n  test_preds_plot[[i]] =\n    data.frame(\n      pred = c(\n        rep(NA, n_timesteps + (i - 1)),\n        test_preds[[i]] * train_sd + train_mean,\n        rep(NA, nrow(df_eval_torch) - (i - 1) - n_timesteps - n_forecast)\n      )\n    )\n}\n\ndf_eval_torch_plot0 =\n  bind_cols(df_eval_torch, bind_cols(test_preds_plot))\n\nA visualization of the predictions makes this more clear. Each 7 day segment is making predictions for the next 7 days. The following predictions are for the last two months, with each column a set of 7 predictions for that time point.\n\n\n\n\n\n\n\n\n\nSo for our red line plot, we’ll just use the average prediction at each date to make it comparable to the other plots. In general it looks to be doing okay, even armed with no contextual information. Certainly better than the base ARIMA plot.\n\n\n\n\n\n\n\n\n\nHowever, we can see that there is much information lost just adhering to the series alone.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.804\n\n\nmae\nstandard\n0.584\n\n\nrsq\nstandard\n0.120"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#all",
    "href": "posts/2021-05-time-series/index.html#all",
    "title": "Exploring Time",
    "section": "All",
    "text": "All"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#summary",
    "href": "posts/2021-05-time-series/index.html#summary",
    "title": "Exploring Time",
    "section": "Summary",
    "text": "Summary\n\nARIMA: no real reason to still be doing such a simplified model\nMixed Model: may be just what you need, but may lack in other settings\nGAM: great, more viable than some might suspect, easy implementation\nProphet/Fable: Prophet needs notable work out of the box, though Fable saves you some of that work, and did great in this situation via by-group models\nGBM: can it really be this easy?\nTorch: pretty good even with minimal information\n\nTo get some information on what Torch would do at the next level, i.e. adding additional features and other considerations, see Cody’s post."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#footnotes",
    "href": "posts/2021-05-time-series/index.html#footnotes",
    "title": "Exploring Time",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is also the Purple express line, which is very irregular compared to the others.↩︎\nTechnically we should scale the test set using the mean/sd of the training set, and though with very large data this should not matter, for time series it’s a particular concern as data can ‘shift’ over time.↩︎\nThis follows Bolker’s demo.↩︎\nI always appreciated the depiction of this topic in West, Welch, and Galecki (2022) quite a bit.↩︎\nA year plus later after that statement, it still hasn’t gone beyond 0.1.0, so I don’t think this will continue to be useful for very long. Unfortunate, but honestly, it’s not clear prophet&lt;/span itself can do much better than many other tools.↩︎\nfable.prophet may have a bug enabling the holidays functionality with parallel, so you can just use the original holiday column if you do so (single core doesn’t take too long).↩︎\nWe can also do this with our previous method with a split-by-apply approach. You would obtain the same results, so this serves as a nice supplement to our ‘overall’ metrics.↩︎\nFor the basics of using PyTorch via R, including installation, see the RStudio post.↩︎\nThe blog code actually has several issues, but the github repo should work fine and is what is followed for this demo.↩︎"
  },
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "Content",
    "section": "",
    "text": "Here you’ll find documents of varying technical degree covering things of interest to me, or which I think will be interesting to those I engage with. Generally you’ll find a mix of demonstrations on statistical and machine learning topics, programming, and data processing and visualization. Most focus on application in R as that’s what I used to primarily program with, but you’ll find plenty of Python demonstrations as well."
  },
  {
    "objectID": "documents.html#book",
    "href": "documents.html#book",
    "title": "Content",
    "section": "Book",
    "text": "Book\n Models Demystified\nThis book should be out on CRC Press in 2025. It is a comprehensive overview of the statistical and machine learning landscape, along with related topics. It is designed to be accessible to those with a basic understanding of statistics, but also to provide a deeper dive into the concepts for those with more experience. It covers an array of useful models from simple linear regression to deep learning. The book is designed to be a reference for those who want to understand the models and techniques they are using, and to provide a guide for those who want to learn new techniques."
  },
  {
    "objectID": "documents.html#long-form",
    "href": "documents.html#long-form",
    "title": "Content",
    "section": "Long Form",
    "text": "Long Form\n Mixed Models with R\nThis document focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R. It covers topics such as loss functions, cross-validation, regularization, and bias-variance trade-off, techniques such as penalized regression, random forests, and neural nets, and more.  \n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves, IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc."
  },
  {
    "objectID": "documents.html#blog-posts",
    "href": "documents.html#blog-posts",
    "title": "Content",
    "section": "Blog Posts",
    "text": "Blog Posts\n\nDeep Linear Models\nExploring Time\nProgramming Odds and Ends\nDeep Learning for Tabular Data II\nThe Double Descent Phenomenon\nDeep Learning for Tabular Data\nPractical Bayesian Analysis (I, II)\nMicro-macro Models\nPredictions with an Offset\nFactor Analysis and Related Methods\nConvergence Problems in Mixed Models\nCategorical Random Effects\nMixed Models for Big Data\nFractional Regression\nGroup Comparisons in SEM\n\nEmpirical Bayes\nShrinkage in Mixed Models\n\nMediation Models"
  },
  {
    "objectID": "documents.html#statistical",
    "href": "documents.html#statistical",
    "title": "Content",
    "section": "Statistical",
    "text": "Statistical\n\nModels By Example\n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n\n\nModeling in R\n Data Modeling in R\nThis document demonstrates a wide array of statistical and other models in R. Generic code is provided for standard regression, mixed, additive, survival, and latent variable models, principal components, factor analysis, SEM, cluster analysis, time series, spatial models, zero-altered models, text analysis, Bayesian analysis, machine learning and more. \nThe document is designed for newcomers to R, whether in a statistical sense, or just a programming one. It also should appeal to those working in other packages who are curious how to do the same sorts of things in R.\n\n\nBayesian\n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n MCMC algorithms\nList of MCMC algorithms with brief descriptions.  \n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n\n\nMixed Models\n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Mixed Models Overview\nAn overview that introduces mixed models for those with varying technical/statistical backgrounds.  \n Mixed Models Introduction\nA non-technical document to introduce mixed models for those who have used ANOVA.  \n Clustered Data Situations\nA comparison of standard models, cluster robust standard errors, fixed effect models, mixed models (random effects models), generalized estimating equations (GEE), and latent growth curve models for dealing with clustered data (e.g. longitudinal, hierarchical etc.).  \n Mixed Model Estimation\nDemonstration of mixed models via maximum likelihood and link to additive models. \n Mixed and Growth Curve Models\nA comparison of the mixed model vs. latent variable approach for longitudinal data (growth curve models), with simulation of performance in situations of small sample sizes. \n\n\nLatent Variables/SEM\n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The initial workshop was given to an audience of varying background and statistical skill, but the document should be useful to anyone interested in the techniques covered. It is completely R-based, with special emphasis on the lavaan package. It will continue to be a work in progress, particularly the sections after the SEM chapter. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves. Topics I hope to provide overviews of in the future include other latent variable techniques/extensions such as IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc.  \n Factor Analysis and Related Methods\nThis document gives a brief overview of many matrix factorization, dimension reduction, and latent variable techniques. Here is a list:  \n\nPrincipal Components Analysis - Factor Analysis - Probabilistic Components Analysis - Non-negative Matrix Factorization - Latent Dirichlet Allocation - Structural Equation Modeling - Item Response Theory - Independent Components Analysis - Multidimensional Scaling - t-Distributed Stochastic Neighbor Embedding (t-sne) - Recommender Systems - Hidden Markov Models - Random Effects Models - Bayesian Approaches - Mixture Models - k-means Cluster Analysis - Hierarchical Cluster Analysis - Latent Class Analysis\n\n Latent Variables, Sum Scores, Single Items\nIt is very common to use sum scores of several variables as a single entity to be used in subsequent analysis (e.g. a regression model). Some may even more use a single variable even though multiple indicators are available. Assuming the multiple measures indicate a latent construct, such typical practice would be problematic relative to using estimated factor scores, either constructed as part of a two-stage process or as part of a structural equation model. This document covers simulations in which comparisons in performance are made between latent variable and sum score or single item approaches.  \n Lord’s Paradox\nSummary of Pearl’s 2014 and 2013 technical reports on some modeling situations such as Lord’s Paradox and Simpson’s Paradox that lead to surprising results that are initially at odds with our intuition. Looks particularly at the issue of change scores vs. controlling for baseline.  \n\n\nOther Statistical\n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R.  \n Reliability\nAn unfinished document that ties together some ideas regarding the statistical and conceptual notion of reliability..  \n Fractional Regression\nA quick primer regarding data between zero and one, including zero and one.  \n Categorical Regression Models\nAn overview of regression models for binary, multinomial, and ordinal outcomes, with connections among various types of models.  \n Topic Modeling Demo\nA demonstration of Latent Dirichlet Allocation for topic modeling in R.  \n Comparing Measures of Dependency\nA summary of articles that look at various measures of dependency Pearson’s r, Spearman’s rho, and Hoeffding’s D, and newer ones such as Distance Correlation and Maximal Information Coefficient."
  },
  {
    "objectID": "documents.html#programming",
    "href": "documents.html#programming",
    "title": "Content",
    "section": "Programming",
    "text": "Programming\nCheck the workshops section also for programming-related content.\nPractical Data Science (more details about this document below). The intention was to cover five key topics: basic information processing, programming, modeling, visualization, and publication/presentation.\nExploratory Data Analysis Tools An overview of various packages useful for quick exploration of data.\n FastR\nA notebook on how to make R faster before or irrespective of the machinery used. Topics include avoiding loops, vectorization, faster I/O etc.  \n Engaging the Web with R\nDocument regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach."
  },
  {
    "objectID": "documents.html#workshops",
    "href": "documents.html#workshops",
    "title": "Content",
    "section": "Workshops",
    "text": "Workshops\nI used to give workshops regularly when I worked in academia. Although they generally won’t age well, I have kept the content here for any that might be interested."
  },
  {
    "objectID": "documents.html#miscellaneous",
    "href": "documents.html#miscellaneous",
    "title": "Content",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n R for Social Science\nThis was put together in a couple of days under duress, and is put here in case someone can find it useful (and thus make the time spent on it not completely wasted)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What I do\nI am currently a Senior Machine Learning Scientist for OneSix, where I engage with client projects across multiple industries, helping them get the most from their data to maximize customer satisfaction, increase profitability, and explore new data-enabled territory. In my role, I strive to use the best tools to translate complex results into actionable insights.\nThroughout my career in data science, I have navigated a vast landscape of modeling, visualizing, and understanding data. I have conducted causal inference for marketing campaigns, classified biomedical images to detect pathology, analyzed text to uncover political sentiment, and explored baboon survival rates based on their social status. My experience spans dozens of industries and academic disciplines, helping clients and researchers unlock the full potential of their data.\nAdditionally, I have a strong background in teaching, writing, and conducting workshops on these topics, empowering others to gain expertise and become self-sufficient. Recently, I authored a book - Models Demystified - which offers a comprehensive overview of the statistical and machine learning landscape, along with related topics. It will be available from CRC Press in 2025.\nI am passionate about doing quality work that answers the questions at hand. What drew me to the world of data science and keeps my interest is that it frees me to engage in many different domains, and it provides a great many tools with which to discover more about the things we humans are most interested in. My CV can be found here.\n\n\nPersonal\nI was born and raised in Texas, but have lived a good chunk of my life in the Midwest. I currently live in Ann Arbor with my wife, our daughter, and our dog Sulu. We tend to keep things simple, and like to go on walks and hikes in the neighborhood and surrounding areas of A2, take the occasional trip to some place new, and make big deals out of the little things.\n\n\nAcademic Background\nWhile my interest is in data science generally, I started off majoring in psychology and philosophy as an undergraduate, and eventually obtained a Ph.D. in Experimental Psychology. During graduate school, I became interested in statistics for practical reasons, eventually choosing it as a concentration, and I also started consulting at that time. That turned out to be a good fit for me, and I’ve been exploring and analyzing data ever since.\n\n\n\n\n\n\nMichael Clark\n\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "A lot of the following are things I do for fun or personal interest and development. Here you will find a summary of stuff that I’ve done in the past, but for my latest efforts, check me out on GitHub. Almost all of my work nowadays is for clients and internal use that is not publicly available."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Code",
    "section": "R Packages",
    "text": "R Packages\nI haven’t had time to do much with these anymore, but they are still available GitHub and still mostly functional.\n mixedup\n A package for extracting results from mixed models from several packages that are easy to use and viable for presentation.  \n confusionMatrix\n Given predictions and a target variable, this package provides a wealth of summary statistics that can be calculated from a single confusion matrix, and return tidy results with as few dependencies as possible.  \n 198R\n R with its collar flipped, or the movie Drive if it was all about R programming, writing R code on a beach in Miami as the sun sets, R wearing sunglasses at night, R asking you to take it home tonight because it doesn’t want to let you go until you see the light, Countach &gt; Testarrosa, but Delorean &gt; all except R, R if Automan had lasted longer than 1 season, driving down Mulholland Dr. at night thinking about R code, R playing a cello at the end of a dock on a lake before taking a ride in a badass helicopter, R with its hair all done up with Aquanet… You get the idea.  \n visibly\n This is a collection of functions that I use related to visualization, e.g. the palette generating function (create_palette) and clean visualization themes for ggplot and plotly. In addition, there are visualizations specific to mixed and additive models.  \n 538 football club rankings\n This package grabs the table located at 538, and additionally does some summary by league and country.  \n gammit\n The package provides a set of functions to aid using mgcv (possibly solely) for mixed models. Mostly superseded by mixedup.  \n tidyext\n This package is a collection of functions that do the things I commonly need to do with data while doing other processing within the dataverse. Most of the functionality is now standard in tidyverse, so this is essentially deprecated.  \n lazerhawk\n While the name is more or less explanatory, to clarify, this is a package of miscellaneous functions that were mostly useful to me. Now deprecated.  \nIn addition to these, though they are not publicly available, I’ve created even more involved packages for specific project work."
  },
  {
    "objectID": "code.html#code-snippets",
    "href": "code.html#code-snippets",
    "title": "Code",
    "section": "Code Snippets",
    "text": "Code Snippets\nThe vast majority of these code snippets are conceptual demonstrations of more complicated models. The audience was generally faculty, researchers, and graduate students in applied fields who, like I did, want to go beyond their basic statistical training. However, I hope it helps anyone who happens to stumble across it. I don’t really update this page anymore, as I’ve cleaned and moved much of these over to Model Estimation by Example, so I would look for something you see here in the corresponding chapter of that document. In general, you can find all of my code at GitHub.\n\n\nModel Fitting\nstandard linear regression, standard logistic regression, penalized regression, lasso regression, ridge regression, newton and IRLS, nelder-mead (Python) (R), gradient descent (stochastic), bivariate probit, heckman selection, tobit, naive bayes, multinomial regression, ordinal regression, quantile regression, hurdle poisson, hurdle negbin, zero-inflated poisson, zero-inflated negbin, Cox survival, confirmatory factor analysis, Markov model, hidden Markov model (R) (Python), stochastic volatility, extreme learning machine, Chinese restaurant process, Indian buffet process, One-line models (an exercise), …\n\nMixed models\none factor random effects (R) (Julia) (Matlab), two factor random effects (R) (Julia) (Matlab), mixed model via ML, mixed model, mixed model with correlated random effects, See the documents section for more…\n\n\nBayesian\nBEST t-test, linear regression (Compare with BUGS version, JAGS), mixed model, mixed model with correlated random effects, beta regression, mixed model with beta response (Stan) (JAGS), mixture model, topic model, multinomial models, multilevel mediation, variational bayes regression, gaussian process, horseshoe prior, item response theory, …\n\n\nEM\nEM mixture univariate, EM mixture multivariate, EM probit, EM pca, EM probabilistic pca, EM state space model\n\n\nWiggly\n\nGaussian processses\nGaussian Process noisy, Gaussian Process noise-free, reproducing kernel hilbert space regression, Bayesian Gaussian process, …\n\n\nAdditive models\ncubic spline, …\n\n\n\n\nProgramming Shenanigans\nThis is old stuff I was doing while learning programming languages. Fun at the time, but mostly useless.\nFizzBuzz test (R) (julia) (Python), Reverse a string recursively (R) (Python), Recursive Word Wrap (R) (Python), calculate compound interest recursively, get US Congress roll call data, Scrape xkcd (R) (Python), Shakespearean Insulter, spurious correlation with ratios, R matrix speedups, …"
  },
  {
    "objectID": "code.html#shiny-apps",
    "href": "code.html#shiny-apps",
    "title": "Code",
    "section": "Shiny Apps",
    "text": "Shiny Apps\nFun at the time, these were some my forays into the Shiny world.\n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n Historical Football Data\nMy annual dive into the frustration of Shiny has produced an app to explore historical football/soccer data for various European leagues (Premier, La Liga, Serie A etc.) and MLS. One can create tables for a given country/league and year selected, with some leagues having multiple tiers available, and stretching back many decades. Beyond that, one can get a specific team’s historical finishing position, league games for a specific season, all-time tables, and all-time head-to-head results (within a league).  \n Last Statements of the Texas Executed\nA demonstration of both text analysis and literate programming/document generation with a dynamic and interactive research document. The texts regard the last statements of offenders in Texas. Sadly no longer functional, as the shiny environment appears to not have been preserved. It was actually a very nifty demonstration for its time.  \n A History of Tornados\nBecause I had too much time on my hands and wanted to try out the dashboard feature of R Markdown. Maps tornado activity from 1950-2015. (Archived)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models Demystified",
    "section": "",
    "text": "Some News for the New Year\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nLong time no see…\n\n\n\n\n\n\nmiscellaneous\n\n\n\nNew modeling book under way! \n\n\n\n\n\nMay 20, 2024\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nStuff Going On\n\n\n\n\n\n\nmiscellaneous\n\n\n\nPenalty kicks, class imbalance, tabular deep learning, industry and academia \n\n\n\n\n\nMar 10, 2023\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Linear Models\n\n\n\n\n\n\ndeep learning\n\n\nboosting\n\n\nGLM\n\n\nregression\n\n\nmachine learning\n\n\n\nA demonstration using pytorch \n\n\n\n\n\nOct 10, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Time\n\n\n\n\n\n\nmixed models\n\n\nGAM\n\n\nboosting\n\n\ntime series\n\n\ndeep learning\n\n\n\nDemonstrating some times series approaches\n\n\n\n\n\nAug 10, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning for Tabular Data\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nA continuing exploration \n\n\n\n\n\nMay 1, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Descent\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nRethinking what we thought we knew. \n\n\n\n\n\nNov 13, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nThis is definitely not all you need\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nA summary of findings regarding deep learning for tabular data. \n\n\n\n\n\nJul 19, 2021\n\n\nMichael Clark\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Models {Demystified}},\n  url = {https://m-clark.github.io/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Models Demystified.” n.d. https://m-clark.github.io/."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html",
    "href": "posts/2021-07-15-dl-for-tabular/index.html",
    "title": "This is definitely not all you need",
    "section": "",
    "text": "I’ve been a little perplexed at the lack of attention of deep learning (DL) toward what I consider to be ‘default’ data in my world, often referred to as tabular data, where typically we have a two dimensional input of observations (rows) and features (columns) and inputs are of varying type, scale and source. Despite the ubiquity of such data in data science generally, and despite momentous advances in areas like computer vision and natural language processing, at this time, it’s not very clear what the status of DL for tabular data is.\nThere have been developments in the area recently though, with some modeling approaches, such as TabNet, gaining traction. In June of 2021, I actually came across three papers on Arxiv that were making related claims about the efficacy of DL for tabular data. As in many academic and practical (and life) pursuits, results of these studies are nuanced, so I thought I’d help myself and others by summarizing here."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#motivation",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#motivation",
    "title": "This is definitely not all you need",
    "section": "",
    "text": "I’ve been a little perplexed at the lack of attention of deep learning (DL) toward what I consider to be ‘default’ data in my world, often referred to as tabular data, where typically we have a two dimensional input of observations (rows) and features (columns) and inputs are of varying type, scale and source. Despite the ubiquity of such data in data science generally, and despite momentous advances in areas like computer vision and natural language processing, at this time, it’s not very clear what the status of DL for tabular data is.\nThere have been developments in the area recently though, with some modeling approaches, such as TabNet, gaining traction. In June of 2021, I actually came across three papers on Arxiv that were making related claims about the efficacy of DL for tabular data. As in many academic and practical (and life) pursuits, results of these studies are nuanced, so I thought I’d help myself and others by summarizing here."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#goal",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#goal",
    "title": "This is definitely not all you need",
    "section": "Goal",
    "text": "Goal\nI want to know if, e.g. time and/or resources are limited, whether it will be worth diving into a DL model if I have a satisfactory simpler/easier one ready to implement that does pretty well. Perhaps this answer is already, ‘if it ain’t broke, don’t fix it’, but given the advancements in other data domains, it would be good to assess what the current state of DL with tabular data is."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#caveats",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#caveats",
    "title": "This is definitely not all you need",
    "section": "Caveats",
    "text": "Caveats\n\nI’m not going to do more than give a cursory summary of the articles, and provide no in-depth explanation of the models. For more detail, see the corresponding articles and references for the models therein. You are not going to learn how to use TabNet, NODE, transformers, etc., for tabular data.\nThere are other decent articles on the topic not covered here. Some are referenced in these more recent offerings, so feel free to peruse."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#quick-take",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#quick-take",
    "title": "This is definitely not all you need",
    "section": "Quick Take",
    "text": "Quick Take\nIn case you don’t want any detail, here’s a quick summary based on my impressions from these articles. Right now, if you want to use DL on tabular data, don’t make a fuss of it. A simple architecture, even a standard multi-layer perceptron, will likely do as well as more complicated ones. In general though, the amount of effort put into prep/tuning may not be worth it for many typical tabular data settings, for example, relative to a suitably flexible statistical model (e.g. GAMM) or a default fast boosting implementation like XGBoost. However, DL models are already thinking ‘big data’, so for very large data situations, a DL model might make a great choice, as others may not be computationally very viable. It also will not be surprising at all that in the near future some big hurdle may be overcome as we saw with DL applications in other fields, in which case some form of DL may be ‘all you need’.\nNow, on to the rest!"
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#tabular-data-deep-learning-is-not-all-you-need",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#tabular-data-deep-learning-is-not-all-you-need",
    "title": "This is definitely not all you need",
    "section": "Tabular Data: Deep Learning is Not All You Need",
    "text": "Tabular Data: Deep Learning is Not All You Need\n\nPaper Info\n\nWho: Shwartz-Ziv & Armon\nWhere: Intel\nWhen: 2021-06-21 V1\nArxiv Link\n\n\n\nFrom the Abstract\n\nWe analyze the deep models proposed in four recent papers across eleven datasets, nine of which were used in these papers, to answer these questions. We show that in most cases, each model performs best on the datasets used in its respective paper but significantly worse on other datasets. Moreover, our study shows that XGBoost (Chen and Guestrin, 2016) usually outperforms the deep models on these datasets. Furthermore, we demonstrate that the hyperparameter search process was much shorter for XGBoost.\n\n\n\nOverview\nFor each model they used the data that was implemented in the original model papers by the authors (e.g. the dataset used in the TabNet article), and also used their suggested parameter settings. They tested all the models against their own data, plus the other papers’ data, plus two additional data sets that were not used in any of the original papers.\n\n\nData\nThey use eleven total datasets. Nine datasets are those used in the original papers on TabNet, DNF-Net, and NODE, drawing three datasets from each paper. Additionally, Shwartz-Ziv & Armon use two Kaggle datasets not used in any of those papers. Sample sizes ranged from 7k to 1M, 10-2000 features, with two being numeric targets, while the other target variables ranged from 2-7 classes. Datasets are described in detail in the paper along with links to the source (all publicly available).\n\n\nModels Explored\nBrief summaries of the DL models are found in the paper.\n\nXGBoost\nTabNet\nNeural Oblivious Decision Ensembles (NODE)\nDNF-Net\n1D-CNN\n\n\n\nQuick Summary\n\nNot counting the ensemble methods…\n\nTabNet did best on all of its own data sets, but was not the best model on any other.\nNODE each did best on 2 of its own 3 data sets, but not on any other.\nDNF-Net best on one of its own 3 data sets, but not on any other.\nXGBoost was best on the remaining 5 datasets.\n\n\n\nCounting the ensemble methods…\n\nTabNet did best on 2 of its own 3 data sets, but was not the best model on any other.\nDNF-Net and NODE each did best on one of its own 3 data sets, but not on any other.\nXGBoost was best on one dataset.\n\nOf those, XGB was notably better on ‘unseen’ data, and comparable to the best performing ensemble. A simple ensemble was also very performant. From the paper:\n\nThe ensemble of all the models was the best model with 2.32% average relative increase, XGBoost was the second best with 3.4%, 1D-CNN had 7.5%, TabNet had 10.5%, DNF-Net had 11.8% and NODE had 14.2% (see Tables 2 and 3 in the appendix for full results).\n\nAs a side note, XGBoost + DL was best, but that defeats the purpose in my opinion. Presumably any notably more complicated setting will be potentially better with enough complexity, but unless there is an obvious way on how to add such complexity, it’s mostly an academic exercise. However, as the authors note, if search is automated, maybe the complexity of combining the models is less of an issue.\n\n\n\nOther stuff\n\nKudos\nThe authors cite the No Free Lunch theorem in the second paragraph, something that appears to be lost on many (most?) of these types of papers touting small increases in performance for some given modeling approach.\n\n\nIssues\nThere are always things like training process/settings that are difficult to fully replicate. By the time authors publish any paper, unless exact records are kept, the iterations (including discussions that rule out various paths) are largely lost to time. This isn’t a knock on this paper, just something to keep in mind.\n\n\nOpinion\nI liked this one in general. They start by giving the competing models their best chance with their own settings and data, which was processed and trained in the same way. Even then, those models still either didn’t perform best, and/or performed relatively poorly on any other dataset."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data",
    "title": "This is definitely not all you need",
    "section": "Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data",
    "text": "Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data\n\nPaper Info\n\nWho: Kadra et al.\nWhere: U of Freiburg, Leibniz U (Germany)\nWhen: 2021-06-06 V1\nArxiv Link\n\n\n\nFrom the Abstract\n\nTabular datasets are the last “unconquered castle” for deep learning… In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters.\n\n\nWe empirically assess the impact of these regularization cocktails for MLPs on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.\n\n\nWe emphasize that some of these publications claim to outperform Gradient Boosted Decision Trees (GDBT) [1, 37], and other papers explicitly stress that their neural networks do not outperform GBDT on tabular datasets [38, 22]. In contrast, we do not propose a new kind of neural architecture, but a novel paradigm for learning a combination of regularization methods.**\n\n\n\nOverview\nThis data is more about exploring regularization techniques (e.g. data augmentation, model averaging via dropout) rather than suggesting any particular model is superior. Even in the second paragraph they state their results do not suggest a performance gain over boosting methods. Their focus is on potentially improving DL for tabular data through regularization with two hypotheses:\n\nRegularization cocktails outperform state-of-the-art deep learning architectures on tabular datasets.\nRegularization cocktails outperform Gradient-Boosted Decision Trees, as the most commonly used traditional ML method for tabular data.\n\n\n\nData\nForty total datasets ranging from as little as ~400 observations to over 400k, and between 4 and 2000 features. All were categorical targets, with about half binary. All available at openml.org with target ID provided.\n\n\nModels Explored\nComparison models:\n\nTabNet: (with author’s proposed defaults)\nNODE: (with author’s proposed defaults)\nAutogluon: Tabular: can use other techniques but restricted to ensembles of neural nets for this demo\nASK-GBDT: GB via Auto-sklearn (Note this tool comes from one of the authors )\nXGBoost: Original implementation\nMLP: Multilayer Perceptron - 9 layers with 512 hidden units each.\nMLP+D: MLP with Dropout\nMLP+C: MLP with regularization cocktail\n\n\n\nQuick Summary\n\nTo begin, their regularization cocktail approach is the clear winner on these datasets, having one outright on over 40% of them (based on table 2).\nStandard XGB performed best (or tied for best) 8 of the 40 data sets, while it or ASK-GBDT were best for 12 datasets combined.\nSimple MLP was best once, while MLP with dropout was best 5 times, while the cocktail method was best in general, across 19 datasets.\nThe ‘fancy’ DL models were the worst performers across the board. TabNet never performed best, and NODE only did once, but the latter also repeatedly failed due to memory issues or run-time limitations (this memory issue was mentioned in the previous paper also).\nHead-to-head, the cocktail beat the standard XGB 26 out of 38 times with three ties. So it wins 65% of the time against XGB, 70% against ASK-GBDT, but 60% against either (i.e. some XGB approach).\n\n\n\nOther Stuff\n\nKudos\n\nRecognize that tabular data is understudied in mainstream DL literature\nThey used a lot of datasets\nThey look at the simplest DL models for comparison\n\n\n\nIssues\n\nI wonder why there was not a single numeric outcome among so many datasets. Furthermore, some of the data are image classification (e.g. Fashion-MNIST), so I’m not sure why they’re included. I wouldn’t use a ‘tabular’ technique when standard computer vision approaches already work so well.\nI’m not familiar with the augmentation techniques they mention, which were devised for image classification, but there have been some used for tabular data for a couple decades at this point that were not mentioned, including simple upsampling, or imputation methods (e.g. SMOTE). That’s not a beef with the article at all, I’ve long wondered why people haven’t been using data augmentation for tabular data given it’s success elsewhere (including for tabular data!).\nThey use a standard t-tests of ranks, but if we’re going to use this sort of approach, we’d maybe want to adjust for all the tests done, and probably for all pairwise comparisons (they show such a table for the regularization methods). Depending on the approach and cutoff, the XGB vs. Cocktail difference may not be significant.\nAlso, I couldn’t duplicate these p-values with R’s default settings for Wilcoxon signed rank tests, and there does in fact seem to be inconsistency between the detailed results and Wilcoxon summaries. For example, in the regularization tests of Table 9, Cocktail vs. WD and DO shows two ties in the first four data sets, yet only 1 tie is reported in the comparison chart for both (Figure 4). For the models, Table 2 show 3 ties of XGB & the Cocktail, with 1 for ASK-G and Cocktail, but 2 and 0 are reported for their Wilcoxon tests. It’s not clear what they did for NODE with all the NAs. I do not believe these discrepancies, nor adjusting for multiple comparisons, will change the results (I re-did those myself).\n\n\n\nOpinion\nIf we ignore the regularization focus and just look at the model comparisons, I’m not overly convinced we have a straightforward victory for cocktail vs. GB as implied in the conclusion. Results appear to be in favor of their proposed method, but not enough to be a near-guarantee in a particular setting, so we’re back to square one of just using the easier/faster/better tool. I’m also not sure who was questioning the use of regularization for neural networks or modeling in general, so the comparison to any model without some form of regularization isn’t as interesting to me. What is interesting to me is that we have another round of evidence that the fancier DL models like TabNet do not perform that well relative to GB or simpler DL architectures."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#revisiting-deep-learning-models-for-tabular-data",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#revisiting-deep-learning-models-for-tabular-data",
    "title": "This is definitely not all you need",
    "section": "Revisiting Deep Learning Models for Tabular Data",
    "text": "Revisiting Deep Learning Models for Tabular Data\n\nPaper Info\n\nWho: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko\nWhere: Yandex (Russia)\nWhen: 2021-06-22\nArxiv Link\nSource code\n\n\n\nFrom the Abstract\n\nThe necessity of deep learning for tabular data is still an unanswered question addressed by a large number of research efforts. The recent literature on tabular DL proposes several deep architectures reported to be superior to traditional “shallow” models like Gradient Boosted Decision Trees. However, since existing works often use different benchmarks and tuning protocols, it is unclear if the proposed models universally outperform GBDT. Moreover, the models are often not compared to each other, therefore, it is challenging to identify the best deep model for practitioners.\n\n\nIn this work, we start from a thorough review of the main families of DL models recently developed for tabular data. We carefully tune and evaluate them on a wide range of datasets and reveal two significant findings. First, we show that the choice between GBDT and DL models highly depends on data and there is still no universally superior solution. Second, we demonstrate that a simple ResNet-like architecture is a surprisingly effective baseline, which outperforms most of the sophisticated models from the DL literature. Finally, we design a simple adaptation of the Transformer architecture for tabular data that becomes a new strong DL baseline and reduces the gap between GBDT and DL models on datasets where GBDT dominates.\n\n\n\nOverview\nThis paper compares different models on a variety of datasets. They are interested in the GB vs. DL debate, but like the previous paper, also interested in how well a simpler DL architecture might perform, and what steps might help the more complicated ones do better.\n\n\nData\nThey have 11 datasets with a mix of binary, multiclass and numeric targets. Sizes range from 20K to 1M+. There appears to be some overlap with the first paper (e.g. Higgs, Cover type).\n\n\nModels Explored\n\n‘Baselines’\n\nXGBoost\nCatBoost\nMLP\nResNet\n\n\n\nDL Comparisons\n\nSNN\nNODE\nTabNet\nGrowNet\nDCN V2\nAutoInt\n\nIn addition, they look at ensembles of these models, but this is not of interest to me for this post.\n\n\n\nQuick Summary\nNote that these refer to the ‘single model’ results, not the results for ensembles.\n\nSome form of boosting performed best on 4 of the 11 datasets.\nResNet was best on four classification tasks, but not once for numeric targets.\nAt this point you won’t be surprised at what doesn’t perform as well- TabNet, NODE, and similar. TabNet, DCN, and GrowNet were never the best performer, and the other three were best one time a piece.\nMLP did not perform best on any data, however the authors note that it ‘is often on par or even better than some of the recently proposed DL models’.\nThey also looked at models with a ‘simple’ transformer architecture. Their results suggest better performance than the other DL models, and similar performance to ResNet.\n\n\n\nOther Stuff\n\nKudos\n\nSharing the source code!\nRecognizing that results at this point are complex at best given the lack of standard datasets\n\n\n\nIssues\n\nThey note a distinction between heterogeneous vs. other types of data. They call data heterogeneous if the predictors are of mixed data types (e.g. categorical, numeric, count), while something like pixel data would be homogeneous because all the columns are essentially the same type. The latter isn’t as interesting to me for this sort enterprise, and I think the former is what most are thinking about for ‘tabular’ data, otherwise we’d just call it what it is (e.g. image or text data), and modeling/estimation is generally quite a bit easier when all the data is the same type. I do think it’s important that they point out that GB is better with heterogeneous data, and I think if you only look at such data, you’d likely see GB methods still outperforming or at worst on par with the best DL methods.\n\n\n\nOpinion\nThese results seem consistent with others at this point. Complex DL isn’t helping, and simpler architectures, even standard MLP show good performance. In the end, we still don’t have any clear winner over GB methods."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#overall-assessment",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#overall-assessment",
    "title": "This is definitely not all you need",
    "section": "Overall Assessment",
    "text": "Overall Assessment\nThese papers put together are helpful in painting a picture of where we are at present with deep learning for tabular data, especially with mixed data types. In this setting, it seems that more complicated DL models do not seem to have any obvious gain over simpler architectures, which themselves do not consistently beat boosting methods. It may also be the case that for data of mixed data types/sources, boosting is still the standard to beat.\nEven though these articles are geared toward comparisons to GB/XGBoost, in several settings I’ve applied them, I typically do not necessarily have appreciably greater success compared to a default setting random forest (e.g. from the ranger package in R), or sufficiently flexible statistical model. Unfortunately this comparison is lacking from the papers, and would have been nice to have, especially for smaller data settings where such models are still very viable. I think a viable fast model, preferably one without any tuning required (or which simply is taken off the shelf) should be the baseline.\nIn that light, for tabular data I think one should maybe start with a baseline of a penalized regression with appropriate interactions (e.g. ridge/lasso), or a more flexible penalized approach (GAMM) as a baseline, the latter especially, as it can at least automatically incorporate nonlinear relationships, and tools like mgcv or gpboost in R can do so with very large data (1 million +) in a matter of seconds. In settings of relatively higher dimensions, interactions and nonlinearities should be prevalent enough such that basis function, tree, and DL models should be superior. Whether they are practically so is the key concern even in those settings. With smaller, noisier data of less dimension, I suspect the tuning/time effort with present day DL models for tabular data will likely not be worth it. This may change very soon however, so such an assumption should be regularly checked.\n \nlast updated: 2024-12-27\nNeural Net image source from UC Business Analytics R Programming Guide"
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "title": "Deep Learning for Tabular Data",
    "section": "TLDR: the meta-analysis",
    "text": "TLDR: the meta-analysis\nI collected most of the results from the summarized articles here and those covered in the previous post to see if we come to any general conclusions about which methods are best or work best in certain settings. In the following tables, I excluded those I knew to be image data, as well as datasets where I thought results were indistinguishable across all models tested (e.g. less than 1% difference in accuracy). This left comparisons for 92 datasets across six articles. However, it’s important to note that these were not independent datasets or studies. For example, Gorishniy et al. are the source of two papers and essentially the same testing situations, and other datasets were common across papers (e.g. Higgs Boson). In the rare situations there was a tie, I gave the nod to boosting methods as a. the whole point is to do better than those, b. they are the easier model to implement, and c. they are not always given the same advantages in these studies (e.g. pre-processing).\n\nFeature Type\nThe following shows results by feature type.\n\nHeterogeneous: at least 10% of categorical or numeric data with the rest of the other\nMinimal combo: means any feature inclusion of a different type. In the second table I collapse to ‘any heterogeneous’.\nBoost: Any boosting method (most of the time it’s XGBoost but could include lightGBM or other variant)\nMLP: multilayer perceptron or some variant\nDL_complex: A DL method more complex than MLP and which is typically the focus of the paper\n\nThe results suggest that current DL approaches’ strength is mostly with purely numeric data, and for heterogeneous data, simpler MLP or Boosting will generally prevail. I initially thought that boosting would do even better with heterogeneous data, and I still suspect that with more heterogeneous data and on more equal footing, results would tilt even more.\n\n\n\n\nTable 1: Feature Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nHeterogeneous\nMin. Combo\n\n\n\n\nBoost\n2\n10\n14\n6\n\n\nMLP\n2\n4\n9\n11\n\n\nDL_complex\n0\n22\n7\n5\n\n\n\n\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nAny Combo\n\n\n\n\nBoost\n2\n10\n20\n\n\nMLP\n2\n4\n20\n\n\nDL_complex\n0\n22\n12\n\n\n\n\n\n\n\n\n\n\n\n\nSample/Feature Set Size\nThe following suggests that complex DL methods are going to require a lot of data to perform better. This isn’t that surprising but the difference here is quite dramatic. Interestingly, MLP methods worked well for fewer features. N total in this case means total size reported (not just training).\n\n\n\n\nTable 2: Sample Size\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nN features\nN total\n\n\n\n\nBoost\n209\n133,309\n\n\nDL_complex\n207\n530,976\n\n\nMLP\n114\n114,164\n\n\n\n\n\n\n\n\n\n\n\n\nTarget Type\nIn the following we compare binary (bin), multiclass (mc), and numeric (num) target results1, but there’s no strong conclusion for this. The main thing to glean from this is that these papers do not test numeric targets nearly enough. Across dozens of disciplines and countless datasets that I’ve come across in various settings, if anything, this ratio should be reversed.\n\n\n\n\nTable 3: Target Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nbin\nmc\nnum\n\n\n\n\nBoost\n17\n10\n5\n\n\nDL_complex\n17\n11\n6\n\n\nMLP\n10\n14\n2\n\n\n\n\n\n\n\n\n\n\n\n\nCombinations\nIn the following I look at any heterogeneous, smaller data (N &lt; 200,000). A complex DL model will likely not do great in this setting.\n\n\n\n\nTable 4: Combinations\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nn\n\n\n\n\nBoost\n19\n\n\nDL_complex\n8\n\n\nMLP\n19\n\n\n\n\n\n\n\n\n\n\nNow, on to the details of some of the recent results that were included."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "On Embeddings for Numerical Features in Tabular Deep Learning",
    "text": "On Embeddings for Numerical Features in Tabular Deep Learning\n\nAuthors: Gorishniy, Rubachev, & Babenko\nYear: 2022\nArxiv Link\n\n\nOverview\nYura Gorishniy, Rubachev, and Babenko (2022) pit several architectures against one another, such as standard multilayer perceptron (MLP), ResNet, and their own transformer approach (see Yuri Gorishniy et al. (2021)). Their previous work, which was summarized in my earlier post, was focused on the architecture, while here they focus on embedding approaches. The primary idea is to take the value of some feature and expand it to some embedding space, then use the embedding in lieu of the raw feature. It can essentially be seen as a pre-processing task.\nOne approach they use is piecewise linear encoding (PLE), which they at one point describe as ‘a continuous alternative to the one-hot encoding’2. Another embedding they use is basically a fourier transform.\n\n\nData\n\n12 public datasets mostly from previous works on tabular DL and Kaggle competitions.\nSizes were from ~10K to &gt;1M.\nTarget variables were binary, multiclass, or numeric.\nThe number of features ranged from 8 to 200.\n\n9 of 12 data sets had only numeric features, two had a single categorical feature, and unfortunately, only one of these might be called truly heterogeneous, i.e., with a notable mix of categorical and numeric features3.\n\n\n\nModels Explored\n\nCatBoost\nXGBoost\nMLP, MLP*\nResNet, ResNet*\nTransformer*\n\n* Using proposed embeddings\n\n\nQuick Summary\n\nA mix of results with no clear/obvious winners (results are less distinguishable if one keeps to the actual precision of the performance metrics, and even less so if talking about statistical differences in performance).\n\nSeveral datasets showed no practical difference across any model (e.g. all accuracy results within ~.01 of each other).\n\nEmbedding-based approaches generally tend to improve over their non-embedding counter parts (e.g. MLP + embedding &gt; MLP), this was possibly the clearest result of the paper.\nI’m not sure we could say the same for ResNet, where results were similar with or without embedding\nXGBoost was best on the one truly heterogeneous dataset.\n\n\n\nIn general this was an interesting paper, and I liked the simple embedding approaches used. It was nice to see that they may be useful in some contexts. The fourier transform is something that analysts (including our team at Strong) have used in boosting, so I’m a bit curious why they don’t do Boosting + embeddings for comparison for that or both embedding types. These embeddings can be seen as a pre-processing step, so nothing would keep someone from using them for any model.\nAnother interesting aspect was how little difference there was in model performance. It seemed half the datasets showed extremely small differences between any model type."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "title": "Deep Learning for Tabular Data",
    "section": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training",
    "text": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training\n\nAuthors: Somepalli, Goldblum, Schwarzschild, Bayan-Bruss, & Goldstein\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper applies BERT-style attention over rows and columns, along with embedding/data augmentation. They distinguish the standard attention over features, with intersample attention of rows. In addition, they use CutMix for data augmentation (originally devised for images), which basically combines pairs of observations to create a new observation4. Their model is called SAINT, the Self-Attention and Intersample Attention Transformer.\n\n\nData\n\n16 data sets\nAll classification, 2 multiclass\n6 are heterogeneous, 2 notably so\nSizes 200 to almost 500K\n\n\n\nModels Explored\n\nLogistic Regression (!)\nRandom Forest\nBoosting\n\nCatBoost\nXGBoost\nLightGBM\n\nMLP\nTabNet\nVIME\nTabTransformer\nSAINT\n\n\n\nQuick Summary\n\nIt seems the SAINT does quite well on some of the data, and average AUROC across all datasets is higher than XGB.\nMain table shows only 9 datasets though, which they call ‘representative’ but it’s not clear what that means when you only have 16 to start. One dataset showed near perfect classification for all models so will not be considered. Of the 15 total remaining:\n\nSAINT wins 10 (including 3 heterogeneous)\nBoosting wins 5 (including 2 heterogeneous)\n\nSAINT benefits from data augmentation. This could have been applied to any of the other models, but doesn’t appear to have been done.\nAt least they also used some form of logistic regression as a baseline, though I couldn’t find details on its implementation (e.g. regularization, including interactions). I don’t think this sort of simple baseline is utilized enough.\n\nThis is an interesting result, but somewhat dampened by lack of including numeric targets and more heterogeneous data. The authors include small data settings which is great, and are careful to not generalize despite some good results, which I can appreciate.\nI really like the fact they also compare a simple logistic regression to these models, because if you’re not able to perform notably better relative to the simplest model one could do, then why would we care? The fact that logistic regression is at times competitive and even beats boosting/SAINT methods occasionally gives me pause though. Perhaps some of these data are not sufficiently complex to be useful in distinguishing these methods? It is realistic though. While it’s best not to assume as such, sometimes a linear model is appropriate given the features and target at hand."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
    "text": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning\n\nAuthors: Kossen, Band, Lyle, Gomez, Rainforth, & Gal\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper introduces Non-Parametric Transformers, which focus on holistic processing of multiple inputs, and attempts to consider an entire dataset as input as opposed to a single row. Their model attempts to learn relations between data points to aid prediction. They use a mask to identify prediction points from the non-masked data, i.e. the entire \\(X_{\\textrm{not masked}}\\text{ }\\) data used to predict \\(X_{\\textrm{masked}}\\text{ }\\). The X matrix actually includes the target (also masked vs. not). At prediction, the model is able to make use of the correlations of inputs of training to ultimately make a prediction.\n\n\nData\n\n10 datasets from UCI, 2 are image (CIFAR MNIST)\n4 binary, 2 multiclass, 4 numeric targets\n\n\n\nModels Explored\n\nNPT\nBoosting\n\nGB\nXGB\nCatBoost\nLightGBM\n\nRandom Forest\nTabNet\nKnn\n\n\n\nQuick Summary\n\nGood performance of these models, but not too different from best boosting model for any type of data.\n\nNPT best on binary classification, but similar to CatBoost\nSame as XGB and similar to MLP on multiclass\nBoosting slightly better on numeric targets, but NPT similar\n\nAs seen several times now, TabNet continues to underperform\nk-nn regression worst (not surprising)\n\nWhen I first read the abstract where they say “We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input.”, I immediately was like ‘What about this, that, and those?’. The key phrase was ‘deep learning’, because the authors note later that this has a very long history in the statistical modeling realm. I was glad to see in their background of the research that they explicitly noted the models that came to my mind, like gaussian processes, kernel regression, etc. Beyond that, many are familiar with techniques like knn-regression and predictive mean matching, so it’s definitely not new to consider more than a single data point for prediction. I thought it was good of them to add k-nn regression to the model mix, even though it was not going to do well compared to the other approaches.\nThough the author’s acknowledge a clear thread/history here, I’m not sure this result is the fundamental shift they claim, versus a further extension/expansion into the DL domain. Even techniques that may work on a single input at a time may ultimately be taking advantage of correlations among the inputs (e.g. spatial correlations in images). Also, automatic learning of feature interactions is standard even in basic regularized regression settings, but here their focus is on observation interactions (but see k-nn regression)."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "title": "Deep Learning for Tabular Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn the two reviews on DL for tabular data that I’ve done, it appears there is more work in store for DL methods applied to tabular data. While it’d be nice to have any technique that would substantially improve prediction for such settings, I do have a suspicion results are likely rosier than they are, since that is just about the case for any newly touted technique, and at least in some cases, I don’t think we’re even making apple to apple comparisons.\nThat said, I do feel like some ground has been made for DL applications for tabular data, in that architectures can now more consistently performing as well as boosting methods in certain settings, especially if we include MLP. In the end though, results don’t appear strong enough to warrant a switch from boosting for truly heterogeneous data, or even tabular data in general. I feel like someday we’ll maybe have a breakthrough, but in the meantime, we can just agree that messy data is hard stuff to model, and the best tool is whichever one works for your specific situation."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "title": "Deep Learning for Tabular Data",
    "section": "Guidelines for future research",
    "text": "Guidelines for future research\nI was thinking about what would be a convincing result, the type of setting and setup where if a DL technique was consistently performing statistically better than boosting methods, I’d be impressed. So I’ve made a list of things I’d like to see more of, and which would make for a better story if the DL method were to beat out other techniques.\n\nAlways use heterogeneous data. For giggles let’s say 20%+ of the minority feature type.\nFeatures should at least be minimally correlated, if not notably so.\nImage data results are not interesting (why would we use boosting on this in practice?).\nNumeric targets should at least be as much of focus as categorical targets.\nInclude ‘small’ datasets.\nInclude very structured data (e.g. clustered with repeated observations, geographical points, time series).\nUse a flexible generalized additive or similar penalized regression with interactions as a baseline statistical model.\nMaybe add survival targets to the mix.\nIf using a pre-processing step that is done outside of modeling, this likely should be applied to non-DL methods for better comparison, especially, if we’re only considering predictive accuracy and don’t care too much about interpretation.\nNote your model variants before analyzing any data. Tweaking/torturing model architecture after results don’t pan out is akin to p-hacking in the statistical realm, and likewise wastes both researcher and reader’s time.\nRegarding results…\n\nDon’t claim differences that you don’t have precision to do so, or at least back them up with an actual statistical test.\nIf margin of error in the metrics is overlapping, while statistically they could be different, practically they probably aren’t to most readers. Don’t make a big deal about it.\nIt is unlikely anyone will be interested in three decimal place differences for rmse/acc type metrics, and statistically, results often don’t even support two decimal precision.\nReport how you are obtaining uncertainty in any error estimates.\nIf straightforward, try to give an estimate of total tuning/run times.\n\nWith the datasets\n\nName datasets exactly how they are named at the source you obtained them from, provide direct links\nProvide a breakdown for both feature and target types\nProvide clear delineation of total/training/validation/test sizes"
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "title": "Deep Learning for Tabular Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t refer to numeric targets as ‘regression’ because that’s silly for so many reasons. 😄↩︎\nA quick look suggests it’s not too dissimilar from a b-spline.↩︎\nI’ll let you go ahead and make your own prediction about which method was best on that data set.↩︎\nIt’s not clear to me how well this CutUp approach would actually preserve feature correlations. My gut tells me the feature correlations of this approach would be reduced relative to the observed, since the variability of the new observations is likely reduced. This ultimately may not matter for predictive purposes or their ultimate use in embeddings. However, I wonder if something like SMOTE, random (bootstrap) sampling, other DL methods like autoencoders, or similar approaches might do the same or better.↩︎"
  },
  {
    "objectID": "posts/2023-03-misc/index.html",
    "href": "posts/2023-03-misc/index.html",
    "title": "Stuff Going On",
    "section": "",
    "text": "It’s been a bit so thought I’d force myself to post a couple things I’ve played around with, or that aren’t ready yet for a full post, or won’t be one."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "href": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "title": "Stuff Going On",
    "section": "Football players still don’t know penalty kick basics",
    "text": "Football players still don’t know penalty kick basics\nDid a quick and dirty Bayesian analysis to get posterior probabilities for location, controlling for various factors. As a side note, I won the office world cup challenge with a fancy model of which I will never reveal the details, but may or may not have included lots of guessing and luck."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#tabular-data-post",
    "href": "posts/2023-03-misc/index.html#tabular-data-post",
    "title": "Stuff Going On",
    "section": "Tabular data post",
    "text": "Tabular data post\nI finally did my first post at the Strong blog! It’s a high-level overview of tabular data and deep learning that summarizes some of my previous posts here and here."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#class-imbalance",
    "href": "posts/2023-03-misc/index.html#class-imbalance",
    "title": "Stuff Going On",
    "section": "Class Imbalance",
    "text": "Class Imbalance\nFor my next post at the Strong blog, Elizabeth Monroe and I are working on a similarly high-level overview of issues with class imbalance we’ve been coming across. I will probably provide even more details and simulation results in a post on this site eventually, but here is a preview plot showing (mis)calibration plots at varying degrees of imbalance and different sample sizes."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#two-years-at-strong",
    "href": "posts/2023-03-misc/index.html#two-years-at-strong",
    "title": "Stuff Going On",
    "section": "Two years at Strong",
    "text": "Two years at Strong\nHard to believe for me anyway, but I’ve been out of academia for two years now, after previously spending my professional lifetime there. Aside from some of the obvious differences, one of the more satisfying changes for me has been that the skills I’ve acquired are utilized on a daily basis, and something I need to continuously develop for the job. At Strong our clients want good results in a timely fashion, and though the results might be notably complex, they still need to be sufficiently interpretable as well as reproducible/production-ready. I also have come across more desire for causal explanations from clients, which might be surprising to what is typically assumed for academia vs. industry. Clients obviously require buy-in for what we do, but they ultimately defer to us for the expertise we provide.\nStrong Analytics was a great move for me, because they clearly value the strong academic background of its employees, but are practically minded, and focus on skills that allow one to be nimble enough to get the clients what they need. Just like I was in academia, I am surrounded by a diverse group of smart folks I respect a lot, and am happy to solve some tough problems with. I feel I’ve learned how to get things done in a more efficient manner, and do a better job of explaining what I’ve done to wider audience.\nAmong some things I miss with academia, one was working with faculty and grad students who were just starting with an idea, and continuing a relationship with them until ultimately getting to publication or a successful dissertation defense after a very long journey. Another was giving workshops regularly where you could help people with their initial baby steps into the large world of data science. In general, it was easy to feel personally invested in the individuals you were working with, and their successes felt like your own.\nHowever, in academia it was often a struggle to get buy-in for more complicated methods or new techniques, because the stakes were typically lower and people knew the minimum required to get them published, defended or whatever, and mostly just wanted help getting to that point. There’s nothing wrong with that necessarily, that’s just the practical reality, and a reflection of what’s valued in academia. Despite that, I can say I definitely had some good partnerships with people involved in challenging research that was very rewarding, and those projects made it generally very satisfying to work in academia.\nUltimately though, I’m happy to have made the jump. It’s a bit weird to me how much drama there is on this topic on twitter and elsewhere. It’s really not that big of a deal which route you go, and in the grand scheme of things, almost no one will care if you work in academia or industry but you. There are pros and cons to both, and people should just pick what will make them happier."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#coming-up",
    "href": "posts/2023-03-misc/index.html#coming-up",
    "title": "Stuff Going On",
    "section": "Coming up",
    "text": "Coming up\nWhenever I can get around to it, I’ll try and post on those class imbalance simulations mentioned above, conformal prediction, and some other fun stuff. Stay tuned!"
  },
  {
    "objectID": "posts/2025-news/index.html",
    "href": "posts/2025-news/index.html",
    "title": "Some News for the New Year",
    "section": "",
    "text": "Among many things that have happened recently, I’ve had a daughter, and written a book to come out this year. Fun stuff!\nNot so fun - I’m also attempting to migrate my distill site to quarto and using VS Code, which has been, at best, very difficult. After a lot of effort, it looks like I finally got quarto to use the appropriate python environment (and I’ve given up on trying to get ‘post-specific’ environments to work). Then came the general publishing problems…\nIn the end I may lose a lot of the previous code content, since quarto doesn’t appear to respect the old web cached objects I had associated with prior posts (which included now defunct or notably modified packages). It also has to use a different directory output, which means I have update links along with rerunning old posts.\nI love using Quarto, and highly recommend it for most things (including if you never created a website before), but this particular aspect has been not at all straightforward. So, while my website will ultimately look slightly better, and hopefully be easier to maintain, the old content will be lacking for a while while I try to redo them. Stay tuned."
  },
  {
    "objectID": "posts/2025-news/index.html#update",
    "href": "posts/2025-news/index.html#update",
    "title": "Some News for the New Year",
    "section": "",
    "text": "Among many things that have happened recently, I’ve had a daughter, and written a book to come out this year. Fun stuff!\nNot so fun - I’m also attempting to migrate my distill site to quarto and using VS Code, which has been, at best, very difficult. After a lot of effort, it looks like I finally got quarto to use the appropriate python environment (and I’ve given up on trying to get ‘post-specific’ environments to work). Then came the general publishing problems…\nIn the end I may lose a lot of the previous code content, since quarto doesn’t appear to respect the old web cached objects I had associated with prior posts (which included now defunct or notably modified packages). It also has to use a different directory output, which means I have update links along with rerunning old posts.\nI love using Quarto, and highly recommend it for most things (including if you never created a website before), but this particular aspect has been not at all straightforward. So, while my website will ultimately look slightly better, and hopefully be easier to maintain, the old content will be lacking for a while while I try to redo them. Stay tuned."
  },
  {
    "objectID": "posts/2025-news/index.html#goals",
    "href": "posts/2025-news/index.html#goals",
    "title": "Some News for the New Year",
    "section": "Goals",
    "text": "Goals\nFor 2025 there are a couple posts I hope to do:\n\nA post on class imbalance Elizabeth and I intended for out work blog but which never was published.\nA post I had on conformal prediction that was likewise intended for the work blog.\nSomething new that is of interest\n\nI can also foresee more review based stuff, or just having my code be independent of the post. It’s only every few years that I update my site significantly, but I definitely get tired of trying to maintain this stuff when many things conspire against doing so. You can use specific environments, but then they will still likely unusable in the future if the package functionality or even the versions are no longer supported. I also don’t want a post two years from now to be beholden to a package’s current functionality. Caching would solve a lot of it, but doesn’t seem to respected when other aspects of the computing environment change."
  },
  {
    "objectID": "posts/2025-news/index.html#migration-issues",
    "href": "posts/2025-news/index.html#migration-issues",
    "title": "Some News for the New Year",
    "section": "Migration Issues",
    "text": "Migration Issues\nIssues I came across in case it’s useful to others:\n\nhttps://github.com/quarto-dev/quarto-cli/issues/10276\nhttps://github.com/quarto-dev/quarto-cli/issues/5220\nDeployment error (had to ‘rerun all’ from github itself)\nDefault radian pointing to wrong python environment which would then automatically load that environment and ignore any other env setting.\nhttps://github.com/quarto-dev/quarto-cli/issues/9929 (I think this was because I was in the gh-pages branch and not the main branch)\n\nWhat my ultimate solution was:\nFor Python:\n\nThe only env I could get things to recognize was a conda env in a default location for conda envs. My preference for uv created env, and secondarily, standard py env would not be recognized.\n\nWould not recognize any env in project directories\n\nIn .Rprofile (not .Renviron, not _environment, which were not resepected) put Sys.setenv(RETICULATE_PYTHON = \"~/anaconda3/envs/m-clark-github-io/bin/python\") followed by library(reticulate).\n\nFor R:\n\nI can literally save an RData file, write code for it to load, and when quarto renders it, it will say certain objects not found.\n\nFor publishing:\n\nThe biggest issue was the inability to use the top-level directory as the output_dir as I had before.\nI also now have to change every post file from its previous name to ‘index.qmd’ within the date-named directory in order for previous links to work. I could add an alias to every one of the files and let them redirect, but I prefer the cleaner address, and it’s easier to rename the files collectively than to add aliases to every post.\nI had to discover that you can’t be in the gh-pages branch (which I’m still fuzzy as to the need of). It’s mentioned in the doc, but not stressed or highlighted at all.\n\nOnce I was able to get quarto to render the pages in the first place, it published pretty easily via quarto publish gh-pages. I then had an issue where when publishing it gave some 404 error. I noticed it had updated the gh-pages branch after I pushed a recent change to main, so I pushed that branch as well, then tried to publish and it worked. Honestly they need more documentation as to exactly what the workflow is, especially for a blog (and with actual code, rather than a 2 year old demo website with a title suggesting there is but doesn’t actually have code).\nSo it seems the workflow is something like this:\n\nMake change on main\nPush to main\ngh-pages will magically update?\nPush gh-pages\nPublish"
  }
]