[
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "I used to give workshops regularly when I worked in academia, and I have kept the content here in case anyone who attended wanted to refer back to them. Some were not so much workshops as talks without any expectation of hands-on exercises or similar, so may not be as useful without the in-person context. Some of these, especially programming specific ones, are likely too dated to be useful beyond conceptual content, but the modeling focused ones may still have mostly relevant content."
  },
  {
    "objectID": "workshops.html#recent",
    "href": "workshops.html#recent",
    "title": "Workshops",
    "section": "Recent",
    "text": "Recent\n\nPolars: A notebook and related content providing an overview of polars with comparison to pandas and R package approaches like data.table.\n\nI also have given internal talks on Media Mix Modeling and Models for Tabular Data at our yearly in-person gatherings."
  },
  {
    "objectID": "workshops.html#previous-efforts",
    "href": "workshops.html#previous-efforts",
    "title": "Workshops",
    "section": "Previous efforts",
    "text": "Previous efforts\nThese were among the last workshops I gave before leaving academia.\n\nDistill for R Markdown\nExploratory Data Analysis Tools\nMixed Models with R\nMore Mixed Models\nPatchwork and gganimate\nLibrary Learning Analytics Workshop\nGetting More from RStudio\nLatent Variable Models\nGeneralized Additive Models\nMixed Models\n\n\nTexts\nThese are the texts that serve as the basis for the workshops. At least a few of these were more recently updated.\n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Structural Equation Modeling\nThis document regards a recent workshop given on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The document should be useful to anyone interested in the techniques covered, though it is R-based, with special emphasis on the lavaan package. \n Easy Bayes with rstanarm and brms\nThis workshop provides an overview of the rstanarm and brms packages. Basic modeling syntax is provided, as well as diagnostic checking, model comparison (posterior predictive checks , WAIC/LOO ), and how to get more from the models (marginal effects , posterior probabilities posterior probabilities, etc.). \n Factor Analysis and Related Methods\nThis workshop will expose participants to a variety of related techniques that might fall under the heading of ‘factor analysis’, latent variable modeling, dimension reduction and similar, such as principal components analysis, factor analysis, and measurement models, with possible exposure to and demonstration of latent Dirichlet allocation, mixture models, item response theory, and others. Brief overviews with examples of the more common techniques will be provided. \n Introduction to R Markdown\nThis workshop will introduce participants to the basics of R Markdown. After an introduction to concepts related to reproducible programming and research, demonstrations of standard markdown as well as overviews of different formats will be provided, including exercises. This document has been superseded by Practical Data Science, and will no longer be updated. \n Text Analysis with R\nThis document covers a wide range of topics, including how to process text generally, and demonstrations of sentiment analysis, parts-of-speech tagging, and topic modeling. Exercises are provided for some topics. It has practically no relevance in the modern large language model era."
  },
  {
    "objectID": "workshops.html#been-awhile",
    "href": "workshops.html#been-awhile",
    "title": "Workshops",
    "section": "Been awhile…",
    "text": "Been awhile…\nThese haven’t been given recently and are increasingly out date, but some content may be useful.\n My God, it’s full of STARs! Using astrology to get more from your data. Talk on structured additive regression models, and generalized additive models in particular. \n Become a Bayesian in 10 Minutes This document regards a talk aimed at giving an introduction Bayesian modeling in R via the Stan programming language. It doesn’t assume too much statistically or any prior Bayesian experience. For those with such experience, they can quickly work with the code or packages discussed. I post them here because they exist and provide a quick overview, but you’d get more from the more extensive document. \n Engaging the Web with R Document regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach.  \n Ceci n’est pas une %&gt;% Exploring your data with R. A workshop that introduces some newer modes of data wrangling within R, with an eye toward visualization. Focus on dplyr and magrittr packages. No longer available as the javascript the slides were based on kept producing vulnerabilities for my website. Nowadays, using pipes is standard anyway."
  },
  {
    "objectID": "posts/2025-01-01-news/index.html",
    "href": "posts/2025-01-01-news/index.html",
    "title": "Some News for the New Year",
    "section": "",
    "text": "Among many things that have happened recently, I’ve become a father to a beautiful baby girl, and written a book to come out this year (2025). Fun stuff!\nElsewhere, my employer Strong Analytics merged with OneSix last summer, which has gone well so far. It’s been great to expand our capabilities and personnel, and I’m excited to see what we can do in the future.\nNot so fun - I’ve migrated my distill-based website to quarto, something I actually started about a year and a half ago but never completed. I love using Quarto, and highly recommend it, including if you never created a website before, but this particular aspect was not straightforward. So, while my website looks a little better, and hopefully will be easier to maintain moving forward, some minor content may have gotten lost in the shuffle, or may not match up exactly with the previous version. Feel free to post an issue if you notice anything amiss."
  },
  {
    "objectID": "posts/2025-01-01-news/index.html#update",
    "href": "posts/2025-01-01-news/index.html#update",
    "title": "Some News for the New Year",
    "section": "",
    "text": "Among many things that have happened recently, I’ve become a father to a beautiful baby girl, and written a book to come out this year (2025). Fun stuff!\nElsewhere, my employer Strong Analytics merged with OneSix last summer, which has gone well so far. It’s been great to expand our capabilities and personnel, and I’m excited to see what we can do in the future.\nNot so fun - I’ve migrated my distill-based website to quarto, something I actually started about a year and a half ago but never completed. I love using Quarto, and highly recommend it, including if you never created a website before, but this particular aspect was not straightforward. So, while my website looks a little better, and hopefully will be easier to maintain moving forward, some minor content may have gotten lost in the shuffle, or may not match up exactly with the previous version. Feel free to post an issue if you notice anything amiss."
  },
  {
    "objectID": "posts/2025-01-01-news/index.html#goals",
    "href": "posts/2025-01-01-news/index.html#goals",
    "title": "Some News for the New Year",
    "section": "Goals",
    "text": "Goals\nFor this website in 2025, there are a couple posts I hope to do:\n\nA post on class imbalance Elizabeth and I intended for out work blog, and was mostly complete, but which never was published.\nA post I had on conformal prediction that was likewise intended for the work blog, and which was also mostly complete.\nA copy of the post I did on media mix modeling, which was posted at strong.io before the merger.\nSomething new that is of interest\n\nI might also orient toward general thoughts on various topics as opposed to code-based demos."
  },
  {
    "objectID": "posts/2025-01-01-news/index.html#migration-issues",
    "href": "posts/2025-01-01-news/index.html#migration-issues",
    "title": "Some News for the New Year",
    "section": "Migration Issues",
    "text": "Migration Issues\nAfter a lot of effort, it looks like I finally got my quarto website going. In the end I may have lost some of the previous code content, but my quick assessment is that it likely wasn’t much.\nIssues I came across in case it’s useful to others:\n\nGetting a specific python environment to work for quarto, possibly per post\nissue 10276\nissue 5220\nDeployment error (had to ‘rerun all’ from github repo website itself)\nDefault radian pointing to wrong python environment, which would then automatically load that environment and ignore any other env setting.\nissue 9929 (I think this was because I was in the gh-pages branch when publishing and not the main branch)\n\nWhat my ultimate solution was:\nFor Python:\n\nThe only env I could get things to recognize was a conda env in a default location for conda envs. Not sure why my other approaches didn’t work (my preference is to use an venv via uv), and there have been reticulate issues about using specific environments for years. But maybe it was due to other things (like the radian issue).\n\nAs an example, the quarto rendering would not use a virtual env in the project directory. It’s implied that you can use venvs in the directory of the quarto doc (i.e. folder specific), but I had issues, and at a minimum you’d have to do another step or two to render specifically from that directory venv. It also still begs the question of why you can’t just name the env in the python execute argument of the doc to use whatever environment you need when rendering it in the first place.\n\nIn .Rprofile (not .Renviron, not _environment) put Sys.setenv(RETICULATE_PYTHON = '~/myenvlocation') followed by library(reticulate). The default VS Code venv was not ever used unless I did this. I would literally source activate, do a sanity check which python, and then it would render with a default system env. I’ll revisit this in case this was still mixed in with the Radian issue.\nI feel like maybe relatively little is tested with VS Code/Python for website/blogs, and even less for mixing it with R. So be prepared for some work in that regard. My conclusion at this point is it’s just the same reticulate (not quarto specific) stuff that’s been ongoing for a long time.\n\nFor R/General:\n\nQuarto does not adhere to the VS code project directory for posts by default, so directories are relative to the post file location, rather than the project. I eventually found there is an execute_dir option that can be set to project, but this doesn’t seem to work for yml in the file itself, which still need to refer to the file location for, e.g. the preview image or a bibliography. Also, Python code was relative to the project directory by default, so it was inconsistent with R until I set the execute_dir option.\n\nFor publishing:\n\nA notable issue was the inability to use the top-level directory as the output_dir as I had before.\n\nAs a result, I had to change every post file from its previous name to ‘index.qmd’ within the date-named directory in order for previous links to work. I could add an alias to every one of the files and let them redirect, but I prefer the cleaner address, and it’s easier to rename the files collectively than to add aliases to every post.\n\nI had to discover that you can’t be in the gh-pages branch. It’s mentioned in the doc, but not stressed or highlighted at all.\n\nOnce I was able to get quarto to render the pages in the first place, it published pretty easily via quarto publish gh-pages. I very rarely can still can get a 404 or something else, but that has thus far has been remedied by just re-publishing.\nHonestly they could use more documentation as to exactly what the workflow is for a website blog with actual (python) code, and a useful demo website demonstrating it for GH pages. That sounds specific, but I would imagine not uncommon for python (and a notable amount of R) users. A reproducible example that demonstrates most of the salient features would be nice (jjallaire.github.io is not enough). Thankfully a lot of my distill and previous book setup carried over, and at least part of my issues were probably due to unreasonable expectations.\nSetup Summarized\n(If you never had a website before, you can ignore this and just follow the Quarto docs)\n\nRealize that you’ll have to probably come across a diverse smattering of Quarto documentation over time to know how much and what you’ll likely need to tweak to get things to work. Just reading the website-specific docs will not be enough, and general issues with projects, computations, documents will all potentially affect your website.\nI started with a separate repo to ensure that things were working as I expected, even with my old posts within. I assumed I could essentially copy these to my actual website repo, but there was still a lot to do and I ultimately had to rerender everything. I’d suggest that if you do this, maybe only include an old post or two.\nSet your execute (and output_dir) if desired to whatever you had or now want.\n\nIf you link to other files in subdirectories, you likely need to change the link format. For example, I previously linked to ../folder/x.html to link to m-clark.github.io/folder/x.html, but I had to change these to /folder/x.html.\n\nIf you use Radian for your R console in VS Code, you’ll need to install in your chosen environment and point your workspace rterminal option to use that env for your terminal. You have to do this anyway, which is a real annoyance for using Radian in general.\nCreate a project specific python environment.\n\nYou may have to use conda to get reticulate stuff to work smoothly, but I’m not entirely certain of this. You may not need to do this if you don’t plan on mixing R and Python in the same doc.\nIf you do explicitly use reticulate, I suggest setting the python environment in your .Rprofile file.\n\n\nSo, after setup, it seems the workflow is something like this if you have a blog and use VS Code, python and R:\n\nMake change on main\nPush to main\nPublish\nCross fingers, I still have had a bit of quirkiness from time to time, but usually just publishing/re-rendering again works things out."
  },
  {
    "objectID": "posts/2023-03-misc/index.html",
    "href": "posts/2023-03-misc/index.html",
    "title": "Stuff Going On",
    "section": "",
    "text": "It’s been a bit so thought I’d force myself to post a couple things I’ve played around with, or that aren’t ready yet for a full post, or won’t be one."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "href": "posts/2023-03-misc/index.html#football-players-still-dont-know-penalty-kick-basics",
    "title": "Stuff Going On",
    "section": "Football players still don’t know penalty kick basics",
    "text": "Football players still don’t know penalty kick basics\nDid a quick and dirty Bayesian analysis to get posterior probabilities for location, controlling for various factors. As a side note, I won the office world cup challenge with a fancy model of which I will never reveal the details, but may or may not have included lots of guessing and luck."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#tabular-data-post",
    "href": "posts/2023-03-misc/index.html#tabular-data-post",
    "title": "Stuff Going On",
    "section": "Tabular data post",
    "text": "Tabular data post\nI finally did my first post at the Strong blog! It’s a high-level overview of tabular data and deep learning that summarizes some of my previous posts here and here."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#class-imbalance",
    "href": "posts/2023-03-misc/index.html#class-imbalance",
    "title": "Stuff Going On",
    "section": "Class Imbalance",
    "text": "Class Imbalance\nFor my next post at the Strong blog, Elizabeth Monroe and I are working on a similarly high-level overview of issues with class imbalance we’ve been coming across. I will probably provide even more details and simulation results in a post on this site eventually, but here is a preview plot showing (mis)calibration plots at varying degrees of imbalance and different sample sizes."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#two-years-at-strong",
    "href": "posts/2023-03-misc/index.html#two-years-at-strong",
    "title": "Stuff Going On",
    "section": "Two years at Strong",
    "text": "Two years at Strong\nHard to believe for me anyway, but I’ve been out of academia for two years now, after previously spending my professional lifetime there. Aside from some of the obvious differences, one of the more satisfying changes for me has been that the skills I’ve acquired are utilized on a daily basis, and something I need to continuously develop for the job. At Strong our clients want good results in a timely fashion, and though the results might be notably complex, they still need to be sufficiently interpretable as well as reproducible/production-ready. I also have come across more desire for causal explanations from clients, which might be surprising to what is typically assumed for academia vs. industry. Clients obviously require buy-in for what we do, but they ultimately defer to us for the expertise we provide.\nStrong Analytics was a great move for me, because they clearly value the strong academic background of its employees, but are practically minded, and focus on skills that allow one to be nimble enough to get the clients what they need. Just like I was in academia, I am surrounded by a diverse group of smart folks I respect a lot, and am happy to solve some tough problems with. I feel I’ve learned how to get things done in a more efficient manner, and do a better job of explaining what I’ve done to wider audience.\nAmong some things I miss with academia, one was working with faculty and grad students who were just starting with an idea, and continuing a relationship with them until ultimately getting to publication or a successful dissertation defense after a very long journey. Another was giving workshops regularly where you could help people with their initial baby steps into the large world of data science. In general, it was easy to feel personally invested in the individuals you were working with, and their successes felt like your own.\nHowever, in academia it was often a struggle to get buy-in for more complicated methods or new techniques, because the stakes were typically lower and people knew the minimum required to get them published, defended or whatever, and mostly just wanted help getting to that point. There’s nothing wrong with that necessarily, that’s just the practical reality, and a reflection of what’s valued in academia. Despite that, I can say I definitely had some good partnerships with people involved in challenging research that was very rewarding, and those projects made it generally very satisfying to work in academia.\nUltimately though, I’m happy to have made the jump. It’s a bit weird to me how much drama there is on this topic on twitter and elsewhere. It’s really not that big of a deal which route you go, and in the grand scheme of things, almost no one will care if you work in academia or industry but you. There are pros and cons to both, and people should just pick what will make them happier."
  },
  {
    "objectID": "posts/2023-03-misc/index.html#coming-up",
    "href": "posts/2023-03-misc/index.html#coming-up",
    "title": "Stuff Going On",
    "section": "Coming up",
    "text": "Coming up\nWhenever I can get around to it, I’ll try and post on those class imbalance simulations mentioned above, conformal prediction, and some other fun stuff. Stay tuned!"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html",
    "href": "posts/2022-07-25-programming/index.html",
    "title": "Programming Odds & Ends",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#introduction",
    "href": "posts/2022-07-25-programming/index.html#introduction",
    "title": "Programming Odds & Ends",
    "section": "Introduction",
    "text": "Introduction\nOftentimes I’m looking to gain speed/memory advantages, or maybe just exploring how to do the same thing differently in case it becomes useful later on. I thought I’d start posting them, but then I don’t get around to it. Here are a few that have come up over the past year (or two 😞), and even as I wrote this, more examples kept coming up, so I may come back to add more in the future."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#quick-summary",
    "href": "posts/2022-07-25-programming/index.html#quick-summary",
    "title": "Programming Odds & Ends",
    "section": "Quick summary",
    "text": "Quick summary\nData processing efficiency really depends on context. Faster doesn’t mean memory efficient, and what may be the best in a standard setting can be notably worse in others. Some approaches won’t show their value until the data is very large, or there are many groups to deal with, while others will get notably worse. Also, you may not want an additional package dependency beyond what you’re using, and may need a base R approach. The good news is you’ll always have options!\nOne caveat: I’m not saying that the following timed approaches are necessarily the best/fastest, I mostly stuck to ones I’d try first. You may find even better for your situation! A great resource to keep in mind is the fastverse, which is a collection of packages with speed and efficiency in mind, and includes a couple that are demonstrated here."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#setup-and-orientation",
    "href": "posts/2022-07-25-programming/index.html#setup-and-orientation",
    "title": "Programming Odds & Ends",
    "section": "Setup and orientation",
    "text": "Setup and orientation\nRequired packages.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(dtplyr)\nlibrary(tidyfast)\nlibrary(data.table)\nlibrary(collapse)\nlibrary(vctrs)\nlibrary(bench)   # for timings\n\nAlso, in the following bench marks I turn off checking if the results are equivalent (i.e. check = FALSE) because even if the resulting data is the same, the objects may be different classes, or some objects may even be of the same class, but have different attributes. You are welcome to double check that you would get the same thing as I did. Also, you might want to look at the autoplot of the bench mark summaries, as many results have some variability that isn’t captured by just looking at median/best times."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#fill-in-missing-values",
    "href": "posts/2022-07-25-programming/index.html#fill-in-missing-values",
    "title": "Programming Odds & Ends",
    "section": "Fill in missing values",
    "text": "Fill in missing values\nWe’ll start with the problem of filling in missing values by group. I’ve created a realistic example where the missingness is seen randomly across multiple columns, and differently across groups. I’ve chosen to compare tidyr, tidyfast, the underlying approach of tidyr via vctrs, and data.table.\nNote that only data.table is not last observation carried forward by default (‘down’ in tidyr parlance), so that argument is made explicit. All of these objects will have different attributes or classes. tidyfast for some reason renames the grouping variable to ‘by’. If you wrap all of these in data.frame, that will remove the attributes and give them all the same class, so you can verify they return the same result.\n\nset.seed(1234)\n\nN = 1e5\nNg = 5000\n\ncreate_missing &lt;- function(x) {\n  x[sample(1:length(x), 5)] = NA\n  x\n}\n\n\ndf_missing = tibble(grp = rep(1:Ng, e = N / Ng)) %&gt;%\n  arrange(grp) %&gt;%\n  group_by(grp) %&gt;%\n  mutate(\n    x = 1:n(),\n    y = rpois(n(), 5),\n    z = rnorm(n(), 5),\n    across(x:z, create_missing)\n  ) %&gt;% \n  ungroup()\n\ndf_missing %&gt;% head(5)\n\ndt_missing = as.data.table(df_missing) %&gt;% setkey(grp)\ntf_missing = copy(dt_missing)\n\nbm_fill &lt;-\n  bench::mark(\n    %!in%\n    tidyr    = fill(group_by(df_missing, grp), x:z),  \n    tidyfast = dt_fill(tf_missing, x, y, z, id = grp),\n    vctrs    = df_missing %&gt;% group_by(grp) %&gt;% mutate(across(x:z, vec_fill_missing)),\n    dt = dt_missing[\n      ,\n      .(x = nafill(x, type = 'locf'),\n        y = nafill(y, type = 'locf'),\n        z = nafill(z, type = 'locf')),\n      by = grp\n    ],\n    check = FALSE,\n    iterations = 10\n  )\n\nThis is a great example of where there is a notable speed/memory trade-off. Very surprising how much memory data.table uses1, while not giving much speed advantage relative to the tidyr. Perhaps there is something I’m missing (😏)? Also note that we can get an even ‘tidier’ advantage by using vctrs directly, rather than wrapping it via tidyr, and seeing how easy it is to use, it’s probably the best option.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ntidyfast\n18.3ms\n24.5ms\n39.87MB\n1\n6.06\n\n\nvctrs\n27ms\n28.5ms\n6.58MB\n1.16\n1\n\n\ndt\n111.7ms\n117.8ms\n237.08MB\n4.81\n36.04\n\n\ntidyr\n132.3ms\n147.2ms\n6.59MB\n6.01\n1"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#antijoins",
    "href": "posts/2022-07-25-programming/index.html#antijoins",
    "title": "Programming Odds & Ends",
    "section": "Antijoins",
    "text": "Antijoins\nSometimes you just don’t want that! In the following we have a situation where we want to filter values based on the negation of some condition. Think of a case where certain person IDs are not viable for consideration for analysis. Many times, a natural approach would be to use something like a filter where instead of using vals %in% values_desired, we just negate that with a bang (!) operator. However, another approach is to create a data frame of the undesired values and use an anti_join. When using joins in general, you get a relative advantage by explicitly noting the variables you’re joining on, so I compare that as well for demonstration. Finally, in this particular example we could use data.table’s built-in character match, chin.\n\nset.seed(123)\n\ndf1 = tibble(\n  id = sample(letters, 10000, replace = TRUE)\n)\n\ndf2 = tibble(\n  id = sample(letters[1:10], 10000, replace = TRUE)\n)\n\ndf1_lazy = lazy_dt(df1)\ndf2_lazy = lazy_dt(df2)\n\ndf1_dt = data.table(df1)\ndf2_dt = data.table(df2)\n\nsuppressMessages({\n  bm_antijoin = bench::mark(\n    in_     = filter(df1, !id %in% letters[1:10]),\n    in_dtp  = collect(filter(df1_lazy, !id %in% letters[1:10])),     # not usable until collected/as_tibbled\n    chin    = filter(df1, !id %chin% letters[1:10]),                 # chin for char vector only, from data.table\n    chin_dt = df1_dt[!df1_dt$id %chin% letters[1:10],],              \n    coll    = fsubset(df1, id %!in% letters[1:10]),                  # can work with dt or tidyverse\n    aj      = anti_join(df1, df2, by = 'id'),\n    aj_noby = anti_join(df1, df2),\n\n    iterations = 100,\n    check      = FALSE\n  )\n})\n\nIn this case, the fully data.table approach is best in speed and memory, but collapse is not close behind2. In addition, if you are in the tidyverse, the anti_join function is a very good option. Hopefully the lesson about explicitly setting the by argument is made clear.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nchin_dt\n152µs\n160µs\n206KB\n1\n1\n\n\ncoll\n167µs\n185µs\n268KB\n1.16\n1.3\n\n\naj\n529µs\n559µs\n293KB\n3.5\n1.42\n\n\nchin\n626µs\n662µs\n270KB\n4.15\n1.31\n\n\nin_\n716µs\n804µs\n387KB\n5.04\n1.88\n\n\nin_dtp\n923µs\n988µs\n479KB\n6.2\n2.33\n\n\naj_noby\n9.95ms\n10.962ms\n323KB\n68.72\n1.57"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#lagleaddifferences",
    "href": "posts/2022-07-25-programming/index.html#lagleaddifferences",
    "title": "Programming Odds & Ends",
    "section": "Lag/lead/differences",
    "text": "Lag/lead/differences\nHere we are interested in getting the difference in the current value of some feature from it’s last (or next) value, typically called a lag (lead). Note that it doesn’t have to be the last value, but that is most common. In the tidyverse we have lag/lead functions, or with data.table, we have the generic shift function that can do both. In the following I look at using that function in the fully data.table situation or within a tibble.\n\nset.seed(1234)\n\nN = 1e5\nNg = 5000\n\ndf  = tibble(\n  x = rpois(N, 10),\n  grp = rep(1:Ng, e = N / Ng)\n)\n\ndt = as.data.table(df)\n\nbm_lag = bench::mark(\n  dplyr_lag  = mutate(df, x_diff = x - lag(x)),\n  dplyr_lead = mutate(df, x_diff = x - lead(x)),\n  dt_lag     = dt[, x_diff := x - shift(x)],\n  dt_lead    = dt[, x_diff := x - shift(x, n = -1)],\n  dt_dp_lag  = mutate(df, x_diff = x - shift(x)),\n  dt_dp_lead = mutate(df, x_diff = x - shift(x, n = -1)),\n  coll_lag   = ftransform(df, x_diff = x - flag(x)),\n  coll_lead  = ftransform(df, x_diff = x - flag(x, n = -1)),\n  iterations = 100,\n  check      = FALSE\n)\n\nIn this case, collapse is best, with data.table not far behind, but using the shift function within the tidy approach is a very solid gain. Oddly, lag and lead seem somewhat different in terms of speed and memory.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_lag\n226µs\n260µs\n783.84KB\n1\n1\n\n\ncoll_lead\n250µs\n309µs\n783.84KB\n1.18\n1\n\n\ndt_lag\n316µs\n354µs\n813.87KB\n1.36\n1.04\n\n\ndt_lead\n319µs\n355µs\n813.87KB\n1.36\n1.04\n\n\ndt_dp_lag\n792µs\n818µs\n782.91KB\n3.14\n1\n\n\ndt_dp_lead\n797µs\n892µs\n782.91KB\n3.42\n1\n\n\ndplyr_lead\n994µs\n1.115ms\n1.53MB\n4.28\n2\n\n\ndplyr_lag\n1.044ms\n1.2ms\n1.15MB\n4.61\n1.5\n\n\n\n\n\n\n\nWhat about a grouped scenario? To keep it simple we’ll just look at using lagged values.\n\nbm_lag_grp = bench::mark(\n  dt_lag    = dt[, x_diff := x - shift(x), by = grp],\n  dt_dp_lag = mutate(group_by(df, grp), x_diff = x - shift(x)),\n  dplyr_lag = mutate(group_by(df, grp), x_diff = x - lag(x)),\n  coll_lag  = fmutate(fgroup_by(df, grp), x_diff = x - flag(x)),\n  \n  iterations = 10,\n  check      = FALSE\n)\n\nIn the grouped situation, using a collapse isn’t best for memory, but the speed gain is ridiculous!!\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_lag\n484µs\n548µs\n1.59MB\n1\n3.52\n\n\ndt_lag\n25.692ms\n28.067ms\n462.32KB\n51.2\n1\n\n\ndt_dp_lag\n30.843ms\n32.969ms\n2.61MB\n60.15\n5.77\n\n\ndplyr_lag\n77.156ms\n78.771ms\n5.19MB\n143.7\n11.49"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#firstlast",
    "href": "posts/2022-07-25-programming/index.html#firstlast",
    "title": "Programming Odds & Ends",
    "section": "First/Last",
    "text": "First/Last\nIn this demo, we want to take the first (last) value of each group. Surprisingly, for the same functionality, it turns out that the number of groups matter when doing groupwise operations. For the following I’ll even use a base R approach (though within dplyr’s mutate) to demonstrate some differences.\n\nset.seed(1234)\n\nN = 100000\n\ndf = tibble(\n  x  = rpois(N, 10),\n  id = sample(1:100, N, replace = TRUE)\n)\n\ndt = as.data.table(df)\n\nbm_first = bench::mark(\n  base_first  = summarize(group_by(df, id), x = x[1]),\n  base_last   = summarize(group_by(df, id), x = x[length(x)]),\n  dplyr_first = summarize(group_by(df, id), x = dplyr::first(x)),\n  dplyr_last  = summarize(group_by(df, id), x = dplyr::last(x)),\n  dt_first    = dt[, .(x = data.table::first(x)), by = id],\n  dt_last     = dt[, .(x = data.table::last(x)),  by = id],\n  coll_first  = ffirst(fgroup_by(df, id)),\n  coll_last   = flast(fgroup_by(df, id)),\n  \n  iterations  = 100,\n  check       = FALSE\n)\n\nThe first result is actually not too surprising, in that the fully dt approaches are fast and memory efficient, though collapse is notably faster. Somewhat interesting is that the base last is a bit faster than dplyr’s last (technically nth) approach.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_last\n307µs\n341µs\n783.53KB\n1\n1.72\n\n\ncoll_first\n299µs\n345µs\n783.53KB\n1.01\n1.72\n\n\ndt_last\n698µs\n876µs\n454.62KB\n2.57\n1\n\n\ndt_first\n689µs\n895µs\n454.62KB\n2.62\n1\n\n\nbase_last\n1.944ms\n2.036ms\n2.06MB\n5.97\n4.64\n\n\ndplyr_first\n2.026ms\n2.127ms\n2.06MB\n6.24\n4.64\n\n\nbase_first\n1.984ms\n2.165ms\n2.06MB\n6.35\n4.64\n\n\ndplyr_last\n2.111ms\n2.553ms\n2.06MB\n7.49\n4.64\n\n\n\n\n\n\n\nIn the following, the only thing that changes is the number of groups.\n\nset.seed(1234)\n\nN = 100000\n\ndf = tibble(\n  x = rpois(N, 10),\n  id = sample(1:(N/10), N, replace = TRUE) # &lt;--- change is here\n)\n\ndt = as.data.table(df)\n\nbm_first_more_groups = bench::mark(\n  base_first  = summarize(group_by(df, id), x = x[1]),\n  base_last   = summarize(group_by(df, id), x = x[length(x)]),\n  dplyr_first = summarize(group_by(df, id), x = dplyr::first(x)),\n  dplyr_last  = summarize(group_by(df, id), x = dplyr::last(x)),\n  dt_first    = dt[, .(x = data.table::first(x)), by = id],\n  dt_last     = dt[, .(x = data.table::last(x)),  by = id],\n  coll_first  = ffirst(group_by(df, id)),\n  coll_last   = flast(group_by(df, id)),\n  iterations  = 100,\n  check       = FALSE\n)\n\nNow what the heck is going on here? The base R approach is way faster than even data.table, while not using any more memory than what dplyr is doing (because of the group-by-summarize). More to the point is that collapse is notably faster than the other options, but still a bit heavy memory-wise relative to data.table.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ncoll_last\n2.813ms\n3.006ms\n2.79MB\n1\n3.8\n\n\ncoll_first\n2.838ms\n3.128ms\n2.79MB\n1.04\n3.8\n\n\nbase_first\n10.596ms\n11.4ms\n3.06MB\n3.79\n4.17\n\n\nbase_last\n12.551ms\n13.151ms\n3.06MB\n4.37\n4.17\n\n\ndt_last\n17.588ms\n18.318ms\n752.15KB\n6.09\n1\n\n\ndt_first\n17.671ms\n18.379ms\n752.15KB\n6.11\n1\n\n\ndplyr_first\n20.066ms\n20.943ms\n3.06MB\n6.97\n4.17\n\n\ndplyr_last\n20.916ms\n21.357ms\n3.16MB\n7.1\n4.31"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#coalesceifelse",
    "href": "posts/2022-07-25-programming/index.html#coalesceifelse",
    "title": "Programming Odds & Ends",
    "section": "Coalesce/ifelse",
    "text": "Coalesce/ifelse\nIt’s very often we want to change a single value based on some condition, often starting with ifelse. This is similar to our previous fill situation for missing values, but applies a constant as opposed to last/next value. Coalesce is similar to tidyr’s fill, and is often used in cases where we might otherwise use an ifelse style approach . In the following, we want to change NA values to zero, and there are many ways we might go about it.\n\nset.seed(1234)\nx = rnorm(1000)\nx[x &gt; 2] = NA\n\n\nbm_coalesce = bench::mark(\n  base      = {x[is.na(x)] &lt;- 0; x},\n  ifelse    = ifelse(is.na(x), 0, x),\n  if_else   = if_else(is.na(x), 0, x),\n  vctrs     = vec_assign(x, is.na(x), 0),\n  tidyr     = replace_na(x, 0),\n  fifelse   = fifelse(is.na(x), 0, x),\n  coalesce  = coalesce(x, 0),\n  fcoalesce = fcoalesce(x, 0),\n  nafill    = nafill(x, fill = 0),\n  coll      = replace_NA(x)   # default is 0\n)\n\nThe key result here to me is just how much memory the dplyr if_else approach is using, as well as how fast and memory efficient the base R approach is even with a second step. While providing type safety, if_else is both slow and a memory hog, so probably anything else is better. tidyr itself would be a good option here, and while it makes up for the memory issue, it’s relatively slow compared to other approaches, including the function it’s a wrapper for (vec_assign), which is also demonstrated. Interestingly, fcoalesce and fifelse would both be better options than data.table’s other approach that is explicitly for this task.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nfcoalesce\n1µs\n1µs\n7.86KB\n1\n1\n\n\ncoll\n1µs\n1µs\n7.86KB\n1.03\n1\n\n\nbase\n2µs\n3µs\n7.91KB\n2.17\n1.01\n\n\nfifelse\n3µs\n4µs\n11.81KB\n2.6\n1.5\n\n\nvctrs\n4µs\n4µs\n11.81KB\n3.09\n1.5\n\n\nnafill\n5µs\n6µs\n23.95KB\n4.14\n3.05\n\n\ntidyr\n9µs\n10µs\n11.81KB\n7.09\n1.5\n\n\ncoalesce\n15µs\n17µs\n20.19KB\n11.71\n2.57\n\n\nifelse\n15µs\n17µs\n47.31KB\n12.11\n6.02\n\n\nif_else\n21µs\n25µs\n71.06KB\n17.54\n9.04"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#conditional-slide",
    "href": "posts/2022-07-25-programming/index.html#conditional-slide",
    "title": "Programming Odds & Ends",
    "section": "Conditional Slide",
    "text": "Conditional Slide\nI recently had a problem where I wanted to do a apply a certain function that required taking the difference between the current and last value as we did in the lag demo. The problem was that ‘last’ depended on a specific condition being met. The basic idea is that we want to take x - lag(x) but where the condition is FALSE, we need to basically ignore that value for consideration as the last value, and only use the previous value for which the condition is TRUE. In the following, for the first two values where the condition is met, this is straightforward (6 minus 10). But for the fourth row, 4 should subtract 6, rather than 5, because the condition is FALSE.\n\nset.seed(1234)\n\ndf = tibble(\n  x = sample(1:10),\n  cond = c(TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE),\n  group = rep(c('a', 'b'), e = 5)\n)\n\ndf \n\n# A tibble: 10 × 3\n       x cond  group\n   &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;\n 1    10 TRUE  a    \n 2     6 TRUE  a    \n 3     5 FALSE a    \n 4     4 TRUE  a    \n 5     1 FALSE a    \n 6     8 TRUE  b    \n 7     2 FALSE b    \n 8     7 FALSE b    \n 9     9 TRUE  b    \n10     3 FALSE b    \n\n\nWhile somewhat simple in concept, it doesn’t really work with simple lags, as the answer would be wrong, or sliding functions, because the window is adaptive. I wrote the following function to deal with this. By default, it basically takes our vector under consideration, x, makes it NA where the condition doesn’t hold, then fills in the NA values with the last value using the vec_fill_missing (or a supplied constant/single value). However there is flexibility beyond that type of fill. In addition, the function applied is generic, and could be applied to the newly created variable (.x), or use both the original (x) and the newly created variable.\n\nconditional_slide &lt;-\n  function(x,\n           condition,\n           fun,\n           direction  = c(\"down\"),\n           fill_value = NA,\n           na_value   = NA,\n           ...) {\n    \n    if (!direction %in% c(\"constant\", \"down\", \"up\", \"downup\", \"updown\"))\n      rlang::abort('direction must be one of \"constant\", \"down\", \"up\", \"downup\", \"updown\"')\n    \n    if (length(x) != length(condition))\n      rlang::abort('condition and x must be the same length')\n    \n    # can't use dplyr/dt ifelse since we won't know class type of fill_value\n    conditional_val &lt;- ifelse(direction == 'constant', fill_value, NA)    \n    .x &lt;- ifelse(condition, x, conditional_val)\n    \n    if (direction != 'constant')\n      .x &lt;- vctrs::vec_fill_missing(.x, direction = direction)\n    \n    class(.x) &lt;- class(x)\n    \n    result &lt;- fun(x, .x, ...)\n    \n    if (!is.na(na_value))\n      result[is.na(result)] &lt;- na_value\n    \n    result\n  }\n\nThe first example applies the function, x - lag(x), to our dataset, and which in my case, I also wanted to apply within groups, which caused further problems for some of the available functions I thought would otherwise be applicable. I also show it for another type of problem, taking the cumulative sum, as well as just conditionally changing the values to zero.\n\ndf %&gt;%\n group_by(group) %&gt;%\n mutate(\n   # demo first difference\n   simple_diff = x - dplyr::lag(x),\n   cond_diff = conditional_slide(x, cond, fun = \\(x, .x) x - lag(.x), na_value = 0),\n   \n   # demo cumulative sum\n   simple_cumsum = cumsum(x),\n   cond_cumsum   = conditional_slide(\n     x,\n     cond,\n     fun = \\(x, .x) cumsum(.x),\n     direction = 'constant',\n     fill = 0\n   ),\n   \n   # demo fill last\n   simple_fill_last = vec_fill_missing(x),\n   cond_fill_last = conditional_slide(\n     x,\n     cond,\n     fun = \\(x, .x) .x,\n     direction = 'down'\n   )\n )\n\n# A tibble: 10 × 9\n# Groups:   group [2]\n       x cond  group simple_diff cond_diff simple_cumsum cond_cumsum\n   &lt;int&gt; &lt;lgl&gt; &lt;chr&gt;       &lt;int&gt;     &lt;dbl&gt;         &lt;int&gt;       &lt;int&gt;\n 1    10 TRUE  a              NA         0            10          10\n 2     6 TRUE  a              -4        -4            16          16\n 3     5 FALSE a              -1        -1            21          16\n 4     4 TRUE  a              -1        -2            25          20\n 5     1 FALSE a              -3        -3            26          20\n 6     8 TRUE  b              NA         0             8           8\n 7     2 FALSE b              -6        -6            10           8\n 8     7 FALSE b               5        -1            17           8\n 9     9 TRUE  b               2         1            26          17\n10     3 FALSE b              -6        -6            29          17\n# ℹ 2 more variables: simple_fill_last &lt;int&gt;, cond_fill_last &lt;int&gt;\n\n\nThis is one of those things that comes up from time to time where trying to apply a standard tool likely won’t cut it. You may find similar situations where you need to modify what’s available and create some functionality tailored to your needs."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#take-the-first-true",
    "href": "posts/2022-07-25-programming/index.html#take-the-first-true",
    "title": "Programming Odds & Ends",
    "section": "Take the first TRUE",
    "text": "Take the first TRUE\nSometimes we want the first instance of a condition. For example, we might want the position or value of the first number &gt; than some value. We’ve already investigated using dplyr or data.table’s first, and I won’t do so again here except to say they are both notably slower and worse on memory here. We have a few approaches we might take in base R. Using which would be common, but there is also which.max, that, when applied to logical vectors, gives the position of the first TRUE (which.min gives the position of the first FALSE). In addition, there is the Position function, which I didn’t even know about until messing with this problem.\n\nset.seed(123)\n\nx = sort(rnorm(10000))\n\nmarker = 2\n\nbm_first_true_1 = bench::mark(\n  which     = which(x &gt; marker)[1],\n  which_max = which.max(x &gt; marker),\n  pos       = Position(\\(x) x &gt; marker, x)\n)\n\n\n# make it slightly more challenging\nx = sort(rnorm(1e6))\n\nmarker = 4 \n\nbm_first_true_2 = bench::mark(\n  which     = which(x &gt; marker)[1],\n  which_max = which.max(x &gt; marker),\n  pos       = Position(\\(x) x &gt; marker, x)\n)\n\nInterestingly Position provides the best memory performance, but is prohibitively slower. which.max is probably your best bet.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nwhich\n15µs\n20µs\n79.14KB\n1\n14.19\n\n\nwhich_max\n18µs\n20µs\n39.11KB\n1.01\n7.01\n\n\npos\n1.986ms\n2.158ms\n5.58KB\n109.67\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nwhich\n1.44ms\n1.69ms\n7.63MB\n1\n1400.58\n\n\nwhich_max\n1.79ms\n2ms\n3.81MB\n1.18\n700.29\n\n\npos\n213.55ms\n219.13ms\n5.58KB\n129.4\n1\n\n\n\n\n\n\n\nBut not so fast? The following makes the first case come very quickly, where Position blows the other options out of the water! I guess if you knew this was going to be the case you could take serious advantage.\n\nset.seed(123)\n\nx = sort(rnorm(100000), decreasing = TRUE)\n\nx[1:30] = 4\n\n\nbm_first_true_3 = bench::mark(\n  which     = which(x &lt; marker)[1],\n  which_max = which.max(x &lt; marker),\n  pos       = Position(\\(x) x &lt; marker, x) \n)\n\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\npos_rev\n10µs\n10µs\n5.58KB\n1\n1\n\n\nwhich_max_rev\n110µs\n130µs\n390.67KB\n13.04\n70.04\n\n\nwhich_rev\n160µs\n220µs\n1.14MB\n22.55\n210.09"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#group-by-filteringslicing",
    "href": "posts/2022-07-25-programming/index.html#group-by-filteringslicing",
    "title": "Programming Odds & Ends",
    "section": "Group by filtering/slicing",
    "text": "Group by filtering/slicing\nThe previous situation was the basis for this next demo where we utilize which.max. Here we want to filter in one scenario, such that if all values are zero, we drop them, and in the second, we want to only retain certain values based on a condition. In this latter case, the condition is that at least one non-zero has occurred, in which case we want to keep all of those values from that point on (even if they are zero).\nTo make things more clear, for the example data that follows, we want to drop group 1 entirely, the initial part of group 2, and retain all of group 3.\n\nlibrary(tidyverse)\n\nset.seed(12345)\n\ndf = tibble(\n  group = rep(1:3, e = 10),\n  value = c(\n    rep(0, 10),\n    c(rep(0, 3), sample(0:5, 7, replace = TRUE)), \n    sample(0:10, 10, replace = TRUE)\n  )\n)\n\n\ndf %&gt;% \n  group_by(group) %&gt;% \n  filter(!all(value == 0)) %&gt;% \n  slice(which.max(value &gt; 0):n())\n\n# A tibble: 17 × 2\n# Groups:   group [2]\n   group value\n   &lt;int&gt; &lt;dbl&gt;\n 1     2     5\n 2     2     2\n 3     2     1\n 4     2     3\n 5     2     1\n 6     2     4\n 7     2     2\n 8     3     7\n 9     3     1\n10     3     5\n11     3    10\n12     3     5\n13     3     6\n14     3     9\n15     3     0\n16     3     7\n17     3     6\n\n\nIn the above scenario, we take two steps to illustrate our desired outcome conceptually. Ideally though, we’d like one step, because it is just a general filtering. You might think maybe to change which.max to which and just slice, but this would remove all zeros, when we want to retain zeros after the point where at least some values are greater than zero. Using row_number was a way I thought to get around things.\n\ndf %&gt;% \n  group_by(group) %&gt;% \n  filter(!all(value == 0) & row_number() &gt;= which.max(value &gt; 0))\n\n\nbm_filter_slice = bench::mark(\n  orig = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0)) %&gt;% \n    slice(which.max(value &gt; 0):n()),\n  \n  new = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0) & row_number() &gt;= which.max(value &gt; 0))\n)\n\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\norig\n2ms\n2.2ms\n10.25KB\n1\n1.11\n\n\nnew\n6.1ms\n7.1ms\n9.21KB\n3.28\n1\n\n\n\n\n\n\n\nWell we got it to one operation, but now it takes longer and has no memory advantage. Are we on the wrong track? Let’s try with a realistically sized data set with a lot of groups.\n\nset.seed(1234)\n\nN = 100000\ng = 1:(N/4)\n\ndf = tibble(\n  group = rep(g, e = 4),\n  value = sample(0:5, size = N, replace = TRUE)\n)\n\nbm_filter_slice2 = bench::mark(\n  orig = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0)) %&gt;% \n    slice(which.max(value &gt; 0):n()),\n  \n  new = df %&gt;% \n    group_by(group) %&gt;% \n    filter(!all(value == 0) & row_number() &gt;= which.max(value &gt; 0))\n)\n\nNow we have the reverse scenario. The single filter is notably faster and more memory efficient.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\nnew\n208.6ms\n209.9ms\n12.9MB\n1\n1\n\n\norig\n949.8ms\n949.8ms\n28.3MB\n4.53\n2.19"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#tidy-timings",
    "href": "posts/2022-07-25-programming/index.html#tidy-timings",
    "title": "Programming Odds & Ends",
    "section": "Tidy timings",
    "text": "Tidy timings\n\nOverview\n\nThis tidy timings section comes from a notably old exploration I rediscovered (I think it was originally Jan 2020), but it looks like tidyfast still has some functionality beyond dtplyr, and it doesn’t hurt to revisit. I added a result for collapse. My original timings were on a nicely suped up pc, but the following are on a year and a half old macbook with an M1 processer, and were almost 2x faster.\n\nHere I take a look at some timings for data processing tasks. My reason for doing so is that dtplyr has recently arisen from the dead, and tidyfast has come on the scene, so I wanted a quick reference for myself and others to see how things stack up against data.table.\nSo we have the following:\n\nBase R: Just kidding. If you’re using base R approaches for this aggregate you will always be slower. Functions like aggregate, tapply and similar could be used in these demos, but I leave that as an exercise to the reader. I’ve done them, and it isn’t pretty.\ndplyr: standard data wrangling workhorse package\ntidyr: has some specific functionality not included in dplyr\ndata.table: another commonly used data processing package that purports to be faster and more memory efficient (usually but not always)\ntidyfast: can only do a few things, but does them quickly.\ncollapse: many replacements for base R functions.\n\n\n\nStandard grouped operation\nThe following demonstrates some timings from this post on stackoverflow. I reproduced it on my own machine based on 50 million observations. The grouped operations that are applied are just a sum and length on a vector. As this takes several seconds to do even once, I only do it one time.\n\nset.seed(123)\nn = 5e7\nk = 5e5\nx = runif(n)\ngrp = sample(k, n, TRUE)\n\ntiming_group_by_big = list()\n\n\n# dplyr\ntiming_group_by_big[[\"dplyr\"]] = system.time({\n    df = tibble(x, grp)\n    r.dplyr = summarise(group_by(df, grp), sum(x), n())\n})\n\n# dtplyr\ntiming_group_by_big[[\"dtplyr\"]] = system.time({\n    df = lazy_dt(tibble(x, grp))\n    r.dtplyr = df %&gt;% group_by(grp) %&gt;% summarise(sum(x), n()) %&gt;% collect()\n})\n\n# tidyfast\ntiming_group_by_big[[\"tidyfast\"]] = system.time({\n    dt = setnames(setDT(list(x, grp)), c(\"x\",\"grp\"))\n    r.tidyfast = dt_count(dt, grp)\n})\n\n# data.table\ntiming_group_by_big[[\"data.table\"]] = system.time({\n    dt = setnames(setDT(list(x, grp)), c(\"x\",\"grp\"))\n    r.data.table = dt[, .(sum(x), .N), grp]\n})\n\n# collapse\ntiming_group_by_big[[\"collapse\"]] = system.time({\n     df = tibble(x, grp)\n    r.data.table = fsummarise(fgroup_by(df, grp), x = fsum(x),  n = fnobs(x))\n})\n\ntiming_group_by_big = timing_group_by_big %&gt;% \n  do.call(rbind, .) %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column('package')\n\n\n\n\n\n\n\n\n\npackage\nelapsed\n\n\n\n\ndplyr\n7.03\n\n\ndtplyr\n1.30\n\n\ncollapse\n1.02\n\n\ndata.table\n0.67\n\n\ntidyfast\n0.47\n\n\n\n\n\n\n\nWe can see that all options are notable improvements on dplyr. tidyfast is a little optimistic, as it can count but does not appear to do a summary operation like means or sums.\n\n\nCount\nTo make things more evenly matched, we’ll just do a simple grouped count. In the following, I add a different option for dplyr if all we want are group sizes. In addition, you have to ‘collect’ the data for a dtplyr object, otherwise the resulting object is not actually a usable tibble, and we don’t want to count the timing until it actually performs the operation. You can do this with the collect function or as_tibble.\n\ndata(flights, package = 'nycflights13')\nhead(flights)\n\nflights_dtp = lazy_dt(flights)\n\nflights_dt = data.table(flights)\n\nbm_count_flights = bench::mark(\n  dplyr_base = count(flights, arr_time),\n  dtplyr     = collect(count(flights_dt, arr_time)),\n  tidyfast   = dt_count(flights_dt, arr_time),\n  data.table = flights_dt[, .(n = .N), by = arr_time],\n  iterations = 100,\n  check = FALSE\n)\n\nHere are the results. It’s important to note the memory as well as the time. The faster functions here are taking a bit more memory to do it. If dealing with very large data this could be more important if operations timings aren’t too different.\n\n\n\n\n\n\n\n\nexpression\nmin\nmedian\nmem_alloc\nmedian_relative\nmem_relative\n\n\n\n\ndata.table\n2ms\n2.4ms\n9.07MB\n1\n1.5\n\n\ntidyfast\n2ms\n3.3ms\n9.05MB\n1.38\n1.49\n\n\ndplyr_gs\n3.7ms\n4ms\n6.06MB\n1.65\n1\n\n\ndtplyr\n3.5ms\n4.2ms\n9.06MB\n1.73\n1.5\n\n\ndplyr_base\n9.3ms\n10.5ms\n6.12MB\n4.31\n1.01\n\n\n\n\n\n\n\nJust for giggles I did the same in Python with a pandas DataFrame, and depending on how you go about it you could be notably slower than all these methods, or less than half the standard dplyr approach. Unfortunately I can’t reproduce it here3, but I did run it on the same machine using a df.groupby().size() approach to create the same type of data frame. Things get worse as you move to something not as simple, like summarizing with a custom function, even if that custom function is still simple arithmetic.\nA lot of folks that use Python primarily still think R is slow, but that is mostly just a sign that they don’t know how to effectively program with R for data science. I know folks who use Python more, but also use tidyverse, and I use R more but also use pandas quite a bit. It’s not really a debate - tidyverse is easier, less verbose, and generally faster relative to pandas, especially for more complicated operations. If you start using tools like data.table, then there is really no comparison for speed and efficiency. You can run the following for comparison.\n\nimport pandas as pd\n\n\n# flights = r.flights\nflights = pd.read_parquet('data/flights.parquet')\n\nflights.groupby(\"arr_time\", as_index=False).size()\n\n      arr_time  size\n0          1.0   201\n1          2.0   164\n2          3.0   174\n3          4.0   173\n4          5.0   206\n...        ...   ...\n1406    2356.0   202\n1407    2357.0   207\n1408    2358.0   189\n1409    2359.0   222\n1410    2400.0   150\n\n[1411 rows x 2 columns]\n\ndef test(): \n  flights.groupby(\"arr_time\", as_index=False).arr_time.count()\n \ntest()\n\n\nimport timeit\n\ntimeit.timeit() # see documentation\n\n0.004073165997397155\n\ntest_result = timeit.timeit(stmt=\"test()\", setup=\"from __main__ import test\", number = 100)\n\n# default result is in seconds for the total number of 100 runs\ntest_result/100*1000  # ms per run \n\n3.1555674999253824"
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#summary",
    "href": "posts/2022-07-25-programming/index.html#summary",
    "title": "Programming Odds & Ends",
    "section": "Summary",
    "text": "Summary\nProgramming is a challenge, and programming in a computationally efficient manner is even harder. Depending on your situation, you may need to switch tools or just write your own to come up with the best solution."
  },
  {
    "objectID": "posts/2022-07-25-programming/index.html#footnotes",
    "href": "posts/2022-07-25-programming/index.html#footnotes",
    "title": "Programming Odds & Ends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ndata.table modifies in place, so it technically it doesn’t have anything to fill after the first run. As a comparison, I created new columns as the filled in values, and this made almost no speed/memory difference. I also tried copy(dt_missing)[...], which had a minor speed hit. I also tried using setkey first but that made no difference. Note also that data.table has setnafill, but this apparently has no grouping argument, so is not demonstrated.↩︎\nAs of this writing, I’m new to the collapse package, and so might be missing other uses that might be more efficient.↩︎\nThis is because reticulate still has issues with M1 out of the box, and even then getting it to work can be a pain.↩︎"
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html",
    "href": "posts/2021-10-30-double-descent/index.html",
    "title": "Double Descent",
    "section": "",
    "text": "A co-worker passed along a recent article (Dar, Muthukumar, and Baraniuk 2021) on the topic of double descent in machine learning. I figured I’d summarize some key points I came across while perusing it and some referenced articles. In addition, I’ll provide an accessible example demonstrating the phenomenon."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#what-is-double-descent",
    "href": "posts/2021-10-30-double-descent/index.html#what-is-double-descent",
    "title": "Double Descent",
    "section": "What is double descent?",
    "text": "What is double descent?\n\nBias-variance trade-off\nTo understand double descent you have to revisit the concept of the bias-variance trade-off. Without going into too much detail, the main idea with it is that having an overly complex model leads to overfitting the training data, which results in worse prediction on new data, at least relative to what simpler models would have done. The classic figure looks like the following, where blue is the training error and the red is the test error. Thin lines represent one path of complexity (e.g. across a random sample of the data), while the thicker lines are the average at a particular point of model complexity.\n\nIf we don’t have a sufficiently complex model, both training and test error will be poor, the case of underfitting. Our model is a poor approximation of the true underlying function, and predicts poorly on data both seen and unseen. When we have too much model complexity relative to the size of our data (e.g. more covariates, nonlinear effects, interactions, etc.), we pass into the overfit situation. Essentially, while our model function would result in a decrease in error with the data it’s trained on (lower bias as it better approximates the true underlying function), with too much complexity, you’d also eventually have notable changes in prediction (high variance) with any slight deviation in the underlying training data. We can even get to the point where we fit the training data perfectly, but it will be overly susceptible to the noise in the data, and not do well with unseen observations.\nTo combat this, we usually attempt to find a balance between overly simple and overly complex models. This would be the point where test error is among its lowest point for a desirable level of complexity (e.g. around 20-25 df in the figure above), before it begins to rise again. This may be accomplished more explicitly, for example, picking a model through cross-validation, or more implicitly, for example, through regularization (Belkin et al. (2019)). For more detail on the bias-variance trade-off, you can look at the exposition in the main article noted above, my document here, or any number of places, as it is an extremely well-known idea in machine learning.\n\n\nDouble Descent\nThe funny thing is, it turns out that the above actually only applies to a specific scenario, one which we will call underparameterized models. We can simplify this notion by just thinking of the case where the number of our parameters to estimate is less than or equal to the number of observations we have to work with. Nowadays though, it’s not uncommon to have what we’d call overparameterized models, such as random forests and neural networks, sometimes with even billions of parameters, far exceeding the data size. In this scenario, when we revisit the trade-off, something unusual happens!\n\n\n\nFigure from Dar, Muthukumar, and Baraniuk (2021)\n\n\nSuch models may have near zero training error, yet do well on unseen data. As we increase complexity, we see something like a second bias-variance trade-off beyond the point where the data is perfectly fit (interpolated). This point is where model complexity (e.g. in terms of number of parameters) p equals the number of observations N, and this is where the realm of the overparameterized models begins. Now test error begins to drop again with increasing complexity."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#an-example",
    "href": "posts/2021-10-30-double-descent/index.html#an-example",
    "title": "Double Descent",
    "section": "An example",
    "text": "An example\nI thought it would be amusing to try this with the classic mtcars data set available in base R. With this data, our goal will be to predict fuel consumption in miles per gallon (mpg). First we will split the data into training and test components. We create a model where our number of parameters (p), in this case standard regression coefficients, will equal the number of observations (N). Some of the more technically savvy will know that if the number of features and/or parameters to estimate p equals the number of observations N, a standard linear regression model will fit the data perfectly1, demonstrated below.\n\n\nIf not familiar, the mtcars object is a data frame that comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\nnc = ncol(mtcars) \nnr = nc\nfit_perfect = lm(mpg ~ ., mtcars[1:nr, ])\n# summary(fit_perfect) # not shown, all inferential estimates are NaN\n\n\n\n\n\n\n\n\n\n\nNow let’s look at the test error, our prediction on the unseen data we didn’t use in fitting the model. When we do, we see the usual bias-variance trade-off. Our generalizability capabilities have plummeted, as we have overfit the training data and were unable to accommodate unseen observations. We are even predicting negative mpg in some cases!\n\n\n\n\n\n\n\n\n\n\np ≤ N\nLet’s extend the demonstration more fully. We now create models of increasing complexity, starting with an intercept only model (i.e. just using the mean for prediction), to one where all other columns (10) in the data are predictors. Here I repeatedly sampled mtcars of size \\(N = 10\\) for training, the remainder for test, and also shuffled the columns each time, doing so for a total of 250 times2. Here is the result- the classic bias variance trade-off curve. The larger dot shows the test error minimum, at about 3 covariates (plus intercept). The vertical line denotes our point of interpolation.\n\n\n\nDouble Descent in the underparameterized setting.\n\n\n\n\np &gt; N\nSo with one of the simpler data sets around we were able to demonstrate the bias-variance trade-off clearly. But now let’s try overparameterized models! We don’t need anything fancy or complicated to do this, so for our purposes, I’m just going to add cubic spline basis expansions for the wt, disp, and hp features3. This will definitely be enough to put us in a situation where we have more parameters than data, i.e. p &gt; N, but doesn’t make things too abstract4.\nThe basic linear model approach we might typically use fails to estimate the additional parameters in this situation, so we need a different estimator. Some are familiar with penalized regression techniques such as lasso and ridge regression, and we could use those here. However, I’ll use ridgeless regression, as depicted in Hastie et al. (2019), and which, like ridge regression, is a straightforward variant of the usual least squares regression5. I estimate the coefficients/weights on the training data, and make predictions for the training and test set, calculating their respective errors. Here is an example of the primary function used.\n\nfit_ridgeless = function(X_train, y, X_test, y_test){\n  # get the coefficient estimates\n  b = pseudo_inv(crossprod(X_train)) %*% crossprod(X_train, y)\n  \n  # get training/test predictions\n  predictions_train = X_train %*% b\n  predictions_test  = X_test %*% b\n  \n  # get training/test error\n  rmse_train = sqrt(mean((y - predictions_train[,1])^2))\n  rmse_test  = sqrt(mean((y_test - predictions_test[,1])^2))\n  \n  # return result\n  list(\n    b = b,\n    predictions_train = predictions_train,\n    predictions_test  = predictions_test,\n    rmse_train = rmse_train,\n    rmse_test  = rmse_test\n  )\n}\n\nWe can test the function as follows with as little as 10 observations, where p (all predictor coefficients plus intercept = 11 parameters) is greater than N (10). This demonstrates that the ridgeless approach can provide an estimate for all the parameters (unlike the standard lm function), and we also see very low training error, but relatively high test error (in terms of the root mean square error.)\n\nn = 10\n\nX = as.matrix(cbind(1, mtcars[, -1]))\ny = mtcars$mpg # mpg is the first column\n\nX_train = X[1:n, ]\ny_train = mtcars$mpg[1:n]\nX_test  = X[-(1:n),]\ny_test  = y[-(1:n)]\n\nresult = fit_ridgeless(X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\nb\n\n\n\n\n0.84\n\n\n−1.69\n\n\n0.08\n\n\n−0.08\n\n\n2.76\n\n\n−1.29\n\n\n0.24\n\n\n2.32\n\n\n3.26\n\n\n2.26\n\n\n0.66\n\n\n\n\n\n\n\n\n\n\n\n\n\nrmse_train\nrmse_test\n\n\n\n\n0.05\n5.79\n\n\n\n\n\n\n\nIf we do this for more complex models (max linear features, plus each additional set of features associated with a cubic spline basis expansions), we obtain the following. Now we see the second descent in test error takes form!\n\n\n\nDouble Descent in the overparameterized setting.\n\n\nPutting our results together gives us the double descent curve.\n\n\n\nDouble Descent in the overparameterized setting.\n\n\n\n\nNote that this all holds for the most part with classification problems, including multiclass (or multivariate/class targets).\nWe not only see the double descent pattern, but we can also note that the global test error minimum occurs with the model with the most parameters. The gray dot is the lowest test error with the underparameterized settings, while the dark red is the global test error minimum."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#why-does-this-happen",
    "href": "posts/2021-10-30-double-descent/index.html#why-does-this-happen",
    "title": "Double Descent",
    "section": "Why does this happen?",
    "text": "Why does this happen?\nUnderstanding the double descent phenomenon is an area of active research, and there are some technical issues we won’t cover here. However, we can note a couple things more broadly. When we’re in the underparameterized situation, we ultimately begin to force features that have no association with the target to fit the data anyway. Once you move beyond the point of where these features are useful, test error begins to rise again, until the point of interpolation where test error is even worse than guessing (or just guessing in the classification case).\nBeyond the interpolation point, all models we potentially employ using this estimation technique will have the capacity to fit the training data perfectly, i.e. zero bias. This allows us to fit the remaining noise in the data with the additional features employed by the more complex models. There is no guarantee that among the models you fit that the lowest test error will be found relative to the underparameterized setting. However, the lowest test error to be found is ‘out there’ somewhere6. So adding complexity will potentially allow you to find improved test error.\nAnother way to put it is that we have a single class of models to consider, and under and overparameterized are special cases of that more general class. Any one of these might result in the lowest test error. The overparameterized models, which may contain complex nonlinearities and interactions, are likely to be more compatible with the data than the simpler models7. So odds are good that at least one of them will have a smaller test error as well. In any case, restricting ourselves to the underparameterized setting is definitely no guarantee that we will find the most performant model.\nOne caveat is that the model we used is an example of ‘implicit’ regularization, one in which there is no hyper-parameter to set (or discover through cross-validation), like with ridge and lasso. With other techniques (e.g. optimally chosen ridge regression estimator) we may still be able to achieve optimal test error without complete interpolation, and show a reduced peak.\nDar, Muthukumar, and Baraniuk (2021) note that in the overparameterized setting, we can distinguish the signal part of the error term that reduces as a function of N/p, where the noise part of the error term is a function of p/N. In addition, there is a portion of test error related to model misspecification, which will always decrease with overparameterization. In addition, one must consider both feature correlations as well as correlations among observations. Having more complex covariance structure doesn’t negate the double descent phenomenon, but they suggest that, for example, cases where there is low effective dimension within these additional features will more readily display the double descent.\nAnother issue is that in any given situation it is difficult to know where in the realm of available models we exist presently. So additional complexity, or even additional data, may in fact hurt performance (Nakkiran et al. 2019)."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#conclusion",
    "href": "posts/2021-10-30-double-descent/index.html#conclusion",
    "title": "Double Descent",
    "section": "Conclusion",
    "text": "Conclusion\nThe double descent phenomenon is a quite surprising scenario, especially for those who have only heard of the classical bias-variance trade off. There is still much to learn regarding it, but such research is off and running. For practical purposes, it is worth keeping it in mind to aid us in model selection and thinking about our modeling strategies in general."
  },
  {
    "objectID": "posts/2021-10-30-double-descent/index.html#footnotes",
    "href": "posts/2021-10-30-double-descent/index.html#footnotes",
    "title": "Double Descent",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR2 = 1 in the standard linear model setting.↩︎\nNote that the intercept term is added after data shuffling so when p = 1 it is the intercept only model, i.e. guessing the mean.↩︎\nI used mgcv to so this, then added them in whole for each term to the previously shuffled model matrix. These columns are not shuffled. By default these will add 10 columns each to the model matrix.↩︎\nFor more on generalized additive models, see my document.↩︎\nRidgeless regression has the same form as the ‘normal’ equations for least squares, but instead of \\(\\beta \\sim (X^TX)^{-1} \\cdot X^Ty\\), we have \\(\\beta \\sim (X^TX)^{+} \\cdot X^Ty\\) where the first part is the pseudo-inverse of \\(X\\). It is similar to equations for ridge regression (see my demo here) and can be seen as an approximation to it as the ridge penalty tends toward zero.↩︎\nFox Mulder told me so.↩︎\nBecause nature is just funny that way.↩︎"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html",
    "href": "posts/2021-05-time-series/index.html",
    "title": "Exploring Time",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#intro",
    "href": "posts/2021-05-time-series/index.html#intro",
    "title": "Exploring Time",
    "section": "Intro",
    "text": "Intro\n\nThis post was mostly complete around May 2021, but for various reasons not actually posted until August of 2022. I haven’t changed much aside from adding a section on boosting, and have used the results I conjured up previously (for the most part). However, many package updates since then may mean that parts of the code may not work as well, especially for the torch code. I would also recommend modeltime as starting point for implementing a variety of model approaches for time series data with R. It was still pretty new when this was first written, but has many new features and capabilities, and could do some version of the models shown here.\n\nIt is extremely common to have data that exists over a period of time. For example, we might have yearly sports statistics, daily manufacturing records, server logs that might be occurring many times per second, and similar. There are many approaches we could use to model the data in these scenarios. When there are few time points and they are clustered within other units, like repeated observations of exercise data for many individuals, we often use tools like mixed models for example, and even with many observations in a series, we can still use tools like that. But sometimes there may be no natural clustering, or we might want to use other approaches to handle additional complexity.\nThis post is inspired by a co-worker’s efforts in using PyTorch to analyze Chicago Transit data. Cody Dirks wrote a post where he used a Python module developed by our group at Strong Analytics to analyze the ridership across all the ‘L’. This post can be seen as a demonstration of some simpler models which might also be viable for a given situation such as this, allowing for quick dives, or even as ends in themselves."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#outline",
    "href": "posts/2021-05-time-series/index.html#outline",
    "title": "Exploring Time",
    "section": "Outline",
    "text": "Outline\nThe models we’ll go through are the following:\n\nError models and random effects\nGAM\nMore elaborate time series with seasonal and other effects\nBoosting and Deep learning\n\nIn what follows I will show some more detailed code in the beginning, but won’t show it later for conciseness, focusing mostly just on the basic model code. You can always find the code for these posts on my GitHub."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#quick-summary",
    "href": "posts/2021-05-time-series/index.html#quick-summary",
    "title": "Exploring Time",
    "section": "Quick Summary",
    "text": "Quick Summary\n\nClassical econometrics approaches like ARIMA models may take notable effort to match the flexibility of other approaches one might take with time series. It’s also difficult to believe a specific lag/ma number will hold up with any data change.\nGAMs extend mixed models, and should probably be preferred if a probabilistic approach is desired. Prophet-style approaches would likely take notable effort and still likely lack performance, without adding interpretability.\nFor black box methods, boosting can do very well without much feature engineering, but possibly take a bit more for parameter tuning. Deep learning methods may be your best bet given data size and other data modeling needs."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#data-description",
    "href": "posts/2021-05-time-series/index.html#data-description",
    "title": "Exploring Time",
    "section": "Data Description",
    "text": "Data Description\nAs noted in Cody’s post, over 750,000 people use the Chicago Transit Authority’s ‘L’ system to get around the city. There are 8 interconnected rail lines named after colors- the Red, Blue, Green, Brown, Pink, Orange, Purple, and Yellow, 145 entry/exit stations, and over 2,300 combined trips by its railcars every day1.\nThe city of Chicago provides ridership data that can be accessed publicly.\n\nridership\nstation info\n\nIn Cody’s exploration, he added pertinent information regarding weather, sporting events, and more. You can access the processed data.\nFor our demonstrations we have daily ridership from 2012-2018, and we will use a variety of methods to model this. We will use a normalized ride count (mean of 0, standard deviation of 1) as our target variable.\n\nImport & Setup\nTo get things started we’ll use the tidyverse for some additional data processing, and lubridate for any date processing, for example, converting to weekdays.\n\n# Data Processing\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n# Misc\n\nSTART_DT = '2008-06-01'\nEND_DT   = '2018-12-31'\nSPLIT_DT = '2017-06-01'\n\n\nMain data\nI start with data having already been processed, but as mentioned the source is publicly available. I use data.table to read it in more quickly, but it’s default date class can cause issues with other packages, so I deal with that. I also extract the year, month, weekday, etc.\n\ndf = data.table::fread('data/time-series/processed_df.csv')\n\ndf_start = df %&gt;% \n  as_tibble() %&gt;% \n  select(-contains('_attributes'), -(tsun:wt22)) %&gt;% \n  mutate(\n    date      = as_date(date), # remove IDATE class\n    rides_log = log(rides),\n    year      = year(date),\n    year_fac  = factor(year),\n    month     = month(date, label = TRUE),\n    day       = factor(wday(date, label = TRUE), ordered = FALSE),\n    year_day  = lubridate::yday(date),\n    line      = factor(line),\n    snow_scaled = scale(snow)[, 1],\n    colors = as.character(line),\n    colors = ifelse(colors == 'purple_express', 'purple4', colors),\n    red_line_modernization = \n      ifelse(\n        between(date, as_date('2013-05-19'), as_date('2013-10-20')), \n        1, \n        0\n      )\n  ) %&gt;% \n  arrange(date, line)\n\n\n\n\nTraining and Validation\nWe split our data into training and validation sets, such that everything before 2017-06-01 is used for training, while everything after will be used for testing model performance2.\n\ndf_train = df_start %&gt;% \n  filter(date &lt; SPLIT_DT, !is.na(rides))\n\ndf_validate = df_start %&gt;% \n  filter(date &gt;= SPLIT_DT, !is.na(rides))\n\nred_line_train = df_train %&gt;% \n  filter(line == 'red')\n\nred_line_validate = df_validate %&gt;% \n  filter(line == 'red')\n\n\n\nOther\nHolidays are available via the prophet package, which we’ll be demonstrating a model with later. The data we’re using already has a ‘holiday vs. not’ variable for simplicity, though it comes from a different source. The prophet version has both the actual date and the observed date counted as a holiday, and I prefer to use both.\n\nholidays = prophet::generated_holidays %&gt;% \n  filter(country == 'US') %&gt;% \n  mutate(ds = as.numeric(as_date(ds))) %&gt;%\n  droplevels()\n\nWe’ll take a quick look at the red line similar to Cody’s post, so we can feel we have the data processed as we should.\n\n\n\n\n\n\n\n\n\nWith the data ready to go, we are ready for modeling, so let’s get started!"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#classical-time-series",
    "href": "posts/2021-05-time-series/index.html#classical-time-series",
    "title": "Exploring Time",
    "section": "Classical Time Series",
    "text": "Classical Time Series\n\nIntro\nClassical times series from an econometrics perspective often considers a error model that accounts for the correlation a current observation has with past observations. A traditional example is the so-called autoregressive, or AR, model, which lets a current observation be predicted by past observations up to a certain point. For example, would could start by just using the last observation to predict the current one. Next we extend this to predict the current based on the previous two observations, and so on. How many lags we use is part of the model exploration.\n\\[y_t = \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + \\varepsilon_t\\]\nWe can extend this to include not just past observations but also past residuals, called a moving average. So formally, our ARMA (p, q) model now looks like this for an observation \\(y\\) at time \\(t\\):\n\\[y_t = \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots +\\theta_q \\varepsilon_{t-q})\\]\nWe can also use differencing, for example subtracting the previous time value from the current observation value for all values, to come to the final ARIMA (p, d, q) model. See Hyndman and Athanasopoulos (2021) for more details.\n\n\nModel\nEven base R comes with basic time series models such as this. However, as mentioned, we typically don’t know what to set the values of an ARIMA(p, d, q) to. A quick way to explore this is via the forecast package, which will search over the various hyperparameters and select one based on AIC. Note that fable, a package we will be using later, will also allow such an approach, and if you’d like to go ahead and start using it I show some commented code below.\n\nmodel_arima = forecast::auto.arima(\n  red_line_train$rides_scaled\n)\n\n# model_arima = red_line_train %&gt;%\n#   select(date, rides_scaled) %&gt;%\n#   tsibble::as_tsibble() %&gt;%\n#   fabletools::model(fable::ARIMA(\n#     rides_scaled ~ 0 + PDQ(0,0,0),\n#     stepwise = FALSE,\n#     approximation = FALSE\n#   ))\n# fabletools::report(model_arima)\n\n\n\nExplore\nIn this case we have a selected AR of 3 and MA of 4 for the centered value. But looking at the predictions, we can see this is an almost useless result for any number of days out, and does little better than guessing.\n\nbroom::tidy(model_arima)\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n0.238\n0.048\n\n\nar2\n-0.285\n0.035\n\n\nar3\n0.354\n0.043\n\n\nma1\n-0.725\n0.046\n\n\nma2\n-0.189\n0.046\n\n\nma3\n-0.575\n0.029\n\n\nma4\n0.552\n0.025\n\n\n\n\n\n\n\n\n# plot(acf(residuals(model_arima))) # weekly autocorrelation still exists\n\nred_line_validate %&gt;% \n  slice(1:30)  %&gt;%\n  mutate(pred = predict(model_arima, n.ahead = 30)$pred) %&gt;%\n  # mutate(pred = forecast(model_arima, h = 30)$.mean) %&gt;%\n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'darkred')\n\n\n\n\n\n\n\n\nWe’ll use yardstick to help us evaluate performance for this and subsequent models. In this case however, the visualization told us enough- a basic ARIMA isn’t going cut it.\n\nlibrary(yardstick)\n\n# this function will be used for all subsequent models!\nmetric_score = metric_set(rmse, mae, rsq) \n\n# validation\ndata.frame(\n  pred = predict(model_arima, newdata = red_line_validate, n.ahead = 30)$pred,\n  observed = red_line_validate$rides_scaled[1:30]\n) %&gt;%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.632\n\n\nmae\nstandard\n0.572\n\n\nrsq\nstandard\n0.132\n\n\n\n\n\n\n\nOne nice thing about the forecast package is that it can include additional features via the xreg argument, which is exactly what we need- additional information. Now our model looks something like this, where \\(X\\) is our model matrix of features and \\(\\beta\\) their corresponding regression weights.\n\\[y_t = X_t\\beta + \\alpha_1y_{t-1} + \\dots +\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots +\\theta_q \\varepsilon_{t-q})\\]\nAdding these is not exactly straightforward, since it requires a matrix rather than a data frame, but this is not too big a deal once you are used to creating model matrices.\n\nmm = model.matrix(\n  ~ . - 1, \n  data = red_line_train %&gt;% \n    select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization)\n)\n\nmodel_arima_xreg = forecast::auto.arima(\n  red_line_train$rides_scaled,\n  max.p = 10,\n  max.q = 10,\n  xreg  = mm\n)\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\nar1\n-0.444\n0.018\n\n\nar2\n-0.430\n0.018\n\n\nar3\n-0.370\n0.019\n\n\nar4\n-0.325\n0.019\n\n\nar5\n-0.312\n0.019\n\n\nar6\n-0.356\n0.019\n\n\nar7\n0.307\n0.018\n\n\nar8\n-0.051\n0.017\n\n\nis_weekend\n-1.154\n0.023\n\n\nis_holiday\n-1.045\n0.021\n\n\nis_cubs_game\n0.208\n0.015\n\n\nis_sox_game\n0.072\n0.015\n\n\ntmax_scaled\n0.085\n0.011\n\n\nprcp_scaled\n-0.031\n0.004\n\n\nred_line_modernization\n-0.550\n0.131\n\n\n\n\n\n\n\nThis is looking much better! We can also see how notably different the ARMA structure is relative to the previous model. We also see that adding weekend and holiday effects result in a huge drop in ridership as expected, while baseball games and good weather will lead to an increase.\nIn the following code, we create a model matrix similar to the training data that we can feed into the predict function. The forecast package also offers a glance method if desired.\n\nnd = red_line_validate %&gt;%\n  select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization) %&gt;% \n  model.matrix( ~ . - 1, data = .)\n\npreds = predict(model_arima_xreg, newxreg = nd, n.ahead = nrow(red_line_validate))$pred\n\n\np_arima_red = red_line_validate %&gt;% \n  mutate(pred = preds) %&gt;% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'red') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'ARIMA')\n\np_arima_red\n\n\n\n\n\n\n\n\n\n\nAnd here we can see performance is notably improved (restrict to first 30 obs for a direct comparison to the previous).\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.371\n\n\nmae\nstandard\n0.282\n\n\nrsq\nstandard\n0.747"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#mixed-model-with-ar-structure",
    "href": "posts/2021-05-time-series/index.html#mixed-model-with-ar-structure",
    "title": "Exploring Time",
    "section": "Mixed model with AR Structure",
    "text": "Mixed model with AR Structure\n\nIntro\nMore generally, we can think of that original AR error as a random effect, such that after the linear predictor is constructed, we add a random effect based on the correlation structure desired, in this case, autoregressive. In the mixed model setting, it is actually quite common to use an AR residual structure within a cluster or group, and here we can do so as well, as the data is naturally grouped by line.\nTo make this a bit more clear, we can state the AR effect more formally as follows for a single line at time \\(t\\):\n\\[z_t  \\sim N(0, \\Sigma_{ar})\\] \\[\\Sigma_{ar} = cov(z(s), z(t)) = \\sigma^2\\exp(-\\theta|t-s|)\\]\nWhere t,s are different time points, e.g. within a line.\nIf we were to simulate it for 4 time points, with autocovariance value of .5, we could do so as follows3.\n\nn_clusters   = 1\nn_timepoints = 4\nmu  = 0\nvar = 1  # not actually used if the value is 1\nS = .5^as.matrix(dist(1:n_timepoints))\n\nS\n\nz = MASS::mvrnorm(mu = rep(mu, n_timepoints), Sigma = S)\n\nz\n\nAnd here is our typical model with a single random effect, e.g. for line:\n\\[ y_{tl} \\sim X\\beta + z^{line}_{l} + e_{tl}\\] \\[\\textrm{z}_{l} \\sim N(0, \\sigma_l^2)\\] \\[\\epsilon \\sim N(0, \\sigma_e^2)\\]\nThe X may be at either line or observation level, and potentially the \\(\\beta\\) could vary by line.\nPutting it all together, we’re just adding the AR random effect to the standard mixed model for a single line.\n\\[ y_{tl} \\sim X\\beta + z^{ar}_t +z^{line}_{l} + e_{tl}\\]\n\n\nData Prep\nSo let’s try this! First some minor data prep to add holidays.\n\ndf_train_mixed = df_train %&gt;% \n  mutate(date = as.numeric(date)) %&gt;% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %&gt;% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\ndf_validate_mixed = df_validate %&gt;% \n  mutate(date = as.numeric(date)) %&gt;% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %&gt;% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\n\n\nModel\nFor the model, we can now easily think of it as we do other standard modeling scenarios. Along with standard features, we’ll add random effects for line, day, day x line interaction, etc. Finally we also add an AR random effect. For each line, we have an autoregressive structure for days, such that days right next to each other are correlated, and this correlation tapers off as days are further apart. This is not our only option, but seems straightforward to me.\nDepending on what you include in the model, you may have convergence issues, so feel free to reduce the complexity if needed. For example, most of the day effect is captured by weekend vs. not, and a by line year trend wasn’t really necessary. In addition, the way the AR random effect variance is estimated as noted above, this essentially captures the line intercept variance.\n\nmodel_mixed = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  tmax_scaled + \n  prcp_scaled + \n  snow_scaled +\n  # year_day +\n  ar1(0 + day|line) +     # the 0 + is a nuance of tmb's approach\n  (1|holiday) +           # as RE with all holidays instead of just holiday vs. not\n  (1|year) +     \n  (1 | red_line_modernization:line) +  # the project shifted ridership from red to other lines\n  # (1|day) #+ \n  # (1|line) +\n  (1|day:line) #+\n  # (1 + year_day|line)\n\nlibrary(glmmTMB)\n\nfit_mixed = glmmTMB(model_mixed, data = df_train_mixed)\n\n\n\nExplore\nThe mixed model approach is nice because it is highly interpretable. We get both standard regression coefficients, and variance components to help us understand how the rest of the variance breaks down. For example, I would interpret the following that that line and weekend are the biggest contributors to the variability seen, and that we have high autocorrelation, as expected.\n\nlibrary(mixedup)\n\nsummarise_model(fit_mixed, digits = 4)\n\nextract_cor_structure(fit_mixed, which_cor = 'ar1')\n\nWe can visually inspect how well it matches the data. In the following the colored lines are the predictions, while the observed is gray. It looks like performance tapers for more recent time periods, and holiday effects are not as prevalent for some lines (e.g. yellow). The latter could be helped by adding a holiday:line random effect.\n\nlibrary(glmmTMB)\n\np_mixed = df_validate_mixed %&gt;% \n  droplevels() %&gt;% \n  mutate(pred = predict(fit_mixed, newdata = ., allow.new.levels=TRUE)) %&gt;%\n  mutate(date = as_date(date)) %&gt;% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .1) +\n  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +\n  facet_grid(rows = vars(line), scales = 'free_y') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'Mixed')\n\np_mixed\n\n\n\n\n\n\n\n\nAs before we can measure performance via yardstick. This model does appears to do very well.\n\n# validation\ndata.frame(\n  pred = predict(fit_mixed, newdata = df_validate_mixed, allow.new.levels = TRUE),\n  observed = df_validate_mixed$rides_scaled\n) %&gt;%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.189\n\n\nmae\nstandard\n0.110\n\n\nrsq\nstandard\n0.965\n\n\n\n\n\n\n\nFor more on autocorrelation structure in the mixed model setting, see my mixed model document here4."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#generalized-additive-models",
    "href": "posts/2021-05-time-series/index.html#generalized-additive-models",
    "title": "Exploring Time",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\n\nIntro\nWe can generalize mixed models even further to incorporate nonlinear components, which may include cyclic or other effects. Such models are typically referred to as generalized additive models (GAMs). AR processes themselves can be seen as a special case of gaussian processes, which can potentially be approximated via GAMs. As GAMs can accommodate spatial, temporal, nonlinear, and other effects, they are sometimes more generally referred to as structured additive regression models, or STARs.\n\n\nData Prep\nThe data prep for the GAM is the same as with the mixed model, so we’ll just use that data.\n\ndf_train_gam = df_train_mixed\n\ndf_validate_gam = df_validate_mixed\n\n\n\nModel\nWith data in place we are ready to conduct the model. We have numerous options for how we’d like to take this. However, as an example, I tried various smooths, but didn’t really see much difference, which is actually a good thing. For any further improvements we’d likely have to tweak the core model itself. I also use bam for a quicker result, but this isn’t really necessary, as it didn’t even take a minute to run with standard gam. As with the mixed model, we will use holiday as a random effect, but we add the holiday by line interaction since we saw that need. In addition, our year-day by line interaction should help some with the tailing off of more recent predictions.\n\nlibrary(mgcv)\n\n# for year, use year (numeric) or use year_fac, but for latter, it will not be\n# able to predict any year not in the training data unless you use\n# drop.unused.levels.\nmodel_gam = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  s(tmax_scaled) + \n  s(prcp_scaled) + \n  s(snow_scaled) +\n  s(red_line_modernization, line, bs = 're') +\n  s(holiday,  bs = 're') +\n  s(holiday, line,  bs = 're') +\n  s(year_fac, bs = 're') +      \n  s(day,  bs = 're') + \n  s(line, bs = 're') + \n  s(line, day, bs = 're') + \n  s(year_day, by = line, bs = c('ds', 'fs'))\n\n\n# will take a while!\n# fit_gam = gam(\n#   model_gam, \n#   data     = df_train_gam,\n#   drop.unused.levels = FALSE, \n#   method   = \"REML\"\n# )\n\n# fast even without parallel\nfit_gam = bam(\n  model_gam, \n  data     = df_train_gam,\n  drop.unused.levels = FALSE, \n  method   = \"fREML\",\n  discrete = TRUE     # will fit the model in a second rather than a couple seconds\n  # nthreads = 8,     # this option assumes a cluster is available. not necessary for this.\n)\n\n\n\nExplore\nAs with glmmTMB, I use a custom function to summarize the model, or extract different components from it. From the initial glance we can see things that we expect (e.g. line and weekend effects are large).\n\nmixedup::summarise_model(fit_gam)\n\n\nVariance Components:\n\n\n                       Group                 Effect Variance   SD SD_2.5\n                 tmax_scaled              Intercept     0.01 0.09   0.04\n                 prcp_scaled              Intercept     0.00 0.01   0.00\n                 snow_scaled              Intercept     0.00 0.01   0.00\n                        line red_line_modernization     0.09 0.31   0.19\n                     holiday              Intercept     0.05 0.22   0.14\n                     holiday                   line     0.04 0.21   0.18\n                    year_fac              Intercept     0.00 0.06   0.04\n                         day              Intercept     0.00 0.00   0.00\n                        line              Intercept     0.49 0.70   0.42\n                        line                    day     0.05 0.22   0.18\n           year_day:lineblue              Intercept     0.00 0.00   0.00\n          year_day:linebrown              Intercept     0.00 0.00   0.00\n          year_day:linegreen              Intercept     0.00 0.00   0.00\n         year_day:lineorange              Intercept     0.00 0.00   0.00\n           year_day:linepink              Intercept     0.00 0.00   0.00\n         year_day:linepurple              Intercept     0.00 0.00   0.00\n year_day:linepurple_express              Intercept     0.00 0.00   0.00\n            year_day:linered              Intercept     0.00 0.00   0.00\n         year_day:lineyellow              Intercept     0.00 0.00   0.00\n                    Residual                     NA     0.02 0.13   0.13\n      SD_97.5 Var_prop\n 2.100000e-01     0.01\n 2.000000e-02     0.00\n 2.000000e-02     0.00\n 4.900000e-01     0.13\n 3.300000e-01     0.06\n 2.400000e-01     0.06\n 1.000000e-01     0.01\n 4.162009e+69     0.00\n 1.150000e+00     0.65\n 2.700000e-01     0.06\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 0.000000e+00     0.00\n 1.300000e-01     0.02\n\n\n\nFixed Effects:\n\n\n         Term Value   SE     t P_value Lower_2.5 Upper_97.5\n    Intercept -0.28 0.24 -1.14    0.25     -0.76       0.20\n   is_weekend -0.54 0.06 -8.27    0.00     -0.66      -0.41\n is_cubs_game  0.04 0.00 13.81    0.00      0.03       0.04\n  is_sox_game  0.01 0.00  3.81    0.00      0.00       0.02\n\n\nNow we can visualize test predictions broken about by line. The greater flexibility of the GAM for fixed and other effects allows it to follow the trends more easily than the standard linear mixed model approach.\n\n\n\n\n\n\n\n\n\nWe can also see improvement over our standard mixed model approach, and our best performance yet.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.140\n\n\nmae\nstandard\n0.087\n\n\nrsq\nstandard\n0.981"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#prophet",
    "href": "posts/2021-05-time-series/index.html#prophet",
    "title": "Exploring Time",
    "section": "Prophet",
    "text": "Prophet\n\nIntro\nProphet is an approach from Facebook that uses Stan to estimate a time series model taking various trends, seasonality, and other factors under consideration. By default, it only uses Stan for optimization (e.g. via ‘BFGS’), but you can switch to fully Bayesian if desired, and take advantage of all that the Bayesian approach has to offer.\n\n\nData Prep\nThe prophet package in R takes some getting used to. We have to have specific names for our variables, and unfortunately have to do extra work to incorporate categorical features. We can use recipes (like yardstick, part of the tidymodels ’verse) to set up the data (e.g. one-hot encoding).\n\nlibrary(prophet)\nlibrary(recipes)\n\ndf_train_prophet = df_train %&gt;% \n  arrange(date, line) %&gt;% \n  rename(y  = rides_scaled,\n         ds = date)\n\nrec = recipes::recipe(~., df_train_prophet)\n\ndf_train_prophet = rec %&gt;% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %&gt;% \n  prep(training = df_train_prophet) %&gt;% \n  bake(new_data = df_train_prophet) %&gt;% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\ndf_validate_prophet = df_validate %&gt;% \n  arrange(date, line)%&gt;%\n  rename(ds = date, y = rides_scaled)\n\nrec = recipe(~., df_validate_prophet)\n\ndf_validate_prophet = rec %&gt;% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %&gt;% \n  prep(training = df_train_prophet) %&gt;% \n  bake(new_data = df_validate_prophet) %&gt;% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\n\n\nModel\nWith data in place, we are ready to build the model. Note that later we will compare results to fable.prophet, which will mask some of the functions here, or vice versa depending on which you load first. I would suggest only doing the prophet model, or only doing the fable model, rather than trying to do both at the same time, to avoid any mix-up.\nAfter setting up the model, you have to add additional features in separate steps. Prophet has a nice way to incorporate holidays though. When you run this model, you may have to wait for a minute or so.\n\n# use prophet::prophet in case you have fable.prophet loaded also\nmodel_prophet = prophet::prophet(\n  holidays = generated_holidays %&gt;% filter(country == 'US'),\n  yearly.seasonality = FALSE,\n  seasonality.mode = \"multiplicative\",\n  changepoint.prior.scale = .5\n)\n\nline_names = c(\n  'blue',\n  'brown',\n  'green',\n  'orange',\n  'pink',\n  'purple',\n  'purple_express',\n  'red',\n  'yellow'\n)\n\npredictors = c(\n  'is_weekend',\n  'is_cubs_game',\n  'is_sox_game',\n  # 'is_holiday',\n  'tmax_scaled',\n  'prcp_scaled',\n  'snow_scaled',\n  line_names\n)\n\nfor (i in predictors) {\n  model_prophet = add_regressor(model_prophet, i, standardize = FALSE, mode = 'additive')\n}\n\nmodel_prophet = add_country_holidays(model_prophet, country_name = 'US')\n\nfit_prophet = fit.prophet(model_prophet, df = df_train_prophet)\n\nforecast = predict(fit_prophet, df_validate_prophet)\n\n\n\nExplore\nWe now visualize predictions as we did with the GAM. But one of the nice things with prophet is that you can plot the various parts of the model results via the plot method or prophet_plot_components (not shown). Unfortunately, our baseline effort seems to undersmooth our more popular lines (blue, red), and overreacts to some of the others (purple, yellow). This is because it’s not really changing the predictions according to line.\n\n\n\n\n\n\n\n\n\nWe can also assess performance as before, but note that prophet has it’s own cross-validation capabilities which would be better to utilize if this was your primary tool. Between the previous visualization and these metrics, our first stab doesn’t appear to do as well as the GAM, so you might like to go back and tweak things.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.280\n\n\nmae\nstandard\n0.198\n\n\nrsq\nstandard\n0.925"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#fable",
    "href": "posts/2021-05-time-series/index.html#fable",
    "title": "Exploring Time",
    "section": "Fable",
    "text": "Fable\n\nIntro\nI came across fable.prophet as a possibly easier way to engage prophet. It is an extension of fable and related packages, which are very useful for time series processing and analysis. Note that it is 0.1.0 version development, and hasn’t had much done with it in the past year, so your mileage may vary with regard to utility by the time you read this5. But with it we can specify the model in more of an R fashion, and we now don’t have as much data pre-processing.\n\n\nData Prep\nOne key difference using fable.prophet is that it works with tsibble objects, and thus must have unique date observations. We can do this by setting line as the key6.\n\nlibrary(fable.prophet)\n\ndf_train_fable = df_train_prophet %&gt;% \n  as_tsibble(index = ds, key = line)\n\ndf_validate_fable = df_validate_prophet %&gt;% \n  as_tsibble(index = ds, key = line)\n\nholidays_fable = holidays %&gt;% \n  filter(country == 'US') %&gt;% \n  mutate(ds = as_date(ds)) %&gt;% \n  as_tsibble()\n\n\n\nModel\nBeyond this we use functions within our formula to set the various components. With line as the key, fable is actually running separate prophet models for each line, and we can do so in parallel if desired.\n\nmodel_prophet = fable.prophet::prophet(\n  y ~ \n    growth('linear', changepoint_prior_scale = 0.5) +\n    season(\"week\", type = \"multiplicative\") +\n    holiday(holidays_fable) +\n    xreg(\n      is_weekend,\n      is_cubs_game,\n      is_sox_game,\n      # is_holiday,\n      tmax_scaled,\n      prcp_scaled,\n      snow_scaled\n    ) \n)\n\n# library(future)\n# plan(multisession)\n\n# furrr is used under the hood, and though it wants a seed, it doesn't\n# automatically use one so will give warnings. I don't think it can be passed\n# via the model function, so expect to see ignorable warnings (suppressed here).\n\nfit_fable = model(df_train_fable, mdl = model_prophet)\n\nforecast_fable = fit_fable %&gt;% \n  forecast(df_validate_fable) \n\n# plan(sequential)\n\n\n\nExplore\nWith fable.prophet visualization, we have the more automatic plots, but again we’ll stick with the basic validation plot we’ve been doing.\n\ncomponents(fit_fable)\ncomponents(fit_fable) %&gt;%\n  autoplot()\nforecast_fable %&gt;%\n  autoplot(level = 95, color = '#ff5500')\n\nThis model does well, and better than basic prophet, but we can see its limitations, for example, with the yellow line, and more recent ridership in general.\n\n\n\n\n\n\n\n\n\nAnd we check performance as before. The fable model is doing almost as well as our GAM approach did.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.154\n\n\nmae\nstandard\n0.092\n\n\nrsq\nstandard\n0.979\n\n\n\n\n\n\n\nOne nice thing about the fable approach is its internal performance metrics, which are easily obtained. It will give us results for each line7, validation data results shown. We see that we have more error for the popular lines as before, but in terms of percentage error, the other lines are showing more difficulty. You can find out more about the additional metrics available here.\n\naccuracy(fit_fable)\naccuracy(forecast_fable, df_validate_fable)\n\n\n\n\n\n\n.model\nline\n.type\nME\nRMSE\nMAE\nMPE\nMAPE\nACF1\n\n\n\n\nmdl\nblue\nTest\n0.083\n0.251\n0.180\n-27.007\n62.199\n0.641\n\n\nmdl\nbrown\nTest\n0.001\n0.126\n0.084\n-31.034\n88.711\n0.621\n\n\nmdl\ngreen\nTest\n0.088\n0.118\n0.100\n-105.627\n112.000\n0.660\n\n\nmdl\norange\nTest\n0.035\n0.084\n0.063\n-43.228\n60.244\n0.638\n\n\nmdl\npink\nTest\n0.058\n0.089\n0.072\n-18.386\n20.452\n0.675\n\n\nmdl\npurple\nTest\n0.006\n0.012\n0.009\n-0.633\n0.982\n0.503\n\n\nmdl\npurple_express\nTest\n0.049\n0.107\n0.083\n-41.459\n218.861\n0.752\n\n\nmdl\nred\nTest\n0.045\n0.297\n0.211\n-3.003\n14.880\n0.598\n\n\nmdl\nyellow\nTest\n-0.027\n0.030\n0.027\n2.892\n2.912\n0.726\n\n\n\n\n\n\n\nThe fable results suggests what we already knew from our GAM and mixed model approach, that interactions of the series with line are important. We weren’t easily able to do this with the default prophet (it would likely require adding time x line interaction regresssors), so it’s nice that we have the option here."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#gbm",
    "href": "posts/2021-05-time-series/index.html#gbm",
    "title": "Exploring Time",
    "section": "GBM",
    "text": "GBM\n\nIntro\nThis part is actually new from when I first wrote up this post over a year ago. I basically wanted to test if a boosting approach would work decently out of the box without doing anything special regarding the structure of the data. I don’t add it to the summary visualizations, but provide the standard results here.\n\n\nData Prep\nI’ll use lightgbm which I’ve been increasingly using of late. It requires a matrix object for input, and so some additional processing as well.\n\nlibrary(lightgbm)\n\n# lightgbm only accepts a matrix input\ndf_train_lgb_init = df_train %&gt;% \n  select(\n    rides_scaled,\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %&gt;% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  )\n\nX_train = as.matrix(df_train_lgb_init %&gt;% select(-rides_scaled))\n\nX_test  = df_validate %&gt;% \n  select(\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %&gt;% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  ) %&gt;% \n  as.matrix()\n\n\ndf_train_lgb = lgb.Dataset(\n  X_train,\n  label = df_train_lgb_init$rides_scaled,\n  categorical_feature = c(\n    'is_weekday',\n    'is_holiday',\n    'is_cubs_game',\n    'is_sox_game',\n    'line',\n    'year',\n    'month'\n  )\n)\n\ndf_test_lgb  = lgb.Dataset.create.valid(\n  df_train_lgb, \n  X_test,\n  label = df_validate$rides_scaled\n)\n\n\n\nModel\nTypically we would perform some sort of search over the (many) parameters available to tweak with lightgbm, like the number of trees, learning rate, regularizer parameters and more. I ignore that, but I did fiddle with the learning rate and bumped up the nrounds (trees), but that’s it.\n\nparams = list(\n  objective       = \"regression\"\n  , metric        = \"l2\"\n  , min_data      = 10L\n  , learning_rate = .01\n)\n\nfit_gbm = lgb.train(\n  params    = params\n  , data    = df_train_lgb\n  , nrounds = 2000L\n)\n\n\n\nExplore\nSome may be surprised at how well this does, but regular users of boosting probably are not. We didn’t have to do much and it’s already the best performing model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.130\n\n\nmae\nstandard\n0.076\n\n\nrsq\nstandard\n0.984"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#torch",
    "href": "posts/2021-05-time-series/index.html#torch",
    "title": "Exploring Time",
    "section": "Torch",
    "text": "Torch\nAt this point we have a collection of models that are still relatively interpretable, and mostly within our standard regression model framework. It’s good to see them able to perform very well without too much complexity. However, we still have other methods available that would be more computationally demanding, are more opaque in operations, but which would potentially provide the most accurate forecasts. For this we turn to using PyTorch, which is now available via the torch package in R8.\nIn using torch, we’re going to follow the demo series at the RStudio AI blog 9. It shows in four parts how to use a recurrent neural network. In their example, they use a data set for a single series with (summarized) daily values, similar to our daily counts here. We will use the final model demonstrated in the series, a soi disant seq2seq model that includes an attention mechanism. More detail can be found here. The conceptual gist of the model can be described as taking a set of time points to predict another set of future time points, and doing so for all points in the series.\nTo be clear, they only use a single series, no other information (e.g. additional regressors). So we will do the same, coming full circle to what we started out with, just looking at daily ridership- a single time series for the red line.\n\nData\nAs usual we’ll need some data prep, both for initial training-test split creation, but also specifically for usage with Torch.\n\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(torch)\n\n\ndf_train_torch = df_train %&gt;%\n  filter(line == 'red', year &lt; 2017) %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ndf_validate_torch = df_validate %&gt;%\n  filter(line == 'red', year &gt;= 2017) %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ndf_test_torch = df_validate %&gt;%\n  filter(line == 'red', date &gt; '2017-12-24') %&gt;%\n  pull(rides_scaled) %&gt;%\n  as.matrix()\n\ntrain_mean = mean(df_train_torch)\ntrain_sd   = sd(df_train_torch)\n\n\n\nTorch data\nFor our data, we will use a week behind lag to predict the following week. This seems appropriate for this problem, but for any particular time series problem we’d want to probably think hard about this and/or test different settings.\n\nn_timesteps = 7    # we use a week instead of 14 days in original blog\nn_forecast  = 7    # look ahead one week\n\n\ncta_dataset &lt;- dataset(\n  name = \"cta_dataset\",\n\n  initialize = function(x, n_timesteps, sample_frac = 1) {\n\n    self$n_timesteps &lt;- n_timesteps\n    self$x &lt;- torch_tensor((x - train_mean) / train_sd)\n\n    n &lt;- length(self$x) - self$n_timesteps - 1\n\n    self$starts &lt;- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n\n  },\n\n  .getitem = function(i) {\n\n    start &lt;- self$starts[i]\n    end &lt;- start + self$n_timesteps - 1\n    lag &lt;- 1\n\n    list(\n      x = self$x[start:end],\n      y = self$x[(start+lag):(end+lag)]$squeeze(2)\n    )\n\n  },\n\n  .length = function() {\n    length(self$starts)\n  }\n)\n\nbatch_size = 32\n\ntrain_ds = cta_dataset(df_train_torch, n_timesteps)\ntrain_dl = dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds = cta_dataset(df_validate_torch, n_timesteps)\nvalid_dl = dataloader(valid_ds, batch_size = batch_size)\n\ntest_ds  = cta_dataset(df_test_torch, n_timesteps)\ntest_dl  = dataloader(test_ds, batch_size = 1)\n\n\n\nModel\nI leave it to the blog for details, but briefly, there are four components to the model:\n\nEncoder: takes input, and produces outputs and states via RNN\nDecoder: takes the last predicted value as input and current context to make a new prediction\nSeq2Seq: essentially encodes once, and calls the decoder in a loop\nAttention: allows output from the encoder at a specific time point to provide ‘context’ for the decoder\n\n\nnet =\n  seq2seq_module(\n    \"gru\",\n    input_size     = 1,\n    hidden_size    = 32,\n    attention_type = \"multiplicative\",\n    attention_size = 8,\n    n_forecast     = n_forecast\n  )\n\nb = dataloader_make_iter(train_dl) %&gt;% dataloader_next()\n\nnet(b$x, b$y, teacher_forcing_ratio = 1)\n\n\n\nTraining\nWith data in place, we’re ready to train the model. For the most part, not much is going on here that would be different from other deep learning situations, e.g. choosing an optimizer, number of epochs, etc. We’ll use mean squared error as our loss, and I create an object to store the validation loss over the epochs of training. I played around with it a bit, and you’re probably not going to see much improvement after letting it go for 100 epochs.\n\noptimizer = optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs = 100\n\ntrain_batch &lt;- function(b, teacher_forcing_ratio) {\n\n  optimizer$zero_grad()\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio)\n  target &lt;- b$y\n\n  loss &lt;- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n  loss$backward()\n  optimizer$step()\n\n  loss$item()\n\n}\n\nvalid_batch &lt;- function(b, teacher_forcing_ratio = 0) {\n\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio)\n  target &lt;- b$y\n\n  loss &lt;- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n\n  loss$item()\n\n}\n\n\nall_valid_loss = c()\n\nfor (epoch in 1:num_epochs) {\n\n  net$train()\n  train_loss &lt;- c()\n\n  coro::loop(for (b in train_dl) {\n    loss &lt;- train_batch(b, teacher_forcing_ratio = 0.0)\n    train_loss &lt;- c(train_loss, loss)\n  })\n\n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n\n  net$eval()\n  valid_loss &lt;- c()\n\n  coro::loop(for (b in valid_dl) {\n    loss &lt;- valid_batch(b)\n    valid_loss &lt;- c(valid_loss, loss)\n  })\n  \n  all_valid_loss = c(all_valid_loss, mean(valid_loss))\n\n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\n\nEvaluations\n\nnet$eval()\n\ntest_preds = vector(mode = \"list\", length = length(test_dl))\n\ni = 1\n\ncoro::loop(for (b in test_dl) {\n\n  if (i %% 100 == 0)\n    print(i)\n\n  output &lt;- net(b$x, b$y, teacher_forcing_ratio = 0)\n  preds &lt;- as.numeric(output)\n\n  test_preds[[i]] &lt;- preds\n  i &lt;&lt;- i + 1\n})\n\nFor this visualization, we do things a little different. In our current setup, we have 7 timesteps predicting 7 day windows. We started our test set at the beginning of December so that the first prediction is January first, and then proceeds accordingly.\n\n# same as test\ndf_eval_torch = df_validate %&gt;%\n  filter(line == 'red', date &gt; '2017-12-01') %&gt;%\n  select(rides_scaled, date) %&gt;%\n  as_tsibble()\n\ntest_preds_plot = vector(mode = \"list\", length = length(test_preds))\n\nfor (i in 1:(length(test_preds_plot)-  n_forecast)) {\n  test_preds_plot[[i]] =\n    data.frame(\n      pred = c(\n        rep(NA, n_timesteps + (i - 1)),\n        test_preds[[i]] * train_sd + train_mean,\n        rep(NA, nrow(df_eval_torch) - (i - 1) - n_timesteps - n_forecast)\n      )\n    )\n}\n\ndf_eval_torch_plot0 =\n  bind_cols(df_eval_torch, bind_cols(test_preds_plot))\n\nA visualization of the predictions makes this more clear. Each 7 day segment is making predictions for the next 7 days. The following predictions are for the last two months, with each column a set of 7 predictions for that time point.\n\n\n\n\n\n\n\n\n\nSo for our red line plot, we’ll just use the average prediction at each date to make it comparable to the other plots. In general it looks to be doing okay, even armed with no contextual information. Certainly better than the base ARIMA plot.\n\n\n\n\n\n\n\n\n\nHowever, we can see that there is much information lost just adhering to the series alone.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.804\n\n\nmae\nstandard\n0.584\n\n\nrsq\nstandard\n0.120"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#all",
    "href": "posts/2021-05-time-series/index.html#all",
    "title": "Exploring Time",
    "section": "All",
    "text": "All"
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#summary",
    "href": "posts/2021-05-time-series/index.html#summary",
    "title": "Exploring Time",
    "section": "Summary",
    "text": "Summary\n\nARIMA: no real reason to still be doing such a simplified model\nMixed Model: may be just what you need, but may lack in other settings\nGAM: great, more viable than some might suspect, easy implementation\nProphet/Fable: Prophet needs notable work out of the box, though Fable saves you some of that work, and did great in this situation via by-group models\nGBM: can it really be this easy?\nTorch: pretty good even with minimal information\n\nTo get some information on what Torch would do at the next level, i.e. adding additional features and other considerations, see Cody’s post."
  },
  {
    "objectID": "posts/2021-05-time-series/index.html#footnotes",
    "href": "posts/2021-05-time-series/index.html#footnotes",
    "title": "Exploring Time",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is also the Purple express line, which is very irregular compared to the others.↩︎\nTechnically we should scale the test set using the mean/sd of the training set, and though with very large data this should not matter, for time series it’s a particular concern as data can ‘shift’ over time.↩︎\nThis follows Bolker’s demo.↩︎\nI always appreciated the depiction of this topic in West, Welch, and Galecki (2022) quite a bit.↩︎\nA year plus later after that statement, it still hasn’t gone beyond 0.1.0, so I don’t think this will continue to be useful for very long. Unfortunate, but honestly, it’s not clear prophet&lt;/span itself can do much better than many other tools.↩︎\nfable.prophet may have a bug enabling the holidays functionality with parallel, so you can just use the original holiday column if you do so (single core doesn’t take too long).↩︎\nWe can also do this with our previous method with a split-by-apply approach. You would obtain the same results, so this serves as a nice supplement to our ‘overall’ metrics.↩︎\nFor the basics of using PyTorch via R, including installation, see the RStudio post.↩︎\nThe blog code actually has several issues, but the github repo should work fine and is what is followed for this demo.↩︎"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html",
    "title": "Practical Bayes Part I",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#rhat-effective-sample-size",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#rhat-effective-sample-size",
    "title": "Practical Bayes Part I",
    "section": "Rhat & Effective Sample Size",
    "text": "Rhat & Effective Sample Size\nWe will start with a simple standard regression model that we know is not adequate. We will use default priors2 and run very few iterations.\n\n# no priors, no complexity, all default settings, few iterations\nlibrary(brms)\n\nmodel_start_100 = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 100,\n  verbose = F,\n  seed = 123\n)\n\nWarning: The largest R-hat is 1.1, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\nsummary(model_start_100)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ b1 + b2 + x1 + x2 + x3 \n   Data: main_df (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 100; warmup = 50; thin = 1;\n         total post-warmup draws = 200\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.72      0.15     2.44     3.00 1.10       36      144\nb11           0.83      0.06     0.71     0.94 1.01      140      115\nb21          -0.12      0.15    -0.41     0.15 1.10       35       82\nx1            0.03      0.03    -0.04     0.09 1.00      310      239\nx2           -0.04      0.03    -0.10     0.02 1.00      355      237\nx3            0.28      0.03     0.22     0.35 1.04      349      132\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.10      0.02     1.06     1.15 1.00      220      145\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nAs is common in other R models, for factors, the effect is the stated category vs. the reference. Since we only have binary variables, b11 is show the level 1 vs. level 0 (reference) for the first binary covariate.\nThe warnings about Rhat and effective sample size (ESS) are likely to pop up if you use default iterations for more complex models. They mostly regard the efficiency of the sampling process, and whether you have enough samples to have stable parameter estimates. Ideally Rhat is close to 1.0, and ESS is at least a notable percentage of the total posterior samples (e.g. 50%).\nThe fix for these warnings is usually simple, just let the model run for more iterations beyond your warmup. The default is 2000 iterations, with warmup half of that. Warmup iterations are not used in calculation of parameter estimates, so you can just increase the number of iterations relative to it, or increase both by only increasing the iter argument.\nIn the following, we plot the estimated values across each iteration for each chain, called a trace plot, as well as the density plot of the values from the entire chain. From this we could see that things might be problematic (e.g. we’d want more symmetric density plots for the regression coefficients), but only if you are used to looking at these things, so it will take some practice.\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'combo')\n\n\n\n\n\n\n\n\nTo get some intuition for what you would expect, just plot a series from a normal distribution.\n\nggplot2::qplot(x = 1:1000, y = rnorm(1000), geom = 'line')\n\n\n\n\n\n\n\nggplot2::qplot(x = rnorm(1000), geom = 'density')\n\n\n\n\n\n\n\n\nOther plots allow us to look at the same sorts of things from a different perspective, or break out results by each chain.\n\nmcmc_plot(model_start_100, type = 'areas')\n\n\n\n\n\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'violin')\n\n\n\n\n\n\n\n\nWhile things seem okay, if we single out a particular chain, we might think otherwise. In this case, the Intercept and b2 coefficients may be problematic, given they do not seem to vary as much as the others (chain 2 for example, but also for other chains).\n\nmcmc_plot(model_start_100, highlight = 2, type = 'trace_highlight')\n\n\n\n\n\n\n\n\nSome suggest to look at rank plots instead of traditional trace plots. Really, all we’ve changed is looking for something ‘fuzzy’, to looking for something ‘approximately uniform’, so my opinion is that it’s not much of an improvement visually or intuitively. In general, histograms, which are variants of bar charts, are rarely an improvement for any visualization. If you do use it, you can use an overlay approach to see if the ranks are mixing, but this looks a lot like what I’d be looking for from a trace plot.\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'rank_hist')\n\n\n\n\n\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'rank_overlay')"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#some-details",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#some-details",
    "title": "Practical Bayes Part I",
    "section": "Some details",
    "text": "Some details\nA little more detail will possibly provide additional understanding, and also some guidance, when looking at your own results.\n\nRhat\nThe \\(\\hat{R}\\) (or Rhat) statistic measures the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains. If all chains are at equilibrium, these will be the same and \\(\\hat{R}\\) will be 1.0. If the chains have not converged to a common distribution, the \\(\\hat{R}\\) statistic will be greater than one.\nWhat we want: values near 1.0 (&lt; 1 okay) and less than 1.05\n\nmcmc_plot(model_start_100, type = 'rhat')\n\n\n\n\n\n\n\n\n\n\nESS\nEffective sample size is an estimate of the effective number of independent draws from the posterior distribution of the parameter of interest. Because the draws within a chain are not independent if there is autocorrelation, the effective sample size will typically be smaller than the total number of iterations, but the calculation of this statistic is bit more art than science, and can even be greater than the number of posterior draws. Note also that the plot is based on slightly different values than reported by the brms summary function.\n\nBulk ESS: ESS for the mean/median (n_eff in rstanarm). Tells us whether the parameter estimates are stable.\nTail ESS: ESS for the 5% and 95% quantiles. Tells us whether the interval estimates for the parameters are stable. Tail-ESS can help diagnose problems due to different scales of the chains and slow mixing in the tails.\n\nWhat we want: ESS &gt; 10% percent of total posterior samples for sure, but &gt; 50% is best. At least 100 is desired for decent estimates of autocorrelation. For Bulk-ESS we want &gt; 100 times the number of chains.\n\nmcmc_plot(model_start_100, type = 'neff')  # &lt; .1 problem\n\n\n\n\n\n\n\n\n\n\nTrace plot\nShows the estimated parameter values at each iteration. In general you would like a random bouncing around an average value.\n\nmcmc_plot(model_start_100, type = 'trace')\n\n\n\n\n\n\n\n\nWhat we want: Something like random normal draws over a series. Trace plots in general should look ‘grassy’, or like a ‘fuzzy caterpillar’, which might not be very descriptive, but deviations are usually striking and obvious in my experience. If you see chains that look like they are getting stuck around certain estimates, or separating from one another, this would indicate a serious problem. If the chains are not converging with one another, you were probably already getting warnings and messages.\n\n\nDensity plot\nShows the density of the posterior draws for the parameters.\n\nmcmc_plot(model_start_100, type = 'dens')\n\n\n\n\n\n\n\n\nWhat we want: For the density plots of regression coefficients, these should be roughly normal looking. For variance parameters you may see skewness, especially if the estimate is relatively near zero with smaller data sets. In general, we would not want to see long tails or bimodality for the typical parameters of interest with models you’d be doing with rstanarm and brms.\n\n\nRank plot\nFrom bayesplot help file:\n\nWhereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.\n\n\nmcmc_plot(model_start_100, type = 'rank_hist')\n\n\n\n\n\n\n\n\nWhat we want: Uniform distribution, a good mixing of lines for the overlay version.\n\n\nACF plot\nThe acf, or autocorrelation function plot, is exactly the same thing you’d visualize for any time series. It is a plot of a series of correlations of a parameter with specific lags of itself. Autocorrelation does not bias estimates, but increased autocorrelation may suggest a more inefficient/slower exploration of the parameter space. At lag zero, the series estimates are perfectly correlated with themselves, so that’s where the plot usually starts.\nWhat we want: Quick drop off, but not really that important. By the time you find it’s an issue, your model has already run.\n\nmcmc_plot(model_start_100, type = 'acf')\n\n\n\n\n\n\n\n\n\n\nEfficiency plots\nI have seen these often recommended, but I’m not aware of a package does them, though it seems they may make their way to shinystan at some point. Aki Vehtari has supplied a walk-through and some code (see the resources section below), but there isn’t really documentation for the functions, and they likely won’t work outside of rstan objects, or at least I had limited success with applying them to brms objects. Furthermore, these are getting into waters that are beyond what I’d expect applied users to be wading through.\n\n\nSolution for Rhat/ESS warnings\nTo summarize, the general solution to Rhat and ESS warnings is simply to do more iterations (relative to the warmup). To keep posterior samples and model objects from becoming unwieldy in size3, consider thinning also. Thinning saves only a select amount of the available posterior samples. For example, setting thin = 10 means only every tenth sample will be saved. This will also reduce autocorrelation, as the draws retained after thinning are not as correlated with one another as successive draws would be. However, if you thin too much, you may not have enough for effective sample size.\n\n# default with 4 chains, 1000 warmup 2000 total = 4*(2000 - 1000) = 4000 post-warmup draws\nbrm(model)\n\n# 4*(4000 - 2000) = 8000 posterior draws\nbrm(model, warmup = 2000, iter = 4000)\n\n# 4*(4000 - 2000)/8 = 1000 posterior draws\nbrm(model, warmup = 2000, iter = 4000, thin = 8)"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#bfmi-low",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#bfmi-low",
    "title": "Practical Bayes Part I",
    "section": "BFMI low",
    "text": "BFMI low\nYou may see a warning that says some number of chains had an ‘estimated Bayesian Fraction of Missing Information (BFMI) that was too low’. This implies that the adaptation phase of the Markov Chains did not turn out well, and those chains likely did not explore the posterior distribution efficiently. For more details on this diagnostic, you can see Betancourt’s article, but this will almost surely be too technical for many applied and even more advanced users.\nIn this case, the problem here is often remedied by just adding more iterations. I typically keep to 1000 posterior samples, which makes for nicer visualizations of distributions without creating relatively large model objects. However, you may need more to get a satisfactory result.\n\nmodel_start = update(\n  model_start_100,\n  warmup = 2000,\n  iter = 2250,      # 1000 posterior draws\n  cores = 4,\n  seed = 123\n) \n\nIn this case, we no longer have any warnings, and even one of our more problematic coefficients looks fine now.\n\nsummary(model_start)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ b1 + b2 + x1 + x2 + x3 \n   Data: main_df (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2250; warmup = 2000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.72      0.20     2.32     3.10 1.00     1583      938\nb11           0.83      0.07     0.69     0.97 1.00     1711      895\nb21          -0.12      0.20    -0.50     0.29 1.01     1509      864\nx1            0.03      0.03    -0.04     0.10 1.00     1504      947\nx2           -0.03      0.04    -0.11     0.05 1.00     1279      803\nx3            0.28      0.04     0.21     0.35 1.00     1136      810\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.11      0.02     1.06     1.15 1.00     1357      735\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(model_start, par = 'b2')\n\n\n\n\n\n\n\n\n\nSolution for low BFMI\nIf there aren’t other serious problems, add more iterations to deal with low BFMI. In some cases, switching from a heavy-tailed prior (e.g. student t) to something else (e.g. normal) would be helpful, but some other approaches typically would involve having to write Stan code directly to reparameterize the model. Otherwise, you may need to approach it similarly to the problem of divergent transitions."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#tree-depth",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#tree-depth",
    "title": "Practical Bayes Part I",
    "section": "Tree Depth",
    "text": "Tree Depth\nTree depth is a more technical warning that has to do with the details of Hamiltonian Monte Carlo. Practically speaking:\n\nLack of convergence and hitting the maximum number of leapfrog steps (equivalently maximum tree depth) are indicative of improper posteriors ~ Stan User Guide\n\nSometimes you’ll get a warning about hitting maximum tree depth, and without getting overly technical, the fix is easy enough. Just set the maximum higher.\n\nSolution for max tree depth\nUse the control argument to increase the value beyond the default of 10.\n\nmodel_update_treedepth = update(\n  model,\n  control = list(max_tree_depth = 15)\n)"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#divergent-transitions",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#divergent-transitions",
    "title": "Practical Bayes Part I",
    "section": "Divergent Transitions",
    "text": "Divergent Transitions\nDivergent transitions are a technical issue that indicates something may be notably wrong with the data or model (technical details). They indicate that the sampling process has ‘gone off the rails’, and that the divergent iteration’s results, and anything based on them (i.e. subsequent draws, parameter estimates), can’t be trusted. Unlike the other problems we’ve discussed, this is more difficult to navigate.\nWhy might this happen?\n\ninsufficient data for the model’s complexity\npoor model\nhigh collinearity\nimproper or otherwise problematic priors\nseparability (logistic regression)\nany number of other things\n\nAs an example, I’ll make an overly complex model with only a small random sample of the data, improper priors, and use very few warmups/iterations.\n\nmodel_problem = brm(\n  bf(\n    y ~ b1*b2*x1 + x2*b2 + x3*b2 + (1 + x1 + b2|group),\n    sigma ~ x1 + b1*b2\n  ),\n  data   = main_df %&gt;% slice_sample(prop = .1),\n  family = student,\n  cores  = 4,\n  warmup = 5,\n  iter   = 1005,\n  thin   = 4,\n  seed   = 123\n)\n\nWarning: There were 206 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: The largest R-hat is 2.21, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\nSo what do we do in this case? Well let’s start with visual inspection.\n\nVisualization: Pairs plot\nA diagnostic tool that is typically suggested to look at with divergent transitions is the pairs plot. It is just a scatterplot matrix of the parameters estimates (and log posterior value), but it suffers from a few issues. The plot is slow to render even with few parameters, and simply too unwieldy to use for many typical modeling situations. If you somehow knew in advance which parameters were causing issues, you could narrow it down by only looking at those parameters. But if you knew which parameters were the problem, you wouldn’t need the pairs plot.\nAnother issue is that it isn’t what you think it is at first glance. The upper diagonal is not just the flipped coordinates of the lower diagonal like every other scatterplot matrix you’ve seen. The chains are split such that half are used for the above diagonal plots, and the other for the lower, with the split being based on the amount of numerical error (above or below the median). I suspect this may not help applied users interpret things, but the gist is, that if your red points show up only on the upper diagonal, changing the adapt_delta part of the control argument may help (see below), otherwise it likely won’t4.\nLet’s take a look at the pairs plot anyway. I’ll use hex bins instead of standard points because the point plots have no transparency by default. In addition, we’ll use a density plot on the diagonal, instead of the histogram.\n\nmcmc_plot(\n  model_problem,\n  pars = c('b_b11', 'b_b21', 'b_x1'),\n  type = 'pairs',\n  diag_fun = 'dens',\n  off_diag_fun = 'hex',\n  fixed = TRUE\n)\n\n\n\n\n\n\n\n\nWith problematic cases, what you might see on the off-diagonal plots is some sort of ‘funneling’, which would indicate where the sampler is getting stuck in the parameter space. However, this visual notion isn’t defined well, as it may be happening without being obvious, displaying just a bump, or just some weird patterns as above. But you’ll also regularly see correlated parameters, but it’s unclear whether these might necessarily be a problem in a given situation.\nFor the initial model we ran, the pairs plot for all parameters takes several seconds to produce, and even with the hex option, it is still difficult to parse without closer inspection. It shows the intercept and b2 parameters to be notably correlated, possibly indirectly due to the poor priors.\n\nmcmc_plot(\n  model_start_100,\n  type = 'pairs',\n  off_diag_fun = 'hex',\n  diag_fun = 'dens'\n)\n\n\n\n\n\n\n\n\nWhat we want: Roughly little correlation among parameters, mostly symmetric densities for typical regression parameters.\n\n\nVisualization: Parallel Coordinates Plot\nIt is also suggested to look at parallel coordinates plots, but unfortunately there are issues with these plots as well. The order of the variable/parameter axis is arbitrary, and yet the order can definitely influence your perception of any patterns. Also, unless everything is on similar scales, they simply aren’t going to be very useful, but even if you scale your data in some fashion, the estimates given divergent transitions may be notably beyond a reasonable scale.\nAs in our pairs plot, we’d be looking for a pattern among the divergences, specifically a concentration for a parameter where the lines seemingly converge to a point. If this isn’t the case, the divergences are probably false positives5. I had to add some ggplot options to help this to be more legible, and you will likely have to as well. In the following, you might think the b_sigma_x1 coefficient for dispersion is a problem, which might suggest we need to rethink the prior for it. In reality it’s likely just that it’s being estimated to be near zero, as it should be, especially since non-divergent transitions are also bouncing around that value. For the most part we don’t see much pattern here.\n\nmcmc_parcoord(\n  model_problem,\n  pars = vars(matches('^b')),\n  size = .25, \n  alpha = .01,\n  np = nuts_params(model_problem),  # without this div trans won't be highlighted\n  np_style = parcoord_style_np(\n    div_color = \"#ff5500\",\n    div_size = 1,\n    div_alpha = .1\n  )\n) +\n  guides(x = guide_axis(n.dodge = 2)) +\n  theme(\n    axis.text.x = element_text(size = 6),\n    panel.grid.major.x = element_line(color = '#00AAFF80', size = .1)\n  )\n\n\n\n\n\n\n\n\nWhat we want: Roughly no obvious pattern. If the divergence lines are not showing any particular concentration, it could be that these are false positives.\n\n\nSolution for divergent transitions\nUnfortunately the solution to divergent transitions is usually not straightforward. The typical starting point for solving the problem of divergent transitions is to use the control argument to increase adapt_delta, for example, from .80 to .996, and let your model have more warmup/total iterations, which is the primary issue here. In the cases I see for myself and clients, increasing adapt_delta rarely helps, but it doesn’t hurt to try. I often will just start with it increased for more complex models, just to save messing with it later.\n\nmodel = brm(..., control = list(adapt_delta = .99))\n\nAside from that you will have to look more deeply, including issues with priors, model specification, and more. I find this problem often comes from poor data (e.g. not scaled, possible separation in logistic models, etc.), combined with a complex model (e.g. complicated random effects structure), and beyond that, the priors may need to be amended. You should at least not have uniform priors for any parameter, and as we’ll see in Part II, you can use a simulation approach to help choose better priors. Often these data and prior checks are a better approach to solving the problem than doing something after the fact. You may also need to simplify the model to better deal with your data nuances. As an example, for mixed models, it may be that some variance components are not necessary.\nSome solutions offered on the forums assume you are coding in Stan directly, such as reparameterizing your model, using specific types of priors, etc. If you are writing Stan code rather than using a modeling package, you definitely need to double check it, as typos or other mistakes can certainly result in a problematic model. However, this post is for those using modeling packages, so I will not offer such remedies, and they are usually not obvious anyway."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#other-messages",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#other-messages",
    "title": "Practical Bayes Part I",
    "section": "Other messages",
    "text": "Other messages\nCertain builds of rstan for some types of settings (e.g. specific operating systems) will often have warnings or other messages. Sometimes it looks like a bunch of gobbledygook, which is typically something happening at the C++ level. If your model runs and produces output despite these messages, you can typically ignore them in most cases. Even then, you should look it up on the forums just to be sure.\n\nParser warnings\nParser warnings are either a deprecation warning, or another more serious kind (Jacobian). The latter will not happen if you’re using higher level interfaces (e.g. brms), rather than programming in Stan directly. The other kind, deprecation warnings, are not something you can do anything about, but the developer of the package will likely need to make minor changes to the code to avoid them in the future. I’ve never seen parser warnings from using rstanarm or brms.\n\n\nCompilation warnings\nCompiler warnings happen regularly and indicate something going on at the compiler level, typically that something in Stan is being compiled but not used. You can ignore these.\n\n\nPackage warnings\nLike any good package, when things go unexpectedly, or just to be informative, modeling packages like rstanarm and brms will provide you messages or warnings. These do not have to do with the Stan part of things. For example, brms will warn you that it will drop cases with missing values.\n\nRows containing NAs were excluded from the model.\n\nSome issues can be more subtle. For example, you may get a message that the model is compiling but then nothing happens. This might be because of a typo in a distribution name for your priors, or some similar goof7.\n\n\nSolutions for other messages\nIf you are using a package to interface with Stan and not having an issue with the model (i.e. it runs, converges), these messages can largely be ignored, unless it is a warning from the package itself, which typically should be investigated."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#model-comparison-problems",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#model-comparison-problems",
    "title": "Practical Bayes Part I",
    "section": "Model Comparison Problems",
    "text": "Model Comparison Problems\nFor a discussion of loo and related issues, see Part II."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#but-its-slow",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#but-its-slow",
    "title": "Practical Bayes Part I",
    "section": "But it’s slow!",
    "text": "But it’s slow!\nAnother problem people seem to be concerned with is the speed of the analysis. If you have a lot of data, or more to the point, a lot of parameters, your model can be very slow. For standard models with rstanarm and brms, there may be no real benefit doing it Bayesian style if you have millions of data points and simpler models. If your model is only taking a couple minutes, then you really have nothing to complain about- watch some YouTube or something while it runs. If your model takes on the order of hours, work with less data or simpler models until you have your modeling code, plots, etc. squared away. At that point you can run your primary analysis and wait it out. A slow model may also be indicative of a poorly specified/understood model, so you may have to think hard about how you are approaching the problem.\n\nSolutions for a slow model\n\nFor any model, Bayesian or otherwise, doing things like standardizing, logging or other variable transformations will put parameters in more reasonable domains, resulting in a more manageable estimation process. For example, if you standardize predictors, then the coefficients are on similar scales and a Normal(0, 1) prior can typically be applied.\nUsing more informative priors can avoid exploring areas of the posterior that aren’t going to lead to plausible results. We will show how to do this in Part II.\nUse less iterations if there are no other issues.\nIf possible, work with less data or simpler models until ready for the full model.\nIf possible, work with a different version of the model that can still answer the question of interest. For example, if an ordinal model is causing a lot of issues, and you’re not interested in category specific probabilities, just treat the target variable as numeric8.\nStop fishing/p-hacking/torturing your data so that you don’t have to run dozens of models. You’re likely not to care if the models takes hours if you only run them once or twice."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#shinystan",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#shinystan",
    "title": "Practical Bayes Part I",
    "section": "shinystan",
    "text": "shinystan\nThe shinystan package makes exploring model diagnostics actually fun! Using launch_shinystan on a model opens a browser window, providing interactive visualizations to help you see what’s going on with it. You can look at many of the previous plots, plus a few others we haven’t shown.\nUnfortunately the documentation in the browser doesn’t really tell you what to look for in these plots. The glossary contains information that is likely overly technical for applied users, and if there is a problem, there’s not really a whole lot to go on. In addition, some plots are difficult to use (e.g. trying to assess whether overlapping histograms are similar), or are probably only useful if you have very obvious problem (e.g. lots of divergent transitions). As an example, consider the tree depth plots. What would be good here?\n\nThe documentation tells you the value should be somewhere between 0 and whatever max_treedepth is set at. If they are ‘large’, the documentation states the problem could be due to different things, but the solutions are to either reparameterize of the model (probably not possible unless using Stan code directly), or just increase the value. It doesn’t seem to tell you what those plots are supposed to look like, and unfortunately that severely limits their utility. The divergence and energy plots are similarly under-explained. Many refer users to Betancourt’s wonderful articles on the details, but these are far too technical for those not already steeped in the approach9.\nAll that said, luckily there is a nice walkthrough if you do have a hankering to go down that path, and provides more details on the statistics and what you should be looking for in the visualizations."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#general",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#general",
    "title": "Practical Bayes Part I",
    "section": "General",
    "text": "General\n\nAki Vehtari’s website has demos and explanation for things like model comparison, Rhat/Neff, and more.\nbayesplot Vignette for Diagnostic Plots\nJeffrey Arnold’s Bayesian Notes has nice examples of many models and good summaries otherwise.\n\nBayesian Basics, an applied overview of Bayesian analysis with Stan as the backdrop."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#priors",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#priors",
    "title": "Practical Bayes Part I",
    "section": "Priors",
    "text": "Priors\nPrior Choice Recommendations by the Stan group.\n\nrstanarm vignette"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#rhatess",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#rhatess",
    "title": "Practical Bayes Part I",
    "section": "Rhat/ESS",
    "text": "Rhat/ESS\n\nStan Reference on Effective Sample Size\nVehtari et al. on Rank plots for details.\nVehtari’s appendix for the above (probably more accessible)\nVehtari’s code for efficiency plots, Code for functions"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#divergence",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#divergence",
    "title": "Practical Bayes Part I",
    "section": "Divergence",
    "text": "Divergence\n\nStan Reference on Divergent Transitions\nDivergent Transitions A Primer\nCase study: Divergences and Bias"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#other-warnings",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#other-warnings",
    "title": "Practical Bayes Part I",
    "section": "Other warnings",
    "text": "Other warnings\n\nBrief Guide to Stan’s Warnings"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#visual-diagnostics",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#visual-diagnostics",
    "title": "Practical Bayes Part I",
    "section": "Visual diagnostics",
    "text": "Visual diagnostics\n\nVisual MCMC diagnostics"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#misc",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#misc",
    "title": "Practical Bayes Part I",
    "section": "Misc",
    "text": "Misc\n\nGelman’s advice for slow models\nbrms vignettes\nUse CmdStan to save memory\nStan case studies"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-i/index.html#footnotes",
    "href": "posts/2021-02-28-practical-bayes-part-i/index.html#footnotes",
    "title": "Practical Bayes Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI will spare the details of my opinions, which are almost entirely negative.↩︎\nFor this model, they are uniform/improper priors for the regression coefficients and half-t for the residual variance. You can always use prior_summary on a brms model to see this.↩︎\nWith more posterior samples comes slower visualizations and possibly other computations.↩︎\nAnother issue is that you could change how the chains are split and it could potentially dramatically change the how the pattern of divergent transitions looks.↩︎\nThis seems counter to the common suggestion on forums and GitHub issues that even 1 divergent transition renders results suspect. I’ve never seen results meaningfully change for something with just a couple divergent transitions to one that has none, and often when there are that few, even rerunning the model will result in no divergences.↩︎\nIn my experience, there isn’t a need to guess some value between .80 and .99 as the time differences are typically not great, say between .9, .95, and .99, unless your model already takes a very long time. Also, if it doesn’t work at .99, it won’t at .9999 either.↩︎\nI’m definitely not speaking from experience here or anything! Nomral distributions do exist, I’m sure of it! 😬↩︎\nIt is often suggested in the Stan world to reparameterize models. However, this advice doesn’t really apply in the case of using rstanarm or brms (i.e. where you aren’t writing Stan code directly), and it assumes a level of statistical expertise many would not have, or even if they do, the route to respecifying the model may not be obvious.↩︎\nBetancourt, whose work I admire greatly, typically makes my head spin.↩︎"
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html",
    "title": "Micro-macro models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#introduction",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#introduction",
    "title": "Micro-macro models",
    "section": "Introduction",
    "text": "Introduction\nEvery once in a while, it comes up that someone has clustered data, with covariates that vary at different levels, and where mixed models or similar would normally be implemented, but in which the target variable only varies at the cluster level (or ‘group’ level- I will use the terms interchangeably). Though the outcome is at the cluster level, the individual may still want to use information from lower-level/within-cluster variables. Such situations are generically referred to as micro-macro models, to distinguish between the standard setting where the target varies at the lower level (which does not require a special name). An example might be using team member traits to predict team level scores. While conceptually one wants to use all available information in a model, normally we just run a model at the cluster (team) level using summaries of variables that would otherwise vary within the cluster, for example, using mean scores or proportions. Not only is it natural, it makes conceptual sense, and as such it is the default approach. Alternatives include using the within cluster variables as predictors, but this wouldn’t be applicable except in balanced settings where they would represent the same thing for each group, and even in the balanced settings collinearity might be a notable issue. So how would we deal with this?\n\nPrequsites\nFor the following you should have familiarity with mixed/multilevel models, and it would help to have an understanding of factor analysis and structural equation modeling."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#predicting-group-level-outcomes",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#predicting-group-level-outcomes",
    "title": "Micro-macro models",
    "section": "Predicting Group-Level Outcomes",
    "text": "Predicting Group-Level Outcomes\nCroon and van Veldhoven Croon and Veldhoven (2007) (CV) present a group-level regression model (e.g. a basic linear model) as follows.\n\\[y_g = \\beta_0 + \\xi_g\\beta_1 + Z_g\\beta_2 + \\epsilon_g\\]\nIn this depiction, \\(y_g\\) is the group level target variable, the \\(Z_g\\) represent the typical observed group-level covariates and corresponding coefficients (\\(\\beta_2\\)). If this were the entirety of the model, there would be no ‘levels’ to consider and we could use a standard model, say OLS regression. In the case we are interested in, some variables vary within these clusters, while others do not. Again, normally we might do a mixed model, but remember, \\(y_g\\) only varies at the group level, so that won’t really work.\nIn this setting then, \\(\\xi_g\\) represents an aggregated effect of the lower level variables. In standard practice it would just be the calculated mean, proportion, or some other metric with values for each cluster. In the CV depiction however, it is a latent (or perhaps several) latent variables and their corresponding effects \\(\\beta_1\\).\nIf we assume a single \\(\\xi_g\\) variable, the model for the underlying within-cluster variables is the standard latent variable model, a.k.a factor analysis. With an observed multivariate \\(x\\), e.g. repeated observations of some measure for an individual or, as before, team member scores, we have the latent linear model as follows:\n\\[\\textbf{x}_{ig} = \\xi_g\\lambda + v_{ig}\\]\nwhere \\(x_{ig}\\) are the (possibly repeated) observations \\(i\\) for a group/individual \\(g\\), \\(\\lambda\\) are the factor loadings and variances are constant. We can now see the full model as a structural equation model as follows for a situation with five observations per group."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-aggregate-approaches",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-aggregate-approaches",
    "title": "Micro-macro models",
    "section": "Issues with Aggregate Approaches",
    "text": "Issues with Aggregate Approaches\nCV suggest that simple aggregation, e.g. using a group mean, will result in problems, specifically biased estimates. They simulate data that varies the number of groups/clusters, the number of observations within groups, the intraclass correlation of observations within a group. In most of the cases they explore, the bias for the aggregate mean effect is notable, and there is sometimes small bias for the group level covariates, if they are collinear with the aggregate covariate. We will duplicate this approach later.\nAn approach to adjusting the group mean is offered by CV, with the structural model implied. These adjusted group means, or in their parlance, best linear unbiased predictors (BLUPs), result in a bias-free result. The notion of a BLUP will be familiar to those who use mixed models, as that is what the random effects are for a standard linear mixed model. As such, later on we’ll take a look at using a mixed model as a possible solution. In any case, once the adjusted means are calculated, you can then run your standard regression with the bias mostly eliminated."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-adjustment",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#issues-with-adjustment",
    "title": "Micro-macro models",
    "section": "Issues with Adjustment",
    "text": "Issues with Adjustment\nIt turns out the the weighting calculation proffered by CV is somewhat complicated, not easily implemented, and rarely used. Foster-Johnson & Kromrey Foster-Johnson and Kromrey (2018) (FJK) looked further into its utility, as well as other possible solutions that might be easier to implement. As far as type I error rate goes, FJK demonstrated that using the CV adjusted group means offers no advantage over unadjusted, and even showed less statistical power. They suggested that a standard correction for heteroscedasticity (White’s) might be enough. In applying corrected standard errors for both unadjusted and adjusted group means, FJK found there to be additional power for both approaches, but if anything still favored the standard group mean. What’s more, while the bias remained, there was actually notable variability in the adjusted mean results. FJK’s final recommendation was to use the usual group means with robust standard errors, easily implemented in any statistical package.\nI will add that the adjustment still uses an underlying factor model of equal loadings and variances across the observations. For notably reliable scales this might be reasonable, but it isn’t a necessity. In repeated measures settings for example, we might see decreased variability across time, or practice effects, which might make the assumption more tenuous."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#my-perspective",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#my-perspective",
    "title": "Micro-macro models",
    "section": "My Perspective",
    "text": "My Perspective\nMy first glance at the issue raised by CV immediately called to mind the standard measurement model typically employed for factor analysis, i.e. a latent linear model. So my interpretation was that we are simply talking about a well known fact in measurement: that reliability of the measure is key in using a mean or sum score, and decreased reliability attenuates the correlation among the variables in question. I even did a simulation demonstrating the problem a while back. So in this case, I’m interested in the issue from a reliability perspective.\nIt turns out that factor models and mixed models share a lot in common. Those familiar with growth curve models know that they are equivalent to mixed models, but the comparison is a more general one of random effects methods. To demonstrate the equivalence, I’ll use a cleaned up version of the Big 5 data in the psych package. Specifically, we’ll use the five items that belong to the Agreeableness measure.\nFirst we make the data in both wide and long. The former makes it amenable to factor analysis, while the latter is what we need for a mixed model.\n\n# data prep for long and wide format\n\nagree_df = noiris::big_five %&gt;% \n  select(A1:A5) %&gt;% \n  drop_na()\n\nagree_long = agree_df %&gt;% \n  mutate(id = factor(row_number())) %&gt;% \n  pivot_longer(-id, names_to = 'variable', values_to = 'value')\n\nThe standard factor model will have to be constrained to have equal loadings and item variances. In addition, we’ll estimate the intercepts, but otherwise this is your basic factor analysis.\n\n # or use growth() to save some of the model tedium\ncfa_model_agree = \"\n  agree =~ a*A1 + a*A2 + a*A3 + a*A4 + a*A5\n  \n  A1 ~~ var*A1\n  A2 ~~ var*A2\n  A3 ~~ var*A3\n  A4 ~~ var*A4\n  A5 ~~ var*A5\n\"\n\nlibrary(lavaan)\n\ncfa_fit_agree = cfa(cfa_model_agree, data = agree_df, meanstructure = T) \n\nsummary(cfa_fit_agree)\n\nlavaan 0.6.17 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n  Number of equality constraints                     4\n\n  Number of observations                          2709\n\nModel Test User Model:\n                                                      \n  Test statistic                               744.709\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  agree =~                                            \n    A1         (a)    1.000                           \n    A2         (a)    1.000                           \n    A3         (a)    1.000                           \n    A4         (a)    1.000                           \n    A5         (a)    1.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .A1               -2.412    0.026  -94.340    0.000\n   .A2                4.797    0.026  187.611    0.000\n   .A3                4.599    0.026  179.859    0.000\n   .A4                4.682    0.026  183.107    0.000\n   .A5                4.551    0.026  177.982    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .A1       (var)    1.201    0.016   73.607    0.000\n   .A2       (var)    1.201    0.016   73.607    0.000\n   .A3       (var)    1.201    0.016   73.607    0.000\n   .A4       (var)    1.201    0.016   73.607    0.000\n   .A5       (var)    1.201    0.016   73.607    0.000\n    agree             0.571    0.022   25.621    0.000\n\n\nWhen we run the mixed model, we get the same variance and intercept estimates.\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nlibrary(mixedup) # for post-processing\n\nmixed_fit = lmer(value ~ -1 + variable + (1 |id), data = agree_long,  REML = FALSE)\nsummarise_model(mixed_fit, digits = 3)\n\nComputing profile confidence intervals ...\n\n\n\nVariance Components:\n\n\n    Group    Effect Variance    SD SD_2.5 SD_97.5 Var_prop\n       id Intercept    0.571 0.755  0.727   0.785    0.322\n Residual        NA    1.201 1.096  1.081   1.111    0.678\n\n\n\nFixed Effects:\n\n\n       Term  Value    SE       t P_value Lower_2.5 Upper_97.5\n variableA1 -2.412 0.026 -94.340   0.000    -2.462     -2.362\n variableA2  4.797 0.026 187.611   0.000     4.747      4.847\n variableA3  4.599 0.026 179.859   0.000     4.549      4.649\n variableA4  4.682 0.026 183.107   0.000     4.632      4.732\n variableA5  4.551 0.026 177.982   0.000     4.501      4.601\n\n\nWe can also see that the estimated factor scores agree with the estimated random effects.\n\n\n\n\n\nindex\nEstimated.Factor.Scores\nEstimated.Random.Effects\n\n\n\n\n1\n-0.453\n-0.453\n\n\n2\n-0.312\n-0.312\n\n\n3\n-0.594\n-0.594\n\n\n4\n-0.031\n-0.031\n\n\n5\n-0.453\n-0.453\n\n\n6\n-0.031\n-0.031\n\n\n2704\n-0.453\n-0.453\n\n\n2705\n-1.720\n-1.720\n\n\n2706\n-0.312\n-0.312\n\n\n2707\n-0.453\n-0.453\n\n\n2708\n-1.297\n-1.297\n\n\n2709\n-1.157\n-1.157\n\n\n\n\n\n\n\nUsually when the term BLUP comes up it is in reference to the random effects estimated from a linear mixed model. As such, I thought it might be interesting to see how a mixed or factor model might be used to deal with the bias. I also thought it was a bit odd that neither CV nor FJK actually conduct the implied SEM (but see the paper co-authored by the lavaan package author Devlieger, Mayer, and Rosseel (2016)), so I wanted to look at that too."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#model-setup",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#model-setup",
    "title": "Micro-macro models",
    "section": "Model Setup",
    "text": "Model Setup\nFor our demonstration, I will create some data as CV did and run a variety of models to see what we get. My focus is on bias, not coverage or power, as I think FJK covered those aspects plenty. The models in particular are:\n\nStandard linear model: a basic group level analysis using unadjusted means.\nRandom effects: a group level model using estimated factor scores using lavaan, or the BLUPs from lme4, or those with heterogeneous variance via glmmTMB1. These involved a two-step approach, with the factor/mixed model followed by the standard linear model.\nStructural equation model: A full, single-step SEM via lavaan. This model has the ability to account for the correlation of the Z and latent variable. It is exactly as CV depict in their Figure 1 and Figure @ref(fig:sem-plot) above.\nAdjusted means: Use CV’s approach"
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#data-setup",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#data-setup",
    "title": "Micro-macro models",
    "section": "Data Setup",
    "text": "Data Setup\nI made a function2 to create data with the values shown in CV (p. 52) for a single aggregate \\(X\\) and single group-level covariate \\(Z\\). Using their notation, the model that generates the data is the following:\n\\[y_g = .3 + .3Z_g + .3\\xi_g + \\epsilon_g\\] \\[x_{ig} = \\xi + \\nu_g\\]\nAs there, \\(\\sigma^2_\\epsilon\\) is .35. While they look at a variety of situations, I’ll just consider a single scenario for our purposes, where the correlation of the \\(Z\\) and \\(\\xi\\) was .3, the intraclass correlation of the observed \\(x_{ig}\\) was .1 (i.e. \\(\\sigma^2_\\nu\\) = 9), the number of groups was 100 and the number of observations per group was balanced at 10 (row 16 of their table 1). I simulated 1000 such data sets so that we could examine the mean value of the estimated coefficients. I first started by analyzing the result with a factor analysis, and if there are any problems such as negative variances or lack of convergence, the data is regenerated, as that will also help with any issues the mixed model would have. So the final 1000 data sets don’t have convergence issues or other problems that might make the results a little wonky."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#results",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#results",
    "title": "Micro-macro models",
    "section": "Results",
    "text": "Results\nHere are the results. We can first take a peek at the estimated scores from the two-step approaches. The CV adjustment appears closely matched to the true score at first, but we see it’s range is very wild, which is what FJK found also. Interestingly, the BLUPs from the mixed models have less variance than the true scores. The factor score is in keeping with the BLUPs, but appears also to have notable extremes, but far less than the CV adjustment. We’ll talk about why these extremes may arise later.\n\n\n\nEstimated scores\n\n\nVariable\nN\nMean\nSD\nMin\nQ1\nMedian\nQ3\nMax\n% Missing\n\n\n\n\nTrue\n1e+05\n0.01\n1.00\n-4.31\n-0.67\n0\n0.68\n4.44\n0\n\n\nCV Adj\n1e+05\n0.01\n1.07\n-22.97\n-0.65\n0\n0.66\n25.64\n0\n\n\nUnadjusted\n1e+05\n0.00\n1.39\n-6.32\n-0.93\n0\n0.94\n6.96\n0\n\n\nBLUP_mixed\n1e+05\n0.00\n0.74\n-4.20\n-0.48\n0\n0.48\n3.98\n0\n\n\nBLUP_mixed_hetvar\n1e+05\n0.00\n0.74\n-4.18\n-0.48\n0\n0.47\n3.97\n0\n\n\nFactor Score\n1e+05\n0.00\n0.87\n-7.29\n-0.47\n0\n0.46\n6.54\n0\n\n\n\n\n\n\n\nNow let’s look at the bias in the estimates.\n\n\n\nPercent bias\n\n\nModel\nIntercept\nZ\nX\n\n\n\n\nUnadjusted\n0.149\n14.494\n-49.877\n\n\nBLUP_mixed\n0.051\n14.494\n-1.828\n\n\nBLUP_mixed_hetvar\n0.053\n14.647\n-1.673\n\n\nFactor Score\n0.063\n16.502\n11.968\n\n\nCV Adj\n-0.663\n-2.368\n7.992\n\n\nSEM\n-0.017\n-1.777\n4.744\n\n\nTrue\n-0.567\n-0.380\n0.513\n\n\n\n\n\n\n\nThe results suggest a couple things. First, the results of CV were duplicated for the unadjusted setting, where the group level covariate has a slight bias upward, but the aggregate is severely downwardly biased3. We can also see that a two-step approach using BLUPs from a mixed model (with or without heterogeneous variances), or factor scores, either eliminate or notably reduce the bias for the aggregate score, but still have issue with the group level covariate. This is because of the correlation between the group level and lower level covariates, which if zero, would result in no bias, and has long been a known issue with mixed models. The factor scores had some very wild results at times, even after overcoming basic inadmissible results. In the end, we see that the calculated adjustment and SEM both essentially eliminate the bias by practical standards. It is worth noting that the bias for either the factor analysis or SEM would be completely eliminated if the model adds a regression of the latent variable onto the group level covariate \\(Z\\).\nNote that in practice, a two-step approach, such as using the mixed model BLUPs or factor scores, comes with the same issue of using an estimate rather than observed score that we have using the mean. Even if there is no bias, the estimated uncertainty would be optimistic as it doesn’t take into account the estimation process. This uncertainty decreases with the number of observations per group (or number of items from the factor analytic perspective), but would technically need to be dealt with, e.g. using ‘factor score regression’ Devlieger, Mayer, and Rosseel (2016) or more simply, just doing the SEM."
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#reliability",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#reliability",
    "title": "Micro-macro models",
    "section": "Reliability",
    "text": "Reliability\nInterestingly, if we look at the reliability of the measure, we shouldn’t be surprised at the results. Reliability may be thought of as the amount of variance in an observed score that is true score variance Revelle and Condon (2019). Since the underlying construct is assumed unidimensional, we can examine something like coefficient \\(\\alpha\\), which gives a sense of how reliable the mean or total score would be. Doing so reveals a fairly poor measure for 10 observations per group under the CV settings. The mean coefficient \\(\\alpha\\) is 0.52, the max of which is 0.74, which, from a measurement model perspective, would be unacceptable4. This is all to say that we have rediscovered attenuation in correlation due to (lack of) reliability, something addressed by Spearman over a century ago5.\nIn actual repeated measures, or with constructed scales, it’s probably unlikely we would have this poor of a measure. Indeed, if we think a mean is appropriate in the first place, we are probably assuming that the scores are something that can be meaningfully combined in the first place, because if a latent construct doesn’t actually explain the observations well, then what is the point of estimating it?\nIn our current context, we can create a more reliable measure by decreasing the variance value for \\(\\sigma^2_\\nu\\) which is the residual variance for the observed items at the lower level. Decreasing it from 9 to 1, puts the observed scores in a notably better place (\\(\\alpha\\) = 0.91), and if we actually have a reliable measure (or even just increase the number of observations per group, as noted by CV), the results show hardly any bias for the group level effect and a near negligible one for the mean effect.\n\n\n\nPercent bias\n\n\nModel\nIntercept\nZ\nX\n\n\n\n\nUnadjusted\n-0.011\n2.698\n-9.651"
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#summary",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#summary",
    "title": "Micro-macro models",
    "section": "Summary",
    "text": "Summary\nIn the end we relearn a valuable, but very old lesson. The take home story here, at least to me, is to have a reliable measure and/or get more observations per group if you can, which would be the same advice for any clustered data situation. If you do have a reliable measure, such as a proportion of simple counts, or a known scale with good properties, using the mean should not give you too much pause. As a precaution, you might go ahead and use White’s correction as suggested by FJK. If you have enough data and the model isn’t overly complicated, consider doing the SEM.\nlast updated: 2025-01-02"
  },
  {
    "objectID": "posts/2020-08-31-micro-macro-mlm/index.html#footnotes",
    "href": "posts/2020-08-31-micro-macro-mlm/index.html#footnotes",
    "title": "Micro-macro models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI wasn’t sure in the mixed model whether to include the item and or group level Z as fixed effects. Results did not change much, so I went with a mixed model with no fixed effects to make them closer to the scale of the mean scores.↩︎\nAll code is contained within the R markdown file that produced this post.↩︎\nFor those who may not have access to the article, the values for percentage bias in CV were as follows: for the unadjusted model, the bias for the coefficients under these conditions was 0.6, 15.3, -50.4, and for the adjusted model, -1.1, -1.3, 5.0.↩︎\nA typical cutoff for coefficient \\(\\alpha\\) for a good measure is .8. We can actually use a ‘G-theory’ approach and calculate this by hand \\(\\frac{1}{1+9/10}\\), where 1 is the variance CV fixed for the true score, and 9 is residual variance. \\(\\frac{1}{1+9}\\) is the \\(\\rho_x\\), i.e. intraclass correlation, that they have in Table 1. In the better scenario \\(\\rho_x\\) = \\(\\frac{1}{1+4}\\) = .2 and the reliability is \\(\\frac{1}{1+4/10}\\) = .71, which is notably better, though still substandard. Even then we can see from their table dramatic decreases in bias from that improvement in reliability.↩︎\nThe lack of reliability is likely the culprit behind the wider range in the estimated factor scores as well.↩︎"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html",
    "href": "posts/2020-06-15-predict-with-offset/index.html",
    "title": "Predictions with an offset",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub. Also for this particular post, Stata is not something I’ve used in years nor have access to. So for those interested who do have access, I can only show you the code, since at the time, it wasn’t something I could save. But since I’m here, check out the fantastic marginaleffects package in R."
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#introduction",
    "href": "posts/2020-06-15-predict-with-offset/index.html#introduction",
    "title": "Predictions with an offset",
    "section": "Introduction",
    "text": "Introduction\nGetting predictions in R is and always has been pretty easy for the vast majority of packages providing modeling functions, as they also provide a predict method for the model objects. For those in the Stata world, they typically use margins for this, but when they come to R, there is no obvious option for how to go about it in the same way1. Likewise, some in the R world catch a whiff of Stata’s margins and would want something similar, but may not be sure where to turn.\nA little digging will reveal there are several packages that will provide the same sort of thing. In addition, there are numerous resources for both R and Stata for getting marginal results (i.e. predictions). However, here we note the issues that arise when models include an offset. Offsets are commonly used to model rates when the target variable is a count, but are used in other contexts as well. The Stata documentation for the margins command offers no specific details of how the offset/exposure is treated, and some R packages appear not to know what to do with it, or offer few options to deal with it. So even when the models are identical, marginal estimates might be different in R and Stata. Here we’ll try to sort some of this out."
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#get-some-data",
    "href": "posts/2020-06-15-predict-with-offset/index.html#get-some-data",
    "title": "Predictions with an offset",
    "section": "Get some data",
    "text": "Get some data\nWe will use the Insurance data from the MASS package which most with R will have access to. From the helpfile:\n\nThe data given in data frame Insurance consist of the numbers of policyholders of an insurance company who were exposed to risk, and the numbers of car insurance claims made by those policyholders.\n\n\nDistrict: district of residence of policyholder (1 to 4): 4 is major cities.\nGroup: group of car with levels &lt;1 litre, 1–1.5 litre, 1.5–2 litre, &gt;2 litre\nAge: the age of the insured in 4 groups labelled &lt;25, 25–29, 30–35, &gt;35\nHolders: number of policyholders\nClaims: number of claims\n\nWe do a bit of minor processing, and I save the data as a Stata file in case anyone wants to play with it in that realm.\n\nlibrary(tidyverse)\n\nset.seed(123)\n\ninsurance = MASS::Insurance %&gt;%\n  rename_all(tolower) %&gt;%\n  mutate(\n    # create standard rather than ordered factors for typical output\n    age   = factor(age, ordered = FALSE),\n    group = factor(group, ordered = FALSE),\n    \n    # create a numeric age covariate for later\n    age_num = case_when(\n      age == '&lt;25'   ~ sample(18:25, n(), replace = T),\n      age == '25-29' ~ sample(25:29, n(), replace = T),\n      age == '30-35' ~ sample(30:35, n(), replace = T),\n      age == '&gt;35'   ~ sample(36:75, n(), replace = T),\n    ),\n    \n    # for stata consistency\n    ln_holders = log(holders)\n  )\n\n\nhaven::write_dta(insurance, 'data/insurance.dta')\n\nLet’s take a quick peek to get our bearings.\n\n\n$`Numeric Variables`\n# A tibble: 4 × 10\n  Variable       N  Mean     SD   Min    Q1 Median     Q3     Max `% Missing`\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 holders       64 365.  623.     3   46.8  136    328.   3582              0\n2 claims        64  49.2  71.2    0    9.5   22     55.5   400              0\n3 age_num       64  35.2  15.8   18   25     29.5   35.5    75              0\n4 ln_holders    64   4.9   1.48   1.1  3.84   4.91   5.79    8.18           0\n\n$`Categorical Variables`\n# A tibble: 12 × 4\n   Variable Group  Frequency   `%`\n   &lt;chr&gt;    &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 district 1             16    25\n 2 district 2             16    25\n 3 district 3             16    25\n 4 district 4             16    25\n 5 group    &lt;1l           16    25\n 6 group    &gt;2l           16    25\n 7 group    1-1.5l        16    25\n 8 group    1.5-2l        16    25\n 9 age      &lt;25           16    25\n10 age      &gt;35           16    25\n11 age      25-29         16    25\n12 age      30-35         16    25"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#model",
    "href": "posts/2020-06-15-predict-with-offset/index.html#model",
    "title": "Predictions with an offset",
    "section": "Model",
    "text": "Model\nStarting out, we run a model in as simple a form as possible. I use just a standard negative binomial with a single covariate age, so we can clearly see how the ouptut is being produced. Note that age has four categories as seen above.\n\nnb_glm_offset = MASS::glm.nb(claims ~  age + offset(ln_holders), data = insurance)\n\nsummary(nb_glm_offset)\n\n\nCall:\nMASS::glm.nb(formula = claims ~ age + offset(ln_holders), data = insurance, \n    init.theta = 28.40119393, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.59233    0.09071 -17.554  &lt; 2e-16 ***\nage25-29    -0.12697    0.11743  -1.081   0.2796    \nage30-35    -0.25340    0.11558  -2.193   0.0283 *  \nage&gt;35      -0.41940    0.10583  -3.963  7.4e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(28.4012) family taken to be 1)\n\n    Null deviance: 86.761  on 63  degrees of freedom\nResidual deviance: 67.602  on 60  degrees of freedom\nAIC: 444.38\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  28.40 \n          Std. Err.:  9.80 \n\n 2 x log-likelihood:  -434.385 \n\n\nNow we run it with Stata. We get the same result, so this means we can’t get different predictions if we do the same thing in both R or Stata2.\n\nnbreg claims i.age, offset(ln_holders) nolog"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#emmeans",
    "href": "posts/2020-06-15-predict-with-offset/index.html#emmeans",
    "title": "Predictions with an offset",
    "section": "emmeans",
    "text": "emmeans\nFirst let’s use emmeans, a very popular package for getting estimated marginal means, to get the predicted counts for each age group.\n\nlibrary(emmeans)\n\nemmeans(\n  nb_glm_offset,\n  ~ age,\n  type = \"response\",\n  offset = mean(insurance$ln_holders)  #  default\n)\n\n\n\n\n\n\nage\nresponse\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\n&lt;25\n27.437\n2.489\nInf\n22.968\n32.776\n\n\n25-29\n24.166\n1.802\nInf\n20.880\n27.969\n\n\n30-35\n21.295\n1.525\nInf\n18.507\n24.505\n\n\n&gt;35\n18.038\n0.983\nInf\n16.211\n20.072\n\n\n\n\n\n\n\nHow is this result obtained? It is just the prediction at each value of the covariate, with the offset held at its mean. We can duplicate this result by using the predict method and specifying a data frame with the values of interest.\n\nnd = data.frame(\n  age = levels(insurance$age),\n  ln_holders = mean(insurance$ln_holders)\n)\n\npredict(\n  nb_glm_offset,\n  newdata = nd,\n  type = 'response'\n)\n\n\n\n\n\n\nprediction\n\n\n\n\n27.437\n\n\n24.166\n\n\n21.295\n\n\n18.038\n\n\n\n\n\n\n\nAs an explicit comparison, the intercept represents group ‘&lt;25’, and if we exponentiate and add the mean offset we get:\n\\(exp(Intercept + \\overline{ln\\_holders}) = e^{-1.59+4.90} = \\qquad\\) 27.437"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#stata-basic-margins",
    "href": "posts/2020-06-15-predict-with-offset/index.html#stata-basic-margins",
    "title": "Predictions with an offset",
    "section": "Stata: basic margins",
    "text": "Stata: basic margins\nNow let’s look at Stata. First we want just the basic margins output.\n\nmargins age \n\nThese values, while consistent in pattern, are much different than the emmeans output, so what is going on?\n\nR by hand\nIn this model, we only have the age covariate and the offset, so there really isn’t much to focus on besides the latter. To replicate the Stata output in R, we will use all values of the offset for every level of age, and subsequently get an average prediction for each age group. First, we create a data frame for prediction using expand.grid, get the predictions for all those values, then get mean prediction per group.\n\npredictions = \n  expand.grid(\n    age        = levels(insurance$age), \n    ln_holders = insurance$ln_holders\n  ) %&gt;% \n  mutate(prediction = predict(nb_glm_offset, newdata = ., type = 'response')) %&gt;% \n  group_by(age) %&gt;% \n  summarise(avg_prediction = mean(prediction)) \n\n\n\n\n\n\nage\navg_prediction\n\n\n\n\n&lt;25\n74.257\n\n\n25-29\n65.403\n\n\n30-35\n57.635\n\n\n&gt;35\n48.820\n\n\n\n\n\n\n\n\n\nemmeans\nThe emmeans doesn’t appear to allow one to provide all values of the offset, as adding additional values just applies them to each group and then recycles. In this case, it would just use the first four values of ln_holders for each age group respectively, which is not what we want.\n\nemmeans(\n  nb_glm_offset,\n  ~ age,\n  type = \"response\",\n  offset = insurance$ln_holders\n)\n\n age   response    SE  df asymp.LCL asymp.UCL\n &lt;25       40.1  3.64 Inf      33.6      47.9\n 25-29     47.3  3.53 Inf      40.9      54.8\n 30-35     38.8  2.78 Inf      33.8      44.7\n &gt;35      224.7 12.25 Inf     201.9     250.0\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\ninsurance$ln_holders[1:4]\n\n[1] 5.283204 5.575949 5.505332 7.426549\n\n\nIf we add the offset to the spec argument, it still just fixes it at the mean (and I tried variations on the spec). So at least using the standard approaches with this model does not appear to give you the same thing as Stata.\n\nemmeans(\n  nb_glm_offset,\n  ~ age + offset(ln_holders),\n  type = \"response\"\n)\n\n age   ln_holders response    SE  df asymp.LCL asymp.UCL\n &lt;25          4.9     27.4 2.489 Inf      23.0      32.8\n 25-29        4.9     24.2 1.802 Inf      20.9      28.0\n 30-35        4.9     21.3 1.525 Inf      18.5      24.5\n &gt;35          4.9     18.0 0.983 Inf      16.2      20.1\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nUnfortunately Stata has the opposite issue. Trying to set the offset to the mean results in an error, and using atmeans doesn’t change the previous result.\n\nmargins age, atmeans\nmargins age, at(ln_holders = 10.27)\n\n\n\nmargins\nThe margins package explicitly attempts to duplicate Stata’s margins command, but here we can see it has an issue with the offset.\n\nlibrary(margins)\n\nmargins(\n  nb_glm_offset,\n  variables = 'age',\n  type = \"response\"  # default\n)\n\nError in offset(ln_holders): could not find function \"offset\"\n\n\nThe offset function is part of the stats package of the base R installation, so I tried rerunning the model using stats::offset, but this makes the offset just like any other covariate, i.e. it did not have a fixed coefficient of 1. Changing the model to a standard glm class with poisson and moving the offset to the offset argument did work, and produces the results for the differences in predictions for each group from the reference group (dydx in Stata), but we’ll visit this type of result later3. However, the offset argument is not available to glm.nb, so we’re stuck for now.\n\npois_glm_offset = glm(\n  claims ~  age,\n  data   = insurance,\n  family = 'poisson',\n  offset = ln_holders\n)\n\nmargins(\n  pois_glm_offset,\n  variables = 'age',\n  type = \"response\"  # default\n)\n\n age25-29 age30-35 age&gt;35\n   -10.32   -18.46 -28.79\n\nexpand.grid(\n    age        = levels(insurance$age), \n    ln_holders = insurance$ln_holders\n  ) %&gt;% \n  mutate(prediction = predict(pois_glm_offset, newdata = ., type = 'response')) %&gt;% \n  group_by(age) %&gt;% \n  summarise(avg_prediction = mean(prediction)) %&gt;% \n  mutate(diff = avg_prediction - avg_prediction[1]) \n\n# A tibble: 4 × 3\n  age   avg_prediction  diff\n  &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 &lt;25             73.4   0  \n2 25-29           63.1 -10.3\n3 30-35           55.0 -18.5\n4 &gt;35             44.7 -28.8"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#stata-over",
    "href": "posts/2020-06-15-predict-with-offset/index.html#stata-over",
    "title": "Predictions with an offset",
    "section": "Stata: over",
    "text": "Stata: over\nIn Stata, with categorical values we can also use the over approach. What do we get in this case?\n\nmargins, over(age)\n\nThese are very different from our previous results for Stata, so what’s happening here?\n\nR by hand\nThis can be duplicated with the predict function as follows. While similar to the previous approach, here only the observed values of the offset for each group are used. We then make predictions for all values of the data and average them by group.\n\npredictions_over = insurance %&gt;%\n  group_by(age) %&gt;%\n  group_modify(\n    ~ data.frame(\n        prediction = mean(\n          predict(nb_glm_offset, newdata = ., type = 'response')\n        )\n      ),\n    keep = TRUE\n  ) \n\n\n\n\n\n\nage\nprediction\n\n\n\n\n&lt;25\n14.471\n\n\n25-29\n26.162\n\n\n30-35\n29.677\n\n\n&gt;35\n141.098\n\n\n\n\n\n\n\nThe pattern is actually in the opposite direction, which is unexpected, but probably just reflects the fact that we just don’t have much data. However, it’s good to note that these respective approaches would not necessarily tell you the same thing.\n\n\nemmeans\nI currently don’t know of an equivalence for emmeans in this offset case, and initial searches didn’t turn up much, though it is hard to distinguish specific ‘average predictions’ from many other similar scenarios. I attempted the following, which keeps the values of ln_holders, but it only keeps unique ones, and it’s not reproducing what I would expect, although it implies that it is averaging over the offset values.\n\nrg = ref_grid(nb_glm_offset, cov.keep = c('ln_holders'))\n\nem_over = emmeans(rg, ~age, type = 'response')  \n\ndata.frame(em_over)\n\n    age  response        SE  df asymp.LCL  asymp.UCL\n1   &lt;25  7.814354 0.7088454 Inf   6.54154   9.334825\n2 25-29 17.042911 1.2709131 Inf  14.72545  19.725092\n3 30-35 20.304984 1.4542076 Inf  17.64579  23.364916\n4   &gt;35 96.993166 5.2865742 Inf  87.16592 107.928350\n\n\n\n\nmargins\nThe over approach for the margins package is not explicitly supported. The package author states:\n\nAt present, margins() does not implement the over option. The reason for this is also simple: R already makes data subsetting operations quite simple using simple [ extraction. If, for example, one wanted to calculate marginal effects on subsets of a data frame, those subsets can be passed directly to margins() via the data argument (as in a call to prediction()).\n\nIt would look something like the following, but we still have the offset problem for this negative binomial class, so I don’t show a result.\n\ninsurance %&gt;% \n  group_by(age) %&gt;% \n  group_map(~margins(nb_glm_offset, .), keep = TRUE)"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#stata-dydx",
    "href": "posts/2020-06-15-predict-with-offset/index.html#stata-dydx",
    "title": "Predictions with an offset",
    "section": "Stata: dydx",
    "text": "Stata: dydx\n\nCategorical Covariate\nSometimes people want differences as you move from one level (e.g. the reference level) to the next for some covariate, the ‘average marginal effect’. In Stata this is obtained with the dydx option.\n\nmargins, dydx(age)\n\nIn R, we can get this from our initial predictions that used all offset values by just taking the differences in the predicted values.\n\npredictions %&gt;% \n  mutate(dydx = avg_prediction - avg_prediction[1])\n\n\n\n\n\n\nage\navg_prediction\ndydx\n\n\n\n\n&lt;25\n74.257\n0.000\n\n\n25-29\n65.403\n-8.854\n\n\n30-35\n57.635\n-16.622\n\n\n&gt;35\n48.820\n-25.437\n\n\n\n\n\n\n\n\n\nContinuous predictor\nNow we’ll consider a continuous covariate. Here we’ll again just focus on a simple example where we rerun the model, but with age as numeric rather than binned4. For comparison we’ll set the numeric age values at roughly the midpoint of the binned categories. We can do this using the at option.\n\nmargins, at(age_num = (21, 27, 32, 50))\n\nAgain, we can duplicate this with the basic predict function. We just predict at that value of the covariate for all values of the offset, and get the average prediction as we did before.\n\nnb_glm_offset_cont = MASS::glm.nb(claims ~  age_num + offset(ln_holders), data = insurance)\n\npredictions = expand.grid(\n  age_num = c(21, 27, 32, 50),\n  ln_holders = insurance$ln_holders\n) %&gt;% \n  mutate(pred = \n           predict(\n             nb_glm_offset_cont,\n             newdata = .,\n             type = 'response'\n           )\n  ) %&gt;% \n  group_by(age_num) %&gt;% \n  summarise(average_prediction = mean(pred))\n\n\n\n\n\n\nage_num\naverage_prediction\n\n\n\n\n21\n67.500\n\n\n27\n64.366\n\n\n32\n61.865\n\n\n50\n53.639\n\n\n\n\n\n\n\nWe can also get the dydx for the continuous covariate, which is the derivative of the target with respect to the covariate. In linear models, this is just the regression coefficient, but here we have to do things a little differently.\n\nmargins, dydx(age_num)\n\nAs noted for the categorical case, this value is the average marginal effect. As the Stata reference describes:\n\nIt is not necessarily true that dydx() = 0.5 means that “y increases by 0.5 if x increases by 1”. It is true that “y increases with x at a rate such that, if the rate were constant, y would increase by 0.5 if x increased by 1”\n\nThis qualified interpretation may not be of much value in contexts where the rate is not constant, but we can still see what Stata is doing.\n\n\nR by hand\nFor dydx, when it comes to continuous covariates, there isn’t an obvious change in the covariate to use (i.e. the dx) to evaluate at each point, as is the case with categorical variables, which can use a reference group. So what we do is use a small arbitrary difference (\\(\\epsilon\\)) for the covariate at its observed values, get the predictions for the values above and below the observed value, and then average those differences in predicted values. For comparison to Stata, I set \\(\\epsilon\\) to the value used by the margins command. Note that we are only using the observed values for the offset.\n\nh = function(x, epsilon = 1e-5) max(abs(x), 1) * sqrt(epsilon)\n\nage_dx_plus = insurance %&gt;% \n  select(age_num, ln_holders) %&gt;% \n  mutate(age_num = age_num + h(age_num))\n\nage_dx_minus = insurance %&gt;% \n  select(age_num, ln_holders) %&gt;% \n  mutate(age_num = age_num - h(age_num))\n\npredictions_dydx = \n  tibble(\n    dy = \n      predict(nb_glm_offset_cont, age_dx_plus,  type = 'response') -\n      predict(nb_glm_offset_cont, age_dx_minus, type = 'response'), \n    dx   = age_dx_plus$age_num - age_dx_minus$age_num,\n    dydx = dy/dx\n  )\n\n# summarise(predictions_dydx, ame = mean(dydx))\n\n\n\n\n\n\name\n\n\n\n\n-0.42552\n\n\n\n\n\n\n\nSo we just get predictions for a small difference in age for each value of age, and average that difference in predictions.\n\n\nemmeans\nThe emmeans package is primarily geared toward factor variables, but does have support for numeric variables interacting with factors. However, this isn’t what we’re really looking for here.\n\n\nmargins\nWe can however use the margins package for this, and it provides the same result as before. For whatever reason, it doesn’t have an issue with the offset if we use the lower level dydx function.\n\ndydx(\n  insurance,\n  nb_glm_offset_cont,\n  variable = 'age_num',\n  eps = 1e-5\n) %&gt;%\n  summarise(ame = mean(dydx_age_num))\n\n\n\n\n\n\name\n\n\n\n\n-0.42552\n\n\n\n\n\n\n\nFor more on the dydx case for continuous variables in general, see the resources."
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#other-complications",
    "href": "posts/2020-06-15-predict-with-offset/index.html#other-complications",
    "title": "Predictions with an offset",
    "section": "Other complications",
    "text": "Other complications\nObviously models will have more than one covariate, and in the particular case that was brought to my attention, there were also random effects. I may explore more in the future, but the general result should hold in those circumstances. As a quick example5, we can get the same age results for both, by getting the age group predictions with all values of the dataset (not just the offset).\n\nnb_glm_offset_full = MASS::glm.nb(\n  claims ~  age + group + district + offset(ln_holders), \n  data = insurance\n)\n\nsummary(nb_glm_offset_full)\n\n\nCall:\nMASS::glm.nb(formula = claims ~ age + group + district + offset(ln_holders), \n    data = insurance, init.theta = 449932.7775, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.82174    0.07679 -23.723  &lt; 2e-16 ***\nage25-29    -0.19101    0.08286  -2.305 0.021155 *  \nage30-35    -0.34495    0.08138  -4.239 2.25e-05 ***\nage&gt;35      -0.53667    0.06996  -7.671 1.70e-14 ***\ngroup1-1.5l  0.16133    0.05054   3.192 0.001412 ** \ngroup1.5-2l  0.39281    0.05500   7.142 9.23e-13 ***\ngroup&gt;2l     0.56341    0.07232   7.791 6.67e-15 ***\ndistrict2    0.02587    0.04302   0.601 0.547637    \ndistrict3    0.03853    0.05052   0.763 0.445686    \ndistrict4    0.23421    0.06168   3.797 0.000146 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(449932.8) family taken to be 1)\n\n    Null deviance: 236.212  on 63  degrees of freedom\nResidual deviance:  51.416  on 54  degrees of freedom\nAIC: 390.74\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  449933 \n          Std. Err.:  4185437 \nWarning while fitting theta: iteration limit reached \n\n 2 x log-likelihood:  -368.745 \n\n\n\nnbreg claims i.age i.group i.district, offset(ln_holders) nolog\nmargins age\n\nTo do this with predict, we make predictions for all observed values as if they were at each level of age. Then we average them for each age group, just like we did before.\n\npredictions_full_model =  \n  map_df(1:4, function(i) mutate(insurance, age = levels(age)[i])) %&gt;% \n  mutate(\n    age = factor(age, levels(insurance$age)),   # convert back to factor\n    prediction = predict(nb_glm_offset_full, newdata = ., type = 'response')\n  )  %&gt;% \n  group_by(age) %&gt;% \n  summarise(avg_prediction = mean(prediction)) \n\n\n\n\n\n\nage\navg_prediction\n\n\n\n\n&lt;25\n76.397\n\n\n25-29\n63.113\n\n\n30-35\n54.109\n\n\n&gt;35\n44.669"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#r-packages",
    "href": "posts/2020-06-15-predict-with-offset/index.html#r-packages",
    "title": "Predictions with an offset",
    "section": "R Packages",
    "text": "R Packages\nTo summarize R’s capabilities with Stata-like margins with models using an offset, we have a few options we can note. First, we can get the values using the predict method. Then there are the packages to help with getting these types of predictions. margins explicitly attempts to replicate Stata-like margins for standard and some more complex models, but there doesn’t appear to be documentation on how the offset is dealt with by default. Furthermore, care must be taken if it isn’t an explicitly supported model. As we have also seen, emmeans provides many predictions of the sort discussed here, supports many more models, produces results in a nice format, and has plotting capabilities. However, it’s mostly suited toward factor variables.\nBeyond those, ggeffects uses predict and emmeans under the hood, so offers a nice way to do the same sorts of things, but with a more viable plot as a result. Other packages and functions are available for specific settings. For example, conditional_effects in the brms package provides predictions and visualization for the bayesian setting."
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#summary",
    "href": "posts/2020-06-15-predict-with-offset/index.html#summary",
    "title": "Predictions with an offset",
    "section": "Summary",
    "text": "Summary\nHopefully this will clarify the discrepancies between R and Stata with models using an offset. Honestly, I pretty much always use the predict function with my specified data values because I know what it’s doing and I can understand the results without hesitation regardless of model or package used. Furthermore, if one knows their data at all, it should be possible to specify covariate values that are meaningful pretty easily. On the other hand, getting predictions at averages can cause conceptual issues with categorical variables in many settings, and getting average effects often also can be hard to interpret (e.g. nonlinear relationships).\nOne thing you don’t get with some of the averaged predictions using the predict function are interval estimates, but this could be obtained via bootstrapping. Otherwise, most predict methods provide the standard error for a prediction with an additional argument (e.g. se.fit = TRUE), so if you getting predictions at key values of the variables it is trivial to get the interval estimate. In general, most R packages are just using predict under the hood, so being familiar with it will likely get you what you need on its own."
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#resources",
    "href": "posts/2020-06-15-predict-with-offset/index.html#resources",
    "title": "Predictions with an offset",
    "section": "Resources",
    "text": "Resources\n\nReference\nStata reference for margins\nemmeans\nmargins\nggeffects\n\n\nNotes\nMarginal Effects- Rich Williams notes- 1\nMarginal Effects- Rich Williams notes- 2\nMarginal Effects- Rich Williams notes- 3\nMarginal Effects Stata Article by Rich Williams\nJosh Errickson’s comparisons of Stata, emmeans, and ggeffects\nUCLA IDRE FAQ (Margins command section)\nStata FAQ (based on old mfx command)"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#appendix",
    "href": "posts/2020-06-15-predict-with-offset/index.html#appendix",
    "title": "Predictions with an offset",
    "section": "Appendix",
    "text": "Appendix\nJust for giggles, I did an average marginal effect for a GAM, though I find little utility in it for the relationship shown. Confirmed via gratia and margins.\n\nlibrary(mgcv)\nset.seed(2)\ndat &lt;- gamSim(1, n = 400, dist = \"normal\", scale = 2)\n\nGu & Wahba 4 term additive model\n\nb &lt;- gam(y ~ s(x2), data = dat)\n\nvisibly::plot_gam(b)\n\n\n\n\n\n\n\n# set change step\nh = 1e-5\n\nb_dx_plus = dat %&gt;% \n  select(x2) %&gt;% \n  mutate(x2 = x2 + h)\n\nb_dx_minus = dat %&gt;% \n  select(x2) %&gt;% \n  mutate(x2 = x2 - h)\n\npredictions_dydx = \n  tibble(\n    x2 = dat$x2,\n    dy = \n      predict(b, b_dx_plus,  type = 'response') -\n      predict(b, b_dx_minus, type = 'response'), \n    dx   = b_dx_plus$x2 - b_dx_minus$x2,\n    dydx = dy/dx\n  ) \n\ngratia_result  = gratia::derivatives(b, newdata = dat, eps = h)\nmargins_result = margins::dydx(dat, b, variable = 'x2', eps = h^2)  # note that margins uses the h function specified previously\n\nall.equal(as.numeric(predictions_dydx$dydx), gratia_result$derivative)\n\n[1] \"names for current but not for target\" \n[2] \"Mean relative difference: 5.59627e-05\"\n\nall.equal(as.numeric(predictions_dydx$dydx), as.numeric(margins_result$dydx_x2))\n\n[1] TRUE\n\nsummarise(\n  predictions_dydx, \n  ame = mean(dydx), \n  ame_gratia = mean(gratia_result$derivative),\n  ame_margins = mean(margins_result$dydx_x2)\n) \n\n# A tibble: 1 × 3\n    ame ame_gratia ame_margins\n  &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1  1.76       1.76        1.76"
  },
  {
    "objectID": "posts/2020-06-15-predict-with-offset/index.html#footnotes",
    "href": "posts/2020-06-15-predict-with-offset/index.html#footnotes",
    "title": "Predictions with an offset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot the least of which is that most outside of econometrics don’t call predictions margins, since these days we aren’t adding results to the margin of a hand-written table.↩︎\nFor those in the R world, the i.age tells Stata to treat the age factor as, well, a factor. Stata’s alpha is 1/theta from R’s output.↩︎\nThe margins package does do predictions rather than the marginal effects, but it, like others, is just a wrapper for the predict method, and doesn’t appear to average them, so I don’t demonstrate that.↩︎\nThere is rarely a justifiable reason to discretize age as near as I can tell, and doing so inevitably results in less satisfying and less reliable conclusions.↩︎\nThere is a weird print issue where the Stata output isn’t showing the coefficient for one of the levels of group, but the model is correct and was verified directly using Stata.↩︎"
  },
  {
    "objectID": "posts/2020-03-23-covid/index.html",
    "href": "posts/2020-03-23-covid/index.html",
    "title": "Exploring the Pandemic",
    "section": "",
    "text": "I once did quite a deep dive into the available covid data when the pandemic was at its height. The data was always changing then and as you can maybe guess, it’s changed a lot since then. I don’t care enough to try and make that post all work again, but in the interest of posterity I show some plots that came out of that. My main takeaway was that epidemiological models were absolute rubbish for predicting anything realtime, and the data/reporting was so inconsistent as to be almost meaningless as far as making specific claims. That said, it was a fun modeling and visualization exercise.\nNote that I began the post early on and updated it sometime in the summer.\n\nWorld\n\n\n\n\n\n\nUS State Level\n   \n\n\nCounties\n\n\n\nMichigan\n \n\n\nModel based\n  \n\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{clark2020,\n  author = {Clark, Michael},\n  title = {Exploring the {Pandemic}},\n  date = {2020-03-23},\n  url = {https://m-clark.github.io/posts/2020-03-23-covid/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nClark, Michael. 2020. “Exploring the Pandemic.” March 23,\n2020. https://m-clark.github.io/posts/2020-03-23-covid/."
  },
  {
    "objectID": "posts/2020-03-01-random-categorical/index.html",
    "href": "posts/2020-03-01-random-categorical/index.html",
    "title": "Categorical Effects as Random",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\nPrerequisites: familiarity with mixed models"
  },
  {
    "objectID": "posts/2020-03-01-random-categorical/index.html#introduction",
    "href": "posts/2020-03-01-random-categorical/index.html#introduction",
    "title": "Categorical Effects as Random",
    "section": "Introduction",
    "text": "Introduction\nIt’s often the case where, for mixed models, we want to look at random ‘slopes’ as well as random intercepts, such that coefficients for the fixed effects are expected to vary by group. This is very common in longitudinal settings, were we want to examine an overall trend, but allow the trend to vary by individual.\nIn such settings, when time is numeric, things are straightforward. The variance components are decomposed into parts for the intercept, the coefficient for the time indicator, and the residual variance (for linear mixed models). But what happens if we have only three time points? Does it make sense to treat it as numeric and hope for the best?\nThis came up in consulting because someone had a similar issue, and tried to keep the format for random slopes while treating the time indicator as categorical. This led to convergence issues, so we thought about what models might be possible. This post explores that scenario.\nPackages used:\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(mixedup)  # http://m-clark.github.io/mixedup"
  },
  {
    "objectID": "posts/2020-03-01-random-categorical/index.html#machines",
    "href": "posts/2020-03-01-random-categorical/index.html#machines",
    "title": "Categorical Effects as Random",
    "section": "Machines",
    "text": "Machines\n\nThe Data\nLet’s start with a very simple data set from the nlme package, which comes with the standard R installation. The reason I chose this is because Doug Bates has a good treatment on this topic using that example (starting at slide 85), which I just extend a bit.\nHere is the data description from the help file.\n\nData on an experiment to compare three brands of machines used in an industrial process are presented in Milliken and Johnson (p. 285, 1992). Six workers were chosen randomly among the employees of a factory to operate each machine three times. The response is an overall productivity score taking into account the number and quality of components produced.\n\nSo for each worker and each machine, we’ll have three scores. Let’s look.\n\nmachines = nlme::Machines\n\n# for some reason worker is an ordered factor.\nmachines = machines %&gt;% \n  mutate(Worker = factor(Worker, levels = 1:6, ordered = FALSE))\n\n\n\n\n\n\n\nWorker\nMachine\nscore\n\n\n\n\n1\n1\nA\n52.0\n\n\n2\n1\nA\n52.8\n\n\n3\n1\nA\n53.1\n\n\n19\n1\nB\n62.1\n\n\n20\n1\nB\n62.6\n\n\n21\n1\nB\n64.0\n\n\n\n\n\n\n\nThis duplicates the plot in Bates’ notes and visually describes the entire data set. There likely is variability due to both workers and machines.\n\n\n\n\n\n\n\n\n\n\n\nRandom Effects Models\nThe random effects of potential interest are for worker and machine, so how do we specify this? Let’s try a standard approach. The following is the type of model tried by our client.\n\nmodel_m_slope = lmer(score ~ Machine + (1 + Machine | Worker), machines)\n\nThis was exactly the same issue our client had- problematic convergence. This could be more of an issue with lme4, and we could certainly explore tweaks to make the problem go away (or use a different package like glmmTMB), but let’s go ahead and keep it.\n\nsummarize_model(model_m_slope, ci = FALSE, cor_re = TRUE)\n\n\nVariance Components:\n\n\n    Group    Effect Variance   SD Var_prop\n   Worker Intercept    16.64 4.08     0.25\n   Worker  MachineB    34.55 5.88     0.53\n   Worker  MachineC    13.62 3.69     0.21\n Residual        NA     0.92 0.96     0.01\n\n\n\nFixed Effects:\n\n\n      Term Value   SE     t P_value Lower_2.5 Upper_97.5\n Intercept 52.36 1.68 31.15    0.00     49.06      55.65\n  MachineB  7.97 2.42  3.29    0.00      3.22      12.71\n  MachineC 13.92 1.54  9.04    0.00     10.90      16.94\n\n\nWe get the variance components we expect, i.e. the variance attributable to the intercept (i.e. Machine A), as well as for the slopes for the difference in machine B vs. A, and C vs. A. We also see the correlations among the random effects. It’s this part that Bates acknowledges is hard to estimate, and incurs estimating potentially notably more parameters than typical random effects models. We have different options that will be available to us though, so let’s try some.\nLet’s start with the simplest, most plausible models. The first would be to have at least a worker effect. The next baseline model could be if we only had a machine by worker effect, i.e. a separate effect of each machine for each worker, essentially treating the interaction term as the sole clustering unit.\n\nmodel_base_w  = lmer(score ~ Machine + (1 | Worker), machines)\nmodel_base_wm = lmer(score ~ Machine + (1 | Worker:Machine), machines)\n\nExamining the random effects makes clear the difference between the two models. For our first baseline model, we only have 6 effects, one for each worker. For the second we have an effect of each machine for each worker.\n\nextract_random_effects(model_base_w)  # only 6 effects\nextract_random_effects(model_base_wm) # 6 workers by 3 machines = 18 effects\n\n\n\n\n\n\ngroup_var\neffect\ngroup\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nWorker\nIntercept\n1\n1.210\n1.032\n-0.813\n3.234\n\n\nWorker\nIntercept\n2\n-1.594\n1.032\n-3.618\n0.429\n\n\nWorker\nIntercept\n3\n6.212\n1.032\n4.188\n8.235\n\n\nWorker\nIntercept\n4\n-0.069\n1.032\n-2.093\n1.954\n\n\nWorker\nIntercept\n5\n2.949\n1.032\n0.925\n4.972\n\n\nWorker\nIntercept\n6\n-8.707\n1.032\n-10.731\n-6.684\n\n\n\n\n\n\n\n\n\n\ngroup_var\neffect\ngroup\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nWorker:Machine\nIntercept\n1:A\n0.275\n0.553\n-0.808\n1.359\n\n\nWorker:Machine\nIntercept\n1:B\n2.556\n0.553\n1.473\n3.640\n\n\nWorker:Machine\nIntercept\n1:C\n0.920\n0.553\n-0.164\n2.004\n\n\nWorker:Machine\nIntercept\n2:A\n0.209\n0.553\n-0.874\n1.293\n\n\nWorker:Machine\nIntercept\n2:B\n-0.749\n0.553\n-1.833\n0.334\n\n\nWorker:Machine\nIntercept\n2:C\n-4.402\n0.553\n-5.486\n-3.318\n\n\nWorker:Machine\nIntercept\n3:A\n7.118\n0.553\n6.035\n8.202\n\n\nWorker:Machine\nIntercept\n3:B\n7.647\n0.553\n6.563\n8.731\n\n\nWorker:Machine\nIntercept\n3:C\n4.490\n0.553\n3.407\n5.574\n\n\nWorker:Machine\nIntercept\n4:A\n-1.113\n0.553\n-2.196\n-0.029\n\n\nWorker:Machine\nIntercept\n4:B\n2.391\n0.553\n1.307\n3.475\n\n\nWorker:Machine\nIntercept\n4:C\n-1.493\n0.553\n-2.577\n-0.409\n\n\nWorker:Machine\nIntercept\n5:A\n-0.981\n0.553\n-2.064\n0.103\n\n\nWorker:Machine\nIntercept\n5:B\n4.705\n0.553\n3.621\n5.789\n\n\nWorker:Machine\nIntercept\n5:C\n5.416\n0.553\n4.332\n6.499\n\n\nWorker:Machine\nIntercept\n6:A\n-5.509\n0.553\n-6.593\n-4.426\n\n\nWorker:Machine\nIntercept\n6:B\n-16.550\n0.553\n-17.634\n-15.467\n\n\nWorker:Machine\nIntercept\n6:C\n-4.931\n0.553\n-6.014\n-3.847\n\n\n\n\n\n\n\nAs a next step, we’ll essentially combine our two baseline models.\n\nmodel_w_wm = lmer(score ~ Machine + (1 | Worker) + (1 | Worker:Machine), machines)\n\nNow we have 6 worker effects plus 18 machine within worker effects1.\n\nextract_random_effects(model_w_wm)\n\n\n\n\n\n\ngroup_var\neffect\ngroup\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nWorker:Machine\nIntercept\n1:A\n-0.750\n2.015\n-4.699\n3.198\n\n\nWorker:Machine\nIntercept\n1:B\n1.500\n2.015\n-2.449\n5.449\n\n\nWorker:Machine\nIntercept\n1:C\n-0.114\n2.015\n-4.063\n3.834\n\n\nWorker:Machine\nIntercept\n2:A\n1.553\n2.015\n-2.396\n5.501\n\n\nWorker:Machine\nIntercept\n2:B\n0.607\n2.015\n-3.342\n4.555\n\n\nWorker:Machine\nIntercept\n2:C\n-2.997\n2.015\n-6.945\n0.952\n\n\nWorker:Machine\nIntercept\n3:A\n1.778\n2.015\n-2.171\n5.726\n\n\nWorker:Machine\nIntercept\n3:B\n2.299\n2.015\n-1.649\n6.248\n\n\nWorker:Machine\nIntercept\n3:C\n-0.815\n2.015\n-4.763\n3.134\n\n\nWorker:Machine\nIntercept\n4:A\n-1.039\n2.015\n-4.988\n2.909\n\n\nWorker:Machine\nIntercept\n4:B\n2.417\n2.015\n-1.531\n6.366\n\n\nWorker:Machine\nIntercept\n4:C\n-1.414\n2.015\n-5.363\n2.534\n\n\nWorker:Machine\nIntercept\n5:A\n-3.457\n2.015\n-7.405\n0.492\n\n\nWorker:Machine\nIntercept\n5:B\n2.152\n2.015\n-1.796\n6.101\n\n\nWorker:Machine\nIntercept\n5:C\n2.853\n2.015\n-1.095\n6.802\n\n\nWorker:Machine\nIntercept\n6:A\n1.916\n2.015\n-2.032\n5.865\n\n\nWorker:Machine\nIntercept\n6:B\n-8.976\n2.015\n-12.924\n-5.027\n\n\nWorker:Machine\nIntercept\n6:C\n2.487\n2.015\n-1.462\n6.435\n\n\nWorker\nIntercept\n1\n1.045\n1.981\n-2.839\n4.928\n\n\nWorker\nIntercept\n2\n-1.376\n1.981\n-5.259\n2.507\n\n\nWorker\nIntercept\n3\n5.361\n1.981\n1.478\n9.244\n\n\nWorker\nIntercept\n4\n-0.060\n1.981\n-3.943\n3.823\n\n\nWorker\nIntercept\n5\n2.545\n1.981\n-1.339\n6.428\n\n\nWorker\nIntercept\n6\n-7.514\n1.981\n-11.397\n-3.631\n\n\n\n\n\n\n\nIf you look closely at these effects, and add them together, you will get a value similar to our second baseline model, which is probably not too surprising. For example in the above model 1:B + 1 = 1.5 + 1.045. Looking at the initial model, the estimated random effect for 1:B was 2.556. Likewise if we look at the variance components, we can see that the sum of the non-residual effect variances for model_w_wm equals the variance of model_base_wm (36.8). So this latest model allows us to disentangle the worker and machine effects, where our baseline models did not.\nNext we’ll do the ‘vector-valued’ model Bates describes. This removes the intercept portion of the formula in the original random slopes model, but is otherwise the same. We can look at the results here, but I will hold off description for comparing it to other models. Note that at least have no convergence problem.\n\nmodel_m_vv = lmer(score ~ Machine + (0 + Machine | Worker), machines)\n\nsummarize_model(model_m_vv, ci = 0, cor_re = TRUE)\n\n\nVariance Components:\n\n\n    Group   Effect Variance   SD Var_prop\n   Worker MachineA    16.64 4.08     0.15\n   Worker MachineB    74.39 8.63     0.67\n   Worker MachineC    19.27 4.39     0.17\n Residual       NA     0.92 0.96     0.01\n\n\n\nFixed Effects:\n\n\n      Term Value   SE     t P_value Lower_2.5 Upper_97.5\n Intercept 52.36 1.68 31.15    0.00     49.06      55.65\n  MachineB  7.97 2.42  3.29    0.00      3.22      12.71\n  MachineC 13.92 1.54  9.04    0.00     10.90      16.94\n\n\n\n\nSummarize All the Models\nNow let’s extract the fixed effect and variance component summaries for all the models.\n\nmodel_list = mget(ls(pattern = 'model_'))\n\nfe = map_df(model_list, extract_fixed_effects, .id = 'model')\n\nvc = map_df(model_list, extract_vc, ci_level = 0, .id = 'model')\n\nFirst, let’s look at the fixed effects. We see that there are no differences in the coefficients for the fixed effect of machine, which is our only covariate in the model. However, there are notable differences for the estimated standard errors. Practically we’d come to no differences in our conclusions, but the uncertainty associated with them would be different.\n\n\n\n\n\nmodel\nterm\nvalue\nse\nt\np_value\nlower_2.5\nupper_97.5\n\n\n\n\nmodel_base_w\nIntercept\n52.356\n2.229\n23.485\n0.000\n47.986\n56.725\n\n\nmodel_base_w\nMachineB\n7.967\n1.054\n7.559\n0.000\n5.901\n10.032\n\n\nmodel_base_w\nMachineC\n13.917\n1.054\n13.205\n0.000\n11.851\n15.982\n\n\nmodel_base_wm\nIntercept\n52.356\n2.486\n21.062\n0.000\n47.483\n57.228\n\n\nmodel_base_wm\nMachineB\n7.967\n3.515\n2.266\n0.023\n1.076\n14.857\n\n\nmodel_base_wm\nMachineC\n13.917\n3.515\n3.959\n0.000\n7.026\n20.807\n\n\nmodel_m_slope\nIntercept\n52.356\n1.681\n31.152\n0.000\n49.062\n55.650\n\n\nmodel_m_slope\nMachineB\n7.967\n2.421\n3.291\n0.001\n3.221\n12.712\n\n\nmodel_m_slope\nMachineC\n13.917\n1.540\n9.036\n0.000\n10.898\n16.935\n\n\nmodel_m_vv\nIntercept\n52.356\n1.681\n31.151\n0.000\n49.061\n55.650\n\n\nmodel_m_vv\nMachineB\n7.967\n2.421\n3.291\n0.001\n3.222\n12.712\n\n\nmodel_m_vv\nMachineC\n13.917\n1.540\n9.036\n0.000\n10.898\n16.935\n\n\nmodel_w_wm\nIntercept\n52.356\n2.486\n21.062\n0.000\n47.483\n57.228\n\n\nmodel_w_wm\nMachineB\n7.967\n2.177\n3.660\n0.000\n3.700\n12.233\n\n\nmodel_w_wm\nMachineC\n13.917\n2.177\n6.393\n0.000\n9.650\n18.183\n\n\n\n\n\n\n\nHere are the variance components, there are definitely some differences here, but, as we’ll see, maybe not as much as we suspect.\n\n\n\n\n\nmodel\ngroup\neffect\nvariance\nsd\nvar_prop\n\n\n\n\nmodel_base_w\nWorker\nIntercept\n26.487\n5.147\n0.726\n\n\nmodel_base_w\nResidual\nNA\n9.996\n3.162\n0.274\n\n\nmodel_base_wm\nWorker:Machine\nIntercept\n36.768\n6.064\n0.975\n\n\nmodel_base_wm\nResidual\nNA\n0.925\n0.962\n0.025\n\n\nmodel_m_slope\nWorker\nIntercept\n16.639\n4.079\n0.253\n\n\nmodel_m_slope\nWorker\nMachineB\n34.554\n5.878\n0.526\n\n\nmodel_m_slope\nWorker\nMachineC\n13.617\n3.690\n0.207\n\n\nmodel_m_slope\nResidual\nNA\n0.925\n0.962\n0.014\n\n\nmodel_m_vv\nWorker\nMachineA\n16.640\n4.079\n0.150\n\n\nmodel_m_vv\nWorker\nMachineB\n74.395\n8.625\n0.669\n\n\nmodel_m_vv\nWorker\nMachineC\n19.270\n4.390\n0.173\n\n\nmodel_m_vv\nResidual\nNA\n0.925\n0.962\n0.008\n\n\nmodel_w_wm\nWorker:Machine\nIntercept\n13.909\n3.730\n0.369\n\n\nmodel_w_wm\nWorker\nIntercept\n22.858\n4.781\n0.606\n\n\nmodel_w_wm\nResidual\nNA\n0.925\n0.962\n0.025\n\n\n\n\n\n\n\nWe can see that the base_wm model has (non-residual) variance 36.768. This equals the total of the two (non-residual) variance components of the w_wm model 13.909 + 22.858, which again speaks to the latter model decomposing a machine effect into worker + machine effects. This value also equals the variance of the vector-valued model divided by the number of groups (16.64 + 74.395 + 19.27) / 3.\nWe can see that the estimated random effects from the vector-valued model (m_vv) are essentially the same as from the baseline, interaction-only model. However, the way it is estimated allows for incorporation of correlations among the machine random effects, so they are not identical (but pretty close).\n\nextract_random_effects(model_m_vv)\n\nextract_random_effects(model_base_wm) \n\n\n\n\n\n\ngroup_var\neffect\ngroup\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nWorker\nMachineA\n1\n0.312\n0.541\n-0.749\n1.373\n\n\nWorker\nMachineB\n1\n2.553\n0.551\n1.474\n3.632\n\n\nWorker\nMachineC\n1\n0.930\n0.545\n-0.137\n1.998\n\n\nWorker\nMachineA\n2\n0.184\n0.541\n-0.877\n1.245\n\n\nWorker\nMachineB\n2\n-0.803\n0.551\n-1.882\n0.276\n\n\nWorker\nMachineC\n2\n-4.282\n0.545\n-5.350\n-3.215\n\n\nWorker\nMachineA\n3\n6.969\n0.541\n5.908\n8.030\n\n\nWorker\nMachineB\n3\n7.779\n0.551\n6.700\n8.858\n\n\nWorker\nMachineC\n3\n4.474\n0.545\n3.406\n5.541\n\n\nWorker\nMachineA\n4\n-1.024\n0.541\n-2.085\n0.037\n\n\nWorker\nMachineB\n4\n2.328\n0.551\n1.249\n3.407\n\n\nWorker\nMachineC\n4\n-1.415\n0.545\n-2.482\n-0.347\n\n\nWorker\nMachineA\n5\n-0.850\n0.541\n-1.911\n0.211\n\n\nWorker\nMachineB\n5\n4.726\n0.551\n3.647\n5.805\n\n\nWorker\nMachineC\n5\n5.323\n0.545\n4.256\n6.391\n\n\nWorker\nMachineA\n6\n-5.592\n0.541\n-6.653\n-4.531\n\n\nWorker\nMachineB\n6\n-16.584\n0.551\n-17.663\n-15.505\n\n\nWorker\nMachineC\n6\n-5.030\n0.545\n-6.098\n-3.963\n\n\n\n\n\n\n\n\n\n\ngroup_var\neffect\ngroup\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nWorker:Machine\nIntercept\n1:A\n0.275\n0.553\n-0.808\n1.359\n\n\nWorker:Machine\nIntercept\n1:B\n2.556\n0.553\n1.473\n3.640\n\n\nWorker:Machine\nIntercept\n1:C\n0.920\n0.553\n-0.164\n2.004\n\n\nWorker:Machine\nIntercept\n2:A\n0.209\n0.553\n-0.874\n1.293\n\n\nWorker:Machine\nIntercept\n2:B\n-0.749\n0.553\n-1.833\n0.334\n\n\nWorker:Machine\nIntercept\n2:C\n-4.402\n0.553\n-5.486\n-3.318\n\n\nWorker:Machine\nIntercept\n3:A\n7.118\n0.553\n6.035\n8.202\n\n\nWorker:Machine\nIntercept\n3:B\n7.647\n0.553\n6.563\n8.731\n\n\nWorker:Machine\nIntercept\n3:C\n4.490\n0.553\n3.407\n5.574\n\n\nWorker:Machine\nIntercept\n4:A\n-1.113\n0.553\n-2.196\n-0.029\n\n\nWorker:Machine\nIntercept\n4:B\n2.391\n0.553\n1.307\n3.475\n\n\nWorker:Machine\nIntercept\n4:C\n-1.493\n0.553\n-2.577\n-0.409\n\n\nWorker:Machine\nIntercept\n5:A\n-0.981\n0.553\n-2.064\n0.103\n\n\nWorker:Machine\nIntercept\n5:B\n4.705\n0.553\n3.621\n5.789\n\n\nWorker:Machine\nIntercept\n5:C\n5.416\n0.553\n4.332\n6.499\n\n\nWorker:Machine\nIntercept\n6:A\n-5.509\n0.553\n-6.593\n-4.426\n\n\nWorker:Machine\nIntercept\n6:B\n-16.550\n0.553\n-17.634\n-15.467\n\n\nWorker:Machine\nIntercept\n6:C\n-4.931\n0.553\n-6.014\n-3.847\n\n\n\n\n\n\n\nEven the default way that the extracted random effects are structured implies this difference. In the vector-valued model we have a multivariate normal draw for 3 machines (i.e. 3 variances and 3 covariances) for each of six workers. In the baseline model, we do not estimate any covariances and assume equal variance to draw for 18 groups (1 variance).\n\nranef(model_m_vv)\n\n$Worker\n    MachineA    MachineB   MachineC\n1  0.3119847   2.5532230  0.9303029\n2  0.1838934  -0.8033384 -4.2822788\n3  6.9692298   7.7793441  4.4735177\n4 -1.0238759   2.3284494 -1.4146165\n5 -0.8496428   4.7261139  5.3231867\n6 -5.5915891 -16.5837920 -5.0301120\n\nwith conditional variances for \"Worker\" \n\nranef(model_base_wm)\n\n$`Worker:Machine`\n    (Intercept)\n1:A   0.2754686\n1:B   2.5563490\n1:C   0.9200653\n2:A   0.2093562\n2:B  -0.7492747\n2:C  -4.4019889\n3:A   7.1181097\n3:B   7.6470094\n3:C   4.4901388\n4:A  -1.1128933\n4:B   2.3910678\n4:C  -1.4930400\n5:A  -0.9806684\n5:B   4.7050044\n5:C   5.4157135\n6:A  -5.5093728\n6:B -16.5501559\n6:C  -4.9308887\n\nwith conditional variances for \"Worker:Machine\" \n\n\nNow let’s compare the models directly via AIC. As we would expect if we dummy coded a predictor vs. running a model without the intercept (e.g. lm(score ~ machine), vs. lm(score ~ -1 + machine)), the random slope model and vector-valued models are identical and produce the same AIC. Likewise the intercept variance of the former is equal to the first group variance of the vector-valued model.\n\n\n\n\n\nmodel_base_w\nmodel_base_wm\nmodel_m_slope\nmodel_m_vv\nmodel_w_wm\n\n\n\n\n296.878\n231.256\n228.311\n228.311\n227.688\n\n\n\n\n\n\n\nWhile such a model is doing better than either of our baseline models, it turns out that our other approach is slightly better, as the additional complexity of estimating the covariances and separate variances wasn’t really worth it.\nAt this point we’ve seen a couple of ways of doing a model in this situation. Some may be a little too simplistic for a given scenario, others may not capture the correlation structure the way we’d want. In any case, we have options to explore."
  },
  {
    "objectID": "posts/2020-03-01-random-categorical/index.html#simulation",
    "href": "posts/2020-03-01-random-categorical/index.html#simulation",
    "title": "Categorical Effects as Random",
    "section": "Simulation",
    "text": "Simulation\nThe following is a simplified approach to creating data in this scenario, and allows us to play around with the settings to see what happens.\n\nData Creation\nFirst we need some data. The following creates a group identifier similar to Worker in our previous example, a cat_var like our Machine, and other covariates just to make it interesting.\n\n# for simplicity keeping to 3 cat levels\nset.seed(1234)\nng = 5000     # n groups\ncat_levs = 3  # n levels per group\nreps = 4      # number of obs per level per cat\n\nid = rep(1:ng, each = cat_levs * reps)           # id indicator (e.g. like Worker)\ncat_var = rep(1:cat_levs, times = ng, e = reps)  # categorical variable (e.g. Machine)\nx = rnorm(ng * cat_levs * reps)                  # continuous covariate\nx_c = rep(rnorm(ng), e = cat_levs * reps)        # continuous cluster level covariate\n\nSo we have the basic data in place, now we need to create the random effects. There are several ways we could do this, including more efficient ones, but this approach focuses on a conceptual approach and on the model that got us here, i.e. something of the form (1 + cat_var | group). In this case we assume this model is ‘correct’, so we’re going to create a multivariate normal draw of random effects for each level of the cat_var, which is only 3 levels. The correlations depicted are the estimates we expect from our model for the random effects2.\n\n# as correlated  (1, .5, .5) var, (1, .25, .25) sd \ncov_mat = lazerhawk::create_corr(c(.1, .25, .25), diagonal = c(1, .5, .5))\n\ncov2cor(cov_mat)  # these will be the estimated correlations for the random_slope model\n\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.1414214 0.3535534\n[2,] 0.1414214 1.0000000 0.5000000\n[3,] 0.3535534 0.5000000 1.0000000\n\n\nNow we create the random effects by drawing an effect for each categorical level for each group.\n\n# take a multivariate normal draw for each of the groups in `id`\nre_id_cat_lev = mvtnorm::rmvnorm(ng, mean = rep(0, 3), sigma = cov_mat) %&gt;% \n  data.frame()\n\nhead(re_id_cat_lev)\n\n          X1          X2          X3\n1  0.5618465  0.62780841  1.15175459\n2  0.6588685  0.78045939  0.25942427\n3  0.2680315 -0.06403496  1.30173301\n4 -0.3711184  0.37579392 -0.03242486\n5 -1.1306064 -0.08450038 -0.38165685\n6 -0.7537223  0.65320469  0.33269260\n\n\nNow that we have the random effects, we can create our target variable. We do this by adding our first effect to the intercept, and the others to their respective coefficients.\n\ny = \n  # fixed effect = (2, .5, -.5)\n  2  + .5*x - .5*x_c +   \n  # random intercept\n  rep(re_id_cat_lev[, 1], each = cat_levs * reps) +                           \n  # .25 is the fixef for group 2 vs. 1\n  (.25 + rep(re_id_cat_lev[, 2], each = cat_levs * reps)) * (cat_var == 2) +  \n  # .40 is the fixef for group 3 vs. 1\n  (.40 + rep(re_id_cat_lev[, 3], each = cat_levs * reps)) * (cat_var == 3) +  \n  rnorm(ng * cat_levs * reps, sd = .5)\n\nNow we create a data frame so we can see everything together.\n\ndf = tibble(\n    id,\n    cat_var,\n    x,\n    x_c,\n    y,\n    re_id = rep(re_id_cat_lev[, 1], each = cat_levs*reps),\n    re_id_cat_lev2 = rep(re_id_cat_lev[, 2], each = cat_levs*reps),\n    re_id_cat_lev3 = rep(re_id_cat_lev[, 3], each = cat_levs*reps)\n  ) %&gt;% \n  mutate(\n    cat_var = factor(cat_var),\n    cat_as_num = as.integer(cat_var),\n    id = factor(id)\n  )\n\ndf %&gt;% print(n = 30)\n\n# A tibble: 60,000 × 9\n   id    cat_var       x    x_c     y re_id re_id_cat_lev2 re_id_cat_lev3\n   &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 1     1       -1.21   -1.43  3.04  0.562         0.628           1.15 \n 2 1     1        0.277  -1.43  3.35  0.562         0.628           1.15 \n 3 1     1        1.08   -1.43  3.46  0.562         0.628           1.15 \n 4 1     1       -2.35   -1.43  2.64  0.562         0.628           1.15 \n 5 1     2        0.429  -1.43  4.59  0.562         0.628           1.15 \n 6 1     2        0.506  -1.43  3.61  0.562         0.628           1.15 \n 7 1     2       -0.575  -1.43  3.54  0.562         0.628           1.15 \n 8 1     2       -0.547  -1.43  2.98  0.562         0.628           1.15 \n 9 1     3       -0.564  -1.43  5.28  0.562         0.628           1.15 \n10 1     3       -0.890  -1.43  4.06  0.562         0.628           1.15 \n11 1     3       -0.477  -1.43  4.31  0.562         0.628           1.15 \n12 1     3       -0.998  -1.43  4.70  0.562         0.628           1.15 \n13 2     1       -0.776   0.126 2.25  0.659         0.780           0.259\n14 2     1        0.0645  0.126 2.48  0.659         0.780           0.259\n15 2     1        0.959   0.126 2.99  0.659         0.780           0.259\n16 2     1       -0.110   0.126 2.55  0.659         0.780           0.259\n17 2     2       -0.511   0.126 3.94  0.659         0.780           0.259\n18 2     2       -0.911   0.126 3.22  0.659         0.780           0.259\n19 2     2       -0.837   0.126 3.98  0.659         0.780           0.259\n20 2     2        2.42    0.126 4.59  0.659         0.780           0.259\n21 2     3        0.134   0.126 3.58  0.659         0.780           0.259\n22 2     3       -0.491   0.126 3.31  0.659         0.780           0.259\n23 2     3       -0.441   0.126 2.66  0.659         0.780           0.259\n24 2     3        0.460   0.126 3.42  0.659         0.780           0.259\n25 3     1       -0.694   0.437 0.985 0.268        -0.0640          1.30 \n26 3     1       -1.45    0.437 2.08  0.268        -0.0640          1.30 \n27 3     1        0.575   0.437 2.62  0.268        -0.0640          1.30 \n28 3     1       -1.02    0.437 1.24  0.268        -0.0640          1.30 \n29 3     2       -0.0151  0.437 2.24  0.268        -0.0640          1.30 \n30 3     2       -0.936   0.437 2.57  0.268        -0.0640          1.30 \n# ℹ 59,970 more rows\n# ℹ 1 more variable: cat_as_num &lt;int&gt;\n\n\n\n\nRun the Models & Summarize\nWith everything in place, let’s run four models similar to our previous models from the Machine example:\n\nThe baseline model that does not distinguish the id from cat_var variance.\nThe random slope approach (data generating model)\nThe vector valued model (equivalent to #2)\nThe scalar model that does not estimate the random effect correlations\n\n\nm_interaction_only = lmer(y ~ x + x_c + cat_var + (1 | id:cat_var), df)\nm_random_slope = lmer(y ~ x + x_c + cat_var + (1 + cat_var | id), df)    \nm_vector_valued = lmer(y ~ x + x_c + cat_var + (0 + cat_var | id), df)\nm_separate_re = lmer(y ~ x + x_c + cat_var + (1 | id) + (1 | id:cat_var), df)\n\n\nmodel_mixed = list(\n  m_interaction_only = m_interaction_only,\n  m_random_slope = m_random_slope,\n  m_vector_valued = m_vector_valued,\n  m_separate_re = m_separate_re\n)\n\n# model summaries if desired\n# map(model_mixed, summarize_model, ci = 0, cor_re = TRUE)\nfe = map_df(model_mixed, extract_fixed_effects, .id = 'model') \nvc = map_df(model_mixed, extract_vc, ci_level = 0, .id = 'model')\n\nLooking at the fixed effects, we get what we should but, as before, we do see differences in the standard errors.\n\n\n\n\n\nmodel\nterm\nvalue\nse\nt\np_value\nlower_2.5\nupper_97.5\n\n\n\n\nm_interaction_only\nIntercept\n2.006\n0.018\n111.051\n0\n1.971\n2.042\n\n\nm_interaction_only\nx\n0.497\n0.002\n212.962\n0\n0.492\n0.501\n\n\nm_interaction_only\nx_c\n-0.489\n0.010\n-46.776\n0\n-0.509\n-0.469\n\n\nm_interaction_only\ncat_var2\n0.256\n0.026\n10.026\n0\n0.206\n0.306\n\n\nm_interaction_only\ncat_var3\n0.389\n0.026\n15.228\n0\n0.339\n0.439\n\n\nm_random_slope\nIntercept\n2.006\n0.015\n136.943\n0\n1.977\n2.035\n\n\nm_random_slope\nx\n0.497\n0.002\n217.676\n0\n0.493\n0.502\n\n\nm_random_slope\nx_c\n-0.495\n0.014\n-34.636\n0\n-0.523\n-0.467\n\n\nm_random_slope\ncat_var2\n0.256\n0.011\n23.038\n0\n0.234\n0.278\n\n\nm_random_slope\ncat_var3\n0.389\n0.011\n34.969\n0\n0.367\n0.411\n\n\nm_vector_valued\nIntercept\n2.006\n0.015\n136.940\n0\n1.977\n2.035\n\n\nm_vector_valued\nx\n0.497\n0.002\n217.677\n0\n0.493\n0.502\n\n\nm_vector_valued\nx_c\n-0.495\n0.014\n-34.635\n0\n-0.523\n-0.467\n\n\nm_vector_valued\ncat_var2\n0.256\n0.011\n23.038\n0\n0.234\n0.278\n\n\nm_vector_valued\ncat_var3\n0.389\n0.011\n34.971\n0\n0.367\n0.411\n\n\nm_separate_re\nIntercept\n2.006\n0.018\n111.045\n0\n1.971\n2.042\n\n\nm_separate_re\nx\n0.497\n0.002\n216.586\n0\n0.493\n0.502\n\n\nm_separate_re\nx_c\n-0.489\n0.017\n-28.883\n0\n-0.522\n-0.456\n\n\nm_separate_re\ncat_var2\n0.256\n0.011\n23.077\n0\n0.234\n0.278\n\n\nm_separate_re\ncat_var3\n0.389\n0.011\n35.050\n0\n0.367\n0.411\n\n\n\n\n\n\n\nThe variance components break down as before.\n\n\n\n\n\nmodel\ngroup\neffect\nvariance\nsd\nvar_prop\n\n\n\n\nm_interaction_only\nid:cat_var\nIntercept\n1.570\n1.253\n0.864\n\n\nm_interaction_only\nResidual\nNA\n0.247\n0.497\n0.136\n\n\nm_random_slope\nid\nIntercept\n1.011\n1.006\n0.450\n\n\nm_random_slope\nid\ncat_var2\n0.494\n0.703\n0.220\n\n\nm_random_slope\nid\ncat_var3\n0.495\n0.704\n0.220\n\n\nm_random_slope\nResidual\nNA\n0.247\n0.497\n0.110\n\n\nm_vector_valued\nid\ncat_var1\n1.011\n1.006\n0.204\n\n\nm_vector_valued\nid\ncat_var2\n1.709\n1.307\n0.345\n\n\nm_vector_valued\nid\ncat_var3\n1.990\n1.411\n0.401\n\n\nm_vector_valued\nResidual\nNA\n0.247\n0.497\n0.050\n\n\nm_separate_re\nid:cat_var\nIntercept\n0.246\n0.496\n0.135\n\n\nm_separate_re\nid\nIntercept\n1.324\n1.151\n0.728\n\n\nm_separate_re\nResidual\nNA\n0.247\n0.497\n0.136\n\n\n\n\n\n\n\nIn this case, we know the model with correlated random effects is the more accurate model, and this is born out via AIC.\n\n\n\n\n\nm_interaction_only\nm_random_slope\nm_vector_valued\nm_separate_re\n\n\n\n\n135592.8\n122051.9\n122051.9\n123745.2\n\n\n\n\n\n\n\n\n\nChange the model orientation\nNow I will make the vector_valued model reduce to the separate_re model. First, we create a covariance matrix that has equal variances/covariances (i.e. compound symmetry), and for demonstration, we will apply the random effects a little differently. So, when we create the target variable, we make a slight alteration to apply it to the vector valued model instead.\n\nset.seed(1234)\n\ncov_mat = lazerhawk::create_corr(c(0.1, 0.1, 0.1), diagonal = c(.5, .5, .5))\n\ncov2cor(cov_mat)  # these will now be the estimated correlations for the vector_valued model\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.2  0.2\n[2,]  0.2  1.0  0.2\n[3,]  0.2  0.2  1.0\n\nre_id_cat_lev = mvtnorm::rmvnorm(ng, mean = rep(0, 3), sigma = cov_mat) %&gt;% \n  data.frame()\n\ny = 2  + .5*x - .5*x_c +   # fixed effect = (2, .5, -.5)\n  rep(re_id_cat_lev[, 1], each = cat_levs * reps) * (cat_var == 1) +     # added this\n  rep(re_id_cat_lev[, 2], each = cat_levs * reps) * (cat_var == 2) +\n  rep(re_id_cat_lev[, 3], each = cat_levs * reps) * (cat_var == 3) +\n  .25 * (cat_var == 2) +  # .25 is the fixef for group 2 vs. 1\n  .40 * (cat_var == 3) +  # .40 is the fixef for group 3 vs. 1\n  rnorm(ng * cat_levs * reps, sd = .5)\n\n\ndf = tibble(\n    id,\n    cat_var  = factor(cat_var),\n    x,\n    x_c,\n    y\n  )\n\nRerun the models.\n\nm_random_slope  = lmer(y ~ x + x_c + cat_var + (1 + cat_var | id), df)  # still problems!\nm_vector_valued = lmer(y ~ x + x_c + cat_var + (0 + cat_var | id), df)   \nm_separate_re   = lmer(y ~ x + x_c + cat_var + (1 | id) + (1 | id:cat_var), df)\n\nExamine the variance components.\n\nmodel_mixed = list(\n  m_random_slope  = m_random_slope,\n  m_vector_valued = m_vector_valued,\n  m_separate_re   = m_separate_re\n)\n\n# model summaries if desired\n# map(model_mixed, summarize_model, ci = 0, cor_re = TRUE)\n\n# fixed effects if desired\n# fe = map_df(model_mixed, extract_fixed_effects, .id = 'model')\n\nvc = map_df(model_mixed, extract_vc, ci_level = 0, .id = 'model')\n\n\n\n\n\n\nmodel\ngroup\neffect\nvariance\nsd\nvar_prop\n\n\n\n\nm_random_slope\nid\nIntercept\n0.491\n0.700\n0.213\n\n\nm_random_slope\nid\ncat_var2\n0.786\n0.886\n0.341\n\n\nm_random_slope\nid\ncat_var3\n0.778\n0.882\n0.337\n\n\nm_random_slope\nResidual\nNA\n0.251\n0.501\n0.109\n\n\nm_vector_valued\nid\ncat_var1\n0.491\n0.700\n0.283\n\n\nm_vector_valued\nid\ncat_var2\n0.497\n0.705\n0.287\n\n\nm_vector_valued\nid\ncat_var3\n0.492\n0.702\n0.284\n\n\nm_vector_valued\nResidual\nNA\n0.251\n0.501\n0.145\n\n\nm_separate_re\nid:cat_var\nIntercept\n0.395\n0.628\n0.530\n\n\nm_separate_re\nid\nIntercept\n0.099\n0.314\n0.133\n\n\nm_separate_re\nResidual\nNA\n0.251\n0.501\n0.337\n\n\n\n\n\n\n\nIn this case, we know the true case regards zero correlations and equal variances, so estimating them is adding complexity we don’t need, thus our simpler model wins (-log likelihoods are essentially the same).\n\n\n\n\n\nparameter\nm_random_slope\nm_vector_valued\nm_separate_re\n\n\n\n\nLL\n-59818.9\n-59818.9\n-59819.72\n\n\nAIC\n119661.8\n119661.8\n119655.45"
  },
  {
    "objectID": "posts/2020-03-01-random-categorical/index.html#summary",
    "href": "posts/2020-03-01-random-categorical/index.html#summary",
    "title": "Categorical Effects as Random",
    "section": "Summary",
    "text": "Summary\nHere we’ve demonstrated a couple of different ways to specify a particular model with random slopes for a categorical covariate. Intuition may lead to a model that is not easy to estimate, often leading to convergence problems. Sometimes, this model may be overly complicated, and a simpler version will likely have less estimation difficulty. Try it out if you run into trouble!"
  },
  {
    "objectID": "posts/2020-03-01-random-categorical/index.html#footnotes",
    "href": "posts/2020-03-01-random-categorical/index.html#footnotes",
    "title": "Categorical Effects as Random",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough I use the word ‘within’, do not take it to mean we have nested data.↩︎\nI use a personal package to create the matrix where I can just specify the lower diagonal, as this comes up a lot for simulation. Feel free to just create the matrix directly given it’s just a 3x3.↩︎"
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html",
    "href": "posts/2019-08-20-fractional-regression/index.html",
    "title": "Fractional Regression",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#introduction",
    "href": "posts/2019-08-20-fractional-regression/index.html#introduction",
    "title": "Fractional Regression",
    "section": "Introduction",
    "text": "Introduction\nIt is sometimes the case that you might have data that falls primarily between zero and one. For example, these may be proportions, grades from 0-100 that can be transformed as such, reported percentile values, and similar. If you had the raw counts where you also knew the denominator or total value that created the proportion, you would be able to just use standard logistic regression with the binomial distribution. Similarly, if you had a binary outcome (i.e. just zeros and ones), this is just a special case, so the same model would be applicable. Alternatively, if all the target variable values lie between zero and one, beta regression is a natural choice for which to model such data. However, if the variable you wish to model has values between zero and one, and additionally, you also have zeros or ones, what should you do?\nSome suggest adding a ‘fudge factor’ to the zeros or ones to put all values on the (0, 1) interval, so that beta regression could still be employed. Others might implement zero/one-inflated beta regression if a larger percentage of the observations are at the boundaries. However, as we will see, you already have more standard tools that are appropriate for this modeling situation, and this post will demonstrate some of them.\n\nRelated models\n\nBinomial logistic for binary and count/proportional data, i.e. \\(x\\) successes out of \\(n\\) trials (can use standard glm tools)\nBeta regression for (0, 1), i.e. only values between 0 and 1 (see betareg, DirichletReg, mgcv, brms packages)\nZero/One-inflated binomial or beta regression for cases including a relatively high amount of zeros and ones (brms, VGAM, gamlss)"
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#stata-example",
    "href": "posts/2019-08-20-fractional-regression/index.html#stata-example",
    "title": "Fractional Regression",
    "section": "Stata example",
    "text": "Stata example\nIt might seem strange to start with an example using Stata1, but if you look this sort of thing up, you’ll almost certainly come across the Stata demonstration using the fracreg command. For comparison we’ll use the data in the corresponding documentation. The data regards the expected participation rate in 401(k) plans for a cross-section of firms2. They define participation rate (prate) as the fraction of eligible employees in a firm that participate in a 401(k) plan. This is modeled by the matching rate of employee 401(k) contributions (mrate), the (natural) log of the total number of employees (ltotemp), the age of the plan (age), and whether the 401(k) plan is the only retirement plan offered by the employer (sole). Here we do not use quadratic effects for ltotemp and age as in the Stata documentation, though we do use an additive modeling approach later that could be implemented for the same purpose instead3.\nThe following shows the distribution of the target variable. There are no zeroes in the participation rate, however the amount of ones is 33.2%.\n\n\n\n\n\n\n\n\n\n\nThe following specifies a fractional regression with logit link. Probit and heteroscedastic probit are also available.\n\nuse http://www.stata-press.com/data/r14/401k\n\nfracreg logit prate mrate c.ltotemp c.age i.sole\n\n\n\n. use http://www.stata-press.com/data/r14/4. \n. fracreg logit prate mrate c.ltotemp c.age i.sole\n\nIteration 0:   log pseudolikelihood = -1985.1469  \nIteration 1:   log pseudolikelihood = -1689.2659  \nIteration 2:   log pseudolikelihood = -1681.1055  \nIteration 3:   log pseudolikelihood = -1681.0263  \nIteration 4:   log pseudolikelihood = -1681.0263  \n\nFractional logistic regression                  Number of obs     =      4,075\n                                                Wald chi2(4)      =     685.26\n                                                Prob &gt; chi2       =     0.0000\nLog pseudolikelihood = -1681.0263               Pseudo R2         =     0.0596\n\n------------------------------------------------------------------------------\n             |               Robust\n       prate |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n       mrate |   1.157832   .0749231    15.45   0.000     1.010986    1.304679\n     ltotemp |  -.2072429   .0141468   -14.65   0.000      -.23497   -.1795157\n         age |   .0345786   .0027604    12.53   0.000     .0291684    .0399888\n             |\n        sole |\n  only plan  |   .1655762   .0506445     3.27   0.001     .0663147    .2648377\n       _cons |   2.391717   .1061292    22.54   0.000     2.183707    2.599726\n------------------------------------------------------------------------------\n\n\nPerhaps not surprisingly, all of the covariates are statistically notable. With the logistic link, the coefficients can be exponentiated to provide odds ratios4. Stata’s is one of the few tools that is specifically advertised to model such outcomes, but as we’re about to see, you don’t need Stata’s command, or even a special package in R, once you know what’s going on."
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#r-glm",
    "href": "posts/2019-08-20-fractional-regression/index.html#r-glm",
    "title": "Fractional Regression",
    "section": "R GLM",
    "text": "R GLM\nIt turns out that the underlying likelihood for fractional regression in Stata is the same as the standard binomial likelihood we would use for binary or count/proportional outcomes. In the following, \\(y\\) is our target variable, \\(X\\beta\\) is the linear predictor, and \\(g(.)\\) is the link function, for example, the logit.\n\\[\\mathcal{L} \\sim y(\\ln{g(X\\beta)}) + (1-y)(1-\\ln{g(X\\beta)})\\]\nAs such, we can just use glm like we would for count or binary outcomes. It will warn you that the outcome isn’t integer as it expects, but in this case we can just ignore the warning.\n\nd = haven::read_dta('data/401k.dta')\n\nmodel_glm = glm(\n  prate ~ mrate + ltotemp + age + sole,\n  data = d,\n  family = binomial\n)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n# summary(model_glm)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.392\n0.227\n10.520\n0.000\n\n\nmrate\n1.158\n0.147\n7.887\n0.000\n\n\nltotemp\n-0.207\n0.029\n-7.143\n0.000\n\n\nage\n0.035\n0.006\n5.678\n0.000\n\n\nsole\n0.166\n0.104\n1.591\n0.112\n\n\n\n\n\n\n\nSo the model runs fine, and the coefficients are the same as the Stata example. The only difference regards the standard errors, but we can fix that."
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#robust-standard-errors",
    "href": "posts/2019-08-20-fractional-regression/index.html#robust-standard-errors",
    "title": "Fractional Regression",
    "section": "Robust standard errors",
    "text": "Robust standard errors\nThe difference in the standard errors is that, by default, Stata reports robust standard errors. We can use the sandwich package to get them in R. The lmtest package provides a nice summary table.\n\nlibrary(lmtest)\nlibrary(sandwich)\n\nse_glm_robust = coeftest(model_glm, vcov = vcovHC(model_glm, type=\"HC\"))\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.392\n0.106\n22.539\n0.000\n\n\nmrate\n1.158\n0.075\n15.455\n0.000\n\n\nltotemp\n-0.207\n0.014\n-14.651\n0.000\n\n\nage\n0.035\n0.003\n12.528\n0.000\n\n\nsole\n0.166\n0.051\n3.270\n0.001\n\n\n\n\n\n\n\nSo now we have the same result via a standard R generalized linear model and Stata. Likewise, you could just use the glm command in Stata with the vce(robust) option."
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#quasibinomial",
    "href": "posts/2019-08-20-fractional-regression/index.html#quasibinomial",
    "title": "Fractional Regression",
    "section": "Quasibinomial",
    "text": "Quasibinomial\nWe could also use the quasibinomial family. Quasi-likelihoods are similar to standard likelihood functions, but technically do not relate to any particular probability distribution5. Using this family would provide the same result as the previous glm, but without the warning.\n\n\nFrom the R help file for ?family: The quasibinomial and quasipoisson families differ from the binomial and poisson families only in that the dispersion parameter is not fixed at one, so they can model over-dispersion.\nAlso, as noted in the StackExchange link in the references, while by default the variance estimate is ‘robust’, possibly leading to standard errors that are similar, the basic result is not the same as using the robust standard errors.\n\nmodel_quasi = glm(\n  prate ~ mrate + ltotemp + age + sole,\n  data = d,\n  family = quasibinomial\n)\n\n# summary(model_quasi)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.392\n0.105\n22.684\n0.000\n\n\nmrate\n1.158\n0.068\n17.006\n0.000\n\n\nltotemp\n-0.207\n0.013\n-15.402\n0.000\n\n\nage\n0.035\n0.003\n12.244\n0.000\n\n\nsole\n0.166\n0.048\n3.431\n0.001\n\n\n\n\n\n\n\nWe can get robust standard errors for the quasi-likelihood approach as well, but they were already pretty close.\n\nse_glm_robust_quasi = coeftest(model_quasi, vcov = vcovHC(model_quasi, type=\"HC\"))\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.392\n0.106\n22.539\n0.000\n\n\nmrate\n1.158\n0.075\n15.455\n0.000\n\n\nltotemp\n-0.207\n0.014\n-14.651\n0.000\n\n\nage\n0.035\n0.003\n12.528\n0.000\n\n\nsole\n0.166\n0.051\n3.270\n0.001"
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#mixed-model-with-per-observation-random-effect",
    "href": "posts/2019-08-20-fractional-regression/index.html#mixed-model-with-per-observation-random-effect",
    "title": "Fractional Regression",
    "section": "Mixed model with per-observation random effect",
    "text": "Mixed model with per-observation random effect\nIt turns out that we can also use a mixed model approach. For some distributions such as binomial and poisson, the variance is directly tied to the mean function, and so does not have to be estimated. In these scenarios, we can insert a per-observation random effect and estimate the associated variance. This extra source of variance can account for overdispersion, similar to what the scale parameter estimate does for the quasibinomial.\nI initially attempted to do so using the popular mixed model package lme4 and its glmer function, with an observation level random effect. While I’ve had success using this package with such models in the past, in this particular instance, all failed to converge with default optimization settings across multiple optimizers. As such, those results are not shown.\n\nd$id = 1:nrow(d)\n\n## model_glmm = lme4::glmer(\n##   prate ~ mrate + ltotemp + age + sole + (1 | id),\n##   data = d,\n##   family = binomial\n## )\n## \n## summary(model_glmm, cor=F)\n## \n## test_models = lme4::allFit(model_glmm)\n## \n## summary(test_models)\n\n\nWe have options though. The glmmTMB package was able to estimate the model.\n\n\nlibrary(glmmTMB)\n\nmodel_glmm = glmmTMB(\n  prate ~ mrate + ltotemp + age + sole + (1 | id),\n  data = d,\n  family = binomial,\n  REML = TRUE\n)\n\n# summary(model_glmm)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.392\n0.227\n10.520\n0.000\n\n\nmrate\n1.158\n0.147\n7.887\n0.000\n\n\nltotemp\n-0.207\n0.029\n-7.143\n0.000\n\n\nage\n0.035\n0.006\n5.678\n0.000\n\n\nsole\n0.166\n0.104\n1.591\n0.112\n\n\n\n\n\n\n\nWe can maybe guess why glmer was struggling. The extra variance is estimated by glmmTMB to be basically zero.\nLately, I’ve been using mgcv to do most of my mixed models, so we can try a GAM instead. The following is equivalent to the glm-quasibinomial approach before.\n\nlibrary(mgcv)\n\nmodel_gam_std = gam(\n  prate ~ mrate + ltotemp + age + sole, \n  data = d, \n  family = quasibinomial\n)\n\n# summary(model_gam_std)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.392\n0.104\n22.908\n0.000\n\n\nmrate\n1.158\n0.067\n17.174\n0.000\n\n\nltotemp\n-0.207\n0.013\n-15.554\n0.000\n\n\nage\n0.035\n0.003\n12.365\n0.000\n\n\nsole\n0.166\n0.048\n3.464\n0.001\n\n\n\n\n\n\n\nThe following adds the per observation random effect as with the mixed model. Unlike with lme4 or glmmTMB, you can technically use the quasi family here as well, but I will follow Bates’ thinking and avoid doing so6. I will also calculate the robust standard errors.\n\nmodel_gam_re = gam(\n  prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),\n  data = d,\n  family = binomial,\n  method = 'REML'\n)\n\n# summary(model_gam_re)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.392\n0.227\n10.519\n0.000\n\n\nmrate\n1.158\n0.147\n7.887\n0.000\n\n\nltotemp\n-0.207\n0.029\n-7.143\n0.000\n\n\nage\n0.035\n0.006\n5.678\n0.000\n\n\nsole\n0.166\n0.104\n1.591\n0.112"
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#summarized-results",
    "href": "posts/2019-08-20-fractional-regression/index.html#summarized-results",
    "title": "Fractional Regression",
    "section": "Summarized results",
    "text": "Summarized results\nThe following tables show the results of the models. The first table regards the estimated coefficients, the second the standard errors. There are no differences for the coefficients. For standard errors, some approaches are definitely working better than others.\n\n\n\nCoefficients\n\n\nbaseline_glm\nstata\nglm_robust\nglm_robust_quasi\ngam_re_robust\ngam_std\nglmm_std\n\n\n\n\n2.3917\n-2.3917\n2.3917\n2.3917\n2.3917\n2.3917\n2.3917\n\n\n1.1578\n1.1578\n1.1578\n1.1578\n1.1578\n1.1578\n1.1578\n\n\n-0.2072\n-0.2072\n-0.2072\n-0.2072\n-0.2072\n-0.2072\n-0.2072\n\n\n0.0346\n0.0346\n0.0346\n0.0346\n0.0346\n0.0346\n0.0346\n\n\n0.1656\n0.1656\n0.1656\n0.1656\n0.1656\n0.1656\n0.1656\n\n\n\n\n\n\n\n\nStandard Errors\n\n\nbaseline_glm\nstata\nglm_robust\nglm_robust_quasi\ngam_re_robust\ngam_std\nglmm_std\n\n\n\n\n0.2274\n0.1061\n0.1061\n0.1061\n0.1061\n0.1044\n0.2274\n\n\n0.1468\n0.0749\n0.0749\n0.0749\n0.0749\n0.0674\n0.1468\n\n\n0.0290\n0.0141\n0.0141\n0.0141\n0.0141\n0.0133\n0.0290\n\n\n0.0061\n0.0028\n0.0028\n0.0028\n0.0028\n0.0028\n0.0061\n\n\n0.1041\n0.0506\n0.0506\n0.0506\n0.0506\n0.0478\n0.1041"
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#conclusion",
    "href": "posts/2019-08-20-fractional-regression/index.html#conclusion",
    "title": "Fractional Regression",
    "section": "Conclusion",
    "text": "Conclusion\nFractional data occurs from time to time. While Stata and R have specific functionality for such outcomes, more commonly used statistical tools can be used, which might provide additional means of model exploration. In the demo above, a standard glm with robust errors would be fine, and the simplest to pull off. With that as a basis, other complexities could be incorporated in more or less a standard fashion."
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#references",
    "href": "posts/2019-08-20-fractional-regression/index.html#references",
    "title": "Fractional Regression",
    "section": "References",
    "text": "References\nStata demo\nStata reference on fracreg command\nMcCullagh P. and Nelder, J. A. (1989) Generalized Linear Models. London: Chapman and Hall.\nPapke & Wooldridge. (1996) Econometric Methods For Fractional Response Variables With An Application To 401 (K) Plan Participation Rates. link\nRamalho, E., Ramalho, J. & Coelho, L. (2016) Exponential Regression of Fractional-Response Fixed-Effects Models with an Application to Firm Capital Structure. Journal of Econometric Methods. link\nRamalho, E., Ramalho, J. & Murteira, J. (2011) Alternative Estimating And Testing Empirical Strategies For Fractional Regression Models. link\nStackExchange has some more useful discussion, e.g. more on standard error differences between the approaches and other context link, link2"
  },
  {
    "objectID": "posts/2019-08-20-fractional-regression/index.html#footnotes",
    "href": "posts/2019-08-20-fractional-regression/index.html#footnotes",
    "title": "Fractional Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGiven that I’m an avid R user. But if that was not apparent, then using Stata is possibly no surprise at all! 😄↩︎\nI added the original data, which has the raw values and many more observations, to my noiris package.↩︎\nI actually played with this a bit. The quadratic would be okay for age, but log firm size has a little more going on and mrate should also be allowed to wiggle. There would also be some interesting smooth interactions. In short, a generalized additive model is pretty much always a better option than trying to guess polynomials.↩︎\nIn Stata you can just add the option , or to the end of the model line.↩︎\nThis is in fact what fracreg in Stata is doing.↩︎\nFrom Doug Bates: In many application areas using ‘pseudo’ distribution families, such as quasibinomial and quasipoisson, is a popular and well-accepted technique for accommodating variability that is apparently larger than would be expected from a binomial or a Poisson distribution. This amounts to adding an extra parameter, like σ, the common scale parameter in a LMM, to the distribution of the response. It is possible to form an estimate of such a quantity during the IRLS algorithm but it is an artificial construct. There is no probability distribution with such a parameter. I find it difficult to define maximum likelihood estimates without a probability model. It is not clear how this ‘distribution which is not a distribution’ could be incorporated into a GLMM. This, of course, does not stop people from doing it but I don’t know what the estimates from such a model would mean.↩︎"
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html",
    "href": "posts/2019-06-21-empirical-bayes/index.html",
    "title": "Empirical Bayes",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#introduction",
    "href": "posts/2019-06-21-empirical-bayes/index.html#introduction",
    "title": "Empirical Bayes",
    "section": "Introduction",
    "text": "Introduction\nA couple of folks I work with in different capacities independently came across an article by Data Camp’s David Robinson1 demonstrating empirical bayes. It provides a nice and simple example of how to create a prior from the observed data, allowing it to induce shrinkage in estimates, in that case, career batting averages of Major League Baseball players. This would better allow one to compare someone that had only a relatively few at-bats to those that had longer careers.\nIt is a simple and straightforward demo, and admits that it doesn’t account for many other things that could be brought into the model, but that’s also why it’s effective at demonstrating the technique. However, shrinkage of parameter estimates can be accomplished in other ways, so I thought I’d compare it to two of my preferred ways to do so - a fully Bayesian approach and a random effects/mixed-model approach.\nI demonstrate shrinkage in mixed models in more detail here and here, and I’m not going to explain Bayesian analysis in general, but feel free to see my doc on it. This post is just to provide a quick comparison of techniques."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#data-setup",
    "href": "posts/2019-06-21-empirical-bayes/index.html#data-setup",
    "title": "Empirical Bayes",
    "section": "Data Setup",
    "text": "Data Setup\nWe’ll start as we typically do, with the data. The following just duplicates David’s code from the article. Nothing new here. If you want the details, please read it.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(Lahman)\n\ncareer &lt;- Batting %&gt;%\n  filter(AB &gt; 0) %&gt;%\n  anti_join(Pitching, by = \"playerID\") %&gt;%  # This removes Babe Ruth!\n  group_by(playerID) %&gt;%\n  summarize(H = sum(H), AB = sum(AB)) %&gt;%\n  mutate(average = H / AB)\n\n# use names along with the player IDs\ncareer &lt;- People %&gt;%\n  as_tibble() %&gt;%\n  select(playerID, nameFirst, nameLast) %&gt;%\n  unite(name, nameFirst, nameLast, sep = \" \") %&gt;%\n  inner_join(career, by = \"playerID\") \n\ncareer_filtered &lt;- career %&gt;%\n  filter(AB &gt;= 500)\n\nWith data in place, we can get the empirical bayes estimates. Again, this is just the original code. As a reminder, we assume a beta distribution for batting average, and the mean of the filtered data is 0.258. This finds the corresponding \\(\\alpha\\) and \\(\\beta\\) values for the beta distribution using MASS.\n\n\nThe beta distribution can be reparameterized as having a mean and variance: \\[\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\] \\[\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\]\n\nm &lt;- MASS::fitdistr(career_filtered$average, \n                    dbeta,\n                    start = list(shape1 = 1, shape2 = 10))\n\nalpha0 &lt;- m$estimate[1]\nbeta0 &lt;- m$estimate[2]\n\ncareer_eb &lt;- career %&gt;%\n  mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))\n\n\nWe use the estimated parameters as input for the beta prior. Let’s examine what we’ve got.\n\n\n\n\n\n\n\nJust to refresh, we can see how the EB estimates are able to guess something more meaningful for someone with just a few at-bats than say, a 0 batting average. Even for Ody Abbot there, we would guess something closer to the overall average than their .186 average after 70 plate appearances. With Frank Abercrombie, who had no hits in a measly 4 at bats, with so little information, we’d give him the benefit of the doubt of being average.\n\n\nFrom Wikipedia:\n\nFrancis Patterson Abercrombie (January 2, 1851 - November 11, 1939) was an American professional baseball player who played in the National Association for one game as a shortstop in 1871. Born in Fort Towson, Oklahoma, then part of Indian Territory, he played for the Troy Haymakers. He died at age 88 in Philadelphia, Pennsylvania.\n\nPretty sure that does not qualify for Wikipedia’s notability standards, but oh well."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#models",
    "href": "posts/2019-06-21-empirical-bayes/index.html#models",
    "title": "Empirical Bayes",
    "section": "Models",
    "text": "Models\nAs mentioned, I will compare the empirical bayes results to those of a couple of other approaches. They are:\n\nBayesian mixed model on full data (using brms)\nStandard mixed model on full data (using lme4)\nBayesian mixed model on filtered data (at bats greater than 500)\nStandard mixed model on filtered data\n\nThe advantages to these are that using a fully Bayesian approach allows us to not approximate the Bayesian and just do it. In the other case, the standard mixed model provides shrinkage with a penalized regression approach which also approximates the Bayesian, but doesn’t require any double dipping of the data to get at a prior, or any additional steps aside from running the model.\nIn both cases, we can accomplish the desired result with just a standard R modeling approach. In particular, the model is a standard binomial model for counts. With base R glm, we would do something like the following:\n\nglm(cbind(H, AB-H) ~ ..., data = career_eb, family = binomial)\n\nThe model is actually for the count of successes out of the total, which R has always oddly done in glm as cbind(# successes, # failures) rather than the more intuitive route (my opinion). The brms package will make it more obvious, but glmer uses the glm approach. The key difference for both models relative to the standard binomial is that we add a per-observation random effect for playerID2."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#bayesian-model",
    "href": "posts/2019-06-21-empirical-bayes/index.html#bayesian-model",
    "title": "Empirical Bayes",
    "section": "Bayesian Model",
    "text": "Bayesian Model\nWe’ll start with the full Bayesian approach using brms. This model will struggle a bit3, and takes a while to run, as it’s estimating 9983 parameters. But in the end we get what we want.\n\n\nI later learned the Bayesian model’s depicted here are essentially the same as in the example for one of the Stan vignettes.\n\n# in case anyone wants to use rstanarm I show it here\n# library(rstanarm)\n# bayes_full = stan_glmer(cbind(H, AB-H) ~ 1 + (1|playerID),\n#                         data = career_eb,\n#                         family = binomial)\n\nlibrary(brms)\nbayes_full = brm(H|trials(AB) ~ 1 + (1|playerID), \n                 data = career_eb,\n                 family = binomial,\n                 seed = 1234,\n                 iter = 1000,\n                 thin = 4,\n                 cores = 4)\n\nWith the posterior predictive check we can see right off the bat4 that this approach estimates the data well. Our posterior predictive distribution for the number of hits is hardly distinguishable from the observed data.\n\n\n\n\n\n\n\n\n\nAgain, the binomial model is for counts (out of some total), in this case, the number of hits. But if we wanted proportions, which in this case are the batting averages, we could just divide this result by the AB (at bats) column. Here we can see a little more nuance, especially that the model shies away from the lower values more, but this would still be a good fit by any standards."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#mixed-model",
    "href": "posts/2019-06-21-empirical-bayes/index.html#mixed-model",
    "title": "Empirical Bayes",
    "section": "Mixed Model",
    "text": "Mixed Model\nThe lme4 model takes the glm approach as far as syntax goes cbind(successes, non-successes). Very straightforward, and fast, as it doesn’t actually estimate the random effects, but instead predicts them. The predicted random effects are in fact akin to empirical bayes estimates5.\n\nglmer_full = lme4::glmer(cbind(H, AB-H) ~ 1 + (1|playerID), \n                         data = career_eb,\n                         family = binomial)"
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#filtered-data-models",
    "href": "posts/2019-06-21-empirical-bayes/index.html#filtered-data-models",
    "title": "Empirical Bayes",
    "section": "Filtered Data Models",
    "text": "Filtered Data Models\nSince David’s original ‘prior’ was based only on observations for those who had at least 500+ at bats (essentially a full season), the following re-runs the previous models just for the filtered data set, to see how those comparisons turn out.\n\nbayes_filtered = brm(H|trials(AB) ~ 1 + (1|playerID), \n                     data = career_eb %&gt;% filter(AB &gt;= 500),\n                     family = binomial,\n                     iter = 1000,\n                     seed = 1234,\n                     thin = 4,\n                     cores = 4)\n\nglmer_filtered = lme4::glmer(cbind(H, AB-H) ~ 1 + (1|playerID), \n                             data = career_eb %&gt;% filter(AB &gt;= 500),\n                             family = binomial)"
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#prediction-comparisons",
    "href": "posts/2019-06-21-empirical-bayes/index.html#prediction-comparisons",
    "title": "Empirical Bayes",
    "section": "Prediction Comparisons",
    "text": "Prediction Comparisons\nNow we’re ready to make some comparisons. We’ll combine the fits from the models to the original data set.\n\ncareer_other = career_eb %&gt;% \n  mutate(\n    bayes_estimate = fitted(bayes_full)[,1] / AB,\n    glmer_estimate = fitted(glmer_full),\n  )\n\ncareer_other_filtered = career_filtered %&gt;% \n  mutate(\n    bayes_filtered_estimate = fitted(bayes_filtered)[,1] / AB,\n    glmer_filtered_estimate = fitted(glmer_filtered),\n  ) %&gt;% \n  select(playerID, contains('filter'))\n\ncareer_all = left_join(career_other, \n                       career_other_filtered)\n\n\n\n\n\n\n\nWe can see that the fully Bayesian and standard mixed models are essentially giving us the same values. We start to see slight differences with the EB estimates, especially for those with fewer at-bats. When there is less data, the EB estimates appear to pull more sharply toward the prior.\n\nTop and bottom predictions\nIf we just look at the top 10, we would not come to any different conclusions (only full data models shown).\n\ntop_10_eb = career_all %&gt;% \n  top_n(10, eb_estimate) %&gt;% \n  select(playerID, eb_estimate)\n\ntop_10_bayes = career_all %&gt;% \n  top_n(10, bayes_estimate) %&gt;% \n  select(playerID, bayes_estimate)\n\ntop_10_mixed = career_all %&gt;% \n  top_n(10, glmer_estimate) %&gt;% \n  select(playerID, glmer_estimate)\n\n\n\n\n\n\nplayerID\neb_estimate\nbayes_estimate\nglmer_estimate\n\n\n\n\ndelahed01\n0.342\n0.342\n0.342\n\n\ngehrilo01\n0.337\n0.337\n0.337\n\n\ngwynnto01\n0.336\n0.335\n0.336\n\n\nhamilbi01\n0.340\n0.340\n0.341\n\n\nheilmha01\n0.338\n0.338\n0.339\n\n\nhornsro01\n0.355\n0.355\n0.355\n\n\njacksjo01\n0.350\n0.351\n0.350\n\n\nkeelewi01\n0.338\n0.339\n0.338\n\n\nlajoina01\n0.336\n0.336\n0.336\n\n\nterrybi01\n0.337\n0.338\n0.337\n\n\n\n\n\n\n\nSame for the bottom 10, although we see a little more wavering on the fitted values, as some of these are the ones who have relatively fewer at bats, and would see more shrinkage as a result.\n\nbottom_10_eb = career_all %&gt;% \n  top_n(-10, eb_estimate) %&gt;% \n  select(playerID, eb_estimate)\n\nbottom_10_bayes = career_all %&gt;% \n  top_n(-10, bayes_estimate) %&gt;% \n  select(playerID, bayes_estimate)\n\nbottom_10_mixed = career_all %&gt;% \n  top_n(-10, glmer_estimate) %&gt;% \n  select(playerID, glmer_estimate)\n\n\n\n\n\n\nplayerID\neb_estimate\nbayes_estimate\nglmer_estimate\n\n\n\n\narmbrch01\n0.199\n0.197\n0.197\n\n\nbakerge01\n0.196\n0.193\n0.194\n\n\nbergebi01\n0.178\n0.179\n0.178\n\n\neastehe01\n0.196\n0.196\n0.195\n\n\ngladmbu01\n0.196\n0.194\n0.194\n\n\nhedgeau01\n0.199\n0.198\n0.198\n\n\nhumphjo01\n0.195\n0.193\n0.193\n\n\noylerra01\n0.191\n0.190\n0.190\n\n\ntraffbi01\n0.201\n0.199\n0.199\n\n\nvukovjo01\n0.195\n0.193\n0.193\n\n\n\n\n\n\n\n\n\nExtreme predictions\nNow let’s look at some more extreme predictions. Those who averaged 0 or 1 for their lifetime batting average. Note that none of these will have very many plate appearances, and will show the greatest shrinkage. As a reminder, the filtered models did not include any of these individuals, and so are not shown."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#visualizing-the-results",
    "href": "posts/2019-06-21-empirical-bayes/index.html#visualizing-the-results",
    "title": "Empirical Bayes",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nThe following reproduces David’s plots. I start with his original image, altered only to be consistent with my visualization choices that use different color choices, add transparency, and allow size to reflect the number of at bats. Here is his explanation:\n\nThe horizontal dashed red line marks \\(y=\\alpha_0/\\alpha_0+\\beta_0=0.259\\) - that’s what we would guess someone’s batting average was if we had no evidence at all. Notice that points above that line tend to move down towards it, while points below it move up. The diagonal red line marks \\(x=y\\). Points that lie close to it are the ones that didn’t get shrunk at all by empirical Bayes. Notice that they’re the ones with the highest number of at-bats (the brightest blue): they have enough evidence that we’re willing to believe the naive batting average estimate.\n\n\n\n\n\n\n\n\n\n\nAgain, this is the same plot, but the size (along with color), which represents the number of at-bats, shows more clearly how observations don’t exhibit as much shrinkage when there is enough information.\nHere is the same plot against the full bayes estimates. The original lines are kept, but I add lines representing the average of the whole data, and the intercept from the Bayesian analysis (which is essentially the same as with the mixed model). In this case, estimates are pulled toward the estimated model mean.\n\n\n\n\n\n\n\n\n\n\nWe can also look at the density plots for more perspective. The full data models for the Bayesian and mixed models are basically coming to the same conclusions. The filtered data estimates center on the filtered data mean batting average as expected, but the mixed and Bayesian models show more variability in the estimates, as they are not based on the full data. As the EB prior is based on the filtered data, the distribution of values is similar to the others, but shifted to the filtered data mean. Thus it is coming to slightly different conclusions about the expected batting averages in general6."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#summary",
    "href": "posts/2019-06-21-empirical-bayes/index.html#summary",
    "title": "Empirical Bayes",
    "section": "Summary",
    "text": "Summary\nHere we have enhanced the original empirical bayes story with the addition of full bayes estimates and those from a standard mixed model. In terms of approximating Bayesian results, the empirical bayes are similar, but shifted due to the choice of prior. On the practical side, the mixed model would be easier to run and appears to more closely approximate the full bayes approach. In your own situation, any of these might be viable."
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#addendum",
    "href": "posts/2019-06-21-empirical-bayes/index.html#addendum",
    "title": "Empirical Bayes",
    "section": "Addendum",
    "text": "Addendum\n\nBeta-binomial\nDavid mentioned the beta-binomial distribution in his post. In this case, the prior for the probability of the binomial is assumed to be beta distributed. The brms vignette shows an example of how to use this distribution, and the following reproduces it for our data. You will likely need more iterations and possibly other fiddling to obtain convergence.\n\nbeta_binomial2 &lt;- custom_family(\n  \"beta_binomial2\", \n  dpars = c(\"mu\", \"phi\"),\n  links = c(\"logit\", \"log\"), \n  lb = c(NA, 0),\n  type = \"int\", \n  vars = \"trials[n]\"\n)\n\n\nstan_funs &lt;- \"\n  real beta_binomial2_lpmf(int y, real mu, real phi, int T) {\n    return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi);\n  }\n  int beta_binomial2_rng(real mu, real phi, int T) {\n    return beta_binomial_rng(T, mu * phi, (1 - mu) * phi);\n  }\n\"\n\n\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\") +\n  stanvar(as.integer(career_eb$AB), name = \"trials\")\n\nfit2 &lt;- brm(\n  H ~ (1|playerID), \n  data = career_eb, \n  family = beta_binomial2, \n  stanvars = stanvars,\n  seed = 1234,\n  iter = 1000,\n  cores = 4\n)\n\nsummary(fit2)\n\nexpose_functions(fit2, vectorize = TRUE)\n\npredict_beta_binomial2 &lt;- function(i, draws, ...) {\n  mu &lt;- draws$dpars$mu[, i]\n  phi &lt;- draws$dpars$phi\n  N &lt;- draws$data$trials[i]\n  beta_binomial2_rng(mu, phi, N)\n}\n\nfitted_beta_binomial2 &lt;- function(draws) {\n  mu &lt;- draws$dpars$mu\n  trials &lt;- draws$data$trials\n  trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE)\n  mu * trials\n}\n\npp_check(fit2)\nfitted(fit2, summary = T)[,1]\nfitted(fit2, summary = T)[,1]/career_eb$AB\nprior_summary(fit2)\n\n\n\n## Using stan for just an intercept only model to get the parameter estimates\n# takes about a minute per chain\n\nmodel_code = \"\ndata {\n  int N;\n  int H [N];\n  int AB [N];\n\n}\n\nparameters {\n  real alpha;  // setting lower = 0 provides serious estimation problems, without, just warnings\n  real beta;\n}\n\nmodel {\n  H ~ beta_binomial(AB, alpha, beta);\n}\n\ngenerated quantities {\n  vector[N] avg;\n  int pred [N];\n\n  pred = beta_binomial_rng(AB, alpha, beta);\n\n  for (n in 1:N) avg[n] = 1.0*pred[n] / AB[n];\n}\n\"\n\nlibrary(rstan)\ndata_list = list(H = career_eb$H, AB = career_eb$AB, N = nrow(career_eb))\n\nbayes_beta_binomial = stan(model_code = model_code,\n                           data  = data_list,\n                           seed  = 1234,\n                           iter  = 1000,\n                           cores = 4)\n\nprint(bayes_beta_binomial, digits = 3)\n# dr_bin = c(78.661, 224.875)           # beta estimates from DR post\n# dr_betabin = c(75, 222)               # beta binomial estimates from DR pots\n# stan_betabin = c(74.784, 223.451)     # stan estimates from above ~ .251 avg"
  },
  {
    "objectID": "posts/2019-06-21-empirical-bayes/index.html#footnotes",
    "href": "posts/2019-06-21-empirical-bayes/index.html#footnotes",
    "title": "Empirical Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDavid is actually responsible for me starting to do blog posts this year as a supplement to my more involved documents and workshops. At his talk at the RStudio conference, he laid out the benefits of blogging and in general doing anything to share one’s work with the greater community, and it made sense to me to use this as a less formal outlet.↩︎\nThis is possible for mixed models for counts like binomial and poisson (and other distributions) where we don’t estimate the residual variance, as it is determined by the nature of the distribution’s mean-variance relationship. In this case, it allows us to deal with overdispersion in this model via the random effect variance. See the GLMM FAQ.↩︎\nOne chain always struggled with the brms defaults, but diagnostics were okay.↩︎\nNot sorry!↩︎\nSee Bates’ comment.↩︎\nI actually redid the empirical bayes based on the full data. One issue was that fitting the beta required nudging the 0s and 1s, because the beta distribution doesn’t include 0 and 1. In the end, the resulting estimates mostly followed the original data. It had a very large and long tail for values less than .200, and on the the other end, estimated some batting averages over .500.↩︎"
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html",
    "href": "posts/2019-03-12-mediation-models/index.html",
    "title": "Mediation Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\nUpdated January 02, 2025. Code can be downloaded here."
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#introduction",
    "href": "posts/2019-03-12-mediation-models/index.html#introduction",
    "title": "Mediation Models",
    "section": "Introduction",
    "text": "Introduction\nIn some situations we may consider the indirect effect of some variable on an outcome or result. As an example, poor living conditions at home in childhood may decrease learning outcomes in school, which subsequently have a negative effect on later quality of life, for example, lifetime income earnings. In another case we might consider a single variable collected at multiple time points, such that there exists an effect of the variable at time 1 on time 2, and time 2 on time 3. The basic idea is something like:\n\\[\\mathcal{A} \\rightarrow \\mathcal{B} \\rightarrow \\mathcal{C}\\]\nIn other words, \\(\\mathcal{A}\\) leads to \\(\\mathcal{B}\\), and then \\(\\mathcal{B}\\) leads to \\(\\mathcal{C}\\). With mediation models, we posit an intervening variable between the normal covariate \\(\\rightarrow\\) outcome path that we might have in the standard regression setting, and these models allow us to investigate such behaviors. In the above, the intervening variable, or mediator, is \\(\\mathcal{B}\\). It is often the case that we still might have a direct effect of \\(\\mathcal{A}\\) on \\(\\mathcal{C}\\), but as with the model in general, this would be theoretically motivated.\nMediation analysis is very popular in social science disciplines, though by no means restricted to those, and usually conducted under the guise of structural equation modeling (SEM), which itself is a specific orientation of graphical models more generally1. The graphical model of a mediation model might look like the following.\n\n\nConfounding and mediation are not distinguishable statistically in the standard linear model setting, only conceptually. One way to think about it is that confounding doesn’t require a causal relationship, and/or could be a common cause between the variable of interest and the outcome. See MacKinnon et al.\n\nIn this case, a and b reflect the indirect path of the effect of \\(\\mathrm{X}\\) on the outcome through the mediator, while c' is the direct effect of \\(\\mathrm{X}\\) on the outcome after the indirect path has been removed (c would be the effect before positing the indirect effect, and c - c' equals the indirect effect). The total effect of \\(\\mathrm{X}\\) is the combined indirect and direct effects.\nI should note a few things based on what I see in consulting across dozens of disciplines. To begin, it seems very few people who think they need a mediation model actually do. For example, if you cannot think of your model in temporal or physical terms, such that \\(\\mathrm{X}\\) necessarily leads to the mediator, which then necessarily leads to the outcome, you likely do not need a mediation model. If you could see the arrows going either direction, again, you probably don’t need such a model. Also, if when describing your model, everyone thinks you’re talking about an interaction (a.k.a. moderation), you might not need this. And finally, as one might suspect, if there is no strong correlation between key variables (\\(\\mathrm{X}\\)) and mediator (path a), and if there is no strong correlation between mediator and the outcome(s) (path b), you probably don’t need this. While nothing will stop you from doing mediation analysis, without such prerequisites, you will almost certainly have a weak and probably more confusing model than you otherwise would have.\nIn short, mediation works best when there are strongly implied causal connections among the variables. Even then, such a model should be compared to simpler model of no mediation2. In any case, there are a few very easy ways to investigate such models in R, and that is the goal here, just to demonstrate how you can get started."
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#data",
    "href": "posts/2019-03-12-mediation-models/index.html#data",
    "title": "Mediation Models",
    "section": "Data",
    "text": "Data\nFor demonstration of mediation models with the different packages, we will use the jobs data set that comes with the mediation package. Here is the description.\nJob Search Intervention Study (JOBS II). JOBS II is a randomized field experiment that investigates the efficacy of a job training intervention on unemployed workers. The program is designed to not only increase reemployment among the unemployed but also enhance the mental health of the job seekers. In the JOBS II field experiment, 1,801 unemployed workers received a pre-screening questionnaire and were then randomly assigned to treatment and control groups. Those in the treatment group participated in job-skills workshops. In the workshops, respondents learned job-search skills and coping strategies for dealing with setbacks in the job-search process. Those in the control condition received a booklet describing job-search tips. In follow-up interviews, the two key outcome variables were a continuous measure of depressive symptoms based on the Hopkins Symptom Checklist, and a binary variable, representing whether the respondent had become employed.\nHere is a description of the variables in this demonstration. There are others available you might also want to play around with.\n\necon_hard: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: Indicator variable for sex. 1 = female\nage: Age in years.\neduc: Factor with five categories for educational attainment.\njob_seek: A continuous scale measuring the level of job-search self-efficacy with values from 1 to 5. The mediator variable.\ndepress2: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: Indicator variable for whether participant was randomly selected for the JOBS II training program. 1 = assignment to participation.\n\n\ndata(jobs, package = 'mediation')"
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#model",
    "href": "posts/2019-03-12-mediation-models/index.html#model",
    "title": "Mediation Models",
    "section": "Model",
    "text": "Model\nGiven this data the models for the mediator and outcome are as follows:\n\\[\n\\begin{aligned}\n\\mathrm{\\color{#00b294}{job\\_seek}} &\\sim \\mathrm{\\color{#b2001d}{treatment} + econ\\_hard + sex + age} \\\\\n\\mathrm{depression} &\\sim \\mathrm{\\color{#b2001d}{treatment} + econ\\_hard + sex + age + \\color{#00b294}{job\\_seek}}\n\\end{aligned}\n\\]\nThus we expect the job skills training to have a negative effect on depression (i.e. an increase in well-being), but at least part of this would be due to a positive effect on job search.\nAs a graphical model, we might depict it succinctly as follows."
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#packages",
    "href": "posts/2019-03-12-mediation-models/index.html#packages",
    "title": "Mediation Models",
    "section": "Packages",
    "text": "Packages\nWe will look at the following packages to demonstrate how one can conduct mediation analysis in R:\n\nmediation\nlavaan\npsych\nbrms\n\nWhile these will be the focus, I’ll also note some other alternatives, including Python and Stata.\n\nmediation\nWe will start with the mediation package, as it basically requires no more programming ability to conduct than one possesses already from running standard regression models in R. The package provides the average causal mediation effect, defined as follows from the help file and Imai’s articles3:\n\nThe average causal mediation effect (ACME) represents the expected difference in the potential outcome when the mediator took the value that would realize under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nNote how this definition is focused on expected or predicted values conditional on the treatment value. This notion of counterfactuals, or what would the observation look like under the opposite setting, has a long history in modeling at this point. Think of it this way, if one is in the treatment group, they would have a specific value for the mediator, and, given that, they would then have a specific expected value for the outcome. However, we could posit the same observation as being in the control group as well, and assess the effect on the outcome through the mediator just the same. We can assess the potential outcomes while holding the treatment constant. Thinking of outcome changes given the value of the mediator makes no assumption about the model type. This is how the mediation package is able to incorporate different models for the mediator vs. the outcome. For example, the mediator could be binary, requiring a logistic regression model, while the outcome model might be a survival model.\n\n\nAs this document is a tools-based demo and not for depth, see the works of Judea Pearl for more details.\nIn our example, we will stick with standard (normal) linear models. Note also, that while our treatment is a binary variable, this generalizes to the continuous case, where we consider the result of a one unit movement on the ‘treatment’. For the mediation package to work, we simply run our respective models for the mediator and outcome, then use the mediate function to get the final result.\n\nlibrary(mediation)\n\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome  &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximation\n# ?mediate\nmediation_result &lt;- mediate(\n  model.m = model_mediator, \n  model.y = model_outcome, \n  sims = 500,\n  treat = \"treat\",\n  mediator = \"job_seek\"\n)\n\ndetach(package:mediation)\ndetach(package:MASS)\n\n\n\nThe result is based on simulations of a multivariate normal draw of the coefficients given their estimated covariance matrix. The algorithm is summarized as follows (from Imai et al. 2010)\n\nFit models for the observed outcome and mediator variables.\nSimulate model parameters from their sampling distribution.\nRepeat the following three steps:\n\nsimulate the potential values of the mediator,\nsimulate the potential outcomes given the simulated values of the mediator,\ncompute the causal mediation effects.\n\nCompute summary statistics such as point estimates and confidence intervals.\n\nWith this approach we can obtain the average difference and corresponding quantiles based on the simulated draws.\n\nsummary(mediation_result)\nplot(mediation_result)\n\n\n\n\n\n\n\nEstimate\n95% CI Lower\n95% CI Upper\np-value\n\n\n\n\nACME\n-0.016\n-0.040\n0.009\n0.168\n\n\nADE\n-0.041\n-0.124\n0.043\n0.384\n\n\nTotal Effect\n-0.057\n-0.146\n0.034\n0.204\n\n\nProp. Mediated\n0.242\n-1.213\n2.737\n0.292\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results above demonstrate that the ACME is not statistically distinct from zero, or no mediation. The average direct effect is negative but likewise not statistically notable, neither is the total effect (indirect + direct effect). Also provided is the soi disant ‘proportion mediated’, which is the ratio of the indirect effect to the total. However this is not a proportion, and can even be negative, and so is mostly a meaningless number.\n\nPros\n\nStandard R models and syntax\nMultiple types of models for both mediator and outcome\nProvides multiple results simultaneously\nGood documentation and associated articles are freely available\nCan do ‘moderated’ mediation\n\n\n\nLimitations\n\nUse of MASS4\nSimple random effects models\nFunctionality maybe limited with some model complexities\nNo latent variable capabilities\n\n\n\n\nlavaan\nIn the specific case where both mediation and outcome models are standard linear models with a normal distribution for the target variable, the indirect effect is equivalent to the product of the a and b paths in the previous diagram. The direct effect is the c' path. A comparison of standalone direct effect, which we might call c, vs this estimated direct effect in the mediation model c', is such that c - c' = a*b. What was mentioned earlier might now be more clear, if either a or b are nearly zero, then the indirect effect can only be nearly zero, so it is prudent to investigate such relationships beforehand.\n\n\n\nThis product-of-paths (or difference in coefficients) approach is the one we will take with the lavaan package, and in fact, as of this writing, that is our only way of going about it. lavaan is specifically geared toward structural equation modeling, such as factor analysis, growth models, and mediation models like we’re conducting here, and is highly recommended for such models. While it is limited to the standard linear model case to assess mediation, it is the only one of our tools that can incorporate latent variables readily5. For example, we could have our depression outcome as a latent variable underlying the individual questionnaire items. In addition, we could also incorporate multiple mediators and multiple outcomes.\n\n\nlavaan can still estimate the model with binary or ordinal variables, there just is no way to produce the proper indirect effect, at least not without a lot more effort.\nTo keep things as we have been discussing, I will label the a, b and c' paths in lavaan according to how they have been depicted previously. Otherwise lavaan is very easy to use, and in the case of observed variables, uses standard R formula notation for the models. Beyond that we define the effects of interest that we want to calculate with the := operator. We specify the model in its entirety as a simple character string, then use the sem function to do the analysis.\n\nlibrary(lavaan)\n\nsem_model = '\n  job_seek ~ a*treat + econ_hard + sex + age\n  depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\n \n  # direct effect\n  direct := c\n \n  # indirect effect\n  indirect := a*b\n \n  # total effect\n  total := c + (a*b)\n'\n\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)  # compare with ACME in mediation\n\nlavaan 0.6.17 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           899\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws              500\n  Number of successful bootstrap draws             500\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  job_seek ~                                          \n    treat      (a)    0.066    0.050    1.317    0.188\n    econ_hard         0.053    0.023    2.282    0.023\n    sex              -0.008    0.048   -0.158    0.874\n    age               0.005    0.002    1.878    0.060\n  depress2 ~                                          \n    treat      (c)   -0.040    0.045   -0.896    0.370\n    econ_hard         0.149    0.021    7.239    0.000\n    sex               0.107    0.040    2.641    0.008\n    age               0.001    0.002    0.321    0.748\n    job_seek   (b)   -0.240    0.029   -8.392    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .job_seek          0.524    0.030   17.550    0.000\n   .depress2          0.373    0.021   17.654    0.000\n\nR-Square:\n                   Estimate\n    job_seek          0.011\n    depress2          0.120\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    direct           -0.040    0.045   -0.895    0.371\n    indirect         -0.016    0.012   -1.263    0.206\n    total            -0.056    0.046   -1.218    0.223\n\n\nWe see the same output before and can compare our indirect parameter to the ACME we had before, the direct effect is compared to the ADE, and the total compares to the previous total effect. The values are essentially the same.\nNote also that the output shows the \\(R^2\\) value for both models. In the case of job_seek, we can see that the reason we’re not finding much in the way of mediation is because the covariates involved do not explain any variation in the mediator to begin with. Preliminary investigation would have saved us the trouble in this case.\n\nPros\n\nCan handle multiple mediators\nCan handle multiple ‘treatments’\nCan handle multiple outcomes\nCan use latent variables\nSome multilevel support\nCan do moderated mediation and mediated moderation (though not for latent variables)\n\n\n\nLimitations\n\nRequires additional coding to estimate the indirect effect\nSingle random effects\nWhile the models could incorporate binary or ordinal variables for the mediator/outcomes, there is no straightforward way to calculate the indirect effect in the manner of the mediation package in those settings.\n\n\n\n\npiecewiseSEM\nThe piecewiseSEM package works very similar to the mediation package. The nice thing about this relative to the mediation package is that piecewiseSEM can handle additional types of models, as well as provide additional output (e.g. standardized results), additional options (e.g. multigroup, correlated residuals), and visualization of the model.\n\nlibrary(piecewiseSEM)\n\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome  &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\nmediation_result &lt;-  psem(model_mediator, model_outcome, data = jobs)\n\nsummary(mediation_result)\n\n\nStructural Equation Model of mediation_result \n\nCall:\n  job_seek ~ treat + econ_hard + sex + age\n  depress2 ~ treat + econ_hard + sex + age + job_seek\n\n    AIC\n 3661.375\n\n---\nTests of directed separation:\n\n No independence claims present. Tests of directed separation not possible.\n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 0 with P-value = 1 and on 0 degrees of freedom\nFisher's C = NA with P-value = NA and on 0 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error  DF Crit.Value P.Value Std.Estimate    \n  job_seek     treat   0.0656    0.0515 894     1.2748  0.2027       0.0425    \n  job_seek econ_hard   0.0532    0.0246 894     2.1612  0.0309       0.0720   *\n  job_seek       sex  -0.0076    0.0487 894    -0.1567  0.8755      -0.0052    \n  job_seek       age   0.0046    0.0023 894     1.9779  0.0482       0.0658   *\n  depress2     treat  -0.0403    0.0435 893    -0.9255  0.3550      -0.0291    \n  depress2 econ_hard   0.1485    0.0208 893     7.1323  0.0000       0.2248 ***\n  depress2       sex   0.1068    0.0411 893     2.5957  0.0096       0.0818  **\n  depress2       age   0.0006    0.0020 893     0.3306  0.7410       0.0104    \n  depress2  job_seek  -0.2400    0.0282 893    -8.4960  0.0000      -0.2682 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n  job_seek   none      0.01\n  depress2   none      0.12\n\n\nWe can use it’s plotting capabilities to create a quick visualization of the model.\n\n\nThe plot is using Diagrammer and graphviz under the hood, which is appealing to me as I use those anyway, so could use the generated code as a starting point.\n\nplot(mediation_result)\n\n\nUnfortunately, there is no automatic way to calculate the indirect effects at present, so one would have to bootstrap the results by hand.\n\n\nThere used to be a package to calculate indirect effects of a psem object, semEff, but it currently doesn’t work.\n\nPros\n\nStandard R models and syntax\nMultiple types of models for both mediator and outcome\nSome SEM-style results (e.g. fit, standardized coefficients, AIC)\nQuick plot of results\nCan handle multiple mediators, ‘treatments’, and outcomes\n\n\n\nLimitations\n\nDoesn’t automatically calculate indirect effects\nNo latent variable capabilities\n\n\n\n\npsych\nThe psych package takes advantage of the fact that in the standard linear model case, one can obtain the results via the appropriate regression models based on the covariance matrices alone. It’s very similar to lavaan, although using an ordinary least squares approach as opposed to maximum likelihood. The nice thing here is a syntax that allows you to focus only on the effect of interest, or include everything, which is nice if you were interested in the indirect effects for economic hardship, age, and sex as well.\nFor this demo we’ll use the cleaned up version using the -, instead of +, for the non-treatment effects. This just means they are included with the models, but results are not shown concerning them. The mediator is identified with (). Another bonus is a quick plot of the results, showing the difference between the unadjusted and adjusted direct effects, and the appropriate bootstrapped interval.\n\nlibrary(psych)\n\nmediation_psych = mediate(\n  depress2 ~ treat + (job_seek) - econ_hard - sex - age, \n  data = jobs,\n  n.iter = 500\n)\n\n\n\n\n\n\n\nmediation_psych\n\n\nMediation/Moderation Analysis \nCall: mediate(y = depress2 ~ treat + (job_seek) - econ_hard - sex - \n    age, data = jobs, n.iter = 500)\n\nThe DV (Y) was  depress2* . The IV (X) was  Intercept* treat* . The mediating variable(s) =  job_seek* . Variable(s)  partialled out were econ_hard sex age\n\nTotal effect(c) of  Intercept*  on  depress2*  =  -0.06   S.E. =  0.05  t  =  -1.24  df=  894   with p =  0.22\nDirect effect (c') of  Intercept*  on  depress2*  removing  job_seek*  =  -0.04   S.E. =  0.04  t  =  -0.93  df=  893   with p =  0.35\nIndirect effect (ab) of  Intercept*  on  depress2*  through  job_seek*   =  -0.02 \nMean bootstrapped indirect effect =  -0.02  with standard error =  0.01  Lower CI =  -0.04    Upper CI =  0.01\n\n\nError in x$direct[i + 1, j]: subscript out of bounds\n\nsummary(mediation_psych)\n\nCall: mediate(y = depress2 ~ treat + (job_seek) - econ_hard - sex - \n    age, data = jobs, n.iter = 500)\n\nDirect effect estimates (traditional regression)    (c') X + M on Y \n          depress2*   se     t  df     Prob\nIntercept      2.21 0.15 14.91 893 4.60e-45\ntreat         -0.04 0.04 -0.93 893 3.55e-01\njob_seek      -0.24 0.03 -8.50 893 8.14e-17\n\nR = 1.07 R2 = 1.15   F = -3510.4 on 2 and 893 DF   p-value:  1 \n\n Total effect estimates (c) (X on Y) \n          depress2*   se     t  df     Prob\nIntercept      1.33 0.11 12.08 894 3.10e-31\ntreat         -0.06 0.05 -1.24 894 2.15e-01\n\n 'a'  effect estimates (X on M) \n          job_seek   se     t  df      Prob\nIntercept     3.67 0.13 29.33 894 5.65e-133\ntreat         0.07 0.05  1.27 894  2.03e-01\n\n 'b'  effect estimates (M on Y controlling for X) \n         depress2*   se    t  df     Prob\njob_seek     -0.24 0.03 -8.5 893 8.14e-17\n\n 'ab'  effect estimates (through all  mediators)\n      depress2*  boot   sd lower upper\ntreat     -0.02 -0.02 0.01 -0.04  0.01\n\n\nSame results, different packaging, but possibly the easiest route yet as it only required one function call. The psych package also handles multiple mediators and outcomes as a bonus.\n\nPros\n\nEasiest syntax, basically a one line model\nQuick plot of results\nCan handle multiple mediators, ‘treatments’, and outcomes\nCan do ‘moderated’ mediation\n\n\n\nLimitations\n\nLimited to standard linear model (lm)\nUse of MASS\n\n\n\n\nbrms\nFor our next demo we come to what I feel is the most powerful package, brms. The name stands for Bayesian Regression Modeling with Stan, and Stan is a powerful probabilistic programming language for Bayesian analysis. I won’t go into details about Bayesian analysis, but feel free to see my document that does.\nWe generally do as we have before, specifying the mediator model and the outcome model. brms doesn’t do anything special for mediation analysis, but its hypothesis function can allow us to test the product-of-paths approach. Furthermore, the sjstats package will essentially provide the results in the same way the mediation package does for us, and for that matter, the mediation package is basically an attempt at a Bayesian solution using frequentist methods anyway. If we did have different distributions for the outcome and mediator, we’d have an relatively easy time getting these average prediction values and their differences, as Bayesian approaches are always thinking about posterior predictive distributions. In any case, here is the code.\n\nlibrary(brms)\n\nmodel_mediator &lt;- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcome  &lt;- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\n\nmed_result = brm(\n  model_mediator + model_outcome + set_rescor(FALSE), \n  data = jobs\n)\nsave(med_result, file = 'data/mediation_brms.RData')\n\n\nload('data/mediation_brms.RData')\nsummary(med_result)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: job_seek ~ treat + econ_hard + sex + age \n         depress2 ~ treat + job_seek + econ_hard + sex + age \n   Data: jobs (Number of observations: 899) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\njobseek_Intercept      3.67      0.12     3.43     3.91 1.00     6703     3749\ndepress2_Intercept     2.21      0.15     1.92     2.50 1.00     6173     3091\njobseek_treat          0.07      0.05    -0.03     0.17 1.00     6321     2709\njobseek_econ_hard      0.05      0.02     0.00     0.10 1.00     6266     2656\njobseek_sex           -0.01      0.05    -0.10     0.09 1.00     5741     2655\njobseek_age            0.00      0.00     0.00     0.01 1.00     6538     2846\ndepress2_treat        -0.04      0.04    -0.12     0.04 1.00     5458     3102\ndepress2_job_seek     -0.24      0.03    -0.30    -0.18 1.00     5950     2938\ndepress2_econ_hard     0.15      0.02     0.11     0.19 1.00     7545     3102\ndepress2_sex           0.11      0.04     0.03     0.19 1.00     5600     2699\ndepress2_age           0.00      0.00    -0.00     0.00 1.00     4558     2887\n\nFurther Distributional Parameters:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_jobseek      0.73      0.02     0.69     0.76 1.00     6640     3276\nsigma_depress2     0.61      0.01     0.59     0.64 1.00     6145     2987\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# using brms we can calculate the indirect effect as follows\n# hypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\n# bayestestR provides similar printing as the mediation package\n# print(bayestestR::mediation(med_result), digits=4)\n\n\n\n\n\n\n\n\n\nEffect\nEstimate\nCI_low\nCI_high\n\n\n\n\nDirect Effect (ADE)\n−0.04\n−0.12\n0.04\n\n\nIndirect Effect (ACME)\n−0.02\n−0.04\n0.01\n\n\nMediator Effect\n−0.24\n−0.30\n−0.18\n\n\nTotal Effect\n−0.06\n−0.14\n0.03\n\n\nProportion Mediated\n0.28\n−1.98\n2.54\n\n\n\n\n\n\n\nIn the output, anything with jobseek_* is a result for the mediator model, while depress2_* is for the outcome. We have the same old story at this point, but with the Bayesian approach we have more fun things to look at. For example, we can see that we aren’t actually capturing the skewness of depression outcome well. Our predicted values vs. the observed don’t quite match up. We’re a little better for the mediator, but perhaps still a little high with some of our model-based predictions.\n\npp_check(med_result, resp = 'depress2') + ggtitle('Depression Outcome')\n\n\n\n\n\n\n\npp_check(med_result, resp = 'jobseek') + ggtitle('Mediator')\n\n\n\n\n\n\n\n\n\n\nThe faint lines (yrep) are posterior predictive draws.\n\nPros\n\nStraightforward syntax\nExtremely powerful- Models are mostly limited to one’s imagination\nBasically does what the mediation package approximates\nAll the perks of Bayesian inference: diagnostics, posterior predictive checks, model comparison, etc.\n\n\n\nLimitations\n\nSlower to estimate\n‘By-hand’ calculations needed for going beyond the standard linear model, but this is already a common approach from the Bayesian perspective\nSome comfort with the Bayesian approach required"
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#more-complexity",
    "href": "posts/2019-03-12-mediation-models/index.html#more-complexity",
    "title": "Mediation Models",
    "section": "More complexity",
    "text": "More complexity\nSome of the packages mentioned can handle more complex models or provide additional approaches to investigate indirect effects.\n\nInteractions\nSome models involve interactions either for the mediation model or outcome, and unfortunately this is often referred to as mediated moderation or moderated mediation. I personally don’t see the advantage to giving ambiguous names to what otherwise might be a straightforward concept (if still not-so-straightforward model), but that ship sailed long ago. I’m not going to go into the details, but the idea is that you might have an interaction term somewhere in the model, and the interaction might involve the treatment variable, the mediator, or both.\nSuffice it to say, since we’re using standard modeling tools like lm and extensions of it, incorporating interactions is trivial for all of the above packages, but the product-of-paths type of approach doesn’t hold (a*b != c').\n\n\nGeneralized Linear Models\nIn some cases our mediator or outcome may be binary, count, or something where assuming a normal distribution might not be the best idea. Or we might want to investigate nonlinear relationships among the treatment/mediator/outcome. Or we might have data that has correlated observations like repeated measurements or similar. The mediation package prides itself on this in particular, but brms can do anything it can do and more, though you might have to do a little more work to actually calculate the result. lavaan can actually do a limited set of models for binary and ordinal variables, but getting the appropriate indirect estimate would require a very tedious by-hand approach.\n\n\nMissing data\nOften when dealing with such data, especially in the social sciences, data is often missing on any of the covariates. Sometimes we can drop these if there isn’t too many, but in other cases we will want to do something about it. The packages lavaan, psych, and brms provide one or more ways to deal with the situation (e.g. multiple imputation)."
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#alternatives",
    "href": "posts/2019-03-12-mediation-models/index.html#alternatives",
    "title": "Mediation Models",
    "section": "Alternatives",
    "text": "Alternatives\nWe have been depicting the models as networks of nodes, with arcs/edges/paths connecting them. Our discussion revolves around what are called Directed Acyclic Graphs (DAG) where the arrows can only go one direction with no feedback loops. The result of any outcome variable is a function of the arrows preceding it, and conditionally independent of others. Some theoretical models may relax this, and others may have no arrows at all, i.e. are undirected, such that we are interested in just the connections (e.g. with some social networks).\n\nbnlearn\nThe bnlearn package allows investigation of directed, partially directed, and undirected graphs. In terms of DAGs, we can use it to essentially duplicate the mediation models we’ve been discussing. The nice thing though is that this package will efficiently test paths for inclusion rather than assume them, but we can still impose theoretical constraints as needed. Not only can we then search for the paths of interest in a principled way with bayesian networks and Pearl’s causal graph theory as a basis, we also will have tools to further avoid overfitting via cross-validation.\nFor the initial model, we’ll make sure that paths exist between treatment - mediator, treatment - outcome, and mediator - outcome (the whitelist). We will disallow nonsensical paths like having arrows to the treatment (which was randomly assigned), sex, economic hardship, and age (the blacklist). Otherwise, we’ll see what the data suggests.\n\nwhitelist = data.frame(\n  from = c('treat', 'treat', 'job_seek'),\n  to   = c('job_seek', 'depress2', 'depress2')\n)\n\nblacklist = expand.grid(\n  from = colnames(mediation_result$model.y$model),\n  to   = c('treat', 'sex', 'age', 'econ_hard')\n)\n\n# For simpler output we'll use treatment and sex as numeric (explained later)\nlibrary(dplyr)\n\njobs_trim = jobs %&gt;% \n  select(depress2, treat, econ_hard, sex, age, job_seek) %&gt;% \n  mutate(\n    treat = as.numeric(jobs$treat),\n    sex = as.numeric(jobs$sex)\n    )\n\n\n# extract path coefficients if desired\n# parameters = bn.fit(model, jobs_trim)\n# parameters$job_seek\n# parameters$econ_hard\n# parameters$depress2\n\n\nlibrary(bnlearn)\n\nmodel = gs(jobs_trim, whitelist = whitelist, blacklist = blacklist)\n\nplot(model)\n\n\n\n\n\n\n\n\nWe see in the plot that things have changed a bit. For example, age now only relates to job seeking self-efficacy, and sex only has an effect on depression.\nIf we restrict the paths to only be what they are in our previous examples, we’d get the same results.\n\nlibrary(bnlearn)\n\nwhitelist = data.frame(\n  from = c('treat', 'age', 'sex', 'econ_hard', 'treat', 'job_seek', 'age', 'sex', 'econ_hard'),\n  to   = c('job_seek', 'job_seek','job_seek','job_seek', 'depress2', 'depress2', 'depress2', 'depress2', 'depress2')\n)\n\nblacklist = expand.grid(\n  from = colnames(mediation_result$model.y$model),\n  to   = c('treat', 'sex', 'age', 'econ_hard')\n)\n\n# this no longer guarantees DAG. see note in ?gs; use cextend as below\nmodel = gs(\n  jobs_trim,\n  whitelist = whitelist,\n  blacklist = blacklist,\n  undirected = FALSE  \n)\nplot(model)\n\n\n\n\n\n\n\nparameters = bn.fit(cextend(model), jobs_trim)\n\nparameters$depress2$coefficients\n\n  (Intercept)         treat     econ_hard           sex           age \n 2.2076414333 -0.0402647000  0.1485433818  0.1068048699  0.0006488642 \n     job_seek \n-0.2399549527 \n\nparameters$job_seek$coefficients\n\n (Intercept)        treat    econ_hard          sex          age \n 3.670584908  0.065615003  0.053162413 -0.007637336  0.004586492 \n\n\nThe main thing to note is that the estimated parameters equal the same thing we got with previous packages. It’s essentially equivalent to using lavaan with the default maximum likelihood estimator.\nIf we use treatment and sex as factors, bnlearn will produce conditional models that are different depending on the factor value taken. In other words, one would have a separate model for when treatment == 'treatment' and one for when treatment == control. In our case, this would be identical to allowing everything to interact with treatment, e.g. lm( job_seek ~ treat * (econ_hard + sex + age)), and likewise for the depression model. This would extend to potentially any binary variable (e.g. including sex). If the mediator is a binary variable, this is likely what we’d want to do.\n\n\nPython\nCSCAR director Kerby Shedden has given a Python workshop on mediation models, so I show the statsmodels implementation here. It follows the Imai approach and so can be seen as the Python version of the mediation package. The output is essentially the same as what you would have using treatment as a factor variable, where you get separate results for each treatment category. This is unnecessary for our demo, so you can just compare the ‘average’ results to the previous mediation package results.\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation\nimport numpy as np\nimport pandas as pd\n\njobs = pd.read_csv('data/jobs.csv')\n\noutcome_model = sm.OLS.from_formula(\"depress2 ~ treat + econ_hard + sex + age + job_seek\",\n                                    data = jobs)\n\nmediator_model = sm.OLS.from_formula(\"job_seek ~ treat + econ_hard + sex + age\",\n                                     data = jobs)\n\nmed = Mediation(outcome_model, mediator_model, \"treat\", \"job_seek\")\n\nmed_result = med.fit(n_rep = 500)\n\nprint(np.round(med_result.summary(), decimals = 3))\n\n                          Estimate  Lower CI bound  Upper CI bound  P-value\nACME (control)              -0.016          -0.046           0.016    0.268\nACME (treated)              -0.016          -0.046           0.016    0.268\nADE (control)               -0.040          -0.122           0.048    0.368\nADE (treated)               -0.040          -0.122           0.048    0.368\nTotal effect                -0.056          -0.138           0.036    0.216\nProp. mediated (control)     0.237          -1.543           2.707    0.348\nProp. mediated (treated)     0.237          -1.543           2.707    0.348\nACME (average)              -0.016          -0.046           0.016    0.268\nADE (average)               -0.040          -0.122           0.048    0.368\nProp. mediated (average)     0.237          -1.543           2.707    0.348\n\n\n\n\nStata\nFinally, I provide an option in Stata using its sem command. Stata makes it easy to get the indirect effects in this example, but it does so for every covariate, so the output is a bit verbose to say the least6. For those working with Stata, they do not need a separate SEM package to get these sorts of results.\n\n\nuse \"data\\jobs.dta\"\n\nsem (job_seek &lt;- treat econ_hard sex age) (depress2 &lt;- treat econ_hard sex age job_seek), cformat(%9.3f) pformat(%5.2f)\n\nestat teffects, compact cformat(%9.3f) pformat(%5.2f)\n\n\n\n\n. use 'data\\jobs.dta'\n\n. \n. sem (job_seek &lt;- treat econ_hard sex age) (depress2 &lt;- treat econ_hard sex age job_seek), \ncformat(%9.3f) pformat(%5.2f)\n\nEndogenous variables\n\nObserved:  job_seek depress2\n\nExogenous variables\n\nObserved:  treat econ_hard sex age\n\nFitting target model:\n\nIteration 0:   log likelihood = -7711.0956  \nIteration 1:   log likelihood = -7711.0956  \n\nStructural equation model                       Number of obs     =        899\nEstimation method  = ml\nLog likelihood     = -7711.0956\n\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n       treat |      0.066      0.051     1.28    0.20       -0.035       0.166\n   econ_hard |      0.053      0.025     2.17    0.03        0.005       0.101\n         sex |     -0.008      0.049    -0.16    0.88       -0.103       0.088\n         age |      0.005      0.002     1.98    0.05        0.000       0.009\n       _cons |      3.671      0.125    29.41    0.00        3.426       3.915\n  -----------+----------------------------------------------------------------\n  depress2   |\n    job_seek |     -0.240      0.028    -8.52    0.00       -0.295      -0.185\n       treat |     -0.040      0.043    -0.93    0.35       -0.125       0.045\n   econ_hard |      0.149      0.021     7.16    0.00        0.108       0.189\n         sex |      0.107      0.041     2.60    0.01        0.026       0.187\n         age |      0.001      0.002     0.33    0.74       -0.003       0.004\n       _cons |      2.208      0.148    14.96    0.00        1.918       2.497\n-------------+----------------------------------------------------------------\nvar(e.job_~k)|      0.524      0.025                         0.478       0.575\nvar(e.depr~2)|      0.373      0.018                         0.340       0.409\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0)   =      0.00, Prob &gt; chi2 =      .\n\n. \n. estat teffects, compact cformat(%9.3f) pformat(%5.2f)\n\n\nDirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n       treat |      0.066      0.051     1.28    0.20       -0.035       0.166\n   econ_hard |      0.053      0.025     2.17    0.03        0.005       0.101\n         sex |     -0.008      0.049    -0.16    0.88       -0.103       0.088\n         age |      0.005      0.002     1.98    0.05        0.000       0.009\n  -----------+----------------------------------------------------------------\n  depress2   |\n    job_seek |     -0.240      0.028    -8.52    0.00       -0.295      -0.185\n       treat |     -0.040      0.043    -0.93    0.35       -0.125       0.045\n   econ_hard |      0.149      0.021     7.16    0.00        0.108       0.189\n         sex |      0.107      0.041     2.60    0.01        0.026       0.187\n         age |      0.001      0.002     0.33    0.74       -0.003       0.004\n------------------------------------------------------------------------------\n\n\nIndirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n  -----------+----------------------------------------------------------------\n  depress2   |\n       treat |     -0.016      0.012    -1.26    0.21       -0.040       0.009\n   econ_hard |     -0.013      0.006    -2.10    0.04       -0.025      -0.001\n         sex |      0.002      0.012     0.16    0.88       -0.021       0.025\n         age |     -0.001      0.001    -1.93    0.05       -0.002       0.000\n------------------------------------------------------------------------------\n\n\nTotal effects\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n       treat |      0.066      0.051     1.28    0.20       -0.035       0.166\n   econ_hard |      0.053      0.025     2.17    0.03        0.005       0.101\n         sex |     -0.008      0.049    -0.16    0.88       -0.103       0.088\n         age |      0.005      0.002     1.98    0.05        0.000       0.009\n  -----------+----------------------------------------------------------------\n  depress2   |\n    job_seek |     -0.240      0.028    -8.52    0.00       -0.295      -0.185\n       treat |     -0.056      0.045    -1.24    0.21       -0.144       0.032\n   econ_hard |      0.136      0.022     6.31    0.00        0.094       0.178\n         sex |      0.109      0.043     2.55    0.01        0.025       0.192\n         age |     -0.000      0.002    -0.22    0.82       -0.004       0.004\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#summary",
    "href": "posts/2019-03-12-mediation-models/index.html#summary",
    "title": "Mediation Models",
    "section": "Summary",
    "text": "Summary\nModels with indirect effects require careful theoretical consideration to employ for data analysis. However, if the model is appropriate for your data situation, it is quite easy to get results from a variety of packages in R. Furthermore, one does not need to use a structural equation modeling package to conduct an analysis with indirect effects, and in fact, one can get far using standard R syntax. For strictly observed, i.e. no latent, variables, no SEM tool is necessary, or even recommended.\n\\[\\mathcal{Enjoy\\ your\\ model\\ exploration!}\\]\n\nPackage comparison summarized\nThe following table may help one decide which package to use for their needs given their theoretical considerations.\n\n\n\n\n\n\nmediation\nlavaan\npiecewiseSEM\npsych\nbrms\n\n\n\n\nAutomatic\n•\n\n\n•\n•*\n\n\nMultiple Treatments☺\n•\n•\n•\n•\n•\n\n\nMultiple Mediators\n•\n•\n•\n•\n•\n\n\nMultiple Outcomes\n\n•\n•\n•\n•\n\n\nBeyond SLM†\n•\n•\n•\n\n•\n\n\nRandom Effects\n•\n•\n•\n\n•\n\n\nMissing Values\n\n•\n\n•*\n•\n\n\nLatent Variables\n\n•\n\n\n•*\n\n\n\n* approximately, with some caveats\n\n\n\n\n\n\n\n☺ May require rerunning aspects of the model\n\n\n\n\n\n\n\n† Standard linear model, as estimated by `lm`"
  },
  {
    "objectID": "posts/2019-03-12-mediation-models/index.html#footnotes",
    "href": "posts/2019-03-12-mediation-models/index.html#footnotes",
    "title": "Mediation Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have a much more detailed document on SEM, including mediation analysis.↩︎\nFor some reason you don’t see this in practice much, and one wonders what was done to make the data amenable to such a model if it wasn’t warranted.↩︎\nImai makes his articles available at his website.↩︎\nMASS has been superseded by others for over a decade at this point, and it mostly just tends to muck up your tidyverse and other packages when it’s loaded. It’s a fine package (and was great back in the day), but if you want to use it in a package, it would be good to not load it (or other packages) in the environment just to use a function or two. I mostly just see it used for mvrnorm (multivariate normal distribution) and glm.nb, but there are other packages with that functionality that would provide additional benefits, and not mask dplyr functions, which are among the most commonly used in the R community.↩︎\nbrms is working on it.↩︎\nThe options in the code are there to suppress/minimize what can be.↩︎"
  },
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "Content",
    "section": "",
    "text": "Here you’ll find documents of varying technical degree covering things of interest to me, or which I think will be interesting to those I engage with. Generally you’ll find a mix of demonstrations on statistical and machine learning topics, programming, and data processing and visualization. Most focus on application in R as that’s what I used to primarily program with, but you’ll find plenty of Python demonstrations as well. Be aware that some of the content is a bit dated, but even if the programming aspects are a bit off, the concepts should still be relevant."
  },
  {
    "objectID": "documents.html#my-book",
    "href": "documents.html#my-book",
    "title": "Content",
    "section": "My Book!",
    "text": "My Book!\n Models Demystified\nThis book should be out on CRC Press in 2025. It is a comprehensive overview of the statistical and machine learning landscape, along with related topics. It is designed to be accessible to those with a basic understanding of statistics, but also to provide a deeper dive into the concepts for those with more experience. It covers an array of useful models from simple linear regression to deep learning. The book is designed to be a reference for those who want to understand the models and techniques they are using, and to provide a guide for those who want to learn new techniques."
  },
  {
    "objectID": "documents.html#long-form-docs",
    "href": "documents.html#long-form-docs",
    "title": "Content",
    "section": "Long Form Docs",
    "text": "Long Form Docs\n Mixed Models with R\nThis document focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R. It covers topics such as loss functions, cross-validation, regularization, and bias-variance trade-off, techniques such as penalized regression, random forests, and neural nets, and more.  \n Practical Data Science\nFocus is on common data science tools and techniques in R, including data processing, programming, modeling, visualization, and presentation of results. Exercises may be found in the document, and demonstrations of most content in Python is available via Jupyter notebooks. \n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves, IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc."
  },
  {
    "objectID": "documents.html#blog-posts",
    "href": "documents.html#blog-posts",
    "title": "Content",
    "section": "Blog Posts",
    "text": "Blog Posts\n\nDeep Linear Models\nExploring Time\nProgramming Odds and Ends\nDeep Learning for Tabular Data II\nThe Double Descent Phenomenon\nDeep Learning for Tabular Data\nPractical Bayesian Analysis (I, II)\nMicro-macro Models\nPredictions with an Offset\nFactor Analysis and Related Methods\nConvergence Problems in Mixed Models\nCategorical Random Effects\nMixed Models for Big Data\nFractional Regression\nGroup Comparisons in SEM\n\nEmpirical Bayes\nShrinkage in Mixed Models\n\nMediation Models"
  },
  {
    "objectID": "documents.html#statistical",
    "href": "documents.html#statistical",
    "title": "Content",
    "section": "Statistical",
    "text": "Statistical\n\nModels By Example\n Model Estimation by Example\nThis shows ‘by-hand’ code for various models and estimation approaches, from linear regression to Bayesian multilevel mediation models, and demonstrations from penalized maximum likelihood to stochastic gradient descent.  \n\n\nModeling in R\n Data Modeling in R\nThis document demonstrates a wide array of statistical and other models in R. Generic code is provided for standard regression, mixed, additive, survival, and latent variable models, principal components, factor analysis, SEM, cluster analysis, time series, spatial models, zero-altered models, text analysis, Bayesian analysis, machine learning and more. \nThe document is designed for newcomers to R, whether in a statistical sense, or just a programming one. It also should appeal to those working in other packages who are curious how to do the same sorts of things in R.\n\n\nBayesian\n Bayesian Basics\nThis serves as a conceptual introduction to Bayesian modeling with examples using R and Stan.  \n MCMC algorithms\nList of MCMC algorithms with brief descriptions.  \n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n\n\nMixed Models\n Mixed Models with R\nThis workshop focuses on mixed effects models using R, covering basic random effects models (random intercepts and slopes) as well as extensions into generalized mixed models and discussion of realms beyond. \n Mixed Models Overview\nAn overview that introduces mixed models for those with varying technical/statistical backgrounds.  \n Mixed Models Introduction\nA non-technical document to introduce mixed models for those who have used ANOVA.  \n Clustered Data Situations\nA comparison of standard models, cluster robust standard errors, fixed effect models, mixed models (random effects models), generalized estimating equations (GEE), and latent growth curve models for dealing with clustered data (e.g. longitudinal, hierarchical etc.).  \n Mixed Model Estimation\nDemonstration of mixed models via maximum likelihood and link to additive models. \n Mixed and Growth Curve Models\nA comparison of the mixed model vs. latent variable approach for longitudinal data (growth curve models), with simulation of performance in situations of small sample sizes. \n\n\nLatent Variables/SEM\n Structural Equation Modeling\nThis document (and related workshop) focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. The initial workshop was given to an audience of varying background and statistical skill, but the document should be useful to anyone interested in the techniques covered. It is completely R-based, with special emphasis on the lavaan package. It will continue to be a work in progress, particularly the sections after the SEM chapter. Topics include: graphical models (directed and undirected, including path analysis, bayesian networks, and network analysis), mediation, moderation, latent variable models (including principal components analysis and ‘factor analysis’), measurement models, structural equation models, mixture models, growth curves. Topics I hope to provide overviews of in the future include other latent variable techniques/extensions such as IRT, collaborative filtering/recommender systems, hidden Markov models, multi-group models etc.  \n Factor Analysis and Related Methods\nThis document gives a brief overview of many matrix factorization, dimension reduction, and latent variable techniques. Here is a list:  \n\nPrincipal Components Analysis - Factor Analysis - Probabilistic Components Analysis - Non-negative Matrix Factorization - Latent Dirichlet Allocation - Structural Equation Modeling - Item Response Theory - Independent Components Analysis - Multidimensional Scaling - t-Distributed Stochastic Neighbor Embedding (t-sne) - Recommender Systems - Hidden Markov Models - Random Effects Models - Bayesian Approaches - Mixture Models - k-means Cluster Analysis - Hierarchical Cluster Analysis - Latent Class Analysis\n\n Latent Variables, Sum Scores, Single Items\nIt is very common to use sum scores of several variables as a single entity to be used in subsequent analysis (e.g. a regression model). Some may even more use a single variable even though multiple indicators are available. Assuming the multiple measures indicate a latent construct, such typical practice would be problematic relative to using estimated factor scores, either constructed as part of a two-stage process or as part of a structural equation model. This document covers simulations in which comparisons in performance are made between latent variable and sum score or single item approaches.  \n Lord’s Paradox\nSummary of Pearl’s technical reports on some modeling situations such as Lord’s Paradox and Simpson’s Paradox that lead to surprising results that are initially at odds with our intuition. Looks particularly at the issue of change scores vs. controlling for baseline.  \n\n\nOther Statistical\n Generalized Additive Models\nAn introduction to generalized additive models with an emphasis on generalization from familiar linear models and using the mgcv package in R.  \n Introduction to Machine Learning\nA gentle introduction to machine learning concepts with some application in R.  \n Reliability\nAn unfinished document that ties together some ideas regarding the statistical and conceptual notion of reliability..  \n Fractional Regression\nA quick primer regarding data between zero and one, including zero and one.  \n Categorical Regression Models\nAn overview of regression models for binary, multinomial, and ordinal outcomes, with connections among various types of models.  \n Topic Modeling Demo\nA demonstration of Latent Dirichlet Allocation for topic modeling in R.  \n Comparing Measures of Dependency\nA summary of articles that look at various measures of dependency Pearson’s r, Spearman’s rho, and Hoeffding’s D, and newer ones such as Distance Correlation and Maximal Information Coefficient."
  },
  {
    "objectID": "documents.html#programming",
    "href": "documents.html#programming",
    "title": "Content",
    "section": "Programming",
    "text": "Programming\nCheck the old workshops section also for programming-related content.\nPractical Data Science (more details about this document below). The intention was to cover five key topics: basic information processing, programming, modeling, visualization, and publication/presentation.\nExploratory Data Analysis Tools An overview of various packages useful for quick exploration of data.\n FastR\nA notebook on how to make R faster before or irrespective of the machinery used. Topics include avoiding loops, vectorization, faster I/O etc.  \n Engaging the Web with R\nDocument regarding the use of R for web scraping, extracting data via an API, interactive web-based visualizations, and producing web-ready documents. It serves as an overview of ways one might start to use R for web-based activities as opposed to a hand-on approach."
  },
  {
    "objectID": "documents.html#workshops",
    "href": "documents.html#workshops",
    "title": "Content",
    "section": "Workshops",
    "text": "Workshops\nI used to give workshops regularly when I worked in academia. Although they generally won’t age well, I have kept the content here for any that might be interested."
  },
  {
    "objectID": "documents.html#miscellaneous",
    "href": "documents.html#miscellaneous",
    "title": "Content",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n R for Social Science\nThis was put together in a couple of days under duress, and is put here in case someone can find it useful (and thus make the time spent on it not completely wasted)."
  },
  {
    "objectID": "book.html",
    "href": "book.html",
    "title": "Models Demystified",
    "section": "",
    "text": "I have a book coming out! Models are everywhere, from the weather forecast to the stock market, they can help us make sense of the world and make better decisions. But they can also be confusing and intimidating, and the modeling world tough to navigate. Along with my co-author Seth Berry, our goal with the book is to equip you with a better understanding of how models work and how to use them, including both basic and more advanced techniques, where we attempt to demystify models in data science from linear regression to deep learning. It’s a practical guide to help you understand the models that power the world around you.\nWhile not quite to the finish line, most of the book is written and in the editing stage as of February. It will be published sometime in 2025 on CRC Press as part of the Data Science Series. In the meantime, you can access the book here, and it will continue to be developed as time goes on.\nModels Demystified Table of Contents:\n\nIntroduction\nThinking About Models\nThe Foundation\nUnderstanding the Model\nUnderstanding the Features\nModel Estimation and Optimization\nEstimating Uncertainty\nGeneralized Linear Models\nExtending the Linear Model\nCore Concepts in Machine Learning\nCommon Models in Machine Learning\nExtending Machine Learning\nCausal Modeling\nDealing with Data\nDanger Zone\nParting Thoughts\n\nAll examples are in Python and R, and with separate notebooks you can use yourself for more exploration.\nWe welcome any feedback in the meantime as it continues to develop, so please feel free to create an issue. For contributions, please see the contributing page for more information. Thanks for reading, and hope you enjoy it!\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Models {Demystified}},\n  url = {https://m-clark.github.io/book.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Models Demystified.” n.d. https://m-clark.github.io/book.html."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently a Senior Machine Learning Scientist for OneSix, where I engage with client projects across multiple industries, helping them get the most from their data to maximize customer satisfaction, increase profitability, and explore new data-enabled territory. In my role, I strive to translate complex results into actionable insights.\nThroughout my career in data science, I have navigated a vast landscape of modeling, visualizing, and understanding data. I have conducted causal inference for marketing campaigns, classified biomedical images to detect pathology, analyzed text to uncover political sentiment, and explored baboon survival rates based on their social status. My experience spans dozens of industries and academic disciplines, helping clients and researchers unlock the full potential of their data with statistical analysis, machine learning, and AI.\nAdditionally, I have a strong background in teaching, writing, and conducting workshops on these topics, enabling others to gain expertise and become self-sufficient. Recently, I authored a book - Models Demystified - which offers a comprehensive overview of the statistical and machine learning landscape, along with related topics. It will be available from CRC Press in 2025.\nI enjoy empowering people and helping them discover the secrets hidden within their data, and I am passionate about doing quality work that answers the questions at hand. What drew me to the world of data science - and keeps my interest - is that it frees me to engage in many different domains, and it provides a great many tools with which to discover more about the things we humans are most interested in."
  },
  {
    "objectID": "about.html#what-i-do",
    "href": "about.html#what-i-do",
    "title": "About",
    "section": "",
    "text": "I am currently a Senior Machine Learning Scientist for OneSix, where I engage with client projects across multiple industries, helping them get the most from their data to maximize customer satisfaction, increase profitability, and explore new data-enabled territory. In my role, I strive to translate complex results into actionable insights.\nThroughout my career in data science, I have navigated a vast landscape of modeling, visualizing, and understanding data. I have conducted causal inference for marketing campaigns, classified biomedical images to detect pathology, analyzed text to uncover political sentiment, and explored baboon survival rates based on their social status. My experience spans dozens of industries and academic disciplines, helping clients and researchers unlock the full potential of their data with statistical analysis, machine learning, and AI.\nAdditionally, I have a strong background in teaching, writing, and conducting workshops on these topics, enabling others to gain expertise and become self-sufficient. Recently, I authored a book - Models Demystified - which offers a comprehensive overview of the statistical and machine learning landscape, along with related topics. It will be available from CRC Press in 2025.\nI enjoy empowering people and helping them discover the secrets hidden within their data, and I am passionate about doing quality work that answers the questions at hand. What drew me to the world of data science - and keeps my interest - is that it frees me to engage in many different domains, and it provides a great many tools with which to discover more about the things we humans are most interested in."
  },
  {
    "objectID": "about.html#academic-background",
    "href": "about.html#academic-background",
    "title": "About",
    "section": "Academic Background    ",
    "text": "Academic Background    \nWhile my interest is in data science generally, I started off majoring in psychology and philosophy as an undergraduate, and eventually obtained a Ph.D. in Experimental Psychology. During graduate school, I became interested in statistics for practical reasons, eventually choosing it as a concentration, and I also started consulting at that time. That turned out to be a good fit for me, and I’ve been exploring and analyzing data ever since. After my formal education, I worked as a data science consultant at the Universities of North Texas, Notre Dame and Michigan."
  },
  {
    "objectID": "about.html#personal",
    "href": "about.html#personal",
    "title": "About",
    "section": "Personal  ",
    "text": "Personal  \nI was born and raised in Texas, and aside from high school, was there throughout grad school and beyond. I have also lived a good chunk of my life in the Midwest, and I currently live in Ann Arbor Michigan with my wife, our daughter, and our dog Sulu. We tend to keep things simple, and like to go on walks and hikes in the neighborhood and surrounding areas of A2, take the occasional trip to some place new, and make big deals out of the little things.\nI have a non-academic CV here, but if interested in more details, feel free to inquire via email, or DM me on LinkedIn.\n\n\n\nMichael Clark"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "A lot of the following are things I have done in the past for fun or personal interest and development. For my latest efforts, check me out on GitHub, but note that almost all of my work nowadays is for clients and internal use that is not publicly available. Also, while most of this page regards R, I actually have been doing more in Python lately."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Code",
    "section": "R Packages",
    "text": "R Packages\nI haven’t had time to do much with these anymore, but they are available on GitHub and still mostly functional.\n mixedup\n A package for extracting results from mixed models from several packages that are easy to use and viable for presentation.  \n\n confusionMatrix\n Given predictions and a target variable, this package provides a wealth of summary statistics that can be calculated from a single confusion matrix, and returns tidy results with as few dependencies as possible.  \n\n 198R\n R with its collar flipped, or the movie Drive if it was all about R programming, writing R code on a beach in Miami as the sun sets, R wearing sunglasses at night, R asking you to take it home tonight because it doesn’t want to let you go until you see the light, Countach &gt; Testarrosa, but Delorean &gt; all except R, R if Automan had lasted longer than 1 season, driving down Mulholl Dr. at night thinking about R code, R playing a cello at the end of a dock on a lake before taking a ride in a badass helicopter, R with its hair all done up with Aquanet… You get the idea.  \n\n visibly\n This is a collection of functions that I use related to visualization, e.g. the palette generating function (create_palette) and clean visualization themes for ggplot and plotly. In addition, there are visualizations specific to mixed and additive models.  \n\n 538 football club rankings\n This package grabs the table located at 538, and additionally does some summary by league and country.  \n\n gammit\n The package provides a set of functions to aid using mgcv (possibly solely) for mixed models. Mostly superseded by mixedup.  \n \n tidyext\n This package is a collection of functions that do the things I commonly need to do with data while doing other processing within the dataverse. Most of the functionality is now standard in tidyverse, so this is essentially deprecated.  \n\n lazerhawk\n While the name is more or less explanatory, to clarify, this is a package of miscellaneous functions that were mostly useful to me. Now deprecated.  \n\nIn addition to these, though they are not publicly available, I’ve created even more involved packages for specific project work."
  },
  {
    "objectID": "code.html#code-snippets",
    "href": "code.html#code-snippets",
    "title": "Code",
    "section": "Code Snippets",
    "text": "Code Snippets\nThe vast majority of these code snippets are conceptual demonstrations of more complicated models. The audience was generally faculty, researchers, and graduate students in applied fields who, like I did, want to go beyond their basic statistical training. However, I hope it helps anyone who happens to stumble across it. I don’t really update this page anymore, as I’ve cleaned and moved much of these over to Model Estimation by Example, so I would look for something you see here in the corresponding chapter of that document. In general, you can find all of my code at GitHub.\n\n\nModel Fitting\nstandard linear regression, standard logistic regression, penalized regression, lasso regression, ridge regression, newton and IRLS, nelder-mead (Python) (R), gradient descent (stochastic), bivariate probit, heckman selection, tobit, naive bayes, multinomial regression, ordinal regression, quantile regression, hurdle poisson, hurdle negbin, zero-inflated poisson, zero-inflated negbin, Cox survival, confirmatory factor analysis, Markov model, hidden Markov model (R) (Python), stochastic volatility, extreme learning machine, Chinese restaurant process, Indian buffet process, One-line models (an exercise), …\n\nMixed models\none factor random effects (R) (Julia) (Matlab), two factor random effects (R) (Julia) (Matlab), mixed model via ML, mixed model, mixed model with correlated random effects, See the documents section for more…\n\n\nBayesian\nBEST t-test, linear regression (Compare with BUGS version, JAGS), mixed model, mixed model with correlated random effects, beta regression, mixed model with beta response (Stan) (JAGS), mixture model, topic model, multinomial models, multilevel mediation, variational bayes regression, gaussian process, horseshoe prior, item response theory, …\n\n\nEM\nEM mixture univariate, EM mixture multivariate, EM probit, EM pca, EM probabilistic pca, EM state space model\n\n\nWiggly\n\nGaussian processses\nGaussian Process noisy, Gaussian Process noise-free, reproducing kernel hilbert space regression, Bayesian Gaussian process, …\n\n\nAdditive models\ncubic spline, …\n\n\n\n\nProgramming Shenanigans\nThis is old stuff I was doing while learning programming languages. Fun at the time, but mostly useless.\nFizzBuzz test (R) (julia) (Python), Reverse a string recursively (R) (Python), Recursive Word Wrap (R) (Python), calculate compound interest recursively, get US Congress roll call data, Scrape xkcd (R) (Python), Shakespearean Insulter, spurious correlation with ratios, R matrix speedups, …"
  },
  {
    "objectID": "code.html#shiny-apps",
    "href": "code.html#shiny-apps",
    "title": "Code",
    "section": "Shiny Apps",
    "text": "Shiny Apps\nFun at the time, these were some my forays into the Shiny world.\n Bayesian Demonstration\nA simple interactive demonstration for those just starting on their Bayesian journey.  \n\n Historical Football Data\nThat was a dive into Shiny from long ago to make app to explore historical football/soccer data for various European leagues (Premier, La Liga, Serie A etc.) and MLS. One can create tables for a given country/league and year selected, with some leagues having multiple tiers available, and stretching back many decades. Beyond that, one can get a specific team’s historical finishing position, league games for a specific season, all-time tables, and all-time head-to-head results (within a league).  \n\n Last Statements of the Texas Executed\nA demonstration of both text analysis and literate programming/document generation with a dynamic and interactive research document. The texts regard the last statements of offenders in Texas. Sadly no longer functional, as the shiny environment at shinyapps.io appears to not have been preserved correctly on their servers. For its time, it was actually a very nifty demonstration of latent dirichlet allocation and structured topic modeling.  \n A History of Tornados\nBecause I had too much time on my hands and wanted to try out the dashboard feature of R Markdown. Maps tornado activity from 1950-2015. (Archived)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models Demystified",
    "section": "",
    "text": "Some News for the New Year\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nLong time no see…\n\n\n\n\n\n\nmiscellaneous\n\n\n\nNew modeling book under way! \n\n\n\n\n\nMay 20, 2024\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nStuff Going On\n\n\n\n\n\n\nmiscellaneous\n\n\n\nPenalty kicks, class imbalance, tabular deep learning, industry and academia \n\n\n\n\n\nMar 10, 2023\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Linear Models\n\n\n\n\n\n\ndeep learning\n\n\nboosting\n\n\nGLM\n\n\nregression\n\n\nmachine learning\n\n\n\nA demonstration using pytorch \n\n\n\n\n\nOct 10, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Time\n\n\n\n\n\n\nmixed models\n\n\nGAM\n\n\nboosting\n\n\ntime series\n\n\ndeep learning\n\n\n\nDemonstrating some times series approaches\n\n\n\n\n\nAug 10, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming Odds & Ends\n\n\n\n\n\n\nprogramming\n\n\n\nExplorations in faster data processing and other problems. \n\n\n\n\n\nJul 25, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning for Tabular Data\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nA continuing exploration \n\n\n\n\n\nMay 1, 2022\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Descent\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nRethinking what we thought we knew. \n\n\n\n\n\nNov 13, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nThis is definitely not all you need\n\n\n\n\n\n\ndeep learning\n\n\nmachine learning\n\n\n\nA summary of findings regarding deep learning for tabular data. \n\n\n\n\n\nJul 19, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Bayes Part I\n\n\n\n\n\n\nbayesian\n\n\n\nDealing with common model problems. \n\n\n\n\n\nFeb 28, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Bayes Part II\n\n\n\n\n\n\nbayesian\n\n\n\nTaking a better approach and avoiding issues. \n\n\n\n\n\nFeb 28, 2021\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nModels by Example\n\n\n\n\n\n\nregression\n\n\nmachine learning\n\n\n\nRoll your own to understand more. \n\n\n\n\n\nNov 30, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nMicro-macro models\n\n\n\n\n\n\nmixed models\n\n\nSEM\n\n\nregression\n\n\nfactor analysis\n\n\n\nAn analysis in the wrong direction? Predicting group level targets with lower level covariates. \n\n\n\n\n\nAug 31, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\nexploratory data analysis\n\n\n\nExploring how to explore data. \n\n\n\n\n\nJul 10, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPredictions with an offset\n\n\n\n\n\n\nregression\n\n\nGLM\n\n\n\nReconciling R and Stata Approaches \n\n\n\n\n\nJun 16, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nFactor Analysis with the psych package\n\n\n\n\n\n\nfactor analysis\n\n\nreliability\n\n\n\nMaking sense of the results \n\n\n\n\n\nApr 10, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Pandemic\n\n\n\n\n\n\nvisualization\n\n\n\nProcessing and Visualizing Covid-19 Data \n\n\n\n\n\nMar 23, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence Problems\n\n\n\n\n\n\nregression\n\n\nmixed models\n\n\n\nLack of convergence got ya down? A plan of attack. \n\n\n\n\n\nMar 16, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical Effects as Random\n\n\n\n\n\n\nmixed models\n\n\n\nExploring random slopes for categorical covariates and similar models \n\n\n\n\n\nMar 1, 2020\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nMixed Models for Big Data\n\n\n\n\n\n\nGAM\n\n\nmixed models\n\n\nbig data\n\n\nbayesian\n\n\n\nExplorations of a fast penalized regression approach with mgcv \n\n\n\n\n\nOct 20, 2019\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nFractional Regression\n\n\n\n\n\n\nregression\n\n\nGLM\n\n\nmixed models\n\n\n\nA quick primer regarding data between zero and one, including zero and one \n\n\n\n\n\nAug 20, 2019\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nComparisons of the Unseen\n\n\n\n\n\n\nSEM\n\n\nfactor analysis\n\n\n\nExamining group differences across latent variables \n\n\n\n\n\nAug 5, 2019\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nEmpirical Bayes\n\n\n\n\n\n\nempirical bayes\n\n\nregression\n\n\nmixed models\n\n\nbayesian\n\n\n\nRevisiting an old post \n\n\n\n\n\nJun 21, 2019\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage in Mixed Effects Models\n\n\n\n\n\n\nregression\n\n\nmixed models\n\n\n\nA demonstration of random effects \n\n\n\n\n\nMay 14, 2019\n\n\nMichael Clark\n\n\n\n\n\n\n\n\n\n\n\n\nMediation Models\n\n\n\n\n\n\nSEM\n\n\nmediation\n\n\n\nVarious package options for conducting mediation analysis \n\n\n\n\n\nMar 12, 2019\n\n\nMichael Clark\n\n\n\n\n\n\nNo matching items\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Models {Demystified}},\n  url = {https://m-clark.github.io/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Models Demystified.” n.d. https://m-clark.github.io/."
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#introduction",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#introduction",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Introduction",
    "text": "Introduction\nThe following is a demonstration of shrinkage, sometimes called partial-pooling, as it occurs in mixed effects models. For some background, one can see the section of my document on mixed models here, and the document in general for an introduction to mixed models. Part of the inspiration of this document comes from some of the visuals seen here.\nIt is often the case that we have data such that observations are clustered in some way (e.g. repeated observations for units over time, students within schools, etc.). In mixed models, we obtain cluster-specific effects in addition to those for standard coefficients of our regression model. The former are called random effects, while the latter are typically referred to as fixed effects or population-average effects.\nIn other circumstances, we could ignore the clustering, and run a basic regression model. Unfortunately this assumes that all observations behave in the same way, i.e. that there are no cluster-specific effects, which would often be an untenable assumption. Another approach would be to run separate models for each cluster. However, aside from being problematic due to potentially small cluster sizes in common data settings, this ignores the fact that clusters are not isolated and potentially have some commonality.\nMixed models provide an alternative where we have cluster specific effects, but ‘borrow strength’ from the population-average effects. In general, this borrowing is more apparent for what would otherwise be more extreme clusters, and those that have less data. The following will demonstrate how shrinkage arises in different data situations."
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#analysis",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#analysis",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Analysis",
    "text": "Analysis\nFor the following we run a basic mixed model with a random intercept and random slopes for a single predictor variable. There are a number of ways to write such models, and the following does so for a single cluster \\(c\\) and observation \\(i\\). \\(y\\) is a function of the lone covariate \\(x\\), and otherwise we have a basic linear regression model. In this formulation, the random effects for a given cluster (\\(re_{*c}\\)) are added to each fixed effect (intercept \\(b_0\\) and the effect of \\(x\\), \\(b_1\\)). The random effects are multivariate normally distributed with some covariance. The per observation noise \\(\\sigma\\) is assumed constant across observations.\n\\[\\mu_{ic} = (b_0 + \\mathrm{re}_{0c})+ (b_1+\\mathrm{re}_{1c})*x_{ic}\\] \\[\\mathrm{re}_{0}, \\mathrm{re}_{1} \\sim \\mathcal{N}(0, \\Sigma)\\] \\[y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\]\nSuch models are highly flexible and have many extensions, but this simple model is enough for our purposes.\n\nData\nDefault settings for data creation are as follows:\n\nobs_per_cluster (observations per cluster) = 10\nn_cluster (number of clusters) = 100\nintercept (intercept) = 1\nbeta (coefficient for x) = .5\nsigma (observation level standard deviation) = 1\nsd_int (standard deviation for intercept random effect)= .5\nsd_slope (standard deviation for x random effect)= .25\ncor (correlation of random effect) = 0\nbalanced (fraction of overall sample size) = 1\nseed (for reproducibility) = 888\n\nIn this setting, \\(x\\) is a standardized variable with mean zero and standard deviation of 1. Unless a fraction is provided for balanced, the \\(N\\), i.e. the total sample size, is equal to n_cluster * obs_per_cluster. The following is the function that will be used to create the data, which tries to follow the model depiction above. It requires the tidyverse package to work.\n\ncreate_data &lt;- function(  \n  obs_per_cluster = 10,\n  n_cluster = 100,\n  intercept = 1,\n  beta = .5,\n  sigma = 1,\n  sd_int = .5,\n  sd_slope = .25,\n  cor = 0,\n  balanced = TRUE,\n  seed = 888\n  ) {\n  \n  set.seed(seed)\n\n  cluster = rep(1:n_cluster, each = obs_per_cluster)\n  N = n_cluster * obs_per_cluster\n  x = rnorm(N)\n\n  varmat = matrix(c(sd_int^2, cor, cor, sd_slope^2), 2, 2)\n  \n  re = mvtnorm::rmvnorm(n_cluster, sigma = varmat)\n  colnames(re) = c('Intercept', 'x')\n  \n  y = (intercept + re[cluster, 'Intercept']) + (beta + re[cluster, 'x'])*x + rnorm(N, sd = sigma)\n  \n  df = tibble(\n    y,\n    x,\n    cluster\n  )\n  \n  if (balanced &lt; 0 | balanced &gt; 1) {\n    stop('Balanced should be a proportion to sample.')\n  } else {\n    df = sample_frac(df, balanced)\n  }\n  \n  df\n}\n\nThe plotting functions can be found on GitHub for those interested, but won’t be shown here1."
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#run-the-baseline-model",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#run-the-baseline-model",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Run the baseline model",
    "text": "Run the baseline model\nWe will use lme4 to run the analysis. We can see that the model recovers the parameters fairly well, even with the default of only 1000 observations.\n\ndf = create_data()\n\nlibrary(lme4)\nmod = lmer(y ~ x + (x|cluster), df)\nsummary(mod, cor=F) \n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (x | cluster)\n   Data: df\n\nREML criterion at convergence: 3012.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9392 -0.6352 -0.0061  0.6156  2.8721 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n cluster  (Intercept) 0.29138  0.5398       \n          x           0.05986  0.2447   0.30\n Residual             0.99244  0.9962       \nNumber of obs: 1000, groups:  cluster, 100\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.93647    0.06282   14.91\nx            0.54405    0.04270   12.74"
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#visualize-the-baseline-model",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#visualize-the-baseline-model",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Visualize the baseline model",
    "text": "Visualize the baseline model\nNow it is time to visualize the results. We will use gganimate to bring the shrinkage into focus. We start with the estimates that would be obtained by a fixed effects, or ‘regression-by-cluster’ approach. The movement shown will be of those (default 100) cluster-specific estimates toward the mixed model estimates. On the x axis is the estimate for the intercepts, on the y axis are the estimated slopes of the x covariate. The plots for this and other settings are zoomed in based on the random effect variance inputs for this baseline model, in an attempt to make the plots more consistent and comparable across the different settings. As such, the most extreme points often will not be shown at their starting point.\n\n\n\n\n\nWe see more clearly what the mixed model does. The general result is that cluster-specific effects (lighter color) are shrunk back toward the population-average effects (the ‘black hole’), as the imposed normal distribution for the random effects makes the extreme values less probable. Likewise, those more extreme cluster-specific effects, some of which are not displayed as they are so far from the population average, will generally have the most shrinkage imposed. In terms of prediction, it is akin to introducing bias for the cluster specific effects while lowering variance for prediction of new data, and allows us to make predictions on new categories we have not previously seen - we just assume an ‘average’ cluster effect, i.e. a random effect of 0.\nNow we’ll look at what happens under different data circumstances."
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#more-subject-level-variance",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#more-subject-level-variance",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "More subject level variance",
    "text": "More subject level variance\nWhat happens when we add more subject level variance relative to the residual variance? The mixed model will show relatively less shrinkage, and what were previously less probable outcomes are now more probable2, and thus opting for the clusters to speak for themselves.\n\ndf = create_data(sd_int = 1, sd_slope = 1)"
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#more-slope-variance",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#more-slope-variance",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "More slope variance",
    "text": "More slope variance\nIf we add more slope variance relative to the intercept variance, this more or less changes the orientation of the original plot. The shrinkage will be more along the x axis.\nOne point to keep in mind is that the slope variance is naturally on a very different scale than the intercept variance, usually many times smaller3. This can make the model more difficult to estimate. As such, scaling the covariate (e.g. to mean 0 and standard deviation of 1) is typically recommended, and at least in the linear model case, scaling the target variable can help as well.\n\ndf = create_data(sd_int = .25, sd_slope = 1)"
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#fewer-observations-per-cluster",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#fewer-observations-per-cluster",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Fewer observations per cluster",
    "text": "Fewer observations per cluster\nIf we have fewer observations within each cluster, the more likely extreme values will present in the by-cluster approach, and thus more shrinkage is applied when using a mixed model. In this setting, we have relatively less knowledge about the groups, so we would prefer to lean our cluster estimates toward the population average. In the most extreme case of having only one or two observations per cluster, we could only estimate the fixed effects as the cluster-specific effects4.\n\ndf = create_data(obs_per_cluster = 3)"
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#more-observations-per-cluster",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#more-observations-per-cluster",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "More observations per cluster",
    "text": "More observations per cluster\nThe opposite case is seen with more observations. We see that the estimates do not so easily fly to extreme values to begin with. With enough observations per cluster, you likely will see little shrinkage except with the more extreme cases.\n\ndf = create_data(obs_per_cluster = 100)"
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#unbalanced",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#unbalanced",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Unbalanced",
    "text": "Unbalanced\nWith unbalanced data, we see the combination of having more vs. fewer observations per group. Those clusters with more observations will generally exhibit less shrinkage, and those with fewer observations the opposite, though this is tempered by the relative variance components. In the following, the point size represents the cluster size.\n\ndf = create_data(balanced = .5, sd_int = .5, sd_slope = .5, sigma = .25)"
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#summary",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#summary",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Summary",
    "text": "Summary\nMixed models incorporate some amount of shrinkage for cluster-specific effects. Data nuances will determine the relative amount of ‘strength borrowed’, but in general, such models provide a good way for the data to speak for itself when it should, and reflect an ‘average’ when there is little information. An additional benefit is that thinking about models in this way can be seen as a precursor to Bayesian approaches, which can allow for even more flexibility via priors, and more control over how shrinkage is added to the model."
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#other-demos",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#other-demos",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Other demos",
    "text": "Other demos\nBen Bolker, author lme4 on stackexchange\nTristan Mahr, Plotting partial pooling in mixed-effects models\nDiscussion on cluster size"
  },
  {
    "objectID": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#footnotes",
    "href": "posts/2019-05-14-shrinkage-in-mixed-models/index.html#footnotes",
    "title": "Shrinkage in Mixed Effects Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey are a bit messy, as I’m sort of tricking gganimate to do this sort of plot while supplying numerous options for myself for testing and eventual use.↩︎\nThink of a normal distribution with standard deviation of 1- almost all of the probability is for values falling between 3 and -3. The probability of values beyond that would be less than .001. On the other hand, if the standard deviation is 3, the probability of a value beyond \\(\\pm\\) 3 is ~ .32, i.e. very likely.↩︎\nFor example, if you had income in dollars as a target and education level (in years) as a covariate, the intercept would be on the scale of income, roughly somewhere between 0 - max income (i.e. a range of potentially millions), while the effect of one year of education might only have a range of 0 to a few thousand.↩︎\nIn order to have a random effects model you’d need at least two observations per cluster, though this would only allow you to estimate random intercepts. Note that with unbalanced data, it is fine to have singletons or only very few observations. Singletons can only contribute to the intercept estimate however.↩︎"
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html",
    "title": "Comparisons of the Unseen",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#introduction",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#introduction",
    "title": "Comparisons of the Unseen",
    "section": "Introduction",
    "text": "Introduction\nIn some cases we are interested in looking at group differences with regard to  latent variables. For example, social scientists are interested in race and sex differences on psychological measures, or educational scientists might want to create exams in different languages. We cannot measure many constructs directly, but can get reliable measures of them indirectly, e.g. by asking a series of questions, or otherwise observing multiple instances of activity thought to be related to some construct. There are a variety of ways to assess group differences across latent structure, such as anxiety or verbal ability, and this post provides a demo using lavaan.\nMy motivation for doing this is that it comes up from time to time in consulting, and I wanted a quick reminder for the syntax to refer back to. As a starting point though, you can find some demonstration on the lavaan website. For more on factor analysis, structural equation modeling, and more, see my document."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#multiple-group-analysis",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#multiple-group-analysis",
    "title": "Comparisons of the Unseen",
    "section": "Multiple group analysis",
    "text": "Multiple group analysis\nA common way to assess group differences is via multiple group analysis, which amounts to doing separate structural equation models of some kind across the groups of interest. We will use a classic data set to demonstrate the approach. From the help file:\n\nThe Holzinger and Swineford (1939) dataset consists of mental ability test scores of seventh- and eighth-grade children from two different schools (Pasteur and Grant-White). In the original dataset, there are scores for 26 tests. However, a smaller subset with 9 variables is more widely used in the literature…\n\n\nlibrary(lavaan)\ndata(HolzingerSwineford1939)\n\nThe basic model is a factor analysis with three latent variables, with items for visual-spatial ability (x1-x3), verbal comprehension (x4-x6), and so-called ‘speed’ tests (x7-x9), e.g. for addition and counting, which might be thought of general cognitive processing.\nWith lavaan, we specify the model for three factor (or latent variables). After that, a simple group argument will allow the multigroup analysis, providing the factor analysis for both school groups.\n\nlibrary(tidyverse)\nlibrary(lavaan)\n\nhs_model_baseline &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n'\n\nfit_baseline &lt;- cfa(\n  hs_model_baseline, \n  data = HolzingerSwineford1939, \n  group = \"school\"\n)\n\nsummary(fit_baseline)  \n\nlavaan 0.6.17 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n\n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.851\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     64.309\n    Grant-White                                 51.542\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.394    0.122    3.220    0.001\n    x3                0.570    0.140    4.076    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                1.183    0.102   11.613    0.000\n    x6                0.875    0.077   11.421    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.125    0.277    4.057    0.000\n    x9                0.922    0.225    4.104    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal            0.479    0.106    4.531    0.000\n    speed             0.185    0.077    2.397    0.017\n  verbal ~~                                           \n    speed             0.182    0.069    2.628    0.009\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                4.941    0.095   52.249    0.000\n   .x2                5.984    0.098   60.949    0.000\n   .x3                2.487    0.093   26.778    0.000\n   .x4                2.823    0.092   30.689    0.000\n   .x5                3.995    0.105   38.183    0.000\n   .x6                1.922    0.079   24.321    0.000\n   .x7                4.432    0.087   51.181    0.000\n   .x8                5.563    0.078   71.214    0.000\n   .x9                5.418    0.079   68.440    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.298    0.232    1.286    0.198\n   .x2                1.334    0.158    8.464    0.000\n   .x3                0.989    0.136    7.271    0.000\n   .x4                0.425    0.069    6.138    0.000\n   .x5                0.456    0.086    5.292    0.000\n   .x6                0.290    0.050    5.780    0.000\n   .x7                0.820    0.125    6.580    0.000\n   .x8                0.510    0.116    4.406    0.000\n   .x9                0.680    0.104    6.516    0.000\n    visual            1.097    0.276    3.967    0.000\n    verbal            0.894    0.150    5.963    0.000\n    speed             0.350    0.126    2.778    0.005\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.736    0.155    4.760    0.000\n    x3                0.925    0.166    5.583    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                0.990    0.087   11.418    0.000\n    x6                0.963    0.085   11.377    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.226    0.187    6.569    0.000\n    x9                1.058    0.165    6.429    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal            0.408    0.098    4.153    0.000\n    speed             0.276    0.076    3.639    0.000\n  verbal ~~                                           \n    speed             0.222    0.073    3.022    0.003\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                4.930    0.095   51.696    0.000\n   .x2                6.200    0.092   67.416    0.000\n   .x3                1.996    0.086   23.195    0.000\n   .x4                3.317    0.093   35.625    0.000\n   .x5                4.712    0.096   48.986    0.000\n   .x6                2.469    0.094   26.277    0.000\n   .x7                3.921    0.086   45.819    0.000\n   .x8                5.488    0.087   63.174    0.000\n   .x9                5.327    0.085   62.571    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.715    0.126    5.676    0.000\n   .x2                0.899    0.123    7.339    0.000\n   .x3                0.557    0.103    5.409    0.000\n   .x4                0.315    0.065    4.870    0.000\n   .x5                0.419    0.072    5.812    0.000\n   .x6                0.406    0.069    5.880    0.000\n   .x7                0.600    0.091    6.584    0.000\n   .x8                0.401    0.094    4.249    0.000\n   .x9                0.535    0.089    6.010    0.000\n    visual            0.604    0.160    3.762    0.000\n    verbal            0.942    0.152    6.177    0.000\n    speed             0.461    0.118    3.910    0.000\n\n\nSo we’re left with visual inspection to note whether there are general differences on latent variables among the groups. This is all well and good, but given that none of the parameters will be identical from one group to the next, perhaps we want a more principled approach. Say our question specifically concerns a mean difference between schools on the visual latent variable. How do we go about it?\nNote that at this point the intercepts for the latent variables are zero. They have to be for the model to be identified, much in the same way that at least one factor loading (the first by default) has to be fixed to one. We only have so much information to estimate the parameters in a latent variable setting. Now let’s see how we might go about changing things to get a better understanding of group differences on the latent variables."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#latent-variable-intercepts",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#latent-variable-intercepts",
    "title": "Comparisons of the Unseen",
    "section": "Latent variable intercepts",
    "text": "Latent variable intercepts\nTo get around this limitation, we could try and fix some parameters, thereby freeing the intercepts to be estimated. For example, if we fix the mean of one of the observed variables to be zero instead, we would be able to estimate the intercept for the latent variable. In the following we’ll do this for the visuo-spatial ability construct, which will be our point of focus for group differences going forward.\n\n\nRecall that for the standard latent linear model, the observed variable is the dependent variable . For example, given an observed variable \\(X\\), a latent variable \\(F\\) and loading \\(\\lambda\\): \\[X = b_0 + \\lambda F \\]\nIn the model we we also identify a new parameter, which will be the differences in these latent variable intercepts, simply called diff. I have omitted some output for brevity of space.\n\nhs_model_1 &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  \n  # intercepts: in order to have an identified model, you would have to fix the\n  # intercepts of observed to 0, 1 represents the intercept, 0* fixes it to be 0\n  x1 ~ 0*1   \n\n  # intercept for Pasteur and Grant-White schools\n  visual ~  c(int_p, int_gw)*1    \n   \n  # comparisons\n  diff := int_p - int_gw\n'\n\nfit_1 &lt;- cfa(hs_model_1, \n             data = HolzingerSwineford1939, \n             group = \"school\",\n             meanstructure = T)\nsummary(fit_1, header=F, nd=2)\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2                 0.39     0.12     3.22     0.00\n    x3                 0.57     0.14     4.08     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5                 1.18     0.10    11.61     0.00\n    x6                 0.87     0.08    11.42     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8                 1.12     0.28     4.06     0.00\n    x9                 0.92     0.22     4.10     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal             0.48     0.11     4.53     0.00\n    speed              0.19     0.08     2.40     0.02\n  verbal ~~                                           \n    speed              0.18     0.07     2.63     0.01\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.00                           \n    visual  (int_)     4.94     0.09    52.25     0.00\n   .x2                 4.04     0.61     6.61     0.00\n   .x3                -0.33     0.70    -0.47     0.64\n   .x4                 2.82     0.09    30.69     0.00\n   .x5                 4.00     0.10    38.18     0.00\n   .x6                 1.92     0.08    24.32     0.00\n   .x7                 4.43     0.09    51.18     0.00\n   .x8                 5.56     0.08    71.21     0.00\n   .x9                 5.42     0.08    68.44     0.00\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.30     0.23     1.29     0.20\n   .x2                 1.33     0.16     8.46     0.00\n   .x3                 0.99     0.14     7.27     0.00\n   .x4                 0.43     0.07     6.14     0.00\n   .x5                 0.46     0.09     5.29     0.00\n   .x6                 0.29     0.05     5.78     0.00\n   .x7                 0.82     0.12     6.58     0.00\n   .x8                 0.51     0.12     4.41     0.00\n   .x9                 0.68     0.10     6.52     0.00\n    visual             1.10     0.28     3.97     0.00\n    verbal             0.89     0.15     5.96     0.00\n    speed              0.35     0.13     2.78     0.01\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2                 0.74     0.15     4.76     0.00\n    x3                 0.92     0.17     5.58     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5                 0.99     0.09    11.42     0.00\n    x6                 0.96     0.08    11.38     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8                 1.23     0.19     6.57     0.00\n    x9                 1.06     0.16     6.43     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal             0.41     0.10     4.15     0.00\n    speed              0.28     0.08     3.64     0.00\n  verbal ~~                                           \n    speed              0.22     0.07     3.02     0.00\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.00                           \n    visual  (int_)     4.93     0.10    51.70     0.00\n   .x2                 2.57     0.77     3.35     0.00\n   .x3                -2.56     0.82    -3.12     0.00\n   .x4                 3.32     0.09    35.63     0.00\n   .x5                 4.71     0.10    48.99     0.00\n   .x6                 2.47     0.09    26.28     0.00\n   .x7                 3.92     0.09    45.82     0.00\n   .x8                 5.49     0.09    63.17     0.00\n   .x9                 5.33     0.09    62.57     0.00\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.71     0.13     5.68     0.00\n   .x2                 0.90     0.12     7.34     0.00\n   .x3                 0.56     0.10     5.41     0.00\n   .x4                 0.32     0.06     4.87     0.00\n   .x5                 0.42     0.07     5.81     0.00\n   .x6                 0.41     0.07     5.88     0.00\n   .x7                 0.60     0.09     6.58     0.00\n   .x8                 0.40     0.09     4.25     0.00\n   .x9                 0.53     0.09     6.01     0.00\n    visual             0.60     0.16     3.76     0.00\n    verbal             0.94     0.15     6.18     0.00\n    speed              0.46     0.12     3.91     0.00\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    diff               0.01     0.13     0.08     0.93\n\n\nFor clearer presentation, we’ll look at a table of the specific parameter estimates.\n\n\n\n\n\nterm\nop\nblock\ngroup\nlabel\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nstd.lv\nstd.all\nstd.nox\n\n\n\n\nvisual ~1\n~1\n1\n1\nint_p\n4.941\n0.095\n52.249\n0.000\n4.756\n5.127\n4.718\n4.718\n4.718\n\n\nvisual ~1\n~1\n2\n2\nint_gw\n4.930\n0.095\n51.696\n0.000\n4.743\n5.117\n6.345\n6.345\n6.345\n\n\ndiff := int_p-int_gw\n:=\n0\n0\ndiff\n0.011\n0.134\n0.085\n0.933\n-0.252\n0.275\n-1.627\n-1.627\n-1.627\n\n\n\n\n\n\n\nThe above shows the schools to be not much different from one another on the visual-spatial ability latent variable. But compare this result to the intercepts for x1 in our baseline model. This model would would be identical to comparing the intercepts on whichever observed variable you had fixed to zero. Much like we must scale the latent variable to that of one of the observed variables by fixing the loading to be 1, we essentially come to the same type of issue by fixing its mean to be on that of the observed variable.\nTo make this more explicit, we’ll label the x1 intercepts in our baseline model and look at their difference. I won’t show the model out put and simply focus on the parameter table instead.\n\nhs_model_2 &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  x1 ~ c(a, b)*1   \n   \n  # comparisons\n   diff := a - b\n'\n\nfit_2 &lt;- cfa(hs_model_2, \n           data = HolzingerSwineford1939, \n           group = \"school\",\n           meanstructure = T)\n\n# summary(fit_2)\n\n\n\n\n\n\nterm\nop\nblock\ngroup\nlabel\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\nstd.lv\nstd.all\nstd.nox\n\n\n\n\nx1 ~1\n~1\n1\n1\na\n4.941\n0.095\n52.249\n0.000\n4.756\n5.127\n4.941\n4.183\n4.183\n\n\nx1 ~1\n~1\n2\n2\nb\n4.930\n0.095\n51.696\n0.000\n4.743\n5.117\n4.930\n4.293\n4.293\n\n\ndiff := a-b\n:=\n0\n0\ndiff\n0.011\n0.134\n0.085\n0.933\n-0.252\n0.275\n0.011\n-0.110\n-0.110\n\n\n\n\n\n\n\nSame difference."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#observed-variable-group-differences",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#observed-variable-group-differences",
    "title": "Comparisons of the Unseen",
    "section": "Observed variable group differences",
    "text": "Observed variable group differences\nThe following approach is not the same model, but as we’ll see, would also provide the same result. In this case, each observed variable is affected by the school grouping, and the path coefficient for x1 is the same difference in means as before.\n\nhs_model_3 &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  x1 ~ diff*school \n  x2 + x3 + x4 + x5 + x6 +  x7 + x8 + x9 ~ school\n'\n\nfit_3 &lt;- cfa(hs_model_3, \n             data = HolzingerSwineford1939,\n             meanstructure = T)\n\n# summary(fit_3)\n\nA comparison of all three shows the same results, but that the third model has fewer parameters, as the loadings and latent variable variances are not changing across groups.\n\n\n\n\n\nmodel\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\nfit_1\n0.011\n0.134\n-0.252\n0.275\n\n\nfit_2\n0.011\n0.134\n-0.252\n0.275\n\n\nfit_3\n0.011\n0.134\n-0.252\n0.275\n\n\n\n\n\n\n\n\n\n\nModel N parameters\n\n\nfit_1\nfit_2\nfit_3\n\n\n\n\n60\n60\n39\n\n\n\n\n\n\n\n\n\n\nModel AIC\n\n\nfit_1\nfit_2\nfit_3\n\n\n\n\n7484.395\n7484.395\n7474.493"
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#structural-model",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#structural-model",
    "title": "Comparisons of the Unseen",
    "section": "Structural model",
    "text": "Structural model\nIn the models I see, people would more commonly address such a theoretical question without a multigroup approach, simply regressing the latent variable of interest on the group factor. For lack of a better name, I’ll just call this a structural model in the sense we have an explicit regression model. We can do that here.\n\n# standard cfa with school predicting visual\nhs_model_4a &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  visual ~ diff*school\n  \n  visual ~~ speed + verbal  # lavaan will not estimate this by default\n'\n\nfit_4a = sem(hs_model_4a, data=HolzingerSwineford1939, meanstructure=T)\nsummary(fit_4a)\n\nlavaan 0.6.17 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        31\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                               161.444\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.570    0.100    5.723    0.000\n    x3                0.797    0.111    7.212    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                1.113    0.065   17.021    0.000\n    x6                0.928    0.055   16.741    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.194    0.168    7.124    0.000\n    x9                1.060    0.148    7.152    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~                                            \n    school  (diff)    0.287    0.110    2.612    0.009\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .visual ~~                                           \n    speed             0.241    0.054    4.446    0.000\n    verbal            0.429    0.073    5.846    0.000\n  verbal ~~                                           \n    speed             0.172    0.049    3.483    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                4.500    0.180   24.997    0.000\n   .x2                5.840    0.122   47.989    0.000\n   .x3                1.903    0.151   12.631    0.000\n   .x4                3.061    0.067   45.694    0.000\n   .x5                4.341    0.074   58.452    0.000\n   .x6                2.186    0.063   34.667    0.000\n   .x7                4.186    0.063   66.766    0.000\n   .x8                5.527    0.058   94.854    0.000\n   .x9                5.374    0.058   92.546    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.604    0.105    5.739    0.000\n   .x2                1.137    0.102   11.172    0.000\n   .x3                0.795    0.090    8.879    0.000\n   .x4                0.372    0.048    7.821    0.000\n   .x5                0.448    0.058    7.698    0.000\n   .x6                0.354    0.043    8.254    0.000\n   .x7                0.796    0.081    9.775    0.000\n   .x8                0.470    0.076    6.209    0.000\n   .x9                0.580    0.071    8.195    0.000\n   .visual            0.754    0.135    5.597    0.000\n    verbal            0.978    0.112    8.735    0.000\n    speed             0.387    0.087    4.462    0.000\n\n\nAt first blush, it would seem we are not getting the same result, as the mean difference is now estimated to be 0.287. Our difference is notably larger and significant. We can also see that the fit is different based on AIC and the number of parameters estimated.\n\n\n\n\n\nfit_1\nfit_2\nfit_3\nfit_4a\nfit_baseline\n\n\n\n\n7484.395\n7484.395\n7474.493\n7531.852\n7484.395\n\n\n\n\n\n\n\n\n\n\nfit_1\nfit_2\nfit_3\nfit_4a\nfit_baseline\n\n\n\n\n60\n60\n39\n31\n60\n\n\n\n\n\n\n\n\nStructural model as multigroup\nHowever, we can recover the previous multigroup results by regressing the other observed variables on school as well. This leaves the only group effect remaining to be the effect on x1.\n\nhs_model_4b &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  visual ~ diff*school\n  \n  x2 + x3 + x4 + x5 + x6 +  x7 + x8 + x9 ~ school\n'\n\nfit_4b = sem(hs_model_4b, data=HolzingerSwineford1939, meanstructure=T)\n\n\n\n\n\n\nfit_1\nfit_2\nfit_3\nfit_4a\nfit_4b\nfit_baseline\n\n\n\n\n7484.395\n7484.395\n7474.493\n7531.852\n7526.606\n7484.395\n\n\n\n\n\n\n\n\n\n\nfit_1\nfit_2\nfit_3\nfit_4a\nfit_4b\nfit_baseline\n\n\n\n\n60\n60\n39\n31\n37\n60\n\n\n\n\n\n\n\n\n\n\nmodel\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n1\n0.011\n0.134\n-0.252\n0.275\n\n\n2\n0.011\n0.134\n-0.252\n0.275\n\n\n3\n0.011\n0.134\n-0.252\n0.275\n\n\n4b\n0.011\n0.134\nNA\nNA\n\n\n\n\n\n\n\n\n\nMultigroup as the structural model\nLet’s see if we can get the structural model result from our multigroup approach. In fact we can. The following produces the same coefficient by summing the differences on the observed items. As we will see later, the statistical result is essentially what we’d get by using a linear regression on a sum score of visual items.\n\nhs_model_baseline_2 &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  x1 ~ c(a1, a2)*1\n  x2 ~ c(b1, b2)*1\n  x3 ~ c(c1, c2)*1\n  \n  diff := (a1 - a2) + (b1 - b2) + (c1 - c2)\n'\n\nfit_baseline_2 &lt;- cfa(\n  hs_model_baseline_2, \n  data = HolzingerSwineford1939, \n  group = \"school\"\n)\n\nsummary(fit_baseline_2)\n\nlavaan 0.6.17 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n\n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.851\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     64.309\n    Grant-White                                 51.542\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.394    0.122    3.220    0.001\n    x3                0.570    0.140    4.076    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                1.183    0.102   11.613    0.000\n    x6                0.875    0.077   11.421    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.125    0.277    4.057    0.000\n    x9                0.922    0.225    4.104    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal            0.479    0.106    4.531    0.000\n    speed             0.185    0.077    2.397    0.017\n  verbal ~~                                           \n    speed             0.182    0.069    2.628    0.009\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1        (a1)    4.941    0.095   52.249    0.000\n   .x2        (b1)    5.984    0.098   60.949    0.000\n   .x3        (c1)    2.487    0.093   26.778    0.000\n   .x4                2.823    0.092   30.689    0.000\n   .x5                3.995    0.105   38.183    0.000\n   .x6                1.922    0.079   24.321    0.000\n   .x7                4.432    0.087   51.181    0.000\n   .x8                5.563    0.078   71.214    0.000\n   .x9                5.418    0.079   68.440    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.298    0.232    1.286    0.198\n   .x2                1.334    0.158    8.464    0.000\n   .x3                0.989    0.136    7.271    0.000\n   .x4                0.425    0.069    6.138    0.000\n   .x5                0.456    0.086    5.292    0.000\n   .x6                0.290    0.050    5.780    0.000\n   .x7                0.820    0.125    6.580    0.000\n   .x8                0.510    0.116    4.406    0.000\n   .x9                0.680    0.104    6.516    0.000\n    visual            1.097    0.276    3.967    0.000\n    verbal            0.894    0.150    5.963    0.000\n    speed             0.350    0.126    2.778    0.005\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.736    0.155    4.760    0.000\n    x3                0.925    0.166    5.583    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                0.990    0.087   11.418    0.000\n    x6                0.963    0.085   11.377    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.226    0.187    6.569    0.000\n    x9                1.058    0.165    6.429    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal            0.408    0.098    4.153    0.000\n    speed             0.276    0.076    3.639    0.000\n  verbal ~~                                           \n    speed             0.222    0.073    3.022    0.003\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1        (a2)    4.930    0.095   51.696    0.000\n   .x2        (b2)    6.200    0.092   67.416    0.000\n   .x3        (c2)    1.996    0.086   23.195    0.000\n   .x4                3.317    0.093   35.625    0.000\n   .x5                4.712    0.096   48.986    0.000\n   .x6                2.469    0.094   26.277    0.000\n   .x7                3.921    0.086   45.819    0.000\n   .x8                5.488    0.087   63.174    0.000\n   .x9                5.327    0.085   62.571    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.715    0.126    5.676    0.000\n   .x2                0.899    0.123    7.339    0.000\n   .x3                0.557    0.103    5.409    0.000\n   .x4                0.315    0.065    4.870    0.000\n   .x5                0.419    0.072    5.812    0.000\n   .x6                0.406    0.069    5.880    0.000\n   .x7                0.600    0.091    6.584    0.000\n   .x8                0.401    0.094    4.249    0.000\n   .x9                0.535    0.089    6.010    0.000\n    visual            0.604    0.160    3.762    0.000\n    verbal            0.942    0.152    6.177    0.000\n    speed             0.461    0.118    3.910    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    diff              0.287    0.297    0.965    0.335\n\n\n\n\nGroup difference as an indirect effect\nGoing back to the first structural model hs_model_4a, it might be interesting to some to see that the group difference still regards a difference on the observed x1 observed variable. We can see this more clearly if we set the x1 loading to be estimated rather than fixed at one, then use the product of coefficients approach (a la mediation) to estimate the group difference.\n\nhs_model_4c &lt;- ' \n  visual =~ x2 + a*x1 + x3    # estimate x1 loading vs. scaling by it\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n\n  visual ~ b*school\n  \n  visual ~~ verbal + speed\n  \n  diff := a*b    # \"indirect\" effect of school on x1\n'\n\n# same as fit_4a\nfit_4c = cfa(hs_model_4c, data = HolzingerSwineford1939, meanstructure=T)\n\n\n\n\n\n\nmodel\nlabel\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n4a\ndiff\n0.287\n0.11\n2.612\n0.009\n0.072\n0.503\n\n\n4c\ndiff\n0.287\n0.11\n2.612\n0.009\n0.072\n0.503\n\n\n\n\n\n\n\nAnd what is this value of 0.287? We see it as the group difference on the latent construct rather than simply being a mean difference on x1. However, in fit_4a it is estimated on the metric of x1, which had it’s loading fixed to 1. We will see another interpretation later.\n\n\nYou might be thinking, why does my effect on the latent variable depend on a specific item? Well it shouldn’t. If the items are random observations measured in the same way of the same underlying construct, then the loadings will essentially be equal, and so it wouldn’t matter which one is chosen as a default."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#more-structural-models",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#more-structural-models",
    "title": "Comparisons of the Unseen",
    "section": "More structural models",
    "text": "More structural models\nThe following is equivalent to the result one would get from group.equal = c('loadings', 'intercepts'), but to make things more clear, I show the explicit syntax (commented out are other options one could potentially play with). The first group would have latent variable means at zero, while the second group would be allowed to vary. This is more or less what is desired if we want to know a group difference on the latent structure. The first group mean is arbitrarily set to zero, so the estimated intercept for the second group tells us the relative difference, much like when we are dummy coding with standard regression models.\n\nhs_model_4d &lt;- ' \n  # make loadings equal across groups\n  \n  visual =~ c(1, 1)*x1 + c(v_x2, v_x2)*x2 + c(v_x3, v_x3)*x3\n  verbal =~ c(1, 1)*x4 + c(v_x5, v_x5)*x5 + c(v_x6, v_x6)*x6\n  speed  =~ c(1, 1)*x7 + c(v_x8, v_x8)*x8 + c(v_x9, v_x9)*x9 \n  \n  # make intercepts equal across groups\n  \n  x1 ~ c(0, 0) * 1\n  x2 ~ c(int_x2, int_x2) * 1\n  x3 ~ c(int_x3, int_x3) * 1\n  x4 ~ c(0, 0) * 1\n  x5 ~ c(int_x5, int_x5) * 1\n  x6 ~ c(int_x6, int_x6) * 1\n  x7 ~ c(0, 0) * 1\n  x8 ~ c(int_x8, int_x8) * 1\n  x9 ~ c(int_x9, int_x9) * 1\n  \n  # make covariances equal across groups\n  \n  # visual ~~ c(cov_vv, cov_vv) * verbal + c(cov_visp, cov_visp) * speed\n  # verbal ~~ c(cov_vesp, cov_vesp) * speed\n  \n  # make variances equal\n  \n  # visual ~~ c(vvar, vvar) * visual\n  # verbal ~~ c(tvar, tvar) * verbal\n  # speed  ~~ c(svar, svar) * speed\n  \n  # x1 ~~ c(x1var, x1var) * x1\n  # x2 ~~ c(x2var, x2var) * x2\n  # x3 ~~ c(x3var, x3var) * x3\n  # x4 ~~ c(x4var, x4var) * x4\n  # x5 ~~ c(x5var, x5var) * x5\n  # x6 ~~ c(x6var, x6var) * x6\n  # x7 ~~ c(x7var, x7var) * x7\n  # x8 ~~ c(x8var, x8var) * x8\n  # x9 ~~ c(x9var, x9var) * x9\n  \n  \n  visual ~ c(vis_int_p, vis_int_gw)*1\n  verbal ~ c(verb_int_p, verb_int_gw)*1\n  speed  ~ c(speed_int_p, speed_int_gw)*1\n  \n  \n   \n  # comparisons\n   diff := vis_int_p - vis_int_gw\n'\n\nfit_4d  = sem(hs_model_4d, \n                data=HolzingerSwineford1939, \n                group = 'school',\n                meanstructure=T)\n\nsummary(fit_4d, header=F, nd=2)\n\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2      (v_x2)     0.58     0.10     5.71     0.00\n    x3      (v_x3)     0.80     0.11     7.15     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5      (v_x5)     1.12     0.07    16.97     0.00\n    x6      (v_x6)     0.93     0.06    16.61     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8      (v_x8)     1.13     0.15     7.79     0.00\n    x9      (v_x9)     1.01     0.13     7.67     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal             0.41     0.10     4.29     0.00\n    speed              0.18     0.07     2.69     0.01\n  verbal ~~                                           \n    speed              0.18     0.06     2.90     0.00\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.00                           \n   .x2      (in_2)     3.27     0.50     6.54     0.00\n   .x3      (in_3)    -1.72     0.55    -3.11     0.00\n   .x4                 0.00                           \n   .x5      (in_5)     0.92     0.21     4.36     0.00\n   .x6      (in_6)    -0.66     0.18    -3.75     0.00\n   .x7                 0.00                           \n   .x8      (in_8)     0.84     0.60     1.39     0.17\n   .x9      (in_9)     1.18     0.55     2.16     0.03\n    visual  (vs__)     5.00     0.09    55.76     0.00\n    verbal  (vr__)     2.78     0.09    31.95     0.00\n    speed   (sp__)     4.24     0.07    57.97     0.00\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.56     0.14     3.98     0.00\n   .x2                 1.30     0.16     8.19     0.00\n   .x3                 0.94     0.14     6.93     0.00\n   .x4                 0.45     0.07     6.43     0.00\n   .x5                 0.50     0.08     6.14     0.00\n   .x6                 0.26     0.05     5.26     0.00\n   .x7                 0.89     0.12     7.42     0.00\n   .x8                 0.54     0.09     5.71     0.00\n   .x9                 0.65     0.10     6.80     0.00\n    visual             0.80     0.17     4.64     0.00\n    verbal             0.88     0.13     6.69     0.00\n    speed              0.32     0.08     3.91     0.00\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2      (v_x2)     0.58     0.10     5.71     0.00\n    x3      (v_x3)     0.80     0.11     7.15     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5      (v_x5)     1.12     0.07    16.97     0.00\n    x6      (v_x6)     0.93     0.06    16.61     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8      (v_x8)     1.13     0.15     7.79     0.00\n    x9      (v_x9)     1.01     0.13     7.67     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    verbal             0.43     0.10     4.42     0.00\n    speed              0.33     0.08     4.01     0.00\n  verbal ~~                                           \n    speed              0.24     0.07     3.22     0.00\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.00                           \n   .x2      (in_2)     3.27     0.50     6.54     0.00\n   .x3      (in_3)    -1.72     0.55    -3.11     0.00\n   .x4                 0.00                           \n   .x5      (in_5)     0.92     0.21     4.36     0.00\n   .x6      (in_6)    -0.66     0.18    -3.75     0.00\n   .x7                 0.00                           \n   .x8      (in_8)     0.84     0.60     1.39     0.17\n   .x9      (in_9)     1.18     0.55     2.16     0.03\n    visual  (vs__)     4.85     0.09    52.96     0.00\n    verbal  (vr__)     3.35     0.09    38.16     0.00\n    speed   (sp__)     4.06     0.08    50.75     0.00\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                 0.65     0.13     5.09     0.00\n   .x2                 0.96     0.12     7.81     0.00\n   .x3                 0.64     0.10     6.32     0.00\n   .x4                 0.34     0.06     5.53     0.00\n   .x5                 0.38     0.07     5.13     0.00\n   .x6                 0.44     0.07     6.56     0.00\n   .x7                 0.63     0.10     6.57     0.00\n   .x8                 0.43     0.09     4.91     0.00\n   .x9                 0.52     0.09     6.10     0.00\n    visual             0.71     0.16     4.42     0.00\n    verbal             0.87     0.13     6.66     0.00\n    speed              0.51     0.12     4.38     0.00\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    diff               0.15     0.12     1.21     0.23\n\n\nThe fit is now estimated to be 0.148, as we are now taking into account the intercorrelations of the latent variables. However, there is a more simple and obvious way to do this model. We simply regress all latent variables on school.\n\nhs_model_4e &lt;- ' \n  visual =~ x1 + x2 + x3    # estimate x1 loading vs. scaling by it\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n\n  visual ~ diff*school\n  \n  verbal + speed ~ school\n'\n\nfit_4e = cfa(hs_model_4e, data = HolzingerSwineford1939, meanstructure=T)\n# summary(fit_4e)\n\n\n\n\n\n\nmodel\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\nObserved value differences\n\n\n1\n0.011\n0.134\n-0.252\n0.275\n\n\n2\n0.011\n0.134\n-0.252\n0.275\n\n\n3\n0.011\n0.134\n-0.252\n0.275\n\n\n4b\n0.011\n0.134\nNA\nNA\n\n\nLatent variable differences\n\n\n4a\n0.287\n0.110\n0.072\n0.503\n\n\n4c\n0.287\n0.110\n0.072\n0.503\n\n\n4d\n0.148\n0.122\nNA\nNA\n\n\n4e\n0.147\n0.122\nNA\nNA\n\n\n\n\n\n\n\nThe primary differences we’ve seen thus far can be summarized as follows:\n\n1:3 and 4b: These models are focusing on observed variable differences, specifically on x1.\n4a and 4c: These models are latent variable differences on the visual factor, but do not control for indirect (or backdoor) effect school has on visual through speed and verbal. In that light, we might consider this the total effect of school on the visual factor. Had we regressed the verbal and speed factors on school also, thereby decomposing the total effect into those different paths, we’d get the same result for the school difference on the visual factor as we do in 4d and e\n4d and 4e: This is generally what we want. A simple group difference on a latent variable(s) with other parameters assumed (relatively) equal across groups."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#sumfactor-score",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#sumfactor-score",
    "title": "Comparisons of the Unseen",
    "section": "Sum/Factor score",
    "text": "Sum/Factor score\nWhat would happen if we look at the structural/regression model with the estimated latent variable scores1? How about we go even simpler, by not even running an SEM and simply using sum scores? Let’s see what results.\nWe’ll start with the estimated factor scores.\n\nhs_model_5 &lt;- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9\n'\n\nfit_5 = cfa(hs_model_5, data = HolzingerSwineford1939, meanstructure=T)\n\nHolzingerSwineford1939 = HolzingerSwineford1939 %&gt;% \n  mutate(\n    visual = lavPredict(fit_5)[,'visual'],\n    verbal = lavPredict(fit_5)[,'verbal'],\n    speed  = lavPredict(fit_5)[,'speed']\n  )\n\nlm_1 = lm(visual ~ school + verbal + speed, data = HolzingerSwineford1939)\ncoef(lm_1)  \n\n  (Intercept) schoolPasteur        verbal         speed \n  -0.07231513    0.13953111    0.34744541    0.63626370 \n\n\nThe estimated coefficient is pretty close to that estimated by the SEM when we regressed all the factors on school. Interestingly, if we fix the loadings to be constant, we recover the initial multigroup estimates.\n\nhs_model_5b &lt;- ' \n  visual =~ x1 + l1*x2 + l1*x3\n  verbal =~ x4 + l2*x5 + l2*x6\n  speed  =~ x7 + l3*x8 + l3*x9 \n'\n\nfit_5b = cfa(hs_model_5b,\n             data = lavaan::HolzingerSwineford1939,\n             meanstructure = T)\n\nHolzingerSwineford1939 = HolzingerSwineford1939 %&gt;%\n  mutate(\n    visual = lavPredict(fit_5b)[, 'visual'],\n    verbal = lavPredict(fit_5b)[, 'verbal'],\n    speed  = lavPredict(fit_5b)[, 'speed']\n  )\n\nlm_2 = lm(visual ~ school, data = HolzingerSwineford1939)\ncoef(lm_2)  # same as x1 diffs\n\n  (Intercept) schoolPasteur \n -0.005790171   0.011172061 \n\nlm_3 = lm(x1 ~ school, data = HolzingerSwineford1939)\ncoef(lm_3)\n\n  (Intercept) schoolPasteur \n   4.92988506    0.01135425 \n\n\nNow lets do a sum score. It may not be obvious, but a sum score can be seen as assuming a latent variable model where there is only a single construct and loadings and variances are equal for each item. As such, it is a natural substitute for a latent variable if we don’t want to use SEM, especially if we’re dealing with a notably reliable measure.\n\nHolzingerSwineford1939 = HolzingerSwineford1939 %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    visual_sum = sum(x1, x2, x3),\n    verbal_sum = sum(x4, x5, x6),\n    speed_sum  = sum(x7, x8, x9)\n  ) %&gt;%\n  ungroup()\n\nlm_sum = lm(visual_sum ~ school, HolzingerSwineford1939)\n\ncoef(lm_sum)  # same as structural diffs\n\n  (Intercept) schoolPasteur \n   13.1255747     0.2868184 \n\n\nThat coefficient representing the group difference looks familiar- it’s the same value as we had for models 4a and 4c, where we looked at a group difference only for the visual factor. As with those, this can be seen as a total effect of school on the visual ‘factor’."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#summary-of-differences",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#summary-of-differences",
    "title": "Comparisons of the Unseen",
    "section": "Summary of differences",
    "text": "Summary of differences\nWe can summarize our results as follows. The @ref(multiple-group-analysis) multigroup approach can be seen as an interaction of everything with the grouping variable. In some measurement scenarios, for example, the development of a nationwide achievement exam, this might be desirable as a means to establish measurement invariance (see below). However, I think this is probably rarely a theoretical goal for most applied researchers using SEM. Furthermore, group sizes may be prohibitively small given the number of parameters that need to be estimated. And if we consider other modeling contexts outside of SEM, it is exceedingly rare to interact every covariate with a moderator.\nIn general though, we may very well be interested in a specific group difference on some latent variable, possibly controlling for other effects in some fashion. It is far simpler to specify such a model as above by regressing the latent variable on the group indicator as in the demonstration above, and it is a notably simpler model as well."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#supplemental-measurement-invariance",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#supplemental-measurement-invariance",
    "title": "Comparisons of the Unseen",
    "section": "Supplemental: Measurement invariance",
    "text": "Supplemental: Measurement invariance\nAs a final note, in some cases we are instead looking for similarities across groups among the latent constructs, rather than differences. This is especially the case in scale development, where one would like a measure to be consistent across groups of individuals (e.g. sex, age, race, etc.).\nAside from general problems of ‘accepting the null hypothesis’, the basic idea is to test a restricted model (e.g. loadings, intercepts, etc. are equal) vs. the less restrictive one that assumes the differences across groups exist, and if the general fit of the models is not appreciably different, then one can claim equivalence across groups. As a starting point, we assume configural equivalence, or in other words, that the factor structure is the same. There is no point in testing measurement equivalence if there is not a similar factor structure. The first more restricted model is that the loadings are equivalent. The next is that observed variable intercepts are equivalent, followed by latent variable means, and finally residual variances/covariances.\nI find in consulting and in published reports that researchers think that because they are interested in group differences that they are required to take a measurement invariance approach. This is not the case at all, as our previous models have shown. However, below is a demonstration using semTools. The package used to have a simple function that did exactly what most users want in a way easier than any other SEM package I’ve come across. In an effort to add flexibility and accommodate other data scenarios, they’ve made it much more complicated to do the default scenario, and have unfortunately deprecated the simple approach. I demonstrate both below.\n\nhs_model_4 &lt;- ' \n  visual  =~ x1 + x2 + x3\n  verbal  =~ x4 + x5 + x6\n  speed   =~ x7 + x8 + x9 \n'\n\nsemTools::measurementInvariance(\n  model = hs_model_4, \n  data  = HolzingerSwineford1939, \n  group = \"school\"\n)\n\n\ncat(\"\nMeasurement invariance models:\n\nModel 1 : fit.configural\nModel 2 : fit.loadings\nModel 3 : fit.intercepts\nModel 4 : fit.means\n\nChi-Squared Difference Test\n\n               Df    AIC    BIC  Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nfit.configural 48 7484.4 7706.8 115.85                                  \nfit.loadings   54 7480.6 7680.8 124.04      8.192       6     0.2244    \nfit.intercepts 60 7508.6 7686.6 164.10     40.059       6  4.435e-07 ***\nfit.means      63 7543.1 7710.0 204.61     40.502       3  8.338e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nFit measures:\n\n                 cfi rmsea cfi.delta rmsea.delta\nfit.configural 0.923 0.097        NA          NA\nfit.loadings   0.921 0.093     0.002       0.004\nfit.intercepts 0.882 0.107     0.038       0.015\nfit.means      0.840 0.122     0.042       0.015\n    \")\n\n\nMeasurement invariance models:\n\nModel 1 : fit.configural\nModel 2 : fit.loadings\nModel 3 : fit.intercepts\nModel 4 : fit.means\n\nChi-Squared Difference Test\n\n               Df    AIC    BIC  Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nfit.configural 48 7484.4 7706.8 115.85                                  \nfit.loadings   54 7480.6 7680.8 124.04      8.192       6     0.2244    \nfit.intercepts 60 7508.6 7686.6 164.10     40.059       6  4.435e-07 ***\nfit.means      63 7543.1 7710.0 204.61     40.502       3  8.338e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nFit measures:\n\n                 cfi rmsea cfi.delta rmsea.delta\nfit.configural 0.923 0.097        NA          NA\nfit.loadings   0.921 0.093     0.002       0.004\nfit.intercepts 0.882 0.107     0.038       0.015\nfit.means      0.840 0.122     0.042       0.015\n    \n\n\nNow for the new approach. From a single line of code, we now have to do the following to produce the same result. Great if you need that additional functionality, not so much if you don’t. If you look at the visual latent variable intercepts model , their difference would equal that seen in models 4d/e.\n\ntest.seq &lt;- c(\"loadings\",\"intercepts\",\"means\",\"residuals\")\n\nmeq.list &lt;- list()\n\nfor (i in 0:length(test.seq)) {\n  if (i == 0L) {\n    meq.label &lt;- \"configural\"\n    group.equal &lt;- \"\"\n  } else {\n    meq.label &lt;- test.seq[i]\n    group.equal &lt;- test.seq[1:i]\n  }\n  \n  meq.list[[meq.label]] &lt;- \n    semTools::measEq.syntax(\n      configural.model = hs_model_baseline,\n      data = lavaan::HolzingerSwineford1939,\n      ID.fac = \"auto.fix.first\",\n      group = \"school\",\n      group.equal = group.equal,\n      return.fit = TRUE\n  )\n}\n\nsemTools::compareFit(meq.list)\n\nThe following lavaan models were compared:\n    meq.list.configural\n    meq.list.loadings\n    meq.list.intercepts\n    meq.list.means\n    meq.list.residuals\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\nThis last one just compares the means. We see that assuming no group difference results in a worse model all around.\n\n# means_only\ntest.seq &lt;- c(\"means\")\n\nmeq.list &lt;- list()\n\nfor (i in 0:length(test.seq)) {\n  if (i == 0L) {\n    meq.label &lt;- \"configural\"\n    group.equal &lt;- \"\"\n  } else {\n    meq.label &lt;- test.seq[i]\n    group.equal &lt;- test.seq[1:i]\n  }\n  \n  meq.list[[meq.label]] &lt;- \n    semTools::measEq.syntax(\n      configural.model = hs_model_baseline,\n      data = lavaan::HolzingerSwineford1939,\n      ID.fac = \"auto.fix.first\",\n      group = \"school\",\n      group.equal = group.equal,\n      return.fit = TRUE\n  )\n}\n\nsemTools::compareFit(meq.list)\n\nThe following lavaan models were compared:\n    meq.list.configural\n    meq.list.means\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page."
  },
  {
    "objectID": "posts/2019-08-05-comparing-latent-variables/index.html#footnotes",
    "href": "posts/2019-08-05-comparing-latent-variables/index.html#footnotes",
    "title": "Comparisons of the Unseen",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that lavaan allows one to take this two step approach while estimating proper standard errors given the measurement error associated with the latent variable scores. The function is fsr, but as of this writing, it is undergoing development, has been hidden from the user, and was not working.↩︎"
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html",
    "href": "posts/2019-10-20-big-mixed-models/index.html",
    "title": "Mixed Models for Big Data",
    "section": "",
    "text": "Last updated January 02, 2025. Timings based on original date of posting.\nNOTE: When redoing my website, some of the results were not saved as needed, so it may be possible to see a mismatch with printed results vs. text description. It was also easier to run bam in parallel, but nowadays this requires openmp which likely won’t be recognized even if you have it installed. As of 2023, I have some packages work with it (data.table), and others not (mgcv)."
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#introduction",
    "href": "posts/2019-10-20-big-mixed-models/index.html#introduction",
    "title": "Mixed Models for Big Data",
    "section": "Introduction",
    "text": "Introduction\nWith mixed models, it is easy to run into data that is larger in size than some more typical data scenarios. Consider a cross-sectional data set with 200 individuals. This is fairly small data. Now, if we observe them each five times, as in a longitudinal setting, we suddenly have 1000 observations. There may be less than 200 countries in the world, but if we survey 100s or 1000s of people in many of them, we suddenly have a notable data set size, and still would potentially like to model a country-level random effect. What are our options when dealing with possibly gigabytes of data?\nThis post will demonstrate an approach that can be used with potentially millions of data points, multiple random effects, and possibly other complexities. First we’ll demonstrate how to get typical mixed model results using the approach used for generalized additive models. We’ll compare the output of the GAM, lme4, and even fully Bayesian mixed models. Then we’ll show some timings to compare the speed of the different approaches of common tools, and summarize some findings from other places.\n[Background required:\nFor the following you should have familiarity with mixed models. Knowledge of the lme4 package would be useful but isn’t required. Likewise, knowledge of generalized additive models and mgcv would be helpful, but I don’t think it’s required to follow the demonstration.]{.aside}"
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#r-packages-for-mixed-models-with-large-data",
    "href": "posts/2019-10-20-big-mixed-models/index.html#r-packages-for-mixed-models-with-large-data",
    "title": "Mixed Models for Big Data",
    "section": "R Packages for Mixed Models with Large Data",
    "text": "R Packages for Mixed Models with Large Data\nWhile many tools abound to conduct mixed models for larger data sizes, their limitations can be found pretty quickly. R’s lme4 is a standard, but powerful mixed model tool. More to the point, it is computationally efficient, such that it can handle very large sample sizes for simpler mixed models. For linear mixed models this can include hundreds of thousands of observations with possibly multiple random effects, still running on a basic laptop. For such models, it’s still largely the tool of choice, and its approach has even been copied/ported into other statistical packages.\nWe’ll first create some data to model. This is just a simple random intercepts setting.\n\nset.seed(12358)\nN = 1e6                                  # total sample size\nn_groups = 1000                          # number of groups\ng = rep(1:n_groups, e = N/n_groups)      # the group identifier\n\nx = rnorm(N)                             # an observation level continuous variable\nb = rbinom(n_groups, size = 1, prob=.5)  # a cluster level categorical variable\nb = b[g]\n\nsd_g = .5     # standard deviation for the random effect\nsigma = 1     # standard deviation for the observation\n\nre0 = rnorm(n_groups, sd = sd_g)  # random effects\nre  = re0[g]\n\nlp = 0 + .5*x + .25*b + re        # linear predictor \n\ny = rnorm(N, mean = lp, sd = sigma)               # create a continuous target variable\ny_bin = rbinom(N, size = 1, prob = plogis(lp))    # create a binary target variable\n\nd = tibble(x, b, y, y_bin, g = factor(g))\n\n\nLet’s take a look at the data first.\n\n\n\n\n\nindex\nx\nb\ny\ny_bin\ng\n\n\n\n\n1\n-0.378\n0\n-0.278\n1\n1\n\n\n2\n-0.812\n0\n-0.343\n0\n1\n\n\n3\n0.218\n0\n-0.810\n1\n1\n\n\n4\n1.529\n0\n0.465\n1\n1\n\n\n5\n-1.877\n0\n-1.570\n0\n1\n\n\n6\n-0.427\n0\n0.047\n0\n1\n\n\n999995\n-1.181\n1\n-1.111\n0\n1000\n\n\n999996\n-1.487\n1\n0.563\n1\n1000\n\n\n999997\n-1.236\n1\n-0.603\n0\n1000\n\n\n999998\n0.412\n1\n0.736\n0\n1000\n\n\n999999\n-0.644\n1\n-1.257\n1\n1000\n\n\n1000000\n0.409\n1\n0.520\n1\n1000\n\n\n\n\n\n\n\nNow with the data in place, let’s try lme4 to model the continuous outcome.\nThe time to focus on is elapsed, which is the number of seconds the function took to run.\n\nlibrary(lme4)\n\nsystem.time({\n  mixed_big = lmer(y ~ x + b + (1|g))\n})\n\n   user  system elapsed \n  3.573   0.250   3.902 \n\nsummary(mixed_big, cor = FALSE)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + b + (1 | g)\n\nREML criterion at convergence: 2841256\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6066 -0.6743 -0.0004  0.6744  5.0367 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n g        (Intercept) 0.2509   0.5009  \n Residual             0.9978   0.9989  \nNumber of obs: 1000000, groups:  g, 1000\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 0.0364754  0.0223332   1.633\nx           0.5017616  0.0009987 502.409\nb           0.1904534  0.0317430   6.000\n\n\nThis is great! We just ran a mixed model for 1,000,000 observations and 1,000 groups for our random effect in just a few seconds.\nBut one problem comes as soon as you move to the generalized mixed model, e.g. having a binary outcome, or include additional complexity while still dealing with large data. The following is essentially the same model, but for a binary outcome.\n\nsystem.time({\n  mixed_big_glmm = glmer(y_bin ~ x + b + (1|g), family = binomial)\n})\n\n   user  system elapsed \n 58.090   6.462  65.693 \n\n\nTo begin with, you shouldn’t be worried about models taking a few minutes to run, or even a couple hours. Once you have your model(s) squared away, the testing of which can be done on a smaller sample of the data set, there is no need to repeatedly run it. But in this case we had a greater than 15 fold increase in time for a very simple data scenario. So it’s good to have options when you need them. Let’s turn to those."
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#additive-models-as-mixed-models",
    "href": "posts/2019-10-20-big-mixed-models/index.html#additive-models-as-mixed-models",
    "title": "Mixed Models for Big Data",
    "section": "Additive Models as Mixed Models",
    "text": "Additive Models as Mixed Models\nSimon Wood’s wonderful work on generalized additive models (GAM) and the mgcv package make it one of the better modeling tools in the R kingdom. As his text(S. N. Wood 2017) and other work shows, additive models constructed be posited in a similar way as mixed models, and he exploits this by providing numerous ways to include and explore random effects in the GAM approach. One key difference between the GAM and a standard linear mixed model approach is the way parameters are estimated. For the GAM, the random effects are estimated as are other fixed effect coefficients. Those random effects are penalized, in a similar way as L2/ridge regression. The ‘fixed effects’ are not penalized, and so that part is basically just a generalized linear model. As we will see though, the results will be nearly the same between mgcv and lme4.\nThe following demonstrates the link between the approaches by showing a model that includes a random intercept and slope. We will use the standard mgcv approach for specifying a smooth term, but alternatives are shown for those familiar with the package.\nIf you just use coef on the following gam objects, you will see that the random effects are lumped in with the other estimated coefficients.\n\nlibrary(lme4)\nlibrary(mgcv)\n\nmixed_model = lmer(\n  Reaction ~ Days + (1 | Subject) + (0 + Days | Subject),\n  data = sleepstudy\n)\n\nga_model = gam(\n  Reaction ~  Days + s(Subject, bs = 're') + s(Days, Subject, bs = 're'),\n  data = sleepstudy,\n  method = 'REML'\n)\n\n# Using gamm and gamm4 for the same model\n# ga_model = gamm(\n#   Reaction ~  Days ,\n#   random = list(Subject = ~ 0 + Days),\n#   data = sleepstudy,\n#   method = 'REML'\n# )\n# \n# ga_model = gamm4::gamm4(\n#   Reaction ~  Days,\n#   random =  ~ (Days||Subject),\n#   data = sleepstudy,\n#   REML = TRUE\n# )\n\nNote that we use s to denote a smooth term in the parlance of additive models, and the bs = 're' specifies that we want it as a random effect (as opposed to a spline or other basis function). The second smooth term s(Days, Subject, bs = 're') denotes random coefficients for the Days covariate.\nAs shown, one could use the gamm function for the nlme style, or Wood’s gamm4 package to use the lme4 syntax. These alternate approaches allow for more flexibility in some ways, but will not be useful to us for big data.\n\nComparison of GAM to the Mixed Model\nAside from the syntax, the underlying model between the two is the same, and the following shows that we obtain the same results for both lme4 and mgcv.\n\nsummary(mixed_model, cor = FALSE)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9626 -0.4625  0.0204  0.4653  5.1860 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n Subject   (Intercept) 627.57   25.051  \n Subject.1 Days         35.86    5.988  \n Residual              653.58   25.565  \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.885  36.513\nDays          10.467      1.560   6.712\n\nsummary(ga_model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nReaction ~ Days + s(Subject, bs = \"re\") + s(Days, Subject, bs = \"re\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  251.405      6.885  36.513  &lt; 2e-16 ***\nDays          10.467      1.560   6.712 3.67e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                  edf Ref.df      F  p-value    \ns(Subject)      12.94     17  89.29 1.09e-06 ***\ns(Days,Subject) 14.41     17 104.56  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.794   Deviance explained = 82.7%\n-REML = 871.83  Scale est. = 653.58    n = 180\n\n\nI don’t want to go into the details of the printout for mgcv, but it is worth noting that the parametric part is equivalent to the fixed effects portion of the lme4 output. Likewise the smooth terms output is related to the random effects, but we’ll extract them in a manner more suited to typical mixed model output instead. So let’s compare the variance components, and get them ready for later comparison to bam results. Note, I’ve use several packages for mixed models, so I created a package called mixedup to provide tidier and consistent output, and which is more similar to lme4. I note the corresponding mgcv function where appropriate.\nThe mixedup package is available on GitHub.\n\nlibrary(mixedup)\n\n# extract just the fixed effects for later.\nmixed_fe = extract_fixed_effects(mixed_model, digits = 5)\ngam_fe   = extract_fixed_effects(ga_model, digits = 5)  # coefs with se and confidence interval\n\n# variance components\nlmer_vcov = extract_vc(mixed_model, digits = 5)\ngam_vcov  = extract_vc(ga_model, digits = 5)    # cleaner gam.vcomp\n\n\n\n\nLME Result\n\n\ngroup\neffect\nvariance\nsd\nsd_2.5\nsd_97.5\nvar_prop\n\n\n\n\nSubject\nIntercept\n627.569\n25.051\n15.259\n37.786\n0.477\n\n\nSubject.1\nDays\n35.858\n5.988\n3.964\n8.769\n0.027\n\n\nResidual\nNA\n653.584\n25.565\n22.881\n28.788\n0.496\n\n\n\n\n\n\n\n\n\n\nGAM Result\n\n\ngroup\neffect\nvariance\nsd\nsd_2.5\nsd_97.5\nvar_prop\n\n\n\n\nSubject\nIntercept\n627.571\n25.051\n16.085\n39.015\n0.477\n\n\nSubject\nDays\n35.858\n5.988\n4.025\n8.908\n0.027\n\n\nResidual\nNA\n653.582\n25.565\n22.792\n28.676\n0.496\n\n\n\n\n\n\n\nThe penalty parameter in the GAM model is inversely related to the variance estimate of the random effects. See this demo.\n\n\nThe bam approach\nFor large data, mgcv provides the bam function. For this small data setting we don’t really need it, but we can establish that we would get similar results using it without having to wait. We will see the benefits when we apply bam to large data later. None of our syntax changes, just the function.\n\nba_model = bam(\n  Reaction ~  Days + s(Subject, bs='re') + s(Days, Subject, bs='re'), \n  data = sleepstudy\n)\n\nbam_fe   = extract_fixed_effects(ba_model, digits = 5)\nbam_vcov = extract_vc(ba_model, digits = 5)\n\nHow does it work? The function uses a parallelized approach where possible, essentially working on subsets of the model matrices simultaneously. Details can be found in the references[Li and Wood (2019)](S. N. Wood, Goude, and Shaw 2015a)(S. N. Wood et al. 2017), but basically mgcv parallelizes the parts that can be, and additionally provides an option to discretize the data to work with the minimal information necessary to produce viable estimates. The following uses the discrete option. As there isn’t really anything to discretize with so little data, this is just to demonstrate the syntax.\n\nba_d_model = bam(\n  Reaction ~  Days + s(Subject, bs='re') + s(Days, Subject, bs='re'), \n  data = sleepstudy,\n  discrete = TRUE\n)\n\nbam_d_fe   = extract_fixed_effects(ba_d_model, digits = 5)\nbam_d_vcov = extract_vc(ba_d_model, digits = 5)\n\n\n\nFixed effects comparison\nWe start by comparing the fixed effects of all models run thus far. No surprises here, the results are the same.\n\n\n\nFixed Effects Estimates\n\n\nterm\nmixed\ngam\nbam\nbam_d\n\n\n\n\nIntercept\n251.405\n251.405\n251.405\n251.405\n\n\nDays\n10.467\n10.467\n10.467\n10.467\n\n\n\n\n\n\n\nLet’s examine the standard errors. Note that there are options for the GAM models for standard error estimation, including a Bayesian one. For more details, see ?gamObject, but I will offer the summary:\n\nVe\nfrequentist estimated covariance matrix for the parameter estimators. Particularly useful for testing whether terms are zero. Not so useful for CI’s as smooths are usually biased.\n\n\nVp\nestimated covariance matrix for the parameters. This is a Bayesian posterior covariance matrix that results from adopting a particular Bayesian model of the smoothing process. Particularly useful for creating credible/confidence intervals.\n\n\nVc\nUnder ML or REML smoothing parameter estimation it is possible to correct the covariance matrix Vp for smoothing parameter uncertainty. This is the corrected version.\nWe will use the Bayesian estimates (Vp), but for this setting there are no appreciable differences. I expand the digits to show they are in fact different to some decimal place.\n\n\n\nFixed Effects Standard Errors\n\n\nterm\nmixed\ngam\nbam\nbam_d\n\n\n\n\nIntercept\n6.88538\n6.88540\n6.88538\n6.88538\n\n\nDays\n1.55957\n1.55956\n1.55957\n1.55957\n\n\n\n\n\n\n\n\n\n\nVariance components comparison\nNow we move to the variance component estimates. Reported are the standard deviations for subject level random effects for intercept, Days coefficient, and residual.\n\n\n\nVariance Components Estimates\n\n\n\nIntercept\nDays\nResidual\n\n\n\n\nmixed\n25.051\n5.988\n25.565\n\n\ngam\n25.051\n5.988\n25.565\n\n\nbam\n25.051\n5.988\n25.565\n\n\nbam_d\n25.051\n5.988\n25.565\n\n\n\n\n\n\n\nWe can also look at their interval estimates. We use the profile likelihood for the lme4 mixed model. In this case we can see slightly wider and somewhat different boundary estimates for the variance components, but not too dissimilar.\n\n\n\nInterval Estimates for Variance Components\n\n\nModel\ngroup\neffect\nvariance\nsd\nsd_2.5\nsd_97.5\nwidth\n\n\n\n\nmixed\nSubject\nIntercept\n627.5690\n25.0513\n15.2586\n37.7865\n22.5278\n\n\nDays\n35.8584\n5.9882\n3.9641\n8.7692\n4.8051\n\n\nResidual\nNA\n653.5835\n25.5653\n22.8805\n28.7876\n5.9071\n\n\ngam\nSubject\nIntercept\n627.5712\n25.0514\n16.0854\n39.0150\n22.9297\n\n\nDays\n35.8580\n5.9882\n4.0252\n8.9083\n4.8830\n\n\nResidual\nNA\n653.5822\n25.5652\n22.7918\n28.6763\n5.8845\n\n\nbam\nSubject\nIntercept\n627.5691\n25.0513\n16.0853\n39.0150\n22.9297\n\n\nDays\n35.8582\n5.9882\n4.0253\n8.9083\n4.8830\n\n\nResidual\nNA\n653.5838\n25.5653\n22.7918\n28.6763\n5.8845\n\n\nbam_d\nSubject\nIntercept\n627.5691\n25.0513\n16.0853\n39.0150\n22.9297\n\n\nDays\n35.8582\n5.9882\n4.0253\n8.9083\n4.8830\n\n\nResidual\nNA\n653.5838\n25.5653\n22.7918\n28.6763\n5.8845\n\n\n\n\n\n\n\n\n\nEstimated random effects\nNow let’s look at the random effect estimates.\n\nmixed_re_init = extract_ranef(mixed_model, digits = 5)\ngam_re_init   = extract_ranef(ga_model, digits = 5)\nbam_re_init   = extract_ranef(ba_model, digits = 5)\nbam_d_re_init = extract_ranef(ba_d_model, digits = 5)\n\nWe’ll start with the random effects for the intercept. To several decimal places, we start to see differences, so again we know they aren’t doing exactly the same thing, but they are coming to the same conclusion.\n\n\n\nEstimated Random Intercepts\n\n\nbam_d_re_init\nbam_re_init\ngam_re_init\nmixed_re_init\n\n\n\n\n1.51270\n1.51270\n1.51272\n1.51266\n\n\n-40.37390\n-40.37390\n-40.37397\n-40.37387\n\n\n-39.18104\n-39.18104\n-39.18111\n-39.18103\n\n\n24.51890\n24.51890\n24.51893\n24.51892\n\n\n22.91443\n22.91443\n22.91446\n22.91445\n\n\n9.22197\n9.22197\n9.22199\n9.22198\n\n\n17.15612\n17.15612\n17.15614\n17.15612\n\n\n-7.45173\n-7.45173\n-7.45174\n-7.45174\n\n\n0.57872\n0.57872\n0.57870\n0.57876\n\n\n34.76793\n34.76793\n34.76800\n34.76790\n\n\n-25.75432\n-25.75432\n-25.75436\n-25.75433\n\n\n-13.86504\n-13.86504\n-13.86504\n-13.86506\n\n\n4.91598\n4.91598\n4.91598\n4.91599\n\n\n20.92904\n20.92904\n20.92908\n20.92903\n\n\n3.25865\n3.25865\n3.25865\n3.25864\n\n\n-26.47583\n-26.47583\n-26.47585\n-26.47585\n\n\n0.90565\n0.90565\n0.90565\n0.90565\n\n\n12.42176\n12.42176\n12.42178\n12.42175\n\n\n\n\n\n\n\nRandom effects for the Days coefficient.\n\n\n\nEstimated Random Intercepts\n\n\nbam_d_re_init\nbam_re_init\ngam_re_init\nmixed_re_init\n\n\n\n\n9.32349\n9.32349\n9.32348\n9.32350\n\n\n-8.59917\n-8.59917\n-8.59916\n-8.59918\n\n\n-5.38779\n-5.38779\n-5.38778\n-5.38779\n\n\n-4.96865\n-4.96865\n-4.96865\n-4.96865\n\n\n-3.19393\n-3.19393\n-3.19394\n-3.19394\n\n\n-0.30849\n-0.30849\n-0.30850\n-0.30849\n\n\n-0.28721\n-0.28721\n-0.28721\n-0.28721\n\n\n1.11599\n1.11599\n1.11599\n1.11599\n\n\n-10.90597\n-10.90597\n-10.90596\n-10.90598\n\n\n8.62762\n8.62762\n8.62760\n8.62762\n\n\n1.28069\n1.28069\n1.28069\n1.28069\n\n\n6.75640\n6.75640\n6.75640\n6.75641\n\n\n-3.07513\n-3.07513\n-3.07513\n-3.07514\n\n\n3.51221\n3.51221\n3.51220\n3.51221\n\n\n0.87305\n0.87305\n0.87305\n0.87305\n\n\n4.98379\n4.98379\n4.98379\n4.98379\n\n\n-1.00529\n-1.00529\n-1.00529\n-1.00529\n\n\n1.25840\n1.25840\n1.25840\n1.25840\n\n\n\n\n\n\n\nStandard errors for the random effects. In the balanced design these are essentially constant across clusters. We can see that the Bayesian estimates from mgcv reflect greater uncertainty.\nThe bam results may actually be slightly different for some clusters.\n\n\n\n\nStandard Errors of the Random Coefficients\n\n\nModel\nIntercepts\nDays\n\n\n\n\nmixed\n12.239\n2.335\n\n\ngam\n13.279\n2.673\n\n\nbam\n13.279\n2.673\n\n\nbam_discrete\n13.279\n2.673\n\n\n\n\n\n\n\n\n\nComparisons to Bayesian Estimates\nAs we have noted, one of the differences between lme4 and mgcv output is that the default uncertainty estimates for the GAM are Bayesian. As such, it might be interesting to compare these to a fully Bayes approach. We’ll use rstanarm, which uses the lme4 style syntax.\nFor those familiar with Bayesian models, the Stan group provides a vignette with information about the priors in this model and comparisons to lme4 and gamm4.\n\nlibrary(rstanarm)\nbayes = stan_lmer(Reaction ~ Days + (1|Subject) + (0 + Days|Subject), \n                  data = sleepstudy,\n                  cores = 4)\n\nbayes_fe = extract_fixed_effects(bayes)\nbayes_vc = extract_vc(bayes)\nbayes_re = extract_random_effects(bayes)\n\n\n\n\nBayesian fixed effects\n\n\nterm\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nIntercept\n251.276\n7.054\n237.681\n265.017\n\n\nDays\n10.415\n1.695\n7.156\n13.757\n\n\n\n\n\n\n\n\nBayesian variance components\n\n\ngroup\neffect\nvariance\nsd\nsd_2.5\nsd_97.5\nvar_prop\n\n\n\n\nSubject\nIntercept\n697.634\n26.413\n15.909\n39.397\n0.497\n\n\nSubject\nDays\n43.464\n6.593\n4.253\n9.501\n0.031\n\n\nResidual\nNA\n662.481\n25.739\nNA\nNA\n0.472\n\n\n\n\n\n\n\n\nBayesian random effects\n\n\neffect\ngroup\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nIntercept\n308\n1.421\n12.347\n-22.778\n25.621\n\n\nIntercept\n309\n-39.502\n12.347\n-63.701\n-15.302\n\n\nIntercept\n310\n-38.157\n12.347\n-62.356\n-13.957\n\n\nIntercept\n330\n23.994\n12.347\n-0.205\n48.194\n\n\nIntercept\n331\n22.666\n12.347\n-1.533\n46.866\n\n\nIntercept\n332\n9.140\n12.347\n-15.060\n33.339\n\n\nIntercept\n333\n16.476\n12.347\n-7.724\n40.676\n\n\nIntercept\n334\n-7.232\n12.347\n-31.432\n16.968\n\n\nIntercept\n335\n0.946\n12.347\n-23.254\n25.145\n\n\nIntercept\n337\n33.940\n12.347\n9.741\n58.140\n\n\nIntercept\n349\n-25.148\n12.347\n-49.347\n-0.948\n\n\nIntercept\n350\n-13.861\n12.347\n-38.061\n10.338\n\n\nIntercept\n351\n5.064\n12.347\n-19.136\n29.263\n\n\nIntercept\n352\n20.650\n12.347\n-3.550\n44.849\n\n\nIntercept\n369\n3.039\n12.347\n-21.161\n27.238\n\n\nIntercept\n370\n-25.923\n12.347\n-50.123\n-1.724\n\n\nIntercept\n371\n1.149\n12.347\n-23.051\n25.348\n\n\nIntercept\n372\n12.153\n12.347\n-12.047\n36.353\n\n\nDays\n308\n9.406\n2.417\n4.668\n14.143\n\n\nDays\n309\n-8.680\n2.417\n-13.417\n-3.942\n\n\nDays\n310\n-5.383\n2.417\n-10.120\n-0.645\n\n\nDays\n330\n-4.862\n2.417\n-9.599\n-0.124\n\n\nDays\n331\n-3.052\n2.417\n-7.790\n1.685\n\n\nDays\n332\n-0.274\n2.417\n-5.011\n4.464\n\n\nDays\n333\n-0.119\n2.417\n-4.857\n4.618\n\n\nDays\n334\n1.189\n2.417\n-3.549\n5.927\n\n\nDays\n335\n-10.870\n2.417\n-15.607\n-6.132\n\n\nDays\n337\n8.819\n2.417\n4.081\n13.557\n\n\nDays\n349\n1.280\n2.417\n-3.457\n6.018\n\n\nDays\n350\n6.778\n2.417\n2.040\n11.515\n\n\nDays\n351\n-3.036\n2.417\n-7.773\n1.702\n\n\nDays\n352\n3.589\n2.417\n-1.149\n8.326\n\n\nDays\n369\n0.940\n2.417\n-3.797\n5.678\n\n\nDays\n370\n4.968\n2.417\n0.231\n9.706\n\n\nDays\n371\n-0.979\n2.417\n-5.717\n3.758\n\n\nDays\n372\n1.359\n2.417\n-3.378\n6.097\n\n\n\n\n\n\n\n\nRandom effect standard errors\n\n\neffect\ngroup\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nIntercept\n308\n1.421\n12.347\n-22.778\n25.621\n\n\nIntercept\n309\n-39.502\n12.347\n-63.701\n-15.302\n\n\nIntercept\n310\n-38.157\n12.347\n-62.356\n-13.957\n\n\nIntercept\n330\n23.994\n12.347\n-0.205\n48.194\n\n\nIntercept\n331\n22.666\n12.347\n-1.533\n46.866\n\n\nIntercept\n332\n9.140\n12.347\n-15.060\n33.339\n\n\nIntercept\n333\n16.476\n12.347\n-7.724\n40.676\n\n\nIntercept\n334\n-7.232\n12.347\n-31.432\n16.968\n\n\nIntercept\n335\n0.946\n12.347\n-23.254\n25.145\n\n\nIntercept\n337\n33.940\n12.347\n9.741\n58.140\n\n\nIntercept\n349\n-25.148\n12.347\n-49.347\n-0.948\n\n\nIntercept\n350\n-13.861\n12.347\n-38.061\n10.338\n\n\nIntercept\n351\n5.064\n12.347\n-19.136\n29.263\n\n\nIntercept\n352\n20.650\n12.347\n-3.550\n44.849\n\n\nIntercept\n369\n3.039\n12.347\n-21.161\n27.238\n\n\nIntercept\n370\n-25.923\n12.347\n-50.123\n-1.724\n\n\nIntercept\n371\n1.149\n12.347\n-23.051\n25.348\n\n\nIntercept\n372\n12.153\n12.347\n-12.047\n36.353\n\n\nDays\n308\n9.406\n2.417\n4.668\n14.143\n\n\nDays\n309\n-8.680\n2.417\n-13.417\n-3.942\n\n\nDays\n310\n-5.383\n2.417\n-10.120\n-0.645\n\n\nDays\n330\n-4.862\n2.417\n-9.599\n-0.124\n\n\nDays\n331\n-3.052\n2.417\n-7.790\n1.685\n\n\nDays\n332\n-0.274\n2.417\n-5.011\n4.464\n\n\nDays\n333\n-0.119\n2.417\n-4.857\n4.618\n\n\nDays\n334\n1.189\n2.417\n-3.549\n5.927\n\n\nDays\n335\n-10.870\n2.417\n-15.607\n-6.132\n\n\nDays\n337\n8.819\n2.417\n4.081\n13.557\n\n\nDays\n349\n1.280\n2.417\n-3.457\n6.018\n\n\nDays\n350\n6.778\n2.417\n2.040\n11.515\n\n\nDays\n351\n-3.036\n2.417\n-7.773\n1.702\n\n\nDays\n352\n3.589\n2.417\n-1.149\n8.326\n\n\nDays\n369\n0.940\n2.417\n-3.797\n5.678\n\n\nDays\n370\n4.968\n2.417\n0.231\n9.706\n\n\nDays\n371\n-0.979\n2.417\n-5.717\n3.758\n\n\nDays\n372\n1.359\n2.417\n-3.378\n6.097\n\n\n\n\n\n\n\nWe can see that the mgcv estimates for standard errors of the random effects are close to the average standard errors from the fully Bayesian approach. For the Bayesian result we have 12.347 and 2.417 for Intercept and Days coefficient respectively, while for mgcv this is 13.27913 and 2.67273."
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#back-to-the-initial-problem",
    "href": "posts/2019-10-20-big-mixed-models/index.html#back-to-the-initial-problem",
    "title": "Mixed Models for Big Data",
    "section": "Back to the initial problem",
    "text": "Back to the initial problem\nSo we’ve established that both default gam and bam functions are providing what we want. However, the reason we’re here is to use demonstrate the speed gain we’ll get with big data using mgcv for mixed models. So let’s return to the binary outcome example that took over a minute for lme4 to run.\n\nsystem.time({\n  bam_big &lt;- bam(\n    y_bin ~ x + b + s(g, bs='re'), \n    data = d,\n    nthreads = 8,\n    family = binomial\n  )\n})\n\n\n\n\n    user   system  elapsed \n8164.817  120.570 1298.584 \n\n\nThat didn’t actually improve our situation, and was much worse in time- more than 20 minutes! Remember though, that the mgcv approach has to estimate all those random effect coefficients, while lme4 is able to take advantage of design for mixed models among other things.\nHowever, even here we haven’t used all our secret weapons. Another option with bam works on a modified data set using binned/rounded values for continuous covariates, and working with only the minimum data necessary to estimate the coefficients(S. N. Wood, Goude, and Shaw 2015a). With large enough data, as is the case here, the estimated parameters might not be different at all, while the efficiency gains could be tremendous. Let’s add discrete = TRUE and see what happens.\nWe just need the distinct set of values after rounding.\n\nsystem.time({\n  bam_big_d &lt;- bam(\n    y_bin ~ x + b + s(g, bs='re'), \n    data = d,\n    nthreads = 8,\n    family = binomial, \n    discrete = TRUE\n  )\n})\n\n\n\n\n   user  system elapsed \n 43.542   2.649  12.387 \n    \n\n\nWow! That was almost as fast as lme4 with the linear mixed model! Let’s check the results. We’ll start with the fixed effects. I add some digits to the result so we can see the very slight differences.\n\n\n\nFixed Effects\n\n\nModel\nterm\nvalue\nse\nlower_2.5\nupper_97.5\n\n\n\n\nTrue\n(Intercept)\n0.00000000\nNA\nNA\nNA\n\n\nTrue\nx\n0.50000000\nNA\nNA\nNA\n\n\nTrue\nb\n0.25000000\nNA\nNA\nNA\n\n\nbam_big\nIntercept\n0.03730496\n0.02257804\n-0.00694725\n0.08155716\n\n\nbam_big\nx\n0.50087021\n0.00223195\n0.49649565\n0.50524476\n\n\nbam_big\nb\n0.19083417\n0.03209232\n0.12793430\n0.25373405\n\n\nbam_big_d\nIntercept\n0.03730460\n0.02257842\n-0.00694835\n0.08155755\n\n\nbam_big_d\nx\n0.50087441\n0.00223195\n0.49649987\n0.50524895\n\n\nbam_big_d\nb\n0.19083450\n0.03209286\n0.12793357\n0.25373543\n\n\nmixed_big_glmm\nIntercept\n0.03734700\n0.02254374\n-0.00683792\n0.08153193\n\n\nmixed_big_glmm\nx\n0.50136881\n0.00223342\n0.49699138\n0.50574624\n\n\nmixed_big_glmm\nb\n0.19104484\n0.03205912\n0.12821011\n0.25387957\n\n\n\n\n\n\n\nNow for the variance components.\n\n\n\nVariance Components\n\n\nModel\nsd\nvariance\n\n\n\n\ntrue\n0.500\n0.250\n\n\nbam_big\n0.503\n0.253\n\n\nbam_big_d\n0.503\n0.253\n\n\nlme4\n0.503\n0.253\n\n\n\n\n\n\n\nAnd finally, let’s look at the estimated random effects for the first 5 clusters.\nJust a note, unless you have very many observations per cluster, you should not expect to get very close to the true values of the random effects except on average, which should serve as a caution for any 2-step approach one might undertake using the estimates. The rank correlations of the estimates vs. the true values in this example are 1.0.\n\n\n\nEstimated Random Effects\n\n\ncluster\ntrue\nbam_big\nbam_big_d\nlme4\n\n\n\n\n1\n0.0912404\n-0.036\n-0.036\n-0.036\n\n\n2\n0.1310768\n0.148\n0.148\n0.148\n\n\n3\n-0.0562572\n-0.142\n-0.143\n-0.143\n\n\n4\n0.6194238\n0.556\n0.556\n0.556\n\n\n5\n-0.5022289\n-0.415\n-0.415\n-0.415\n\n\n\n\n\n\n\nSo we’re getting what we should in general."
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#when-to-use-bam",
    "href": "posts/2019-10-20-big-mixed-models/index.html#when-to-use-bam",
    "title": "Mixed Models for Big Data",
    "section": "When to use bam",
    "text": "When to use bam\nThe following are some guidelines for when bam might be preferable compared to other mixed modeling tools. To help with this, I’ve conducted my own examinations on very large data sets of up to one million observations, and included timing results from other relevant studies, which will be presented here.\n\nLinear Mixed Models\nAs we’ll see, in general you’ll probably need very large data for bam to be preferred to lme4 for linear mixed models unless:\n\nYou have complicated structure that begins to bog down lme4\nYou want to add smooth terms1\nYou have memory issues\nYou have a computing setup that can take advantage of bam\n\nThe following shows some timings for lme4, glmmTMB, and mgcv for the linear mixed model case under a variety of settings with large data. In some sense, this is not exactly a fair comparison as mgcv parallelizes computations while lme4 and glmmTMB do not. However, this is also exactly the point of the demonstration - those who can, do. In general though, the lme4 advantage holds until around 500k observations. We can see that the main issue for bam is not so much the sample size, but the number of parameters to estimate.\nFor lme4, I set at least one argument to possibly improve speed/performance for both lme and glmm models, though this only shaved a few seconds for the largest sample size settings for the linear mixed model. For mgcv I only used 12 cores for parallelization so as to be similar to what is common on modern machines (8-12), but anyone with access to a better machine or cluster computing environment would see even more speed gain by utilizing additional cores, so I also looked at 16 cores. For glmmTMB, settings were left at defaults, as I’ve not come across any specific speed recommendations. See Brooks et al.(Brooks et al. 2017) (Brooks et al. (2017)) for more speed comparisons of glmmTMB, mgcv, lme4, and others, as well as the glmmTMB vignette.\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Mixed Models\nFor the generalized setting with binary, count, and other outcomes:\n\nlme4, at least at the time of this writing, will almost certainly start giving convergence warnings even in well-behaved data settings, and as such, will require tweaking to mitigate.\nglmmTMB is probably viable up to 100k and one or two random effects, but may generally be a slower option.\nUse mgcv for same reasons as with linear mixed models, but here it potentially becomes an advantage with as few as 100k.\n\n\n\n\n\n\n\n\n\n\nI have also done some timings on a local machine with as many as 10000 levels for one of the random effects, 1000 for the other, and 5 million observations. Depending on the computational setup, this could take 30 minutes for a linear mixed model and 24 cores, to 2-3 hours for a logistic mixed model using 12 cores.\n\n\nOther options\nWhen looking into mixed models for big data, you typically won’t find much in the way of options. I’ve seen some packages or offerings for some machine learning approaches like random forests2, but this doesn’t address the issue of large data. A Spark module provided by LinkedIn is available, photonML, but it’s not clear how easy it is to implement. Julia has recently made multithreading a viable option for any function. This is notable since Doug Bates, one of the lme4 authors, develops the MixedModels module for Julia. Should multithreading functionality be added, it could be a very powerful tool3.\nAmong proprietary options, SAS and Stata are the more commonly used tools. SAS PROC HPMIXED essentially uses the lme4 approach, but can be faster for well-behaved data. Stata, while commonly used for mixed models, is generally slower than the lme4 even for standard settings, and is likely prohibitively slow for settings above4.\nSAS uses disk rather than RAM for processing, so may be preferred for low RAM devices.\nHere is a summary of other timings of various tools for mixed models.\n\nMcCoach et al. 2018\nThese are the results from McCoach et al.(McCoach et al. 2018) with a standard linear mixed model. Sample size fixed at 10000, with a single grouping factor with only 50 levels. Models included five covariates each with a random slope. In the first five cases, a true variance parameter was set to zero, a situation lme4 handles well5. SAS is very speedy for such settings if data is well-behaved.\n\n\n\n\n\n\n\n\n\n\n\nBrooks et al. 2018 timing as a function of sample size\nBrooks et al. uses the Salamander data from the glmmTMB package. It has a single grouping factor for the random effect with 23 levels. Starting sample size is 644, which is then replicated to produce larger data. This is a negative binomial count model. In this particular setting glmmTMB has an advantage.\n\n\n\n\n\n\n\n\n\n\n\nBrooks et al. 2018 timing as a function of number of levels\nThis data is simulated based on models from the previous, and adds increasing numbers of (balanced) levels to the random effect. This shows a similar effect of the number of levels on mgcv as the simulation presented in this post, though they are not using the functionality of bam.\n\n\n\n\n\n\n\n\n\n\n\nglmmTMB timings\nAs previously noted, depending on the data, and whether the target is assumed gaussian or not, glmmTMB might be preferable. For the following, in the first case a small data set was replicated to create larger data, and in the second, a larger data set was sub-sampled6. The advantage is to glmmTMB in the first case, and lme4 in the second."
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#limitations",
    "href": "posts/2019-10-20-big-mixed-models/index.html#limitations",
    "title": "Mixed Models for Big Data",
    "section": "Limitations",
    "text": "Limitations\nThere are limitations to the use of the mgcv approach.\n\nThe number of parameters to estimate increases with the number of random effect levels, which may void any gains until very large data with complex models\nNo estimation of random effect correlations, e.g. between slopes and intercepts\n\nAll in all, these are pretty minor, and the last one likely will be remedied in a future release."
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#summary",
    "href": "posts/2019-10-20-big-mixed-models/index.html#summary",
    "title": "Mixed Models for Big Data",
    "section": "Summary",
    "text": "Summary\nThe take home point here is that you now have viable tools to run mixed models on even very large data with millions of observations. This doesn’t mean you won’t have to wait for it, especially for more complicated models, but you may even be able to run some of these on standard machines in reasonable times. The alternative estimation procedures may even make otherwise problematic models more feasible in smaller data settings. Good luck!"
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#supplemental",
    "href": "posts/2019-10-20-big-mixed-models/index.html#supplemental",
    "title": "Mixed Models for Big Data",
    "section": "Supplemental",
    "text": "Supplemental\n\nSimulation Settings\nI will set up a repo with the simulation code at some point and link it here. But the settings for the timings can be summarized as follows.\n\nLinear mixed model\nThe following are for the linear mixed model. N is the sample size, balanced refers to whether a random 75% sample was taken with proportion equivalent to the group index (first grouping variable for both 1 and 2 random effect settings), and tau_2 is zero if there is only one random effect, or refers to the standard deviation of the second random effect. Each of these settings was run 5 times, and the previous visualizations display the average timing of those.\n\n\n\n\n\nN\nbalanced\ntau_2\n\n\n\n\n1e+04\n0.75\n0.0\n\n\n1e+04\n0.75\n0.5\n\n\n1e+04\n1.00\n0.0\n\n\n1e+04\n1.00\n0.5\n\n\n5e+04\n0.75\n0.0\n\n\n5e+04\n0.75\n0.5\n\n\n5e+04\n1.00\n0.0\n\n\n5e+04\n1.00\n0.5\n\n\n1e+05\n0.75\n0.0\n\n\n1e+05\n0.75\n0.5\n\n\n1e+05\n1.00\n0.0\n\n\n1e+05\n1.00\n0.5\n\n\n5e+05\n0.75\n0.0\n\n\n5e+05\n0.75\n0.5\n\n\n5e+05\n1.00\n0.0\n\n\n5e+05\n1.00\n0.5\n\n\n1e+06\n0.75\n0.0\n\n\n1e+06\n0.75\n0.5\n\n\n1e+06\n1.00\n0.0\n\n\n1e+06\n1.00\n0.5\n\n\n\n\n\n\n\nHeld constant are:\n\nThe number of covariates: 20, all drawn from a standardized normal distribution\nFixed effect coefficients: drawn from a random uniform (-1, 1)\nResidual standard deviation: 1\nThe number of levels in each factor: 1000 for the first, 100 for the second\nThe standard deviations of the random effects: .5 for both\n\n\n\nGeneralized linear mixed model\nFor the generalized linear mixed model, the settings are the same but we also add a case where the outcome is rare or not in this binary setting (~ 10% prevalence or less).\n\n\n\n\n\nN\nbalanced\ntau_2\nrare\n\n\n\n\n1e+04\n0.75\n0.0\nFALSE\n\n\n1e+04\n0.75\n0.0\nTRUE\n\n\n1e+04\n0.75\n0.5\nFALSE\n\n\n1e+04\n0.75\n0.5\nTRUE\n\n\n1e+04\n1.00\n0.0\nFALSE\n\n\n1e+04\n1.00\n0.0\nTRUE\n\n\n1e+04\n1.00\n0.5\nFALSE\n\n\n1e+04\n1.00\n0.5\nTRUE\n\n\n5e+04\n0.75\n0.0\nFALSE\n\n\n5e+04\n0.75\n0.0\nTRUE\n\n\n5e+04\n0.75\n0.5\nFALSE\n\n\n5e+04\n0.75\n0.5\nTRUE\n\n\n5e+04\n1.00\n0.0\nFALSE\n\n\n5e+04\n1.00\n0.0\nTRUE\n\n\n5e+04\n1.00\n0.5\nFALSE\n\n\n5e+04\n1.00\n0.5\nTRUE\n\n\n1e+05\n0.75\n0.0\nFALSE\n\n\n1e+05\n0.75\n0.0\nTRUE\n\n\n1e+05\n0.75\n0.5\nFALSE\n\n\n1e+05\n0.75\n0.5\nTRUE\n\n\n1e+05\n1.00\n0.0\nFALSE\n\n\n1e+05\n1.00\n0.0\nTRUE\n\n\n1e+05\n1.00\n0.5\nFALSE\n\n\n1e+05\n1.00\n0.5\nTRUE\n\n\n5e+05\n0.75\n0.0\nFALSE\n\n\n5e+05\n0.75\n0.0\nTRUE\n\n\n5e+05\n0.75\n0.5\nFALSE\n\n\n5e+05\n0.75\n0.5\nTRUE\n\n\n5e+05\n1.00\n0.0\nFALSE\n\n\n5e+05\n1.00\n0.0\nTRUE\n\n\n5e+05\n1.00\n0.5\nFALSE\n\n\n5e+05\n1.00\n0.5\nTRUE\n\n\n1e+06\n0.75\n0.0\nFALSE\n\n\n1e+06\n0.75\n0.0\nTRUE\n\n\n1e+06\n0.75\n0.5\nFALSE\n\n\n1e+06\n0.75\n0.5\nTRUE\n\n\n1e+06\n1.00\n0.0\nFALSE\n\n\n1e+06\n1.00\n0.0\nTRUE\n\n\n1e+06\n1.00\n0.5\nFALSE\n\n\n1e+06\n1.00\n0.5\nTRUE\n\n\n\n\n\n\n\n\n\nFunction arguments\nFor g/lmer I set check.derivatives = FALSE and for the GLMM I additionally set nAGQ = 0, as this is precisely the setting one would do so7. I did not mess with the optimizers but it is possible to get a speed gain there in some settings. See performance tips here and demonstrated here.\nFor bam I set the following.\n\ngc.level = 0\nuse.chol = TRUE\nnthreads = 12/16\nchunk.size = 1000\nsamfrac = .1\n\nglmmTMB was left at defaults as I’m not aware of a specific approach for speed gain."
  },
  {
    "objectID": "posts/2019-10-20-big-mixed-models/index.html#footnotes",
    "href": "posts/2019-10-20-big-mixed-models/index.html#footnotes",
    "title": "Mixed Models for Big Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou could use construct the smooth with mgcv and add it to the model matrix for lme4.↩︎\nSee REEMtree, mixRF for example.↩︎\nWhile I haven’t seen it done, so it may have to serve as a later post, it should be possible to use deep learning tools like Keras or fastai by regularizing only weights associated with the random effects. If one takes an actual deep learning approach, then one can estimate functions of the ‘fixed’ covariates (like the smooth terms in typical GAM) and possibly get at correlations of the clusters themselves (a la spatial random effects).↩︎\nSee McCoach reference (McCoach et al. 2018). They also look at HLM and Mplus. However, I haven’t in years consulted with anyone across dozens of disciplines that was using HLM for mixed models. With Mplus, the verbosity of the syntax, plus additional data processing required, plus huge lack of post-processing of the model would negate any speed gain one might get from simply running the model. Couple this with the fact that campus-wide licenses are rare for either, neither could be recommended for mixed models. Note also, that one setting of lmer probably would have negated almost all their reported convergence issues.↩︎\nSee McCoach reference (McCoach et al. 2018). They also look at HLM and Mplus. However, I haven’t in years consulted with anyone across dozens of disciplines that was using HLM for mixed models. With Mplus, the verbosity of the syntax, plus additional data processing required, plus huge lack of post-processing of the model would negate any speed gain one might get from simply running the model. Couple this with the fact that campus-wide licenses are rare for either, neither could be recommended for mixed models. Note also, that one setting of lmer probably would have negated almost all their reported convergence issues.↩︎\n“In general, we expect glmmTMB‘s advantages over lme4 to be (1) greater flexibility (zero-inflation etc.); (2) greater speed for GLMMs, especially those with large number of ’top-level’ parameters (fixed effects plus random-effects variance-covariance parameters). In contrast, lme4 should be faster for LMMs.”↩︎\nSee this R user group thread for a discussion, this stackoverflow exchange involving one of the lme4 contributors, and Bates Julia notebook for more detail.↩︎"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html",
    "href": "posts/2020-03-16-convergence/index.html",
    "title": "Convergence Problems",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\nPrerequisite: Knowledge of regression modeling. Helpful would be to know something about mixed models and optimization.\nPrimary packages used:\nlibrary(tidyverse)\nlibrary(tidyext)\nlibrary(broom)\nlibrary(kableExtra)\nlibrary(visibly)\n\nlibrary(lme4)\nlibrary(mixedup)  # http://m-clark.github.io/mixedup"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#intro",
    "href": "posts/2020-03-16-convergence/index.html#intro",
    "title": "Convergence Problems",
    "section": "Intro",
    "text": "Intro\nIt is not uncommon that complex models lead to difficulties with convergence. Though the following example is a demo with the R package lme4, most of it would potentially apply to any complex modeling situation where convergence problems arise. The goal is provide some steps one can take to get their models back on track. The running example is taken from the data posted at this stackoverflow question. Ben Bolker’s1 response there can be seen as the basis for this post, along with some extensions, updates and other organization."
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#data-setup",
    "href": "posts/2020-03-16-convergence/index.html#data-setup",
    "title": "Convergence Problems",
    "section": "Data Setup",
    "text": "Data Setup\nYou can download the data from here (RDS file), or go to the stackoverflow discussion and paste the code there. There isn’t any real explanation of the variables unfortunately, though you can get a sense of some of them (e.g. Day, replicate, temperature, etc.).\n\n# df = readRDS(\n#   gzcon(\n#     url('https://github.com/m-clark/m-clark.github.io/raw/master/data/convergence.RDS')\n#   )\n# )\n\ndf = readRDS('data/convergence.RDS')\n\ndf = df %&gt;% \n  mutate(\n    SUR.ID = factor(SUR.ID),\n    replicate = factor(replicate),\n    Unit = factor(1:n())\n  )"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#initial-model",
    "href": "posts/2020-03-16-convergence/index.html#initial-model",
    "title": "Convergence Problems",
    "section": "Initial model",
    "text": "Initial model\nThe following is the model that led to the stackoverflow post. It’s fairly complicated with multiple interactions and random effects, modeling the proportion of valid detections via a binomial model. The Unit effect is used to account for overdispersion, a common issue in count modeling.\n\nmodel_mixed_0 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    tm:Area + tm:c.distance + c.distance:Area + c.tm.depth:Area + \n    c.receiver.depth:Area + c.temp:Area + c.wind:Area + c.tm.depth + \n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + replicate + \n    (1|SUR.ID) + (1|Day) + (1|Unit), \n  data = df, \n  family = binomial()\n)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0991801 (tol = 0.002, component 1)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue\n - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\n\nThis gives several warnings, the more egregious of which is that the model has not converged, meaning the estimates may not be trustworthy. So what do we do?"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#step-back",
    "href": "posts/2020-03-16-convergence/index.html#step-back",
    "title": "Convergence Problems",
    "section": "Step back",
    "text": "Step back\nThe first step is to step back and look at the data. Are there issues that can be spotted? Are some essentially collinear with others? In the following, we can see that some variables are only a few levels, and some, like Day, c.tm.depth and c.receiver.depth, are both notably correlated with each other and with other predictor variables.\n\n\n$`Numeric Variables`\n# A tibble: 9 × 10\n  Variable        N  Mean     SD     Min      Q1 Median    Q3    Max `% Missing`\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n1 ValidDetec…   220  6.49  10.1     0       0      1     7     45              0\n2 CountDetec…   220  6.85  10.1     0       0      2     7     45              0\n3 FalseDetec…   220  0.36   0.73    0       0      0     0      4              0\n4 R.det         220  0.52   0.47    0       0      0.68  1      1              0\n5 c.receiver…   220 -0.06   0.31   -0.4    -0.34  -0.18  0.21   0.54           0\n6 c.tm.depth    220 -0.01   0.48   -0.62   -0.55   0.03  0.4    0.78           0\n7 c.temp        220 -0.46   2.38   -4.22   -3.62   0.54  1.55   2.84           0\n8 c.wind        220 -0.26   3.29   -2.97   -2.97  -2.94  1.28   5.88           0\n9 c.distance    220 -6.36 129.   -160    -110    -10    90    190              0\n\n$`Categorical Variables`\n# A tibble: 14 × 4\n   Variable  Group        Frequency   `%`\n   &lt;chr&gt;     &lt;fct&gt;            &lt;int&gt; &lt;dbl&gt;\n 1 SUR.ID    10185               74    34\n 2 SUR.ID    10186               74    34\n 3 SUR.ID    10250               72    33\n 4 tm        CT                 110    50\n 5 tm        PT-04              110    50\n 6 replicate 1                  120    55\n 7 replicate 2                  100    45\n 8 Area      Drug Channel       120    55\n 9 Area      Finger             100    45\n10 Day       03/06/13            60    27\n11 Day       2/22/13             60    27\n12 Day       2/27/13             60    27\n13 Day       3/14/13             28    13\n14 Day       2/26/13             12     5\n\n\n\n\n\n\n\n\nWe can obtain a rough metric of total correlation of a variable with the others by looking at the variance inflation factor (VIF)2. For this, we’ll treat any binary or ordered data as numeric, and we can use the car package to get the VIF. This requires running an arbitrary regression model that includes the covariates of interest, but the value is derived only from the predictor variables. If we just try it with a dummy model, we get an error, since the linear model has perfect collinearity. We find out that Day is probably causing issues, and we’ll see why later.\n\n# the model used to acquire the vif doesn't matter- anything could be used for\n# the target variable\n\n# this produces errors about aliased coefficients\n# car::vif(lm(CountDetections ~ . - Unit - FalseDetections - ValidDetections, dat = df))\n\nattributes(alias(\n  lm(\n    CountDetections ~ . - Unit - FalseDetections - ValidDetections,\n    dat = df\n  )\n)$Complete)$dimnames[[1]]\n\n[1] \"Day3/14/13\" \"c.wind\"    \n\ncar::vif(lm(CountDetections ~ . -Unit -Day -FalseDetections, dat = df))\n\n                      GVIF Df GVIF^(1/(2*Df))\nSUR.ID            1.200902  2        1.046832\ntm                1.066227  1        1.032583\nValidDetections   2.708644  1        1.645796\nreplicate         1.057540  1        1.028368\nArea              5.795510  1        2.407387\nR.det             2.393891  1        1.547220\nc.receiver.depth  8.674740  1        2.945291\nc.tm.depth       13.438407  1        3.665843\nc.temp            2.956315  1        1.719394\nc.wind            3.041363  1        1.743950\nc.distance        2.065335  1        1.437127\n\n\nIt looks like, along with Day, tm.depth is probably going to cause a problem, as more than 90% of its variance is accounted for by the other covariates. So at this point we can consider both it and Day as potential predictors to remove. Wind is less an issue once Day is removed.\nAn alternative approach I tried (not shown) was just running a PCA on the predictor variables. Only six components were needed to account for nearly almost 90% of the total variance, so I think it’s safe to say there is a notable amount of redundancy in this data.\nOne final note is that the target variable is a potential problem as well. More than half of the data is a 1.0 proportion of Valid Detections, and it also turns out this doesn’t appear to be proportional data per se, as sometimes both Valid and False Detections can be zero, meaning such observations don’t contribute meaningfully to the model. This isn’t necessarily a problem as these observations are basically dropped dropped from the model, and you will note in the GLM results that follow that the residual degrees of freedom will be notably lower than the total number of observations (220). This is due to the dropout of those observations, which is about 37% of the data. Even so, the lack of variability may be an underlying issue in general. Especially in the case of interactions, it is likely that some cells have no variability in the outcome."
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#start-simply",
    "href": "posts/2020-03-16-convergence/index.html#start-simply",
    "title": "Convergence Problems",
    "section": "Start simply",
    "text": "Start simply\nSo we know there are some data issues, so let’s start with a model that’s relatively simple but still plausible. If we just look at a GLM without any random effects, can we spot any issues?\n\nmodel_glm_1 = glm(\n  cbind(ValidDetections, FalseDetections) ~\n    tm:Area + tm:c.distance + c.distance:Area + c.tm.depth:Area +\n    c.receiver.depth:Area + c.temp:Area + c.wind:Area + c.tm.depth +\n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + replicate +\n    SUR.ID + Day,\n  data = df,\n  family = binomial(link = 'logit')\n)\n\nsummary(model_glm_1)\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ tm:Area + \n    tm:c.distance + c.distance:Area + c.tm.depth:Area + c.receiver.depth:Area + \n    c.temp:Area + c.wind:Area + c.tm.depth + c.receiver.depth + \n    c.temp + c.wind + tm + c.distance + Area + replicate + SUR.ID + \n    Day, family = binomial(link = \"logit\"), data = df)\n\nCoefficients: (3 not defined because of singularities)\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.125e+01  6.504e+00  -1.730  0.08372 .  \nc.tm.depth                  -8.406e-01  1.228e+00  -0.684  0.49376    \nc.receiver.depth             7.026e+00  7.852e+00   0.895  0.37095    \nc.temp                      -5.514e+00  2.538e+00  -2.172  0.02983 *  \nc.wind                      -6.256e+00  3.349e+00  -1.868  0.06179 .  \ntmPT-04                     -2.053e+00  4.961e-01  -4.139  3.5e-05 ***\nc.distance                  -3.685e-03  2.614e-03  -1.410  0.15864    \nAreaFinger                   5.901e+01  2.775e+01   2.127  0.03346 *  \nreplicate2                   2.700e+00  1.161e+00   2.326  0.02000 *  \nSUR.ID10186                 -2.227e-01  4.456e-01  -0.500  0.61721    \nSUR.ID10250                 -2.995e-01  4.469e-01  -0.670  0.50281    \nDay2/22/13                  -7.114e+01  3.463e+01  -2.054  0.03996 *  \nDay2/26/13                   3.306e+00  1.134e+03   0.003  0.99767    \nDay2/27/13                          NA         NA      NA       NA    \nDay3/14/13                          NA         NA      NA       NA    \ntmPT-04:AreaFinger           4.308e-01  6.066e-01   0.710  0.47759    \ntmPT-04:c.distance          -5.332e-03  3.186e-03  -1.674  0.09419 .  \nAreaFinger:c.distance        1.192e-02  3.855e-03   3.091  0.00199 ** \nAreaFinger:c.tm.depth       -2.815e+00  4.731e+00  -0.595  0.55189    \nAreaFinger:c.receiver.depth -3.211e+01  2.545e+01  -1.262  0.20708    \nAreaFinger:c.temp            2.377e+00  1.902e+00   1.250  0.21141    \nAreaFinger:c.wind                   NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 135.23  on 119  degrees of freedom\nAIC: 253.65\n\nNumber of Fisher Scoring iterations: 17\n\n\nSure enough, there are problems. What’s going on with Day and Area? One issue is that Area only couples with certain days, so having one already tells you a lot about what the other could tell you.\n\nwith(df, table(Day, Area))\n\n          Area\nDay        Drug Channel Finger\n  03/06/13           60      0\n  2/22/13             0     60\n  2/26/13             0     12\n  2/27/13            60      0\n  3/14/13             0     28\n\n\nYou could potentially create a combined type of variable to deal with this for example, but otherwise the problem will persist with both in the model. There are likely remaining collinearities besides, but let’s take Day out for example.\n\nmodel_glm_2 = update(model_glm_1, . ~ . -Day)\n\nsummary(model_glm_2)\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ c.tm.depth + \n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + \n    replicate + SUR.ID + tm:Area + tm:c.distance + Area:c.distance + \n    Area:c.tm.depth + Area:c.receiver.depth + Area:c.temp + Area:c.wind, \n    family = binomial(link = \"logit\"), data = df)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -10.980797   6.323591  -1.736  0.08248 .  \nc.tm.depth                   -0.842695   1.227579  -0.686  0.49242    \nc.receiver.depth              7.003401   7.846631   0.893  0.37211    \nc.temp                       -5.407822   2.465579  -2.193  0.02828 *  \nc.wind                       -6.125119   3.266203  -1.875  0.06075 .  \ntmPT-04                      -2.049088   0.495012  -4.139 3.48e-05 ***\nc.distance                   -0.003755   0.002586  -1.452  0.14644    \nAreaFinger                   11.163412   6.537806   1.708  0.08773 .  \nreplicate2                    2.646675   1.120176   2.363  0.01814 *  \nSUR.ID10186                  -0.223527   0.445491  -0.502  0.61584    \nSUR.ID10250                  -0.300337   0.446936  -0.672  0.50159    \ntmPT-04:AreaFinger            0.425446   0.605567   0.703  0.48233    \ntmPT-04:c.distance           -0.005304   0.003175  -1.670  0.09483 .  \nc.distance:AreaFinger         0.011937   0.003849   3.101  0.00193 ** \nc.tm.depth:AreaFinger        -3.137815   4.366782  -0.719  0.47241    \nc.receiver.depth:AreaFinger -35.342161  17.332674  -2.039  0.04145 *  \nc.temp:AreaFinger             2.222685   1.682491   1.321  0.18648    \nc.wind:AreaFinger             8.259864   3.700848   2.232  0.02562 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 135.27  on 120  degrees of freedom\nAIC: 251.69\n\nNumber of Fisher Scoring iterations: 10\n\n\nWell, at least we got rid of the complete collinearity- no NA remains. However, those familiar with such models can still see that some of these coefficients and their associated standard errors are exceedingly large for this setting, so we shouldn’t really be surprised there would still be issues with the more complicated mixed models. With binary logistic models, large absolute coefficients and their standard errors are usually a sign of collinearity/separation, and we now know from our previous exploration that something similar is going on in this proportional binomial model. Here is a table of the more egregious offenders and a plot of all coefficients, their standard errors, and the VIF (size).\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nvif\n\n\n\n\nc.receiver.depth\n7.003\n7.847\n0.893\n0.372\n21.202\n\n\nc.wind\n-6.125\n3.266\n-1.875\n0.061\n88.124\n\n\nAreaFinger\n11.163\n6.538\n1.708\n0.088\n24.372\n\n\nc.receiver.depth:AreaFinger\n-35.342\n17.333\n-2.039\n0.041\n43.537\n\n\nc.wind:AreaFinger\n8.260\n3.701\n2.232\n0.026\n89.504\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt this point I would not assume anything about the mixed model itself being a problem, and be leaning toward this being primarily a data issue. Or, at the very least, data issues will need to be sorted out. So let’s begin the restart of our mixed model effort by pulling out some of those variables we thought had some collinearity issues. You might have noticed from the GLM that SUR.ID was only 3 levels, so let’s move that to a fixed effect also. In keeping things simple, I’m not including any interactions.\n\nmodel_mixed_1 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance + c.distance:Area + #c.tm.depth:Area + \n    # c.receiver.depth:Area + c.temp:Area + c.wind:Area + #c.tm.depth + \n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + replicate + \n    SUR.ID + \n    (1|Unit), #  + (1|Day)\n  data = df,\n  family = binomial(link = 'logit')\n)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.117672 (tol = 0.002, component 1)\n\nsummarize_model(model_mixed_1, ci = FALSE)\n\n\nVariance Components:\n\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     0.90 0.95     1.00\n\n\n\nFixed Effects:\n\n\n             Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n        Intercept  2.39 0.73  3.26    0.00      0.95       3.82\n c.receiver.depth -3.80 1.08 -3.53    0.00     -5.91      -1.69\n           c.temp  0.25 0.19  1.28    0.20     -0.13       0.63\n           c.wind  0.13 0.15  0.88    0.38     -0.16       0.43\n          tmPT-04 -1.36 0.36 -3.77    0.00     -2.07      -0.65\n       c.distance -0.01 0.00 -5.09    0.00     -0.01      -0.01\n       AreaFinger  1.22 0.62  1.99    0.05      0.02       2.43\n       replicate2 -0.39 0.37 -1.05    0.29     -1.11       0.33\n      SUR.ID10186 -0.13 0.55 -0.23    0.82     -1.20       0.95\n      SUR.ID10250 -0.20 0.54 -0.36    0.72     -1.26       0.87\n\n\nWell, we still have issues, so what else can we try?"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#rescale-variables",
    "href": "posts/2020-03-16-convergence/index.html#rescale-variables",
    "title": "Convergence Problems",
    "section": "Rescale variables",
    "text": "Rescale variables\nLet’s go ahead with the easy part and rescale our variables, which might as well be done with any model. I will standardize the numeric variables.\n\nsc = function(x) scale(x)[, 1]\n\ndf = df %&gt;% \n  mutate(\n    c.receiver.depth_sc = sc(c.receiver.depth),\n    c.tm.depth_sc = sc(c.tm.depth),\n    c.temp_sc     = sc(c.temp),\n    c.wind_sc     = sc(c.wind),\n    c.distance_sc = sc(c.distance),\n  )\n\nmodel_mixed_2 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area + \n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc + c.temp_sc + c.wind_sc + tm + c.distance_sc + Area + replicate + \n    SUR.ID +\n    (1|Unit), \n  data = df,\n  family = binomial(link = 'logit')\n)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.13583 (tol = 0.002, component 1)\n\nsummarize_model(model_mixed_2, ci = FALSE)\n\n\nVariance Components:\n\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     0.87 0.93     1.00\n\n\n\nFixed Effects:\n\n\n                Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n           Intercept  2.48 0.69  3.59    0.00      1.13       3.84\n c.receiver.depth_sc -1.18 0.33 -3.56    0.00     -1.83      -0.53\n           c.temp_sc  0.58 0.45  1.27    0.20     -0.31       1.47\n           c.wind_sc  0.42 0.49  0.86    0.39     -0.54       1.38\n             tmPT-04 -1.36 0.36 -3.78    0.00     -2.07      -0.66\n       c.distance_sc -1.19 0.23 -5.15    0.00     -1.64      -0.73\n          AreaFinger  1.23 0.61  2.01    0.04      0.03       2.43\n          replicate2 -0.39 0.37 -1.07    0.28     -1.11       0.33\n         SUR.ID10186 -0.11 0.54 -0.20    0.84     -1.17       0.95\n         SUR.ID10250 -0.19 0.54 -0.35    0.73     -1.25       0.87\n\n\nSo at least we have the rescaling taken care of, and while that got rid of one warning, we still have the convergence problem. What can we check for next? I looked to see if there was any further imbalance of categorical variables, didn’t spot much issue, but then discovered something else. A couple covariates - c.wind and c.distance - have only five unique values, and for the former, some of those values only occur a few times. In addition, c.wind was unique per day, so was essentially confounded with it. So we can feel fine with having previously removed Day.\n\ndf %&gt;% \n  select(-ends_with('sc')) %&gt;% \n  map_int(n_distinct)\n\n          SUR.ID               tm  ValidDetections  CountDetections \n               3                2               35               35 \n FalseDetections        replicate             Area              Day \n               5                2                2                5 \n           R.det c.receiver.depth       c.tm.depth           c.temp \n              21               30               31               37 \n          c.wind       c.distance             Unit \n               5                5              220 \n\ntable(df$c.wind, df$Day)\n\n              \n               03/06/13 2/22/13 2/26/13 2/27/13 3/14/13\n  -2.96855001         0      60       0       0       0\n  -2.939182972        0       0       0      60       0\n  1.27535159         60       0       0       0       0\n  4.71144999          0       0      12       0       0\n  5.88092439          0       0       0       0      28\n\n\n\nIf we treat these as categorical what happens? I’ll do it just for the GLM again.\n\nmodel_glm_3 = glm(\n  cbind(ValidDetections, FalseDetections) ~\n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area +\n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc + c.temp_sc + factor(c.wind) + tm + factor(c.distance) + Area + replicate +\n    SUR.ID ,\n  data = df,\n  family = binomial(link = 'logit')\n)\n\nsummary(model_glm_3)\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ c.receiver.depth_sc + \n    c.temp_sc + factor(c.wind) + tm + factor(c.distance) + Area + \n    replicate + SUR.ID, family = binomial(link = \"logit\"), data = df)\n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   7.8190     2.2057   3.545 0.000393 ***\nc.receiver.depth_sc           1.3883     2.1912   0.634 0.526349    \nc.temp_sc                    -2.3820     1.4315  -1.664 0.096116 .  \nfactor(c.wind)-2.939182972   -5.0028     4.0691  -1.229 0.218901    \nfactor(c.wind)1.27535159     -7.8429     2.9788  -2.633 0.008466 ** \nfactor(c.wind)4.71144999     17.9094  1180.6959   0.015 0.987898    \nfactor(c.wind)5.88092439     -9.1668     6.3781  -1.437 0.150654    \ntmPT-04                      -1.2459     0.2823  -4.413 1.02e-05 ***\nfactor(c.distance)-110       -0.3953     0.3908  -1.011 0.311786    \nfactor(c.distance)-10        -0.9887     0.4110  -2.405 0.016162 *  \nfactor(c.distance)90         -0.8547     0.5253  -1.627 0.103694    \nfactor(c.distance)190        -3.2681     0.7664  -4.264 2.01e-05 ***\nAreaFinger                        NA         NA      NA       NA    \nreplicate2                    0.7647     0.5249   1.457 0.145120    \nSUR.ID10186                  -0.1224     0.4384  -0.279 0.780057    \nSUR.ID10250                  -0.2700     0.4363  -0.619 0.536010    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 146.17  on 123  degrees of freedom\nAIC: 256.59\n\nNumber of Fisher Scoring iterations: 17\n\n\nAh! So now we see that Area is also accounted for by other factors. In addition, wind is still problematic. Let’s take out Area, while still treating wind and distance as categorical for our diagnostic adventure.\n\nmodel_glm_4 =  update(model_glm_3, . ~ . - Area)\n\nsummary(model_glm_4)\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ c.receiver.depth_sc + \n    c.temp_sc + factor(c.wind) + tm + factor(c.distance) + replicate + \n    SUR.ID, family = binomial(link = \"logit\"), data = df)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   7.8190     2.2057   3.545 0.000393 ***\nc.receiver.depth_sc           1.3883     2.1912   0.634 0.526349    \nc.temp_sc                    -2.3820     1.4315  -1.664 0.096116 .  \nfactor(c.wind)-2.939182972   -5.0028     4.0691  -1.229 0.218901    \nfactor(c.wind)1.27535159     -7.8429     2.9788  -2.633 0.008466 ** \nfactor(c.wind)4.71144999     17.9094  1180.6959   0.015 0.987898    \nfactor(c.wind)5.88092439     -9.1668     6.3781  -1.437 0.150654    \ntmPT-04                      -1.2459     0.2823  -4.413 1.02e-05 ***\nfactor(c.distance)-110       -0.3953     0.3908  -1.011 0.311786    \nfactor(c.distance)-10        -0.9887     0.4110  -2.405 0.016162 *  \nfactor(c.distance)90         -0.8547     0.5253  -1.627 0.103694    \nfactor(c.distance)190        -3.2681     0.7664  -4.264 2.01e-05 ***\nreplicate2                    0.7647     0.5249   1.457 0.145120    \nSUR.ID10186                  -0.1224     0.4384  -0.279 0.780057    \nSUR.ID10250                  -0.2700     0.4363  -0.619 0.536010    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 146.17  on 123  degrees of freedom\nAIC: 256.59\n\nNumber of Fisher Scoring iterations: 17\n\n\nThe remaining collinearity is due to the relatively few observations for that value of wind, but at least most of the other covariates effects have settled down. Let’s try collapsing wind values and officially making it categorical.\n\ndf = df %&gt;% \n  mutate(wind = fct_lump(factor(c.wind), 3, other_level = 'Higher'))\n\ntable(df$wind)\n\n\n -2.96855001 -2.939182972   1.27535159       Higher \n          60           60           60           40 \n\nmodel_mixed_3 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area + \n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc + c.temp_sc + wind + tm + factor(c.distance) + #Area + \n    replicate + SUR.ID +\n    (1|Unit), \n  data = df, \n  family = binomial(link = 'logit')\n)\n\nsummarize_model(model_mixed_3, ci = FALSE)\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     0.78 0.88     1.00\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  3.67 1.16  3.16    0.00      1.40       5.94\n  c.receiver.depth_sc -3.02 1.40 -2.16    0.03     -5.76      -0.28\n            c.temp_sc -1.72 1.70 -1.02    0.31     -5.05       1.60\n     wind-2.939182972  2.73 2.87  0.95    0.34     -2.89       8.35\n       wind1.27535159 -4.20 2.88 -1.46    0.14     -9.85       1.44\n           windHigher  4.41 2.81  1.57    0.12     -1.11       9.92\n              tmPT-04 -1.31 0.36 -3.65    0.00     -2.01      -0.60\n factorc.distance-110 -0.62 0.48 -1.31    0.19     -1.56       0.31\n  factorc.distance-10 -1.44 0.50 -2.87    0.00     -2.42      -0.45\n   factorc.distance90 -1.23 0.62 -1.97    0.05     -2.44      -0.01\n  factorc.distance190 -4.21 0.96 -4.39    0.00     -6.09      -2.33\n           replicate2  0.20 0.58  0.34    0.73     -0.93       1.33\n          SUR.ID10186 -0.06 0.54 -0.12    0.90     -1.12       0.99\n          SUR.ID10250 -0.16 0.54 -0.30    0.76     -1.21       0.89\n\n\nChecking VIF adjusted for the degrees of freedom associated with the covariate (which is greater for categorical variables), we still have some issues. Below I show VIF both with and without wind as an example.\n\n\n\n\n\ncovariate\nVIF_adj_orig\nVIF_adj\n\n\n\n\nc.receiver.depth_sc\n10.017\n1.277\n\n\nc.temp_sc\n9.437\n1.065\n\n\nwind\n3.914\nNA\n\n\ntm\n1.059\n1.047\n\n\nfactor(c.distance)\n1.080\n1.062\n\n\nreplicate\n1.769\n1.101\n\n\nSUR.ID\n1.075\n1.029\n\n\n\n\n\n\n\nLet’s see what happens if we remove wind from the model.\n\nmodel_mixed_4 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area + \n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc +  c.temp_sc  + tm + factor(c.distance) + #Area + \n    replicate + SUR.ID +\n    (1|Unit), \n  data = df,\n  family = binomial(link = 'logit')\n)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0547352 (tol = 0.002, component 1)\n\nsummarize_model(model_mixed_4, ci = FALSE)\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     1.45 1.20     1.00\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  5.62 0.74  7.63    0.00      4.18       7.06\n  c.receiver.depth_sc -1.08 0.19 -5.57    0.00     -1.46      -0.70\n            c.temp_sc  0.50 0.21  2.34    0.02      0.08       0.92\n              tmPT-04 -1.28 0.40 -3.19    0.00     -2.08      -0.49\n factorc.distance-110 -0.99 0.53 -1.86    0.06     -2.04       0.05\n  factorc.distance-10 -1.87 0.55 -3.41    0.00     -2.95      -0.80\n   factorc.distance90 -1.87 0.66 -2.83    0.00     -3.17      -0.58\n  factorc.distance190 -5.60 1.04 -5.38    0.00     -7.65      -3.56\n           replicate2 -0.37 0.40 -0.94    0.35     -1.15       0.40\n          SUR.ID10186 -0.90 0.56 -1.60    0.11     -1.99       0.20\n          SUR.ID10250 -0.98 0.56 -1.74    0.08     -2.07       0.12\n\n\nWe’re doing better, as our max|grad| value is closer to the tolerance value, but we’re still not where we want to be. What else can we do?"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#remove-zero-random-effects",
    "href": "posts/2020-03-16-convergence/index.html#remove-zero-random-effects",
    "title": "Convergence Problems",
    "section": "Remove zero random effects",
    "text": "Remove zero random effects\nIf any variance components estimates are zero we could remove them. However, at this point we already have. Day was zero because it was already accounted for by other covariates. SUR.ID moved to a fixed effect, where it still appears to be a small effect, but at least won’t cause computational problems. The remaining Unit effect does appear to capture some overdispersion, so we can leave it for now."
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#technical-options",
    "href": "posts/2020-03-16-convergence/index.html#technical-options",
    "title": "Convergence Problems",
    "section": "Technical options",
    "text": "Technical options\nAt this point, we have explored some of the more problematic aspects of the data. Some might feel that they are missing out on some of their theoretical priorities by removing some covariates, but if they are confounded with other covariates, their story can’t be easily disentangled anyway. As noted, we could still play around with creation of different categorical effects through further collapsing or combining, but a lot of that would be arbitrary, so must be done with caution.\nIn general, the problems with the model appear to actually be entirely with the data, but we can move on for demonstration. At some point in your own modeling adventure, you may exhaust what you can do data-wise, and still have convergence issues. This leaves exploration of the more technical side of things to see what tweaks can be made to help further. In order to best check these more technical aspects, it helps to know something about the underlying optimization algorithms, or at least optimization in general. And in general, I suggest refraining from this unless the previous steps have failed. It is very common that once the data has been sorted, convergence gets solved as well, so the data must be ruled out. In any case, let’s see what else we can do!\n\nCheck singularity\nThis is a mixed model-specific check3, and in general, checking singularity4 goes along with removing zero random effects. These days, you’ll usually get a singularity warning when it is likely the case. In the past, Bolker suggested checking this problem as follows, but for this example concluded the result wasn’t close enough to zero to be a real concern. The theta below are just our random effect standard deviations, and I would say that the ones besides Unit probably weren’t meaningfully different from zero.\n\nthetas = getME(model_mixed_0, \"theta\")\n\nthetas\n\n  Unit.(Intercept)    Day.(Intercept) SUR.ID.(Intercept) \n       0.681180197        0.008987371        0.007219970 \n\nll = getME(model_mixed_0, \"lower\") # lower bounds on model parameters (random effects parameters only)\n\nmin(thetas[ll == 0])\n\n[1] 0.00721997\n\n\nNowadays lme4 provides the function isSingular which uses the steps above to check the minimum value against some specified tolerance.\n\nisSingular(model_mixed_0, tol = 1e-5)\n\n[1] FALSE\n\n# rePCA(model_mixed_0)  # via PCA of the random-effects variance-covariance estimates\n\n\n\nDouble-checking gradient calculations\nFor the mixed model setting, Bolker notes the following:\n\nOne general problem is that large scaled gradients are often associated with small absolute gradients: we might decide that we’re more interested in testing the (parallel) minimum of these two quantities.\n\nWe can do this as follows for the initial mixed model.\n\nderivs_init  = model_mixed_0@optinfo$derivs\n\nsc_grad_init = with(derivs_init, solve(Hessian, gradient))\n\nmax(abs(sc_grad_init))\n\n[1] 1.526361\n\nmax(pmin(abs(sc_grad_init), abs(derivs_init$gradient)))\n\n[1] 0.09918011\n\n\nWe see that the unscaled gradient results in a lower maximum value, but is still large relative to the tolerance. That value is what is reported in the warning message.\n\nmodel_mixed_0@optinfo$conv$lme4$messages[[1]]\n\n[1] \"Model failed to converge with max|grad| = 0.0991801 (tol = 0.002, component 1)\"\n\n\nIt may be instructive to compare the result to the model where we scaled the inputs. In this case the scaled gradient results in the lower max value.\n\nderivs_model_mixed_2 = model_mixed_2@optinfo$derivs\n\nsc_grad_model_mixed_2 = with(derivs_model_mixed_2, solve(Hessian, gradient))\n\nmax(abs(sc_grad_model_mixed_2))\n\n[1] 0.01645048\n\nmax(pmin(abs(sc_grad_model_mixed_2), abs(derivs_model_mixed_2$gradient)))\n\n[1] 0.01645048\n\n\nBolker also suggests checking if the result varies from using a different calculation, but it’s not clear what we’d do if this was the case. In any event, the results would be similar.\n\ndevfun_init  = update(model_mixed_0, devFunOnly = TRUE)\npars_init    = unlist(getME(model_mixed_0, c(\"theta\", \"fixef\")))\ngrad_init    = numDeriv::grad(devfun_init, pars_init)\nhess_init    = numDeriv::hessian(devfun_init, pars_init)\nsc_grad_init = solve(hess_init, grad_init)\n\nmax(pmin(abs(sc_grad_init), abs(grad_init)))\n\n[1] 0.09917939\n\nmax(pmin(abs(sc_grad_init), abs(derivs_init$gradient)))\n\n[1] 0.09918011\n\n\n\n\nRestart the fit\nAs another step along our technical travails, we can just let the optimizer keep going until it does converge. Many R modeling packages allow for you to access the optimizer and change various settings. Most optimizers have a maxit type of argument to let you set the number of iterations, and we can use update to continue where we left off. Unfortunately there is no standard argument name for the total number of iterations, or even if you do happen to remember, guessing is required as to what we should set it at. So instead, we can just call update iteratively until there is no convergence warning. I check this by seeing if there is any output to @optinfo$conv$lme4, as it will only be there if it doesn’t converge5.\n\nmodel_mixed_5 = model_mixed_4\n\nwhile (length(model_mixed_5@optinfo$conv$lme4) &gt; 0) {\n  pars = getME(model_mixed_5,c(\"theta\",\"fixef\"))\n  model_mixed_5 &lt;-\n    update(model_mixed_5,\n           start = pars,\n           control = glmerControl(optCtrl = list(maxfun = 2e5)))\n}\n\nmax(\n  abs(\n    with(\n      model_mixed_5@optinfo$derivs, solve(Hessian, gradient)\n    )\n  )\n)  \n\n[1] 0.0001935103\n\n# we win!\n\nSo at this point we have converged with no warnings. Hooray for us! However, the following shows us that we were pretty close anyway. The estimates from the last model with warnings and the converged model are nearly identical.\n\nsummarize_model(model_mixed_4, ci = 0)\n\n\nVariance Components:\n\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     1.45 1.20     1.00\n\n\n\nFixed Effects:\n\n\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  5.62 0.74  7.63    0.00      4.18       7.06\n  c.receiver.depth_sc -1.08 0.19 -5.57    0.00     -1.46      -0.70\n            c.temp_sc  0.50 0.21  2.34    0.02      0.08       0.92\n              tmPT-04 -1.28 0.40 -3.19    0.00     -2.08      -0.49\n factorc.distance-110 -0.99 0.53 -1.86    0.06     -2.04       0.05\n  factorc.distance-10 -1.87 0.55 -3.41    0.00     -2.95      -0.80\n   factorc.distance90 -1.87 0.66 -2.83    0.00     -3.17      -0.58\n  factorc.distance190 -5.60 1.04 -5.38    0.00     -7.65      -3.56\n           replicate2 -0.37 0.40 -0.94    0.35     -1.15       0.40\n          SUR.ID10186 -0.90 0.56 -1.60    0.11     -1.99       0.20\n          SUR.ID10250 -0.98 0.56 -1.74    0.08     -2.07       0.12\n\nsummarize_model(model_mixed_5, ci = 0)\n\n\nVariance Components:\n\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     1.46 1.21     1.00\n\n\n\nFixed Effects:\n\n\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  5.63 0.74  7.62    0.00      4.18       7.08\n  c.receiver.depth_sc -1.07 0.19 -5.54    0.00     -1.45      -0.69\n            c.temp_sc  0.50 0.22  2.34    0.02      0.08       0.92\n              tmPT-04 -1.28 0.40 -3.18    0.00     -2.08      -0.49\n factorc.distance-110 -0.99 0.53 -1.85    0.06     -2.03       0.06\n  factorc.distance-10 -1.87 0.55 -3.39    0.00     -2.95      -0.79\n   factorc.distance90 -1.87 0.66 -2.82    0.00     -3.16      -0.57\n  factorc.distance190 -5.59 1.04 -5.37    0.00     -7.63      -3.55\n           replicate2 -0.38 0.40 -0.96    0.34     -1.15       0.40\n          SUR.ID10186 -0.91 0.56 -1.61    0.11     -2.01       0.20\n          SUR.ID10250 -0.99 0.56 -1.76    0.08     -2.09       0.11\n\n\nAs an additional point, one may provide starting estimates from the outset. For example you could run a a simpler model, e.g. standard GLM, and feed the estimates there for the fixed effects coefficients of the mixed model, or even run a model to obtain starting values for the random effects (see this example).\n\n\nChange the optimizer\nAs a last effort among the more technical knobs to turn, we can start fiddling with the optimizer options. The lme4 package has a nice function allFit that will search across several different optimizers (some may require additional packages to be installed). However, in any particular modeling setting you could potentially do this, though often you may not be able to without quite a bit of effort relative to what lme4 allows. I’ll do this with our last non-converged model.\n\nlibrary(optimx)  # required for some optimizers\n\nglmm_all = allFit(model_mixed_4)\n\nbobyqa : [OK]\nNelder_Mead : [OK]\nnlminbwrap : [OK]\noptimx.L-BFGS-B : [OK]\nnloptwrap.NLOPT_LN_NELDERMEAD : [OK]\nnloptwrap.NLOPT_LN_BOBYQA : [OK]\n\nglmm_all_summary = summary(glmm_all)\n\nThe ‘[OK]’ just means there wasn’t an error, however we can see that several have convergence problems, but even a couple of those are almost to the tolerance level. In the end though, the estimates and log likelihoods are not meaningfully different across the optimizers.\n\n\n\n\n\nopt\nOK\nMessage\n\n\n\n\nNelder_Mead\nTRUE\nModel failed to converge with max&#124;grad&#124; = 0.00620032 (tol = 0.002, component 1)\n\n\n\n\n\n\n\n\n\n\n\noptimizer\n(Intercept)\nc.receiver.depth_sc\nc.temp_sc\ntmPT-04\nfactor(c.distance)-110\nfactor(c.distance)-10\nfactor(c.distance)90\nfactor(c.distance)190\nreplicate2\nSUR.ID10186\nSUR.ID10250\nll\n\n\n\n\nbobyqa\n5.633\n-1.072\n0.503\n-1.284\n-0.987\n-1.867\n-1.867\n-5.591\n-0.379\n-0.905\n-0.989\n-121.842\n\n\nNelder_Mead\n5.632\n-1.073\n0.503\n-1.284\n-0.986\n-1.866\n-1.867\n-5.590\n-0.378\n-0.906\n-0.989\n-121.842\n\n\nnlminbwrap\n5.633\n-1.072\n0.503\n-1.284\n-0.987\n-1.867\n-1.867\n-5.591\n-0.379\n-0.905\n-0.989\n-121.842\n\n\noptimx.L-BFGS-B\n5.632\n-1.072\n0.503\n-1.284\n-0.987\n-1.867\n-1.867\n-5.591\n-0.379\n-0.905\n-0.989\n-121.842\n\n\nnloptwrap.NLOPT_LN_NELDERMEAD\n5.633\n-1.072\n0.503\n-1.284\n-0.987\n-1.867\n-1.867\n-5.591\n-0.379\n-0.905\n-0.989\n-121.842\n\n\nnloptwrap.NLOPT_LN_BOBYQA\n5.634\n-1.072\n0.503\n-1.284\n-0.987\n-1.868\n-1.868\n-5.591\n-0.379\n-0.906\n-0.990\n-121.842\n\n\n\n\n\n\n\nThe conclusion would be that our estimates are probably okay regardless of chosen optimizer, but the data likely has issues that still need to be overcome.\nAlong with different optimizers comes trying different packages. Some packages, in this case like glmmTMB or brms, would be viable options. In my playing around with those, glmmTMB converged, but obviously doesn’t magically overcome the collinearity problems. Likewise, brms appeared to converge, but had other estimation issues specific to it. Unlike the others, brms also noted that the zero count observations could not be used."
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#final-step",
    "href": "posts/2020-03-16-convergence/index.html#final-step",
    "title": "Convergence Problems",
    "section": "Final Step",
    "text": "Final Step\nThe final step would be to make some hard decisions about the model. Which predictors are most meaningful? Is this the best target variable available? Is this the right way to think about the distribution of the target? Is the research question clear enough? Is it strongly related to the available data, enough to be answerable?6\nHowever, these are more theoretical problems, not so much statistical ones, and there may be no right answer in the end. That is perfectly fine, and just what you’d report to others. The following model has no issues, but may leave more questions than answers.\n\nmodel_mixed_final = glmer(\n  ValidDetections ~ \n    c.receiver.depth_sc + c.temp_sc + tm + \n    replicate + SUR.ID + (1|Unit),\n    data = df,\n    family = poisson\n    )\n\nsummarize_model(model_mixed_final, ci = 0)\n\n\nVariance Components:\n\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     2.96 1.72     1.00\n\n\n\nFixed Effects:\n\n\n                Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n           Intercept -0.01 0.33 -0.02    0.98     -0.66       0.64\n c.receiver.depth_sc -0.86 0.14 -6.16    0.00     -1.13      -0.58\n           c.temp_sc  0.60 0.14  4.27    0.00      0.33       0.88\n             tmPT-04 -0.62 0.27 -2.27    0.02     -1.16      -0.08\n          replicate2 -0.11 0.27 -0.41    0.68     -0.65       0.42\n         SUR.ID10186  0.92 0.35  2.67    0.01      0.25       1.60\n         SUR.ID10250  1.10 0.35  3.18    0.00      0.42       1.78"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#summary",
    "href": "posts/2020-03-16-convergence/index.html#summary",
    "title": "Convergence Problems",
    "section": "Summary",
    "text": "Summary\nWe can summarize our approach to convergence problems as follows:\n\nStep back and look at your data. Are there issues? Are some variables essentially collinear with others or otherwise can be practically reduced?\nStart with the simplest but still plausible model. Do you spot any additional issues? If there are, these probably need to be remedied before moving to more complex models.\nScale your continuous variables. You should be doing this anyway. If some categorical levels have very relatively few observations, consider collapsing.\nFor mixed models, if some of your random effects only have a few levels, treat them as fixed or see if they are even needed in the model.\nFor mixed models, are any random effect variance estimates zero or nearly zero? Remove.\nAfter some initial technical checks, if possible, restart the model using previous starting values, and run until convergence. Many packages provide some functionality in this regard, but what exactly is provided may be limited.\nChange the optimizer (if a small mixed model using lme4, compare several with allFit function).\n\nIf log-likelihood and parameter estimates are pretty much the same across optimizers (and they are rarely notably different in my experience), pick one and go with it.\n\nUse a different package. The different estimation approach may simply work better for the current problem or provide other opportunities for tweaks. For example, with mixed models one could use glmmTMB or brms (Bayesian) in lieu of lme4.\n\nIn my experience with many clients with many types of data coming across many fields of study, the usual problem with convergence and lme4 (and others) is typically a fixable data problem, or a problematically specified model. In this case, the lme4 developers have worked hard over a number of years to build this awesome tool, and it works very well, so if it is having problems, you should be inspecting your data closely and thinking hard about your model. If it is an lme4 problem, switching optimizers will likely get you to convergence, but going through technical solutions should be a last resort."
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#references",
    "href": "posts/2020-03-16-convergence/index.html#references",
    "title": "Convergence Problems",
    "section": "References",
    "text": "References\nBates et al.(Bates et al. 2015)\n\nStackoverflow question(StackOverflow 2014)\n\nBolker’s answer given a little more cleanly(Bolker 2014?)\n\nHelp file for convergence(Bolker 2020)"
  },
  {
    "objectID": "posts/2020-03-16-convergence/index.html#footnotes",
    "href": "posts/2020-03-16-convergence/index.html#footnotes",
    "title": "Convergence Problems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBen Bolker is one of the primary developers of the lme4 package.↩︎\nFor a basic linear model situation, \\(1-1/VIF = R^2\\), where \\(R^2\\) is a regression model where a covariate is predicted by all the other covariates. We wouldn’t typically use a standard linear regression for binary outcomes or other scenarios, but this provides a quick and rough metric. The car package actually also provides a ‘generalized’ VIF though. In terms of interpretation, it tells us how much the standard error increases relative to the covariate if it was independent of the others. We would be concerned with redundancy of anything of VIF &gt; 10 / \\(R^2\\) &gt; .90, but maybe even less. As a final note, collinearity is basically a sample size issue, as larger data would reduce the standard errors, and we’d likely get a fuller sense of true variability in the covariates.↩︎\nThis outline mostly follows the documentation in the help file for ?convergence for lme4.↩︎\nThink of doing a principal components analysis on the variance-covariance matrix for the random effects. If one of those components is essentially accounting for zero variance, it may suggest at least one of the estimated random effects is not needed.↩︎\nUnfortunately there doesn’t appear to be much documentation on what should be listed in optinfo.↩︎\nTo paraphrase Barthelme slightly: “What is wonderful? Are these results wonderful? Are they significant? Are they what I need?”↩︎"
  },
  {
    "objectID": "posts/2020-04-10-psych-explained/index.html",
    "href": "posts/2020-04-10-psych-explained/index.html",
    "title": "Factor Analysis with the psych package",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\nPrerequisites: familiarity with factor analysis"
  },
  {
    "objectID": "posts/2020-04-10-psych-explained/index.html#introduction",
    "href": "posts/2020-04-10-psych-explained/index.html#introduction",
    "title": "Factor Analysis with the psych package",
    "section": "Introduction",
    "text": "Introduction\nThe psych package is a great tool for assessing underlying latent structure. It can provide reliability statistics, do cluster analysis, principal components analysis, mediation models, and, of course factor analysis. However, it’s been around a very long time, and many things have added to, subtracted, renamed, debugged, etc. And while the package author and noted psychometrician William Revelle even provides a freely available book on the details, it can still be difficult for many to ‘jump right in’ with the package. This is because it provides so much more than other tools, which is great, but which also can be overwhelming. Even I don’t recall what some of the output regards for factor analysis, and I use the package often. While a lot of it doesn’t matter for most use, it’d be nice to have a clean reference, so here it is.\nWhat follows is an explanation of the factor analysis results from the psych package, but much of it carries over into printed results for principal components via principal, reliability via omega, very simple structure via vss and others. Note that this is not an introduction to factor analysis, reliability, and related. It’s assumed you are already familiar with the techniques to some extent, and are interested in using the package for those analyses."
  },
  {
    "objectID": "posts/2020-04-10-psych-explained/index.html#demonstration",
    "href": "posts/2020-04-10-psych-explained/index.html#demonstration",
    "title": "Factor Analysis with the psych package",
    "section": "Demonstration",
    "text": "Demonstration\nWe will use the classic big-five personality measures, which comes with the package, but for our purposes, we’re just going to look at the agreeableness and neuroticism items. See ?bfi for details. With data in place we run a standard factor analysis, in this case, assuming two factors.\n\nlibrary(tidyverse)\nlibrary(psych)\n\ndata(bfi)\n\nbfi_trim = bfi %&gt;% select(matches('^A[1-5]|^N'))\n\nmodel = fa(bfi_trim, 2)\nmodel\n\nFactor Analysis using method =  minres\nCall: fa(r = bfi_trim, nfactors = 2)\nStandardized loadings (pattern matrix) based upon correlation matrix\n     MR1   MR2   h2   u2 com\nA1  0.07 -0.36 0.14 0.86 1.1\nA2  0.05  0.69 0.47 0.53 1.0\nA3  0.03  0.76 0.56 0.44 1.0\nA4 -0.05  0.47 0.24 0.76 1.0\nA5 -0.12  0.60 0.39 0.61 1.1\nN1  0.78 -0.03 0.61 0.39 1.0\nN2  0.76 -0.02 0.58 0.42 1.0\nN3  0.77  0.05 0.58 0.42 1.0\nN4  0.58 -0.08 0.36 0.64 1.0\nN5  0.54  0.08 0.29 0.71 1.0\n\n                       MR1  MR2\nSS loadings           2.44 1.78\nProportion Var        0.24 0.18\nCumulative Var        0.24 0.42\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\n With factor correlations of \n      MR1   MR2\nMR1  1.00 -0.19\nMR2 -0.19  1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  45  with the objective function =  2.82 with Chi Square =  7880.99\ndf of  the model are 26  and the objective function was  0.23 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  2759 with the empirical chi square  396.78  with prob &lt;  5.7e-68 \nThe total n.obs was  2800  with Likelihood Chi Square =  636.27  with prob &lt;  1.6e-117 \n\nTucker Lewis Index of factoring reliability =  0.865\nRMSEA index =  0.092  and the 90 % confidence intervals are  0.085 0.098\nBIC =  429.9\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.92 0.88\nMultiple R square of scores with factors          0.84 0.77\nMinimum correlation of possible factor scores     0.68 0.54\n\n\n\nLoadings\nThat’s a lot of stuff to work though. Let’s go through each part of the printed output.\n\nWhat’s MR, ML, PC etc.? These are factors, and the name merely reflects the fitting method, e.g. minimum residual, maximum likelihood, principal components. The default is minimum residual, so in this case MR.\nWhy are they ‘out of order’? the number assigned is arbitrary, but this has to do with a rotated solution. See the help file for more details, otherwise they are numbered in terms of variance accounted for.\nh2: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. communality.\nu2: 1 - h2. residual variance, a.k.a. uniqueness\ncom: Item complexity. Specifically it is “Hoffman’s index of complexity for each item. This is just \\({(Σ λ_i^2)^2}/{Σ λ_i^4}\\) where \\(λ_i\\) is the factor loading on the ith factor. From Hofmann (1978), MBR. See also Pettersson and Turkheimer (2010).” It equals one if an item loads only on one factor, 2 if evenly loads on two factors, etc. Basically it tells you how much an item reflects a single construct. It will be lower for relatively lower loadings.\n\n\n\nVariance accounted for\n                       MR1  MR2\nSS loadings           2.44 1.78\nProportion Var        0.24 0.18\nCumulative Var        0.24 0.42\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\nThe variance accounted for portion of the output can be explained as follows:\n\nSS loadings: These are the eigenvalues, the sum of the squared loadings. In this case where we are using a correlation matrix, summing across all factors would equal the number of variables used in the analysis.\nProportion Var: tells us how much of the overall variance the factor accounts for out of all the variables.\nCumulative Var: the cumulative sum of Proportion Var.\nProportion Explained: The relative amount of variance explained- Proportion Var/sum(Proportion Var).\nCumulative Proportion: the cumulative sum of Proportion Explained.\n\nThese are contained in model$Vaccounted.\n\n\nFactor correlations\n With factor correlations of \n      MR1   MR2\nMR1  1.00 -0.19\nMR2 -0.19  1.00\nWhether you get this part of the analysis depends on whether or not these are estimated. You have to have multiple factors and a rotation that allows for the correlations.\n\nfactor correlations: the correlation matrix for the factors. \\(\\phi\\) (phi)\nMean item complexity: the mean of com.\n\nThese are contained in model$Phi.\n\n\nModel test results\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  45  and the objective function was  2.82 with Chi Square of  7880.99\nThe degrees of freedom for the model are 26  and the objective function was  0.23 \n\nnull model: The degrees of freedom for the null model that assumes no correlation structure.\nobjective function: The value of the function that is minimized by a specific procedure.\nmodel: The one you’re actually interested in. Where p = Number of items, nf = number of factors then: degrees of freedom = \\[p * (p-1)/2 - p * nf + nf*(nf-1)/2\\] For the null model this is \\(p * (p-1)/2\\).\nChi-square: If f is the objective function value. Then \\[\\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * nf)/3)) * f\\]\nStrangely this is reported for the null but not the primary model result, which comes later.\n\n\n\nNumber of observations\nThe harmonic number of observations is  2759 with the empirical chi square  396.78  with prob &lt;  5.7e-68 \nThe total number of observations was  2800  with Likelihood Chi Square =  636.27  with prob &lt;  1.6e-117 \n\ntotal: the number of rows in the data you supplied for analysis\nharmonic: while one would assume it is the harmonic mean of the number of observations across items, it’s not this exactly, but is instead the harmonic mean of all the pairwise counts of observations (see ?pairwiseCount).\n\nThe \\(\\chi^2\\) reported here regards the primary model. So for your model you can report model$STATISTIC, model$dof, model$PVAL, which is what you see in the printed output for the total number of observations. As this regards the residual correlation matrix, a smaller value is better, as in SEM. The empirical chi-square is based on the harmonic sample size, so might be better, but I’ve never seen it reported.\n\n\nFit indices\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\n...\n\nTucker Lewis Index of factoring reliability =  0.865\nRMSEA index =  0.092  and the 90 % confidence intervals are  0.085 0.098\nBIC =  429.9\nFit based upon off diagonal values = 0.98\nThe nice thing about the psych package is that it reports SEM-style fit indices for standard factor analysis. You can find some more information via ?factor.stats.\n\nTLI: Tucker Lewis fit index, typically reported in SEM. Generally want &gt; .9\nRMSEA: Root mean square error of approximation. Also reported is the so-called ‘test of close fit’.\nRMSR: The (standardized) root mean square of the residuals. Also provided is a ‘corrected’ version, but I doubt this is reported by many.\nFit based upon off diagonal values: This is not documented anywhere I can find. However, you can think of it as 1 - resid^2 / cor^2, or a kind of \\(R^2\\) applied to a correlation matrix instead of raw data. It is calculated via factor.stats.\nBIC: Useful for model comparison purposes only.\n\n\n\nMeasures of factor score adequacy\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.92 0.88\nMultiple R square of scores with factors          0.84 0.77\nMinimum correlation of possible factor scores     0.68 0.54\nUnfortunately these are named in such a way as to be nearly indistinguishable, but there is some documentation for them in ?factor.stats. In general, these tell us how representative the factor score estimates are of the underlying constructs, and can be called indeterminancy indices. Indeterminancy refers to the fact that an infinite number of factor scores can be derived that would be consistent with a given set of loadings. In Revelle’s text, chapter 6.9 goes into detail, while Grice (2001) is a thorough review of the problem.\n\nCorrelation of (regression) scores with factors: square root of the Multiple R square. These can be seen as upper bounds of the determinancy of the factor score estimates that can be computed based on the model. It is essentially the (multiple) correlation of the factor and the observed data, as the name now more clearly suggests.\nMultiple R square...: “The multiple R square between the factors and factor score estimates, if they were to be found. (From Grice, 2001).” Computationally, it is roughly t(model$weights) %*% model$loadings, where the weights are the factor score coefficients, and can be seen as the maximum proportion of determinancy (higher is better). One way you can think about this is as an \\(R^2\\) for a regression model of the items predicting the estimated factor score.\nMinimum correlation...: Not documented, and is only shown as part of the print method, as it is not calculated as part of the factor analysis. But it is \\(2 \\cdot R^2 - 1\\), and so ranges from -1 to +1. If your \\(R^2\\) is less than .5, it will be negative, which is not good.\n\n\n\nMiscellaneous results\nThere is a lot of other stuff in these objects, like a sample size corrected BIC, Grice’s validity coefficients, the actual residuals for the correlation matrix and more.\n\nstr(model, 1)\n\nList of 54\n $ residual     : num [1:10, 1:10] 0.8556 -0.0899 0.0122 0.0377 0.0573 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ dof          : num 26\n $ chi          : num 397\n $ nh           : num 2759\n $ rms          : num 0.04\n $ EPVAL        : num 5.71e-68\n $ crms         : num 0.0526\n $ EBIC         : num 191\n $ ESABIC       : num 273\n $ fit          : num 0.789\n $ fit.off      : num 0.981\n $ sd           : num 0.0382\n $ factors      : num 2\n $ complexity   : Named num [1:10] 1.09 1.01 1 1.02 1.08 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ n.obs        : int 2800\n $ objective    : num 0.228\n $ criteria     : Named num [1:3] 0.228 NA NA\n  ..- attr(*, \"names\")= chr [1:3] \"objective\" \"\" \"\"\n $ STATISTIC    : num 636\n $ PVAL         : num 1.6e-117\n $ Call         : language fa(r = bfi_trim, nfactors = 2)\n $ null.model   : num 2.82\n $ null.dof     : num 45\n $ null.chisq   : num 7881\n $ TLI          : num 0.865\n $ CFI          : num 0.922\n $ RMSEA        : Named num [1:4] 0.0916 0.0855 0.0978 0.9\n  ..- attr(*, \"names\")= chr [1:4] \"RMSEA\" \"lower\" \"upper\" \"confidence\"\n $ BIC          : num 430\n $ SABIC        : num 513\n $ r.scores     : num [1:2, 1:2] 1 -0.227 -0.227 1\n $ R2           : num [1:2] 0.842 0.768\n $ valid        : num [1:2] 0.905 0.852\n $ score.cor    : num [1:2, 1:2] 1 -0.185 -0.185 1\n $ weights      : num [1:10, 1:2] 0.00559 0.00986 -0.00452 -0.01416 -0.03449 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ rotation     : chr \"oblimin\"\n $ hyperplane   : Named num [1:2] 5 5\n  ..- attr(*, \"names\")= chr [1:2] \"MR1\" \"MR2\"\n $ communality  : Named num [1:10] 0.144 0.467 0.563 0.237 0.395 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ communalities: Named num [1:10] 0.144 0.467 0.563 0.237 0.395 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ uniquenesses : Named num [1:10] 0.856 0.533 0.437 0.763 0.605 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ values       : num [1:10] 2.6714 1.5564 0.2727 0.1283 0.0455 ...\n $ e.values     : num [1:10] 3.185 2.11 0.938 0.768 0.71 ...\n $ loadings     : 'loadings' num [1:10, 1:2] 0.0744 0.0541 0.0311 -0.0519 -0.1191 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ model        : num [1:10, 1:10] 0.144 -0.25 -0.277 -0.184 -0.239 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ fm           : chr \"minres\"\n $ rot.mat      : num [1:2, 1:2] 0.857 0.549 -0.38 0.944\n $ Phi          : num [1:2, 1:2] 1 -0.186 -0.186 1\n  ..- attr(*, \"dimnames\")=List of 2\n $ Structure    : 'loadings' num [1:10, 1:2] 0.1413 -0.0748 -0.1098 -0.1402 -0.23 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ method       : chr \"regression\"\n $ scores       : num [1:2800, 1:2] -0.224 0.2186 0.532 -0.2429 -0.0564 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ R2.scores    : Named num [1:2] 0.842 0.768\n  ..- attr(*, \"names\")= chr [1:2] \"MR1\" \"MR2\"\n $ r            : num [1:10, 1:10] 1 -0.34 -0.265 -0.146 -0.181 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ np.obs       : num [1:10, 1:10] 2784 2757 2759 2767 2769 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ fn           : chr \"fa\"\n $ Vaccounted   : num [1:5, 1:2] 2.443 0.244 0.244 0.578 0.578 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ ECV          : Named num [1:2] 0.578 1\n  ..- attr(*, \"names\")= chr [1:2] \"MR1\" \"MR2\"\n - attr(*, \"class\")= chr [1:2] \"psych\" \"fa\"\n\n\nIn turn, these are:\n\nresidual: The residual correlation matrix\ndof: The model degrees of freedom\nchi: The empirical model \\(X^2\\)\nnh: The harmonic sample size\nrms: Root mean square residual\nEPVAL: p-value for the empirical chi-square\ncrms: a ‘corrected’ rms\nEBIC: BIC for the empirical model\nESABIC: sample-size corrected BIC for the empirical model\nfit: Similar to fit.off. General fit index (how well is the observed correlation reproduced)\nfit.off: Fit based on off diagonals. Reported in the output. See above.\nsd: standard deviation of the off-diagonals of the residual correlation matrix.\nfactors: the number of factors\ncomplexity: The complexity scores. See above.\nn.obs: The number of observations (assuming complete data)\nobjective: The objective function for the model\ncriteria: Along with the objective function, additional fitting results\nSTATISTIC: The model-based \\(X^2\\)\nPVAL: The p-value for the model-based \\(X^2\\)\nCall: The function call\nnull.model: \\(X^2\\) test results for the null model\nnull.dof: \\(X^2\\) test results for the null model\nnull.chisq: \\(X^2\\) test results for the null model\nTLI: Tucker-Lewis fit index\nRMSEA: Root mean square error of approximation with upper and lower bounds\nBIC: Bayesian Information Criterion for the model\nSABIC: sample-size corrected BIC for the model\nr.scores: The correlations of the factor score estimates using the specified model, if they were to be found. Comparing these correlations with that of the scores themselves will show, if an alternative estimate of factor scores is used (e.g., the tenBerge method), the problem of factor indeterminacy. For these correlations will not necessarily be the same.\nR2: correlation of factors and estimated factor scores (squared)\nvalid: validity coefficients\nscore.cor: The correlation matrix of course coded (unit weighted) factor score estimates (i.e. sum scores), if they were to be found, based upon the loadings matrix rather than the weights matrix.\nweights: weights used to construct factor scores\nrotation: rotation used\ncommunality: communality scores h2\ncommunalities: So nice they put them twice (actually not entirely equal)\nuniquenesses: Uniquenesses u2\nvalues eigenvalues of the model implied correlation matrix\ne.values: eigenvalues of the correlation matrix\nloadings: the factor loadings\nmodel: the model-implied correlation matrix\nfm: the estimation approach\nrot.mat: matrix used in the rotated solution\nPhi: factor score correlation matrix\nStructure: this is just the loadings (pattern) matrix times the factor intercorrelation matrix (Phi).\nmethod: method used to construct the factor scores\nscores: estimated factor scores\nR2.scores: estimated correlation of factor scores with the factor (squared)\nr: the (possibly smoothed) correlation matrix of the observation\nnp.obs: pairwise sample sizes (used to get the harmonic mean)\nfn: the function used\nVaccounted: the SS loadings output."
  },
  {
    "objectID": "posts/2020-04-10-psych-explained/index.html#additional-notes-for-factor-analysis",
    "href": "posts/2020-04-10-psych-explained/index.html#additional-notes-for-factor-analysis",
    "title": "Factor Analysis with the psych package",
    "section": "Additional Notes for Factor Analysis",
    "text": "Additional Notes for Factor Analysis\n\nMost of the above would apply to other versions of fa and principal for principal components analysis.\nThough rarely done, if you only provide a correlation matrix as your data, you will not get a variety of metrics in the results, nor factor scores.\nCertain rotations will lead to differently named factors, and possibly lacking some output (e.g. varimax won’t have factor correlations)."
  },
  {
    "objectID": "posts/2020-04-10-psych-explained/index.html#other-functions",
    "href": "posts/2020-04-10-psych-explained/index.html#other-functions",
    "title": "Factor Analysis with the psych package",
    "section": "Other Functions",
    "text": "Other Functions\nWhile the previous will help explain factor analysis and related models, a similar issue arises elsewhere with other package functions that might be of interest. I’ll explain some of those here as interest and personal use dictates.\n\nAlpha\nThis is a quick reminder for the results of the reliability coefficient \\(\\alpha\\). For this we’ll just use the agreeableness items to keep things succinct.\n\nBasic Results\n\nagreeableness = bfi_trim[,1:5]\n\n# check.keys will rescale negatively scored items\nalpha_results = alpha(agreeableness, check.keys = TRUE, n.iter=10) \nalpha_results\n\n\nReliability analysis   \nCall: alpha(x = agreeableness, check.keys = TRUE, n.iter = 10)\n\n  raw_alpha std.alpha G6(smc) average_r  S/N   ase mean  sd median_r\n       0.7      0.46    0.53      0.14 0.85 0.009  4.7 0.9     0.32\n\n    95% confidence boundaries \n             lower alpha upper\nFeldt         0.69   0.7  0.72\nDuhachek      0.69   0.7  0.72\nbootstrapped  0.69   0.7  0.72\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r\nA1-      0.72      0.72    0.67     0.397 2.63   0.0087 0.0065 0.375\nA2       0.62      0.30    0.39     0.096 0.43   0.0119 0.1095 0.081\nA3       0.60      0.21    0.31     0.061 0.26   0.0124 0.1014 0.081\nA4       0.69      0.30    0.44     0.099 0.44   0.0098 0.1604 0.104\nA5       0.64      0.24    0.36     0.071 0.31   0.0111 0.1309 0.094\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nA1- 2784  0.58 0.024 -0.39   0.31  4.6 1.4\nA2  2773  0.73 0.665  0.58   0.56  4.8 1.2\nA3  2774  0.76 0.742  0.71   0.59  4.6 1.3\nA4  2781  0.65 0.661  0.50   0.39  4.7 1.5\nA5  2784  0.69 0.719  0.64   0.49  4.6 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.29 0.14 0.12 0.08 0.03 0.01\nA2 0.02 0.05 0.05 0.20 0.37 0.31 0.01\nA3 0.03 0.06 0.07 0.20 0.36 0.27 0.01\nA4 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nA5 0.02 0.07 0.09 0.22 0.35 0.25 0.01\n\n\n\nraw_alpha: Raw estimate of alpha (based on covariances)\nstd.alpha: Standardized estimate. This value is what is typically reported (though most applied researchers would probably not be able to tell you which they reported). Identical to raw if data is already standardized.\nG6 (smc): Guttman’s \\(\\lambda_6\\), the amount of variance in each item that can be accounted for the linear regression of all of the other items\naverage_r: Average inter-item correlation among the items.\nS/N: Signal to noise ratio, \\(n \\cdot r/(1-r)\\) where r is the average_r\nase: standard error for raw \\(\\alpha\\) (used for the confidence interval, bootstrapped would be better)\nmean: the mean of the total/mean score of the items\nsd: the standard deviation of the total/mean score of the items\nmedian_r: median inter-item correlation\n\nAfter the initial statistics, the same stats are reported but for a result where a specific item is dropped. For example, if your \\(\\alpha\\) goes up when an item is dropped, it probably isn’t a good item. In this case, the negatively scored item is probably worst, which isn’t an uncommon result.\n\n\nItem statistics\nNext we get the item statistics, they are as follows.\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nA1- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nA2  2773  0.73  0.75  0.67   0.56  4.8 1.2\nA3  2774  0.76  0.77  0.71   0.59  4.6 1.3\nA4  2781  0.65  0.63  0.47   0.39  4.7 1.5\nA5  2784  0.69  0.70  0.60   0.49  4.6 1.3\n\nn: number of complete observations\nraw.r: correlation of each item with the total score\nstd.r: correlation of each item with the total score if the items were all standardized\nr.cor: item correlation corrected for item overlap and scale reliability\nr.drop item correlation for this item against the scale without this item\nmean: item mean\nsd: item standard deviation\n\n\n\nResponse frequency\nFinally we have information about the missingness of each item. The initial values show the proportion of each level observed, while the last column shows the percentage missing.\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.29 0.14 0.12 0.08 0.03 0.01\nA2 0.02 0.05 0.05 0.20 0.37 0.31 0.01\nA3 0.03 0.06 0.07 0.20 0.36 0.27 0.01\nA4 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nA5 0.02 0.07 0.09 0.22 0.35 0.25 0.01\n\n\nOther Output\nIn addition to these we have a bit more from the output.\n\nstr(alpha_results[5:length(alpha_results)])\n\nList of 11\n $ keys   :List of 1\n  ..$ : chr [1:5] \"-A1\" \"A2\" \"A3\" \"A4\" ...\n $ scores : Named num [1:2800] 4 4.2 3.8 4.6 4 4.6 4.6 2.6 3.6 5.4 ...\n  ..- attr(*, \"names\")= chr [1:2800] \"61617\" \"61618\" \"61620\" \"61621\" ...\n $ nvar   : int 5\n $ boot.ci: Named num [1:3] 0.692 0.702 0.717\n  ..- attr(*, \"names\")= chr [1:3] \"2.5%\" \"50%\" \"97.5%\"\n $ boot   : num [1:10, 1:10] 0.712 0.692 0.711 0.718 0.695 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:10] \"raw_alpha\" \"std.alpha\" \"G6(smc)\" \"average_r\" ...\n $ feldt  :List of 4\n  ..$ lower.ci:'data.frame':    1 obs. of  1 variable:\n  .. ..$ raw_alpha: num 0.685\n  ..$ alpha   :'data.frame':    1 obs. of  1 variable:\n  .. ..$ raw_alpha: num 0.703\n  ..$ upper.ci:'data.frame':    1 obs. of  1 variable:\n  .. ..$ raw_alpha: num 0.72\n  ..$ r.bar   :'data.frame':    1 obs. of  1 variable:\n  .. ..$ raw_alpha: num 0.321\n  ..- attr(*, \"class\")= chr [1:2] \"psych\" \"alpha.ci\"\n $ Unidim :List of 1\n  ..$ Unidim: num 0.237\n $ var.r  : num 0.112\n $ Fit    :List of 1\n  ..$ Fit.off: num 0.879\n $ call   : language alpha(x = agreeableness, check.keys = TRUE, n.iter = 10)\n $ title  : NULL\n\n\nIn turn these are:\n\nkeys: how the items are score (-1 if reverse scored)\nscores: row means/sums depending on the cumulative argument\nnvar: the number of variables/items\nboot.ci: the bootstrapped confidence interval for \\(\\alpha\\) (if requested)\nboot: the bootstrapped values of \\(\\alpha\\) and other statistics (if requested)\nUnidim: index of unidimensionalty. \\(\\alpha\\) is a lower bound of a reliability estimate if the data is not unidimensional. See ?unidim for details.\nvar.r: This doesn’t appear to be documented anywhere, but it is depicted in the Reliability if item is dropped section. It is the variance of the values of the lower triangle of a correlation matrix.\nFit: see Fit based upon off diagonal values for the factor analysis section above.\ncall: the function call\ntitle: title of the results (if requested)\n\n\n\n\nOmega\nOmega is another reliability metric that finally has been catching on. The psych function omega requires a factor analysis to be run behind the scenes, specifically a bifactor model, so most of the output is the same as with other factor analysis. In addition, the results also provide coefficient \\(\\alpha\\) and Guttman’s \\(\\lambda_6\\) that were explained in the alpha section.\nHowever there is a little more to it, so we’ll explain those aspects. The key help files are ?omega and ?schmid.\n\nomega_result = omega(bfi[,1:15])\n\n\n\n\n\n\n\nomega_result\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.81 \nG.6:                   0.83 \nOmega Hierarchical:    0.54 \nOmega H asymptotic:    0.64 \nOmega Total            0.85 \n\nSchmid Leiman Factor loadings greater than  0.2 \n        g   F1*   F2*   F3*   h2   u2   p2\nA1-  0.24              0.30 0.16 0.84 0.36\nA2   0.52              0.44 0.47 0.53 0.58\nA3   0.59              0.45 0.56 0.44 0.63\nA4   0.39              0.28 0.25 0.75 0.62\nA5   0.56              0.31 0.44 0.56 0.70\nC1               0.53       0.31 0.69 0.11\nC2   0.23        0.60       0.42 0.58 0.12\nC3   0.21        0.51       0.32 0.68 0.14\nC4-  0.25        0.59       0.41 0.59 0.15\nC5-  0.28        0.51       0.34 0.66 0.23\nE1-  0.37  0.47             0.37 0.63 0.38\nE2-  0.48  0.54             0.53 0.47 0.43\nE3   0.47  0.36             0.37 0.63 0.61\nE4   0.54  0.46             0.51 0.49 0.57\nE5   0.41  0.32  0.25       0.33 0.67 0.51\n\nWith Sums of squares  of:\n   g  F1*  F2*  F3* \n2.47 1.03 1.60 0.69 \n\ngeneral/max  1.54   max/min =   2.32\nmean percent general =  0.41    with sd =  0.21 and cv of  0.52 \nExplained Common Variance of the general factor =  0.43 \n\nThe degrees of freedom are 63  and the fit is  0.34 \nThe number of observations was  2800  with Chi Square =  961.82  with prob &lt;  6.4e-161\nThe root mean square of the residuals is  0.04 \nThe df corrected root mean square of the residuals is  0.05\nRMSEA index =  0.071  and the 10 % confidence intervals are  0.067 0.075\nBIC =  461.77\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 90  and the fit is  1.58 \nThe number of observations was  2800  with Chi Square =  4407.94  with prob &lt;  0\nThe root mean square of the residuals is  0.13 \nThe df corrected root mean square of the residuals is  0.14 \n\nRMSEA index =  0.131  and the 10 % confidence intervals are  0.128 0.134\nBIC =  3693.57 \n\nMeasures of factor score adequacy             \n                                                 g   F1*  F2*   F3*\nCorrelation of scores with factors            0.78  0.70 0.82  0.60\nMultiple R square of scores with factors      0.61  0.49 0.68  0.36\nMinimum correlation of factor score estimates 0.22 -0.01 0.36 -0.28\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*  F3*\nOmega total for total scores and subscales    0.85 0.77 0.73 0.73\nOmega general for total scores and subscales  0.54 0.40 0.11 0.46\nOmega group for total scores and subscales    0.25 0.36 0.62 0.27\n\n\nThe bifactor model requires a single general factor and minimally three specific factors, as the plot shows. However, you can run it on single factor just to get the omega total statistic, but any less than three factors will produce a warning and some metrics will either be unavailable or not make much sense.\n\nReliability\nAlpha:                 0.81 \nG.6:                   0.83 \nOmega Hierarchical:    0.54 \nOmega H asymptotic:    0.64 \nOmega Total            0.85 \nThe first two pieces of info are as in alpha, the next regard \\(\\omega\\) specifically. \\(\\omega\\) is based on the squared factor loadings. \\(\\omega_{hierarchical}\\) regards just the loadings of the general factor. The asymptotic is the same for a ‘test of infinite items’, and so can be seen as an upper bound. \\(\\omega_{total}\\) is based on all the general and specific factor loadings. I personally like to think of the ratio of \\(\\frac{\\omega_{hier}}{\\omega_{total}}\\), which if very high, say .9 or so, may suggest unidimensionality.\n\n\nLoadings\nSchmid Leiman Factor loadings greater than  0.2 \n        g   F1*   F2*   F3*   h2   u2   p2\nA1-  0.24              0.30 0.16 0.84 0.36\nA2   0.52              0.44 0.47 0.53 0.58\nA3   0.59              0.45 0.56 0.44 0.63\nA4   0.39              0.28 0.25 0.75 0.62\nA5   0.56              0.31 0.44 0.56 0.70\nC1               0.53       0.31 0.69 0.11\nC2   0.23        0.60       0.42 0.58 0.12\nC3   0.21        0.51       0.32 0.68 0.14\nC4-  0.25        0.59       0.41 0.59 0.15\nC5-  0.28        0.51       0.34 0.66 0.23\nE1-  0.37  0.47             0.37 0.63 0.38\nE2-  0.48  0.54             0.53 0.47 0.43\nE3   0.47  0.36             0.37 0.63 0.61\nE4   0.54  0.46             0.51 0.49 0.57\nE5   0.41  0.32  0.25       0.33 0.67 0.51\n\nWith eigenvalues of:\n   g  F1*  F2*  F3* \n2.47 1.03 1.60 0.69 \n\ngeneral/max  1.54   max/min =   2.32\nmean percent general =  0.41    with sd =  0.21 and cv of  0.52 \nExplained Common Variance of the general factor =  0.43 \nThe loadings are for the general and specific factors are provided, as well as the communalities and uniquenesses. In addition there is a column for p2, which is considered a diagnostic tool for the appropriateness of a hierarchical model. It is defined as “percent of the common variance for each variable that is general factor variance”, which is just g2/h2. The line of mean percent general... isn’t documented and is a result of the unexported print.psych.omega function. It wasn’t obvious to me, but these are merely statistics regarding the p2 column (cv is the coefficient of variation).\nNext you get eigenvalue/variance accounted as in standard factor analysis. Then general/max and max/min regard those ratios of the corresponding eigenvalues. Explained common variance is the percent of variance attributable to the general factor (g/sum(all eigenvalues))\n\n\nModel test results & fit\nThe degrees of freedom are 63  and the fit is  0.34 \nThe number of observations was  2800  with Chi Square =  961.82  with prob &lt;  6.4e-161\nThe root mean square of the residuals is  0.04 \nThe df corrected root mean square of the residuals is  0.05\nRMSEA index =  0.071  and the 10 % confidence intervals are  0.067 0.075\nBIC =  461.77\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 90  and the fit is  1.58 \nThe number of observations was  2800  with Chi Square =  4407.94  with prob &lt;  0\nThe root mean square of the residuals is  0.13 \nThe df corrected root mean square of the residuals is  0.14 \n\nRMSEA index =  0.131  and the 10 % confidence intervals are  0.128 0.134\nBIC =  3693.57 \nThe only thing different here relative to the standard factor analysis results is that there are two models considered- a model with general and specific factors and a model with no specific factors.\n\n\nMeasures of factor score adequacy\nMeasures of factor score adequacy             \n                                                 g   F1*  F2*   F3*\nCorrelation of scores with factors            0.78  0.70 0.82  0.60\nMultiple R square of scores with factors      0.61  0.49 0.68  0.36\nMinimum correlation of factor score estimates 0.22 -0.01 0.36 -0.28\nThis first part of the output is the same as standard factor analysis (see above).\n\n\nVariance accounted for by group and specific factors\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*  F3*\nOmega total for total scores and subscales    0.85 0.77 0.73 0.73\nOmega general for total scores and subscales  0.54 0.40 0.11 0.46\nOmega group for total scores and subscales    0.25 0.36 0.62 0.27\nThis part is explained in the ?omega helpfile as:\n\nThe notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory. Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale. Thus, we can find a number of different omega estimates: what percentage of the variance of the items identified with each subfactor is actually due to the general factor. What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor. These results are reported in omega.group object and in the last few lines of the normal output.\n\nAs noted, this is contained in omega_result$omega.group. For the unique factors, these sum very simply as total = general + group. The ones for unique factors pertain only to the loadings and part of the correlation matrix for those items specific to that factor. Take agreeableness for example, we are only concerned with the variance of those items. The ‘general’ part regards the loadings of g for the agreeableness items, the group part the loadings of the agreeableness items, and the ‘total’ is just their sum.\nThe first column, g, just regurgitates \\(\\omega\\) and \\(\\omega_h\\) from the beginning for the first two values, and adds yet another statistic, based only on the sum of variance attributable to each unique factor. Unlike the \\(\\omega_{total}\\), this calculation does not include off-loadings the unique factors have, only the items that are grouped with each factor. In pseudo-code:\nfor (i in specific) {\n specific_var[i] = sum(specific[i]$loadings[specific_items[i]])^2)\n}\n\nvalue = sum(specific_var) / total_var\nThat is the variance uniquely defined by the specific factors. Had it included all the loadings for each specific factor calculation, then group + general = total for g as well.\n\n\n\nUnidimensionality\nRevelle provides an ‘exploratory’ statistic of unidimensionality, or how well a set of variables may be explained by one construct. In practice, you may find multiple factors fit better, e.g. via BIC, but the resulting factors may be highly correlated, so you might still want to consider a single construct. Something like unidim will help make a decision on how viable using a sum score might be for regression or other models. It is explained in the help file as follows:\n\nThe fit FF’ (model implied correlation matrix based on a one factor model) should be identical to the (observed) correlation matrix minus the uniquenesses. unidim is just the ratio of these two estimates. The higher it is, the more the evidence for unidimensionality.\n\nI’ll run it for both the case where there is only a single construct vs. two underlying constructs.\n\nunidim(bfi[,1:5])\n\n\nA measure of unidimensionality \n Call: unidim(keys = bfi[, 1:5])\n\nUnidimensionality index = \n       u      tau      con    alpha     av.r median.r      CFI      ECV \n    0.89     0.90     0.99     0.71     0.33     0.34     0.97     0.85 \n\nunidim adjusted index reverses negatively scored items.\nalpha    Based upon reverse scoring some items.\naverage and median  correlations are based upon reversed scored items\n\nunidim(bfi_trim)\n\n\nA measure of unidimensionality \n Call: unidim(keys = bfi_trim)\n\nUnidimensionality index = \n       u      tau      con    alpha     av.r median.r      CFI      ECV \n    0.45     0.62     0.73     0.75     0.23     0.17     0.60     0.58 \n\nunidim adjusted index reverses negatively scored items.\nalpha    Based upon reverse scoring some items.\naverage and median  correlations are based upon reversed scored items\n\n\nThe values reported are as follows. In general, you’d pay attention to the adjusted results that are based on items that are reverse scored if needed. If there are no reverse scored items (which you generally should be doing), then these adjusted metrics will be identical to the raw metrics.\n\nRaw Unidim: The raw value of the unidimensional criterion\nAdjusted: The unidimensional criterion when items are keyed in positive direction.\nFit1: The off diagonal fit from fa. (explained above)\nalpha: Standardized \\(\\alpha\\) of the keyed items (after appropriate reversals)\nav.r: The average inter-item correlation of the keyed items.\noriginal model: The ratio of the FF’ (model implied correlation matrix based on the loadings) model to the sum(R).\nadjusted model: The ratio of the FF’ model to the sum(R) when items are flipped.\nraw.total: sum(R - uniqueness)/sum(R)\nadjusted total: raw.total ratio with flipped items\n\n\n\nMediate\nThe psych makes even somewhat complicated mediation models about as easily conducted as they can be, assuming you are only dealing with fully observed (no latent) variables and linear models with continuous endogenous variables that are assumed to be normally distributed. Though the output should be straightforward if one understands basic regression as well as the basics of mediation, we demonstrate it here.\n\nInitial Model\nGarcia, Schmitt, Branscome, and Ellemers (2010) report data for 129 subjects on the effects of perceived sexism on anger and liking of women’s reactions to in-group members who protest discrimination. We will predict liking (how much the individual liked the target) while using protest (prot2 yes or no) and sexism (a scale score based on multiple items) as predictors, and a scaled score of the appropriateness of the target’s response as the mediator (respappr).\n\ndata(GSBE)   # The Garcia et al data set; see ?GSBE for details\n\nmodel &lt;- mediate(\n  liking ~  sexism + prot2 + (respappr),\n  data   = Garcia,\n  n.iter = 500,\n  plot   = FALSE\n)   \n\n\n\nVisual Results\nTo start, most of the output of psych is straightforward if you understand what mediation is, as it follows the same depiction and even uses the same labels as most initial demonstrations of mediation I’ve come across. So if it’s confusing, you probably need to review what such models are attempting to accomplish. The visualization it automatically produces is even clearer for storytelling. I reserved plotting for display here so as to make it easier to compare to the printed output.\n\nmediate.diagram(model, digits = 3)\n\n\n\n\n\n\n\n\nWe see the original effects of sexism and prot2 as c, and what they are after including the mediator c', where the difference between those values is equivalent to a * b, i.e. the indirect effect (a is the coefficient from the predictor to mediator, b is from the mediator to the outcome). The rest are standard path/regression coefficients as well.\n\n\nStatistical Results\nNow we will just print the result.\n\nprint(model, digits = 3)\n\n\nMediation/Moderation Analysis \nCall: mediate(y = liking ~ sexism + prot2 + (respappr), data = Garcia, \n    n.iter = 500, plot = FALSE)\n\nThe DV (Y) was  liking . The IV (X) was  sexism prot2 . The mediating variable(s) =  respappr .\n\nTotal effect(c) of  sexism  on  liking  =  0.111   S.E. =  0.116  t  =  0.956  df=  126   with p =  0.341\nDirect effect (c') of  sexism  on  liking  removing  respappr  =  0.096   S.E. =  0.104  t  =  0.923  df=  125   with p =  0.358\nIndirect effect (ab) of  sexism  on  liking  through  respappr   =  0.015 \nMean bootstrapped indirect effect =  0.018  with standard error =  0.056  Lower CI =  -0.09    Upper CI =  0.129\n\nTotal effect(c) of  prot2  on  liking  =  0.471   S.E. =  0.195  t  =  2.417  df=  126   with p =  0.0171\nDirect effect (c') of  prot2  on  liking  removing  respappr  =  -0.105   S.E. =  0.201  t  =  -0.522  df=  125   with p =  0.602\nIndirect effect (ab) of  prot2  on  liking  through  respappr   =  0.576 \nMean bootstrapped indirect effect =  0.563  with standard error =  0.15  Lower CI =  0.306    Upper CI =  0.887\nR = 0.501 R2 = 0.251   F = 13.967 on 3 and 125 DF   p-value:  1.903e-09 \n\n To see the longer output, specify short = FALSE in the print statement or ask for the summary\n\n\nThe output simply shows the same results as the graph. The total effect is the effect of a covariate on the outcome, without the mediator. For example, the effect of sexism on liking without the mediation is 0.111. We can reproduce it as follows. The statistical result is identical to the lm output.\n\nsummary(lm(liking ~  sexism + prot2, data = Garcia))\n\n\nCall:\nlm(formula = liking ~ sexism + prot2, data = Garcia)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3857 -0.6246  0.0599  0.7754  1.7954 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.7468     0.6110   7.768 2.41e-12 ***\nsexism        0.1111     0.1162   0.956   0.3410    \nprot2         0.4711     0.1949   2.417   0.0171 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.03 on 126 degrees of freedom\nMultiple R-squared:  0.0523,    Adjusted R-squared:  0.03726 \nF-statistic: 3.477 on 2 and 126 DF,  p-value: 0.03391\n\n\nThe direct effect is the effect of sexism with the mediator in the model. We can reproduce the effect here. However this version of the model printout currently has a bug where, after the coefficient, it is reporting SE t etc. from the intercept. If you do a summary(model), as we will shortly, you’ll get the correct statistical test until it is fixed.\n\nsummary(lm(liking ~  sexism + prot2 + respappr, data = Garcia))\n\n\nCall:\nlm(formula = liking ~ sexism + prot2 + respappr, data = Garcia)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0210 -0.5337  0.0874  0.6591  2.6760 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.26766    0.60281   5.421 2.94e-07 ***\nsexism       0.09584    0.10379   0.923    0.358    \nprot2       -0.10482    0.20065  -0.522    0.602    \nrespappr     0.40075    0.06958   5.760 6.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9193 on 125 degrees of freedom\nMultiple R-squared:  0.2511,    Adjusted R-squared:  0.2331 \nF-statistic: 13.97 on 3 and 125 DF,  p-value: 6.545e-08\n\n\nThe indirect effect coefficient is the product of the a and b paths: 0.038 * 0.401. Along with this, the bootstrapped interval estimate is provided (you can ignore the mean bootstrapped effect, which is equal to the effect with enough iterations). There is no p-value, but it’s not needed anyway for any of these results.\nAfter that, the same results are provided for the prot2 predictor. Finally, the \\(R^2\\) and F test for the overall model are reported, which are identical to the lm summary results that include all effects. I would suggest reporting the adjusted \\(R^2\\) from that instead.\nA much cleaner result that incorporates the lm results we did can be obtained by summarizing instead of printing the fitted model. All of this output is available as elements of the model object itself.\n\nsummary(model)\n\nCall: mediate(y = liking ~ sexism + prot2 + (respappr), data = Garcia, \n    n.iter = 500, plot = FALSE)\n\nDirect effect estimates (traditional regression)    (c') X + M on Y \n          liking   se     t  df     Prob\nIntercept   3.27 0.60  5.42 125 2.94e-07\nsexism      0.10 0.10  0.92 125 3.58e-01\nprot2      -0.10 0.20 -0.52 125 6.02e-01\nrespappr    0.40 0.07  5.76 125 6.18e-08\n\nR = 0.5 R2 = 0.25   F = 13.97 on 3 and 125 DF   p-value:  6.54e-08 \n\n Total effect estimates (c) (X on Y) \n          liking   se    t  df     Prob\nIntercept   4.75 0.61 7.77 126 2.41e-12\nsexism      0.11 0.12 0.96 126 3.41e-01\nprot2       0.47 0.19 2.42 126 1.71e-02\n\n 'a'  effect estimates (X on M) \n          respappr   se    t  df     Prob\nIntercept     3.69 0.70 5.29 126 5.33e-07\nsexism        0.04 0.13 0.29 126 7.75e-01\nprot2         1.44 0.22 6.45 126 2.15e-09\n\n 'b'  effect estimates (M on Y controlling for X) \n         liking   se    t  df     Prob\nrespappr    0.4 0.07 5.76 125 6.18e-08\n\n 'ab'  effect estimates (through all  mediators)\n       liking boot   sd lower upper\nsexism   0.02 0.02 0.06 -0.09  0.13\nprot2    0.58 0.56 0.15 -0.09  0.13\n\n\nIf you are doing mediation with linear models only, you would be hard-pressed to find an easier tool to use than the psych package. It can incorporate multiple mediators and so-called ‘moderated mediation’ as well. However, just because it is easy to do a mediation model, doesn’t mean you should."
  },
  {
    "objectID": "posts/2020-04-10-psych-explained/index.html#conclusion",
    "href": "posts/2020-04-10-psych-explained/index.html#conclusion",
    "title": "Factor Analysis with the psych package",
    "section": "Conclusion",
    "text": "Conclusion\nThe psych package is very powerful and provides a lot of results from just a single line of code. However, the documentation, while excellent in general, fails to note many pieces of output, or clearly explain it, at least, not without consulting other references (which are provided). Hopefully this saves others some time when they use it. I may add some other functions to explain in time, so check back at some point."
  },
  {
    "objectID": "posts/2020-07-10-eda/index.html",
    "href": "posts/2020-07-10-eda/index.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In R there are many tools available to help you dive in and explore your data. However, in consulting I still see a lot of people using base R’s table and summary functions, followed by a lot of work to get the result into a more presentable format. My own frustrations led to me creating a package (tidyext) for personal use in this area. While that suits me fine, there are tools that can go much further with little effort. Recently, Staniak & Biecek wrote an article in the R Journal exploring several of such packages, so I thought I’d try them out for myself, and take others along with me for that ride.\nAs this will be a workshop/demo, I’ve created a separate repo and document to make it easier to find, so here is the link: https://m-clark.github.io/exploratory-data-analysis-tools/\nThe packages demoed are:\n\narsenal\nDataExplorer\ndataMaid\ngtsummary\njanitor (not explored in the previous article)\nSmartEDA\nsummarytools\nvisdat\n\n\n\n\nTukey"
  },
  {
    "objectID": "posts/2020-07-10-eda/index.html#introduction",
    "href": "posts/2020-07-10-eda/index.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In R there are many tools available to help you dive in and explore your data. However, in consulting I still see a lot of people using base R’s table and summary functions, followed by a lot of work to get the result into a more presentable format. My own frustrations led to me creating a package (tidyext) for personal use in this area. While that suits me fine, there are tools that can go much further with little effort. Recently, Staniak & Biecek wrote an article in the R Journal exploring several of such packages, so I thought I’d try them out for myself, and take others along with me for that ride.\nAs this will be a workshop/demo, I’ve created a separate repo and document to make it easier to find, so here is the link: https://m-clark.github.io/exploratory-data-analysis-tools/\nThe packages demoed are:\n\narsenal\nDataExplorer\ndataMaid\ngtsummary\njanitor (not explored in the previous article)\nSmartEDA\nsummarytools\nvisdat\n\n\n\n\nTukey"
  },
  {
    "objectID": "posts/2020-11-30-models-by-example/index.html",
    "href": "posts/2020-11-30-models-by-example/index.html",
    "title": "Models by Example",
    "section": "",
    "text": "New Book\nI’ve completed a new bookdown document, Models by Example, that converts most of the code from my Miscellaneous R repo. I initially just wanted to update the code, but decided to use a more formal approach to make it cleaner and more accessible. It’s mostly complete, though may be added to on rare occasion, and further cleaned as I find annoying bits here and there. Each topic contains ‘by-hand’ demonstration, such that you can see conceptually how a model is estimated, or technique employed. This can help those that want to dive a little deeper to get a peek behind the curtain of the functions and packages they use, hopefully empowering them to go further with such models.\nTopics covered include the following, and I plan to post a sample chapter soon.\n\nModels\n\nLinear Regression\nLogistic Regression\nOne-factor Mixed Model\nTwo-factor Mixed Model\nMixed Model via ML\nProbit & Bivariate Probit\nHeckman Selection\nMarginal Structural Model\nTobit\nCox Survival\nHurdle Model\nZero-Inflated Model\nNaive Bayes\nMultinomial\nOrdinal\nMarkov Model\nHidden Markov Model\nQuantile Regression\nCubic Spline Model\nGaussian Processes\nNeural Net\nExtreme Learning Machine\nReproducing Kernel Hilbert Space Regression\nConfirmatory Factor Analysis\n\n\n\nBayesian\n\nBasics\nBayesian t-test\nBayesian Linear Regression\nBayesian Beta Regression\nBayesian Mixed Model\nBayesian Multilevel Mediation\nBayesian IRT\nBayesian CFA\nBayesian Nonparametric Models\nBayesian Stochastic Volatility Model\nBayesian Multinomial Models\nVariational Bayes Regression\nTopic Model\n\n\n\nEstimation\n\nMaximum Likelihood\nPenalized Maximum Likelihood\nL1 (lasso) regularization\nL2 (ridge) regularization\nNewton and IRLS\nNelder Mead\nExpectation-Maximization\nGradient Descent\nStochastic Gradient Descent\nMetropolis Hastings\nHamiltonian Monte Carlo\n\n\n\n\n\n\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{clark2020,\n  author = {Clark, Michael},\n  title = {Models by {Example}},\n  date = {2020-11-30},\n  url = {https://m-clark.github.io/posts/2020-11-30-models-by-example/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nClark, Michael. 2020. “Models by Example.” November 30,\n2020. https://m-clark.github.io/posts/2020-11-30-models-by-example/."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html",
    "title": "Practical Bayes Part II",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#outline-for-better-bayesian-analysis",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#outline-for-better-bayesian-analysis",
    "title": "Practical Bayes Part II",
    "section": "Outline for Better Bayesian Analysis",
    "text": "Outline for Better Bayesian Analysis\nWe’ll cover the following steps in more detail, but here is a general outline.\n\nFirst generate ‘fake data’ to assess viability of our priors\nWith adequate priors, start with a simple, but plausible model\nFor simple models you likely do not need many iterations, and for debugging/troubleshooting, starting with few iterations can possibly give you a sense of whether there will be problems1. If you are doing standard GLM or simpler versions of common extensions, even the defaults are likely overkill. For example, a basic linear regression should converge almost immediately.\nIf you have problems at this point, see Part I\n\n\n\nExplore and Visualize the Model Results\n\nVisualize covariate relationships\nAssess model effectiveness\n\nUse posterior predictive checks\nOther avenues\n\n\nPrediction and Model Comparisons\n\nGet basic predictions for observations of interest\nExplore a more viable model\n\nAdd interactions\nAdd nonlinear relations\nAccount for other structure (e.g. random effects)\n\nCompare and/or average models\nUse cross-validation to better assess performance\n\n\nWe’ll now demonstrate these steps."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#check-priors",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#check-priors",
    "title": "Practical Bayes Part II",
    "section": "Check Priors",
    "text": "Check Priors\nThough we did some work to select our prior distributions beforehand, we might still be concerned about how influential our priors were. So how can we check whether our priors were informative? The following uses the bayestestR package to do a simple check of whether the posterior standard deviation is greater than 10% of the prior standard deviation2. Having an informative prior isn’t really a problem in my opinion, unless it’s more informative than you wanted. For example, shrinkage of a coefficient towards zero will generally help avoid overfitting.\n\nprior_summary(model_baseline)\n\n                  prior     class      coef group resp dpar nlpar lb ub\n           normal(0, 1)         b                                      \n           normal(0, 1)         b       b11                            \n           normal(0, 1)         b       b21                            \n           normal(0, 1)         b        x1                            \n           normal(0, 1)         b        x2                            \n           normal(0, 1)         b        x3                            \n student_t(3, 2.9, 2.5) Intercept                                      \n    student_t(10, 1, 1)        sd                                  0   \n    student_t(10, 1, 1)        sd           group                  0   \n    student_t(10, 1, 1)        sd Intercept group                  0   \n    student_t(10, 1, 1)     sigma                                  0   \n       source\n         user\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n         user\n (vectorized)\n (vectorized)\n         user\n\nbayestestR::check_prior(model_baseline)\n\n    Parameter Prior_Quality\n1 b_Intercept uninformative\n2       b_b11 uninformative\n3       b_b21   informative\n4        b_x1 uninformative\n5        b_x2 uninformative\n6        b_x3 uninformative\n\n\nThese results suggest that we might be more informative, but for the intercept, which we largely aren’t too worried about, and for the factor that is highly unbalanced (b2), but which has no obvious solution. I personally would be fine with this result, especially since we took initial care in choosing these priors. If you really wanted to, you could change the priors that were informative."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#explore-and-visualize-results",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#explore-and-visualize-results",
    "title": "Practical Bayes Part II",
    "section": "Explore and Visualize Results",
    "text": "Explore and Visualize Results\nNow that we are feeling pretty good about the results we have, we can explore the model further. We can plot covariate effects easily with brms. The conditional_effects function is what we want here. I show results for one effect below. Without interactions or other things going, on they aren’t very interesting, but it’s a useful tool nonetheless. We’ll come back to this later.\n\nconditional_effects(model_baseline, 'b2')\n\n\n\n\n\n\n\n\nWe can also use the hypothesis function to test for specific types of effects. By default they provide a one-sided probability and uncertainty interval. For starters, we can just duplicate what we saw in the previous summary for the b2 effect. The only benefit is to easily obtain the one-sided p-value (e.g. that b2 is less than zero) and the corresponding evidence ratio, which is just p/(1-p).\n\nhypothesis(model_baseline, 'b21 &lt; 0')\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1  (b21) &lt; 0    -0.14      0.18    -0.45     0.16       3.46      0.78     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nBut we can really try anything, which is the power of this function. As an example, the following tests whether the combined effect of our categorical covariates is greater than twice the value of the x1 effect3.\n\nhypothesis(model_baseline, 'abs(b11) + abs(b21) &gt; 2*x1')\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (abs(b11)+abs(b21... &gt; 0     0.92      0.17     0.67     1.22        Inf\n  Post.Prob Star\n1         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nOne should get used to whatever tools are available for further understanding covariate effects or other parameters. This will likely lead to some of the more interesting discussion of your findings, or at least, notably more interesting than a standard regression table."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-effectiveness",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-effectiveness",
    "title": "Practical Bayes Part II",
    "section": "Model Effectiveness",
    "text": "Model Effectiveness\nIt is one thing to look at specific effects. but a natural question to ask is how useful our model actually is as a whole. This then suggests we need to know how to define such utility. Such an assessment definitely cannot be made with something like ‘statistical significance’. Science of any kind is nothing without prediction, so we we can start there.\n\nPosterior predictive checks\nPosterior predictive checks are a key component of Bayesian analysis. The prior checks we did before are just a special case of this. Here we instead use the posterior distributions of parameters to generate the data, and compare this model-implied/synthetic data to what we actually observe. Doing so can give insight to where the model succeeds and fails.\n\npp_check(model_baseline, ndraws = 100)\n\n\n\n\n\n\n\npp_check(model_baseline, ndraws = 10, type ='error_scatter_avg', alpha = .1)\n\n\n\n\n\n\n\n\nIn this case, we see good alignment between model and data, and no obvious pattern to the types of errors we are getting. It is often the case that we see that the model does not capture the most extreme values well, but that’s not terribly surprising. With simulated data, our situation is more pristine to begin with, but you generally can’t expect such a clean result in practice.\nAs an example, consider predictions with and without random effects. Including the cluster-specific effects for prediction appear to do better with the capturing the tails.\n\n\n\n\n\n\n\n\n\nWe can use the same approach to look at specific statistical measures of interest. For example, the following suggests our model is pretty good at capturing the minimum value, but typically underestimates the maximum value, which we noted earlier, is not especially unexpected in practice, particularly with smaller sample data.\n\npp_check(model_baseline, ndraws = 100, type ='stat', stat='median')\n\n\n\n\n\n\n\npp_check(model_baseline, ndraws = 100, type ='stat', stat = 'max')\n\n\n\n\n\n\n\n\nWe can define any function to use for our posterior predictive check. The following shows how to examine the 10th and 90th quantiles. Minimum and maximum values are unlikely to be captured very well due to their inherent variability, so looking at less extreme quantiles (e.g. 10th or 90th percentile) might be a better way to assess whether the model captures the tails of a distribution.\n\nq10 = function(y) quantile(y, 0.1)\nq90 = function(y) quantile(y, 0.9)\n\npp_check(model_baseline, ndraws = 100, type ='stat', stat = 'q90')\n\npp_check(model_baseline, ndraws = 100, type ='stat_2d', stat = c('q10', 'q90'))\n\n\n\nBayes R-squared\nIn this modeling scenario, we can examine the amount of variance accounted for in the target variable by the covariates. I don’t really recommend this beyond linear models that assume a normal distribution for the target, but people like to report it. Conceptually, it is simply a (squared) correlation of fitted values with the observed target values, so can be seen as descriptive statistic. Since we are Bayesians, we also get a ready-made interval for it, as it is based on the posterior predictive distribution. But to stress the complexity in trying to assess this, in this mixed model we can obtain the result with the random effect included (conditional) or without (unconditional). Both are reasonable ways to express the statistic, but the one including the group effect naturally will be superior, assuming the group-level variance is notable in the first place.\n\nbayes_R2(model_baseline)                   # random effects included\n\n    Estimate  Est.Error      Q2.5     Q97.5\nR2 0.4812032 0.01781973 0.4445528 0.5148587\n\nbayes_R2(model_baseline, re_formula = NA)  # random effects not included\n\n    Estimate  Est.Error       Q2.5    Q97.5\nR2 0.1161024 0.01498238 0.08729654 0.144502\n\n# performance::r2_bayes(model_baseline)    # performance package provides both\n\nTo show the limitation of R2, I rerun the model using a restrictive prior on the intercept. Intercepts for the resulting models are different but the other fixed effects are basically the same. The R2 suggests equal performance of both models.\n\n\n\n\n\nmodel\nR2\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nbaseline\n0.116\n0.015\n0.087\n0.145\n\n\nmodified\n0.116\n0.014\n0.089\n0.143\n\n\n\n\n\n\n\nHowever, a posterior predictive check shows clearly the failure of the modified model to capture the data.\n\n\n\n\n\n\n\n\n\nA variant of R2, the ‘LOO’ R2, is also available via the loo_R2 function. LOO stands for leave-one-out, as in leave-one-out cross-validation. It’s based on the residuals from the leave one out predictions. You can think of it as a better way to obtain an adjusted R2 in this setting. The results suggests that the LOO R2 actually picks up the difference in models, and would be lower for the modified model, even if we included the random effects.\nFor more on Bayesian R2, see the resources section"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#basic-prediction",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#basic-prediction",
    "title": "Practical Bayes Part II",
    "section": "Basic prediction",
    "text": "Basic prediction\nWith models in hand, let’s look at our basic predictive capabilities. We can get fitted values which include ‘confidence’ intervals, or predictions, which include ‘prediction’ intervals that include the uncertainty for a new observation. We can specify these as follows. First we create a small data set to make some predictions on. It will include both values for of the binary covariates, and the means of the numeric covariates (0).\n\nprediction_data = crossing(\n  b1 = 0:1,\n  b2 = 0:1,\n  x1 = 0,\n  x2 = 0,\n  x3 = 0\n)\n\n# fitted values\nhead(fitted(model_baseline))  \n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 3.517482 0.3382849 2.855822 4.167321\n[2,] 3.796906 0.3377263 3.118250 4.453510\n[3,] 3.866082 0.3346731 3.179920 4.511195\n[4,] 4.357828 0.3507970 3.688756 5.026747\n[5,] 3.185495 0.3381892 2.527887 3.825222\n[6,] 3.910027 0.3347265 3.244999 4.558492\n\n# new predictions\ndata.frame(\n  prediction_data,\n  predict(model_baseline, newdata = prediction_data, re_formula = NA)\n)\n\n  b1 b2 x1 x2 x3 Estimate Est.Error      Q2.5    Q97.5\n1  0  0  0  0  0 2.696397  1.069881 0.5093926 4.709590\n2  0  1  0  0  0 2.557696  1.013811 0.5566073 4.603278\n3  1  0  0  0  0 3.450119  1.082413 1.3938133 5.471762\n4  1  1  0  0  0 3.324694  1.008474 1.3336782 5.300122\n\n\nIn general, we’d always like to visualize the predictions. We can do so as we did before with the conditional_effects function, which would also allow us to set specific covariate values. For the third plot of the nonlinear effect below, I modify the basic conditional effects plot that brms provides for a slightly cleaner visualization.\n\nconditional_effects(model_baseline, effects = 'x2', conditions = prediction_data[1,])\n\n\n\n\n\n\n\nconditional_effects(model_interact, effects = 'x1:b2')\n\n\n\n\n\n\n\ninit = conditional_effects(model_interact_nonlin, effects = 'x3', spaghetti = T)\n\n\n\n\n\n\n\n\n\n\nExpanding your story through prediction is essential to helping your audience understand the model on a practical level. You would do well to spend time looking at specific data scenarios, especially in the case of nonlinear models (e.g. GLM) and models with interactions."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-comparison",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-comparison",
    "title": "Practical Bayes Part II",
    "section": "Model Comparison",
    "text": "Model Comparison\nIn typical situations it is good to have competing models, and having additional models allows us to see if improvements can be made in one way or another, both to our models, and potentially to our way of thinking about them. In a general sense, we will go about things very similarly in the Bayesian context that we would elsewhere. However, we’ll also more easily apply other approaches that are not so commonly used (even if they can be).\n\nChoosing a model\nIn traditional contexts, we can use a specific approach to pit competing models against one another, selecting the ‘best’ model based on a particular metric, for example, AIC, cross-validation error, etc. With ‘error metrics’, the model with the lowest value is the winner. In this case, nothing is new in the Bayesian world. Here, we can use estimates like WAIC and LOOIC for model comparison, much like you would AIC to compare models in traditional frameworks. The values themselves don’t tell us much, but in comparing models, lower means less predictive error for these ‘information criteria’ metrics, which is what we want4, and since we’re Bayesian, we will even have estimates of uncertainty for these values as well. We also have cross-validation approaches (which IC metrics approximate), which we will demonstrate later.\nWith our new models added to the mix, we can now make some comparisons using loo_compare. First, we’ll add LOOIC estimates to our models, which are not estimated by default.\n\nmodel_baseline = add_criterion(model_baseline,  'loo')\nmodel_interact = add_criterion(model_interact, 'loo')\nmodel_interact_nonlin = add_criterion(model_interact_nonlin, 'loo')\n\nTo start, we’ll show the LOOIC result for the baseline model. We have the total expected log probability (elpd_loo) for the leave-one-out observations. We also get stuff like p_loo, which is the effective number of parameters. For those familiar with penalized maximum likelihood, these are familiar analogues. However we also get a summary regarding Pareto k values, which we’ll talk about soon.\n\n# example\nloo(model_baseline)\n\n\nComputed from 1000 by 1000 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1500.6 21.9\np_loo        88.0  3.8\nlooic      3001.2 43.9\n------\nMCSE of elpd_loo is 0.4.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.3]).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nLet’s now compare the baseline model to the others models using loo_compare. It shows the ‘best’ (lowest-valued) model first, followed by the others. We get the difference of each elpd vs. the lowest, also get a standard error for this difference, which you could use to help assess how different the values are statistically. Just by this standard, the model that is based on the underlying data generating mechanism is the clear winner, as we would expect.\n\nloo_compare(\n  model_baseline, \n  model_interact,\n  model_interact_nonlin\n)\n\n                      elpd_diff se_diff\nmodel_interact_nonlin   0.0       0.0  \nmodel_interact        -44.5       9.4  \nmodel_baseline        -46.3       9.7  \n\n\nNow let’s compare several metrics available to us. In this particular setting, all are generally in agreement in the rank order of the models, though there appears to be no meaningful difference between the baseline and interaction models.\n\n\n\n\n\nmodel\nR2\nloo_R2\nWAIC\nLOOIC\nELPD\nweight\n\n\n\n\nbaseline\n0.48\n0.43\n2999.15\n3001.18\n-1500.59\n7.680717e-06\n\n\ninteract\n0.49\n0.43\n2995.28\n2997.42\n-1498.71\n4.067051e-06\n\n\ninteract_nonlin\n0.53\n0.48\n2905.87\n2908.49\n-1454.25\n9.999883e-01\n\n\n\n\n\n\n\nFor our ultimate model comparison we want to stick to using the IC values. As far as choosing between WAIC vs. LOOIC, the latter has better diagnostics for noting whether there are potential problems in using it. In practice, they will almost always agree with one another. As we noted previously, LOOIC reflects the ELPD, and this value is used in constructing the model weights shown in the last column5. The model weights can then be used in making final predictions (i.e. model averaging), or just providing a different way for your audience to gauge which model might be preferred.\n\n\nProblems at the loo\nAfter the model issues discussed in Part I, the next most common point of confusion I see people have is with model comparison, and using LOO in particular. Part of the reason is that this is an area of ongoing research and development, and most of the tools and documentation are notably technical. Another reason is that these are not perfect tools. They can fail to show notable problems for models that are definitely misspecified, and flag models that are essentially okay. Sometimes they flag models that other indicators may suggest are better models relatively speaking, which actually isn’t a contradiction, but which may indicate an overfit situation.\nSo in general, no tool is perfect, but in the real world we have to get things, so let’s address a couple issues.\n\nNot so different models\nLet’s start with the case where models do not appear to perform very differently. If two models aren’t very different from one another, the usual response is to go with the simpler model. For example, if we were only comparing the baseline model vs. the interaction model, there really isn’t much difference in terms of LOOIC/ELPD. However, we will have to consider things a little differently in the Bayesian context. Consider the following two thoughts.\n\nThe general issue is that with unregularized estimation such as least squares or maximum likelihood, adding parameters to a model (or making a model more complex) leads to overfitting. With regularized estimation such as multilevel modeling, Bayesian inference, lasso, deep learning, etc., the regularization adds complexity but in a way that reduces the problem of overfitting. So traditional notions of model complexity and tradeoffs are overturned. ~ Andrew Gelman\n\n\nSometimes a simple model will outperform a more complex model… Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well. ~ Radford Neal\n\nThe take-home message here is that simpler is not always better. And to be frank, using penalized (a.k.a. regularized) approaches (e.g. lasso, ridge, mixed models) should probably be our default model in the non-Bayesian context, and it turns out that such approaches actually approximate a Bayesian one with specific priors. In the end, you may have to think about things a little more carefully, and given that you are using methods that can help avoid overfitting, you may instead lean on a more complex model with otherwise similar performing models. And that would be closer to how nature works anyway, which is always more complex than our brains can easily understand.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPareto values\nLet’s look again at the basic result from using the loo function.\n\nloo(model_interact)\n\n\nComputed from 1000 by 1000 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1498.7 22.0\np_loo        89.5  3.9\nlooic      2997.4 44.0\n------\nMCSE of elpd_loo is 0.4.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.2]).\n\nAll Pareto k estimates are good (k &lt; 0.67).\nSee help('pareto-k-diagnostic') for details.\n\n\nWe haven’t yet discussed Pareto values, but it is not uncommon to get a result with some values that are not ‘good’ or ‘ok’. If you happen to see Pareto values in the ‘bad’ or ‘very bad’ group, what does it mean? You can read the definition provided here, but it may not help many due to the background knowledge needed to parse it. However, you can just understand it as an (leave-one-out) extreme value diagnostic, and if it is a problem, it mostly means your LOOIC may not be good for comparing models.\nAs in the standard model setting, ‘outliers’ indicate model incompetence, or in other words, the model is unable to understand such observations. Unless you have reason to suspect something inherently wrong in the data (e.g. an incorrect value/typo), an outlier is a sign that your model is not able to capture the data fully. It definitely is not a reason to remove the observation!\nIf you have Pareto values &gt; .7, you may recalculate LOOIC with the options provided by the loo function or use the reloo function, getting a better estimate that could then be used in, for example, model stacking for prediction. If you don’t discover many outliers, it probably won’t make much difference in your final estimates and conclusions, and so probably isn’t worth the trouble pursing much further. The output for Pareto values doesn’t even save the row identifying information that would make it easy to find which observations are the problem, but you can do something like the following if you need to.\n\npareto = loo(model_interact_nonlin)\n\nproblems = pareto$pointwise %&gt;% \n  data.frame() %&gt;% \n  rowid_to_column() %&gt;% \n  filter(influence_pareto_k &gt; .5) %&gt;% \n  pull(rowid)\n\nmodel_interact_nonlin$data %&gt;% \n  mutate(rank = rank(y)) %&gt;% \n  slice(problems)\n\n          y b1 b2        x1        x2         x3 group rank\n1  8.198540  1  1 0.3636720 0.0652158  1.3760312    68 1000\n2 -1.123859  1  0 0.8573758 0.6268734 -0.3354954    97    3\n\n\nAs we might have expected, the observations with the more extreme target values are likely to be problems (rank closer to 1 or 1000), but for some of these, there is nothing to suggest why they might be difficult, and it’s even harder to speculate in typical modeling situations with more predictors and complexity. Furthermore, outside of additional model complexity, which might then hamper interpretation, there is often little we can do about this, or at least, what we can do is generally not obvious in applied settings."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-averaging",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#model-averaging",
    "title": "Practical Bayes Part II",
    "section": "Model Averaging",
    "text": "Model Averaging\nWith the previous statistics for model comparison we can obtain relative model weights, using the model_weights function. This essentially spreads the total probability of the models across all those being compared. These weights in turn allow us to obtain (weighted) average predictions. The key idea being that we do not select a ‘best’ model, but rather combine their results for predictive purposes6.\nWe can start by comparing the first two models. Adding the interactions helped, and comparing the weights suggests that the interaction model would be contributing most to the averaged predictions.\n\n\nMethod: stacking\n------\n               weight\nmodel_baseline 0.234 \nmodel_interact 0.766 \n\n\nIf we compare the baseline to our most complex model, almost the entirety of the weight is placed on the latter.\n\nloo_model_weights(model_baseline, model_interact_nonlin)\n\nMethod: stacking\n------\n                      weight\nmodel_baseline        0.000 \nmodel_interact_nonlin 1.000 \n\n\nNow we compare all three, with roughly the same conclusion.\n\n\n\n\n\nmodel_baseline\nmodel_interact\nmodel_interact_nonlin\n\n\n\n\n1e-05\n0\n0.99999\n\n\n\n\n\n\n\nNow what about those average predictions? Let’s create a data frame that sets the continuous covariates at their means, and at each level of the categorical covariates. For our purposes here, we will also ignore group effects7. We then will make average predictions for those observations using pp_average.\n\nprediction_data = crossing(\n  b1 = 0:1,\n  b2 = 0:1,\n  x1 = 0,\n  x2 = 0,\n  x3 = 0\n)\n\naverage_predictions = pp_average(\n  model_baseline,\n  model_interact,\n  model_interact_nonlin,\n  newdata = prediction_data,\n  re_formula = NA\n)\n\n\n\n\n\n\nb1\nb2\nx1\nx2\nx3\nEstimate\nEst.Error\nQ2.5\nQ97.5\nBaseline Estimate\nModel Nonlin Est.\n\n\n\n\n0\n0\n0\n0\n0\n2.73\n1.03\n0.67\n4.70\n2.69\n2.73\n\n\n0\n1\n0\n0\n0\n2.29\n0.98\n0.31\n4.21\n2.54\n2.30\n\n\n1\n0\n0\n0\n0\n2.95\n1.03\n1.00\n4.98\n3.48\n2.93\n\n\n1\n1\n0\n0\n0\n3.09\n1.02\n1.07\n5.04\n3.34\n3.08\n\n\n\n\n\n\n\nAs expected, we can see that the averaged predictions are essentially the same as what we would get from the model with all the weight. In other scenarios, you may be dealing with a more nuanced result."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#cross-validation",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#cross-validation",
    "title": "Practical Bayes Part II",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nIn machine learning contexts, cross-validation is the default approach to considerations of model performance. We can do so easily within the Bayesian context as well. I go ahead and do so for a single model, as well as all three models, so we can see how our previous performance metrics might change. In general, prediction on a validation set will be expected to be worse than on the training data8, but it is the better estimate of prediction error.\n\nlibrary(future)\n\nplan(multisession)\n\nmodel_interact_nonlin_cv = kfold(\n  model_interact_nonlin,\n  K = 5,\n  chains = 1,\n  save_fits = TRUE\n)\n\nmodel_all_cv = kfold(\n  model_baseline,\n  model_interact,\n  model_interact_nonlin,\n  K = 5,\n  chains = 1,\n  save_fits = TRUE\n)\n\nplan(sequential)\n\nWith a single cross-validation model in place, we can then make predictions with it to get the test error or other metrics of interest. As we expect, the training error, i.e. that on the original/full data is better than the test error, but the latter is the better estimate of our model error, and thus a better metric for comparing models.\n\ntest_predictions = kfold_predict(model_interact_nonlin_cv)\n\n\ntrain_error = yardstick::rmse_vec(truth    = model_interact_nonlin$data$y,\n                                  estimate = fitted(model_interact_nonlin)[, 1])\n\ntest_error  = yardstick::rmse_vec(truth    = test_predictions$y,\n                                  estimate = colMeans(test_predictions$yrep))\n\n\n\n\n\n\ntrain_error\ntest_error\n\n\n\n\n0.933\n1.051\n\n\n\n\n\n\n\nNow let’s revisit our LOOIC comparison, only now it is based on LOOIC via the cross-validation process. We would come to the same conclusions, but we can see that the differences, while still substantial, are not as great. In addition, other standard metrics can help validate the Bayesian-specific metrics, as RMSE does here.\n\n## model_all_cv$diffs\n\n\n\n\nmodel\nelpd_diff\nse_diff\nelpd_kfold\nrmse\n\n\n\n\nmodel_interact_nonlin\n0.00\n0.00\n-1472.57\n1.06\n\n\nmodel_baseline\n-33.62\n11.33\n-1506.19\n1.09\n\n\nmodel_interact\n-41.59\n11.05\n-1514.16\n1.10\n\n\n\n\n\n\n\n\nVariable Selection\nIf desired, we can use cross-validation to help with feature selection. We’ve already discussed why this really shouldn’t be a concern, namely because there rarely is a reason to throw out variables regardless of how minimally important they might be. Furthermore, interactions among variables are the norm, not the exception. So while a variable might not do well on its own, it can be extremely important in how it interacts with another feature.\nIn any case, one can use the projpred package to get a sense of this, and also why it can be problematic. For starters, we cannot test our nonlinear model due to its complexity9. But we can also see that we would not choose the true underlying model using this approach. In addition, for expediency I had to turn off the random effects, otherwise this would take more time than I wanted to spend for this demo (the group effect would have been the first selected). In short, be prepared for issues that might accompany the complexities in your model10.\n\nlibrary(projpred)\n\nmodel_feature_select_cv = update(model_interact, .~. - (1|group), cores = 4)\nref_model = get_refmodel(model_feature_select_cv)  # reference model structure\n\noptions(mc.cores = parallel::detectCores())\nvar_select = cv_varsel(ref_model)   # will take a very long time\n\nWith results in place we can summarize and visualize our results, similar to how we have done before. This is from summary(var_select). You can see how often features are selected across all cv runs via the observation level folds, and the expected loo results.\n\n\n\n\n\nsize\nranking\ncv_proportions\nelpd\nelpd.se\nelpd.diff\nelpd.diff.se\n\n\n\n\n0\n(Intercept)\nNA\n-1779.6\n21.5\n-63.5\n11.3\n\n\n1\nb1\n1.0\n-1736.4\n21.9\n-20.3\n7.1\n\n\n2\nx3\n1.0\n-1714.5\n21.3\n1.6\n2.3\n\n\n3\nb2\n0.8\n-1723.1\n21.7\n-7.0\n2.1\n\n\n4\nx2\n0.7\n-1725.1\n21.7\n-8.9\n1.9\n\n\n5\nx1\n0.8\n-1720.1\n21.7\n-4.0\n1.7\n\n\n6\nb2:x1\n0.8\n-1721.4\n21.7\n-5.3\n1.3\n\n\n7\nb1:b2\n0.8\n-1715.9\n21.6\n0.2\n0.1\n\n\n\n\n\n\n\nThe plot of elpd (higher better) and rmse (lower better) suggest a possible cutoff of submodel size 2, which would include b1 and x3.\n\n# plot predictive performance on training data\nplot(var_select, stats = c('elpd', 'rmse'))\n\n\n\n\n\n\n\n\nHere is the final rank ordering with density plots for the two best features + one other for comparison. Unexpectedly, the ‘top’ feature is almost centered on zero. Go figure.\n\nranking(var_select)$fulldata\n\n[1] \"b1\"    \"x3\"    \"b2\"    \"x2\"    \"x1\"    \"b2:x1\" \"b1:b2\"\n\nmcmc_areas(\n    as.matrix(ref_model$fit),\n    pars = c('b_b11', 'b_x3', 'b_x1')\n  ) +\n  coord_cartesian(xlim = c(-2, 2))"
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#the-rabbit-hole-of-model-comparsion",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#the-rabbit-hole-of-model-comparsion",
    "title": "Practical Bayes Part II",
    "section": "The rabbit hole of model comparsion",
    "text": "The rabbit hole of model comparsion\nIf you start to look more into this, there are numerous technical articles, whole websites, and various discussions regarding how to go about it. I’m guessing many do not want to try and parse highly technical information, only to still feel confused about what to actually do. Many suggestions amount to ‘your model is probably misspecified’, but without additional thoughts on how to proceed. Some of the data issues that lead to problems are just the reality of data doing what data does. There are also suggestions that posterior predictive checks (PPCs) can be used to detect the problem. But a difficulty here is that these don’t detect anything by themselves without very specific directed action, nor do they typically have a standard metric to report, so the practical utility does have its limits. In addition, it’s not clear to me that issues or problems regarding specific statistics for model comparison (e.g. LOOIC estimation) should be a basis for altering a model, unless there is an obvious path for doing so. And let’s face it, if there was, you’d probably already be taking it.\nFor those that do want to go down the rabbit hole, I have numerous links in the resources section."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#solutions-for-model-comparison",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#solutions-for-model-comparison",
    "title": "Practical Bayes Part II",
    "section": "Solutions for Model Comparison",
    "text": "Solutions for Model Comparison\nWhen doing model comparison, the following summarizes some basic steps you can take.\n\nDon’t assume you’ll have any certainty about some model being ‘best’.\nUse the metrics noted above, e.g. LOOIC, when making comparisons (not R2).\nAvoid the problem and fit the model that includes everything of interest, assuming you have a decent data size to do so. It is likely you can still learn some things about the model by comparing it to others.\nVariable selection is typically just a model comparison problem restated differently, and in a lot of cases I’ve come across, a misguided endeavor. If something is even minimally important, there is no reason to throw it out, as you’d just have worse predictions doing so. With complex models, you can’t assess one variable without consideration of others, so trying to say that one is more important than the others doesn’t really make sense.\nIf some application performance measure is obvious and available to assess, pick a model that does best in that setting.\nIf trying to select among many competing models, e.g. feature selection, you should consider why you are in this situation. If you don’t have much data, then the usual model selection criteria may lead you notably astray. If you have a lot of data, consider why you need to select a subset of predictors and not use all available. If you are somewhere in between, note that you’ll likely spend a lot more time here and still not be confident in the results. However, there are approaches, such as those in the projpred package, that might be useful, but likely will only work for simpler models."
  },
  {
    "objectID": "posts/2021-02-28-practical-bayes-part-ii/index.html#footnotes",
    "href": "posts/2021-02-28-practical-bayes-part-ii/index.html#footnotes",
    "title": "Practical Bayes Part II",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you were coding in Stan directly, you can run a single iteration to see if your code compiles at all.↩︎\nDaniel Lakeland proposes (as a comment in the discussion of the 10% approach outlined) an alternative approach is whether the posterior estimate falls within the 95% highest density interval of the prior. This is available via the method argument in the demonstrated function (method = 'lakeland').↩︎\nAs in the text of the output, this is the same as testing whether abs(b1) + abs(b2) - 2*x1 &gt; 0. In this case the resulting value is greater than zero with high probability.↩︎\nSimilar to AIC, LOOIC is ~ -2*expected log posterior density (ELPD), similar to how we use -2*log likelihood (a.k.a. deviance) in standard approaches for AIC. We don’t add a penalty for parameters here, and I think this is because the regularization is already built in to the modeling process, and the number of parameters might be more difficult to define in the Bayesian context with priors.↩︎\nTechnically we can use WAIC to produce weights like we do with AIC, e.g. exp(waic) / sum(exp(all_waics)), but this isn’t recommended. The stacking approach allows similar models to share their weight, while more unique models will mostly keep their weight as additional models are added.↩︎\nSome might be familiar with Bayesian model averaging. Conceptually we aren’t changing much, but BMA assumes that one of our models is the true model, while the stacking approach underlying these weights does not. It is also different from conventional stacking in machine learning in that we are trying to average posterior predictive distributions, rather than point estimates.↩︎\nIn other words, for prediction we set re_formula = NA.↩︎\nAt the time of this writing, the underlying use of the furrr package defaults to not using a seed in it’s parallelization process, and then warns you that a seed has not been set for each repeated use of a cluster. Passing a seed through the seed argument won’t actually do anything presently here, so one will hope that furrr will change their default behavior. It’s a nuisance that can be ignored though.↩︎\nPerhaps this might be possible in a future release, but there are other complications that might make it problematic still.↩︎\nI revisited this late 2024, and it took hours to run the cv_varsel under these defaults, and I found the resulting objects even less intuitive to work with. They mention in the vignette there are many ways to speed it up, but accuracy will be compromised. In short, this is not something you could iterate over. On the plus side, the results were consistent with what I had seen before, for better or worse.↩︎"
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html",
    "href": "posts/2021-07-15-dl-for-tabular/index.html",
    "title": "This is definitely not all you need",
    "section": "",
    "text": "I’ve been a little perplexed at the lack of attention of deep learning (DL) toward what I consider to be ‘default’ data in my world, often referred to as tabular data, where typically we have a two dimensional input of observations (rows) and features (columns) and inputs are of varying type, scale and source. Despite the ubiquity of such data in data science generally, and despite momentous advances in areas like computer vision and natural language processing, at this time, it’s not very clear what the status of DL for tabular data is.\nThere have been developments in the area recently though, with some modeling approaches, such as TabNet, gaining traction. In June of 2021, I actually came across three papers on Arxiv that were making related claims about the efficacy of DL for tabular data. As in many academic and practical (and life) pursuits, results of these studies are nuanced, so I thought I’d help myself and others by summarizing here."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#motivation",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#motivation",
    "title": "This is definitely not all you need",
    "section": "",
    "text": "I’ve been a little perplexed at the lack of attention of deep learning (DL) toward what I consider to be ‘default’ data in my world, often referred to as tabular data, where typically we have a two dimensional input of observations (rows) and features (columns) and inputs are of varying type, scale and source. Despite the ubiquity of such data in data science generally, and despite momentous advances in areas like computer vision and natural language processing, at this time, it’s not very clear what the status of DL for tabular data is.\nThere have been developments in the area recently though, with some modeling approaches, such as TabNet, gaining traction. In June of 2021, I actually came across three papers on Arxiv that were making related claims about the efficacy of DL for tabular data. As in many academic and practical (and life) pursuits, results of these studies are nuanced, so I thought I’d help myself and others by summarizing here."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#goal",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#goal",
    "title": "This is definitely not all you need",
    "section": "Goal",
    "text": "Goal\nI want to know if, e.g. time and/or resources are limited, whether it will be worth diving into a DL model if I have a satisfactory simpler/easier one ready to implement that does pretty well. Perhaps this answer is already, ‘if it ain’t broke, don’t fix it’, but given the advancements in other data domains, it would be good to assess what the current state of DL with tabular data is."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#caveats",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#caveats",
    "title": "This is definitely not all you need",
    "section": "Caveats",
    "text": "Caveats\n\nI’m not going to do more than give a cursory summary of the articles, and provide no in-depth explanation of the models. For more detail, see the corresponding articles and references for the models therein. You are not going to learn how to use TabNet, NODE, transformers, etc., for tabular data.\nThere are other decent articles on the topic not covered here. Some are referenced in these more recent offerings, so feel free to peruse."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#quick-take",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#quick-take",
    "title": "This is definitely not all you need",
    "section": "Quick Take",
    "text": "Quick Take\nIn case you don’t want any detail, here’s a quick summary based on my impressions from these articles. Right now, if you want to use DL on tabular data, don’t make a fuss of it. A simple architecture, even a standard multi-layer perceptron, will likely do as well as more complicated ones. In general though, the amount of effort put into prep/tuning may not be worth it for many typical tabular data settings, for example, relative to a suitably flexible statistical model (e.g. GAMM) or a default fast boosting implementation like XGBoost. However, DL models are already thinking ‘big data’, so for very large data situations, a DL model might make a great choice, as others may not be computationally very viable. It also will not be surprising at all that in the near future some big hurdle may be overcome as we saw with DL applications in other fields, in which case some form of DL may be ‘all you need’.\nNow, on to the rest!"
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#tabular-data-deep-learning-is-not-all-you-need",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#tabular-data-deep-learning-is-not-all-you-need",
    "title": "This is definitely not all you need",
    "section": "Tabular Data: Deep Learning is Not All You Need",
    "text": "Tabular Data: Deep Learning is Not All You Need\n\nPaper Info\n\nWho: Shwartz-Ziv & Armon\nWhere: Intel\nWhen: 2021-06-21 V1\nArxiv Link\n\n\n\nFrom the Abstract\n\nWe analyze the deep models proposed in four recent papers across eleven datasets, nine of which were used in these papers, to answer these questions. We show that in most cases, each model performs best on the datasets used in its respective paper but significantly worse on other datasets. Moreover, our study shows that XGBoost (Chen and Guestrin, 2016) usually outperforms the deep models on these datasets. Furthermore, we demonstrate that the hyperparameter search process was much shorter for XGBoost.\n\n\n\nOverview\nFor each model they used the data that was implemented in the original model papers by the authors (e.g. the dataset used in the TabNet article), and also used their suggested parameter settings. They tested all the models against their own data, plus the other papers’ data, plus two additional data sets that were not used in any of the original papers.\n\n\nData\nThey use eleven total datasets. Nine datasets are those used in the original papers on TabNet, DNF-Net, and NODE, drawing three datasets from each paper. Additionally, Shwartz-Ziv & Armon use two Kaggle datasets not used in any of those papers. Sample sizes ranged from 7k to 1M, 10-2000 features, with two being numeric targets, while the other target variables ranged from 2-7 classes. Datasets are described in detail in the paper along with links to the source (all publicly available).\n\n\nModels Explored\nBrief summaries of the DL models are found in the paper.\n\nXGBoost\nTabNet\nNeural Oblivious Decision Ensembles (NODE)\nDNF-Net\n1D-CNN\n\n\n\nQuick Summary\n\nNot counting the ensemble methods…\n\nTabNet did best on all of its own data sets, but was not the best model on any other.\nNODE each did best on 2 of its own 3 data sets, but not on any other.\nDNF-Net best on one of its own 3 data sets, but not on any other.\nXGBoost was best on the remaining 5 datasets.\n\n\n\nCounting the ensemble methods…\n\nTabNet did best on 2 of its own 3 data sets, but was not the best model on any other.\nDNF-Net and NODE each did best on one of its own 3 data sets, but not on any other.\nXGBoost was best on one dataset.\n\nOf those, XGB was notably better on ‘unseen’ data, and comparable to the best performing ensemble. A simple ensemble was also very performant. From the paper:\n\nThe ensemble of all the models was the best model with 2.32% average relative increase, XGBoost was the second best with 3.4%, 1D-CNN had 7.5%, TabNet had 10.5%, DNF-Net had 11.8% and NODE had 14.2% (see Tables 2 and 3 in the appendix for full results).\n\nAs a side note, XGBoost + DL was best, but that defeats the purpose in my opinion. Presumably any notably more complicated setting will be potentially better with enough complexity, but unless there is an obvious way on how to add such complexity, it’s mostly an academic exercise. However, as the authors note, if search is automated, maybe the complexity of combining the models is less of an issue.\n\n\n\nOther stuff\n\nKudos\nThe authors cite the No Free Lunch theorem in the second paragraph, something that appears to be lost on many (most?) of these types of papers touting small increases in performance for some given modeling approach.\n\n\nIssues\nThere are always things like training process/settings that are difficult to fully replicate. By the time authors publish any paper, unless exact records are kept, the iterations (including discussions that rule out various paths) are largely lost to time. This isn’t a knock on this paper, just something to keep in mind.\n\n\nOpinion\nI liked this one in general. They start by giving the competing models their best chance with their own settings and data, which was processed and trained in the same way. Even then, those models still either didn’t perform best, and/or performed relatively poorly on any other dataset."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data",
    "title": "This is definitely not all you need",
    "section": "Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data",
    "text": "Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data\n\nPaper Info\n\nWho: Kadra et al.\nWhere: U of Freiburg, Leibniz U (Germany)\nWhen: 2021-06-06 V1\nArxiv Link\n\n\n\nFrom the Abstract\n\nTabular datasets are the last “unconquered castle” for deep learning… In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters.\n\n\nWe empirically assess the impact of these regularization cocktails for MLPs on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.\n\n\nWe emphasize that some of these publications claim to outperform Gradient Boosted Decision Trees (GDBT) [1, 37], and other papers explicitly stress that their neural networks do not outperform GBDT on tabular datasets [38, 22]. In contrast, we do not propose a new kind of neural architecture, but a novel paradigm for learning a combination of regularization methods.**\n\n\n\nOverview\nThis data is more about exploring regularization techniques (e.g. data augmentation, model averaging via dropout) rather than suggesting any particular model is superior. Even in the second paragraph they state their results do not suggest a performance gain over boosting methods. Their focus is on potentially improving DL for tabular data through regularization with two hypotheses:\n\nRegularization cocktails outperform state-of-the-art deep learning architectures on tabular datasets.\nRegularization cocktails outperform Gradient-Boosted Decision Trees, as the most commonly used traditional ML method for tabular data.\n\n\n\nData\nForty total datasets ranging from as little as ~400 observations to over 400k, and between 4 and 2000 features. All were categorical targets, with about half binary. All available at openml.org with target ID provided.\n\n\nModels Explored\nComparison models:\n\nTabNet: (with author’s proposed defaults)\nNODE: (with author’s proposed defaults)\nAutogluon: Tabular: can use other techniques but restricted to ensembles of neural nets for this demo\nASK-GBDT: GB via Auto-sklearn (Note this tool comes from one of the authors )\nXGBoost: Original implementation\nMLP: Multilayer Perceptron - 9 layers with 512 hidden units each.\nMLP+D: MLP with Dropout\nMLP+C: MLP with regularization cocktail\n\n\n\nQuick Summary\n\nTo begin, their regularization cocktail approach is the clear winner on these datasets, having one outright on over 40% of them (based on table 2).\nStandard XGB performed best (or tied for best) 8 of the 40 data sets, while it or ASK-GBDT were best for 12 datasets combined.\nSimple MLP was best once, while MLP with dropout was best 5 times, while the cocktail method was best in general, across 19 datasets.\nThe ‘fancy’ DL models were the worst performers across the board. TabNet never performed best, and NODE only did once, but the latter also repeatedly failed due to memory issues or run-time limitations (this memory issue was mentioned in the previous paper also).\nHead-to-head, the cocktail beat the standard XGB 26 out of 38 times with three ties. So it wins 65% of the time against XGB, 70% against ASK-GBDT, but 60% against either (i.e. some XGB approach).\n\n\n\nOther Stuff\n\nKudos\n\nRecognize that tabular data is understudied in mainstream DL literature\nThey used a lot of datasets\nThey look at the simplest DL models for comparison\n\n\n\nIssues\n\nI wonder why there was not a single numeric outcome among so many datasets. Furthermore, some of the data are image classification (e.g. Fashion-MNIST), so I’m not sure why they’re included. I wouldn’t use a ‘tabular’ technique when standard computer vision approaches already work so well.\nI’m not familiar with the augmentation techniques they mention, which were devised for image classification, but there have been some used for tabular data for a couple decades at this point that were not mentioned, including simple upsampling, or imputation methods (e.g. SMOTE). That’s not a beef with the article at all, I’ve long wondered why people haven’t been using data augmentation for tabular data given it’s success elsewhere (including for tabular data!).\nThey use a standard t-tests of ranks, but if we’re going to use this sort of approach, we’d maybe want to adjust for all the tests done, and probably for all pairwise comparisons (they show such a table for the regularization methods). Depending on the approach and cutoff, the XGB vs. Cocktail difference may not be significant.\nAlso, I couldn’t duplicate these p-values with R’s default settings for Wilcoxon signed rank tests, and there does in fact seem to be inconsistency between the detailed results and Wilcoxon summaries. For example, in the regularization tests of Table 9, Cocktail vs. WD and DO shows two ties in the first four data sets, yet only 1 tie is reported in the comparison chart for both (Figure 4). For the models, Table 2 show 3 ties of XGB & the Cocktail, with 1 for ASK-G and Cocktail, but 2 and 0 are reported for their Wilcoxon tests. It’s not clear what they did for NODE with all the NAs. I do not believe these discrepancies, nor adjusting for multiple comparisons, will change the results (I re-did those myself).\n\n\n\nOpinion\nIf we ignore the regularization focus and just look at the model comparisons, I’m not overly convinced we have a straightforward victory for cocktail vs. GB as implied in the conclusion. Results appear to be in favor of their proposed method, but not enough to be a near-guarantee in a particular setting, so we’re back to square one of just using the easier/faster/better tool. I’m also not sure who was questioning the use of regularization for neural networks or modeling in general, so the comparison to any model without some form of regularization isn’t as interesting to me. What is interesting to me is that we have another round of evidence that the fancier DL models like TabNet do not perform that well relative to GB or simpler DL architectures."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#revisiting-deep-learning-models-for-tabular-data",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#revisiting-deep-learning-models-for-tabular-data",
    "title": "This is definitely not all you need",
    "section": "Revisiting Deep Learning Models for Tabular Data",
    "text": "Revisiting Deep Learning Models for Tabular Data\n\nPaper Info\n\nWho: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko\nWhere: Yandex (Russia)\nWhen: 2021-06-22\nArxiv Link\nSource code\n\n\n\nFrom the Abstract\n\nThe necessity of deep learning for tabular data is still an unanswered question addressed by a large number of research efforts. The recent literature on tabular DL proposes several deep architectures reported to be superior to traditional “shallow” models like Gradient Boosted Decision Trees. However, since existing works often use different benchmarks and tuning protocols, it is unclear if the proposed models universally outperform GBDT. Moreover, the models are often not compared to each other, therefore, it is challenging to identify the best deep model for practitioners.\n\n\nIn this work, we start from a thorough review of the main families of DL models recently developed for tabular data. We carefully tune and evaluate them on a wide range of datasets and reveal two significant findings. First, we show that the choice between GBDT and DL models highly depends on data and there is still no universally superior solution. Second, we demonstrate that a simple ResNet-like architecture is a surprisingly effective baseline, which outperforms most of the sophisticated models from the DL literature. Finally, we design a simple adaptation of the Transformer architecture for tabular data that becomes a new strong DL baseline and reduces the gap between GBDT and DL models on datasets where GBDT dominates.\n\n\n\nOverview\nThis paper compares different models on a variety of datasets. They are interested in the GB vs. DL debate, but like the previous paper, also interested in how well a simpler DL architecture might perform, and what steps might help the more complicated ones do better.\n\n\nData\nThey have 11 datasets with a mix of binary, multiclass and numeric targets. Sizes range from 20K to 1M+. There appears to be some overlap with the first paper (e.g. Higgs, Cover type).\n\n\nModels Explored\n\n‘Baselines’\n\nXGBoost\nCatBoost\nMLP\nResNet\n\n\n\nDL Comparisons\n\nSNN\nNODE\nTabNet\nGrowNet\nDCN V2\nAutoInt\n\nIn addition, they look at ensembles of these models, but this is not of interest to me for this post.\n\n\n\nQuick Summary\nNote that these refer to the ‘single model’ results, not the results for ensembles.\n\nSome form of boosting performed best on 4 of the 11 datasets.\nResNet was best on four classification tasks, but not once for numeric targets.\nAt this point you won’t be surprised at what doesn’t perform as well- TabNet, NODE, and similar. TabNet, DCN, and GrowNet were never the best performer, and the other three were best one time a piece.\nMLP did not perform best on any data, however the authors note that it ‘is often on par or even better than some of the recently proposed DL models’.\nThey also looked at models with a ‘simple’ transformer architecture. Their results suggest better performance than the other DL models, and similar performance to ResNet.\n\n\n\nOther Stuff\n\nKudos\n\nSharing the source code!\nRecognizing that results at this point are complex at best given the lack of standard datasets\n\n\n\nIssues\n\nThey note a distinction between heterogeneous vs. other types of data. They call data heterogeneous if the predictors are of mixed data types (e.g. categorical, numeric, count), while something like pixel data would be homogeneous because all the columns are essentially the same type. The latter isn’t as interesting to me for this sort enterprise, and I think the former is what most are thinking about for ‘tabular’ data, otherwise we’d just call it what it is (e.g. image or text data), and modeling/estimation is generally quite a bit easier when all the data is the same type. I do think it’s important that they point out that GB is better with heterogeneous data, and I think if you only look at such data, you’d likely see GB methods still outperforming or at worst on par with the best DL methods.\n\n\n\nOpinion\nThese results seem consistent with others at this point. Complex DL isn’t helping, and simpler architectures, even standard MLP show good performance. In the end, we still don’t have any clear winner over GB methods."
  },
  {
    "objectID": "posts/2021-07-15-dl-for-tabular/index.html#overall-assessment",
    "href": "posts/2021-07-15-dl-for-tabular/index.html#overall-assessment",
    "title": "This is definitely not all you need",
    "section": "Overall Assessment",
    "text": "Overall Assessment\nThese papers put together are helpful in painting a picture of where we are at present with deep learning for tabular data, especially with mixed data types. In this setting, it seems that more complicated DL models do not seem to have any obvious gain over simpler architectures, which themselves do not consistently beat boosting methods. It may also be the case that for data of mixed data types/sources, boosting is still the standard to beat.\nEven though these articles are geared toward comparisons to GB/XGBoost, in several settings I’ve applied them, I typically do not necessarily have appreciably greater success compared to a default setting random forest (e.g. from the ranger package in R), or sufficiently flexible statistical model. Unfortunately this comparison is lacking from the papers, and would have been nice to have, especially for smaller data settings where such models are still very viable. I think a viable fast model, preferably one without any tuning required (or which simply is taken off the shelf) should be the baseline.\nIn that light, for tabular data I think one should maybe start with a baseline of a penalized regression with appropriate interactions (e.g. ridge/lasso), or a more flexible penalized approach (GAMM) as a baseline, the latter especially, as it can at least automatically incorporate nonlinear relationships, and tools like mgcv or gpboost in R can do so with very large data (1 million +) in a matter of seconds. In settings of relatively higher dimensions, interactions and nonlinearities should be prevalent enough such that basis function, tree, and DL models should be superior. Whether they are practically so is the key concern even in those settings. With smaller, noisier data of less dimension, I suspect the tuning/time effort with present day DL models for tabular data will likely not be worth it. This may change very soon however, so such an assumption should be regularly checked.\n \nlast updated: 2025-01-02\nNeural Net image source from UC Business Analytics R Programming Guide"
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#introduction",
    "title": "Deep Learning for Tabular Data",
    "section": "",
    "text": "In a previous post, I offered a summary of several articles that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g. demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g. XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#tldr-the-meta-analysis",
    "title": "Deep Learning for Tabular Data",
    "section": "TLDR: the meta-analysis",
    "text": "TLDR: the meta-analysis\nI collected most of the results from the summarized articles here and those covered in the previous post to see if we come to any general conclusions about which methods are best or work best in certain settings. In the following tables, I excluded those I knew to be image data, as well as datasets where I thought results were indistinguishable across all models tested (e.g. less than 1% difference in accuracy). This left comparisons for 92 datasets across six articles. However, it’s important to note that these were not independent datasets or studies. For example, Gorishniy et al. are the source of two papers and essentially the same testing situations, and other datasets were common across papers (e.g. Higgs Boson). In the rare situations there was a tie, I gave the nod to boosting methods as a. the whole point is to do better than those, b. they are the easier model to implement, and c. they are not always given the same advantages in these studies (e.g. pre-processing).\n\nFeature Type\nThe following shows results by feature type.\n\nHeterogeneous: at least 10% of categorical or numeric data with the rest of the other\nMinimal combo: means any feature inclusion of a different type. In the second table I collapse to ‘any heterogeneous’.\nBoost: Any boosting method (most of the time it’s XGBoost but could include lightGBM or other variant)\nMLP: multilayer perceptron or some variant\nDL_complex: A DL method more complex than MLP and which is typically the focus of the paper\n\nThe results suggest that current DL approaches’ strength is mostly with purely numeric data, and for heterogeneous data, simpler MLP or Boosting will generally prevail. I initially thought that boosting would do even better with heterogeneous data, and I still suspect that with more heterogeneous data and on more equal footing, results would tilt even more.\n\n\n\n\nTable 1: Feature Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nHeterogeneous\nMin. Combo\n\n\n\n\nBoost\n2\n10\n14\n6\n\n\nMLP\n2\n4\n9\n11\n\n\nDL_complex\n0\n22\n7\n5\n\n\n\n\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nAll Cat\nAll Num\nAny Combo\n\n\n\n\nBoost\n2\n10\n20\n\n\nMLP\n2\n4\n20\n\n\nDL_complex\n0\n22\n12\n\n\n\n\n\n\n\n\n\n\n\n\nSample/Feature Set Size\nThe following suggests that complex DL methods are going to require a lot of data to perform better. This isn’t that surprising but the difference here is quite dramatic. Interestingly, MLP methods worked well for fewer features. N total in this case means total size reported (not just training).\n\n\n\n\nTable 2: Sample Size\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nN features\nN total\n\n\n\n\nBoost\n209\n133,309\n\n\nDL_complex\n207\n530,976\n\n\nMLP\n114\n114,164\n\n\n\n\n\n\n\n\n\n\n\n\nTarget Type\nIn the following we compare binary (bin), multiclass (mc), and numeric (num) target results1, but there’s no strong conclusion for this. The main thing to glean from this is that these papers do not test numeric targets nearly enough. Across dozens of disciplines and countless datasets that I’ve come across in various settings, if anything, this ratio should be reversed.\n\n\n\n\nTable 3: Target Type\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nbin\nmc\nnum\n\n\n\n\nBoost\n17\n10\n5\n\n\nDL_complex\n17\n11\n6\n\n\nMLP\n10\n14\n2\n\n\n\n\n\n\n\n\n\n\n\n\nCombinations\nIn the following I look at any heterogeneous, smaller data (N &lt; 200,000). A complex DL model will likely not do great in this setting.\n\n\n\n\nTable 4: Combinations\n\n\n\n\n  \n  \n\n\n\nwinner_model_type\nn\n\n\n\n\nBoost\n19\n\n\nDL_complex\n8\n\n\nMLP\n19\n\n\n\n\n\n\n\n\n\n\nNow, on to the details of some of the recent results that were included."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#on-embeddings-for-numerical-features-in-tabular-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "On Embeddings for Numerical Features in Tabular Deep Learning",
    "text": "On Embeddings for Numerical Features in Tabular Deep Learning\n\nAuthors: Gorishniy, Rubachev, & Babenko\nYear: 2022\nArxiv Link\n\n\nOverview\nYura Gorishniy, Rubachev, and Babenko (2022) pit several architectures against one another, such as standard multilayer perceptron (MLP), ResNet, and their own transformer approach (see Yuri Gorishniy et al. (2021)). Their previous work, which was summarized in my earlier post, was focused on the architecture, while here they focus on embedding approaches. The primary idea is to take the value of some feature and expand it to some embedding space, then use the embedding in lieu of the raw feature. It can essentially be seen as a pre-processing task.\nOne approach they use is piecewise linear encoding (PLE), which they at one point describe as ‘a continuous alternative to the one-hot encoding’2. Another embedding they use is basically a fourier transform.\n\n\nData\n\n12 public datasets mostly from previous works on tabular DL and Kaggle competitions.\nSizes were from ~10K to &gt;1M.\nTarget variables were binary, multiclass, or numeric.\nThe number of features ranged from 8 to 200.\n\n9 of 12 data sets had only numeric features, two had a single categorical feature, and unfortunately, only one of these might be called truly heterogeneous, i.e., with a notable mix of categorical and numeric features3.\n\n\n\nModels Explored\n\nCatBoost\nXGBoost\nMLP, MLP*\nResNet, ResNet*\nTransformer*\n\n* Using proposed embeddings\n\n\nQuick Summary\n\nA mix of results with no clear/obvious winners (results are less distinguishable if one keeps to the actual precision of the performance metrics, and even less so if talking about statistical differences in performance).\n\nSeveral datasets showed no practical difference across any model (e.g. all accuracy results within ~.01 of each other).\n\nEmbedding-based approaches generally tend to improve over their non-embedding counter parts (e.g. MLP + embedding &gt; MLP), this was possibly the clearest result of the paper.\nI’m not sure we could say the same for ResNet, where results were similar with or without embedding\nXGBoost was best on the one truly heterogeneous dataset.\n\n\n\nIn general this was an interesting paper, and I liked the simple embedding approaches used. It was nice to see that they may be useful in some contexts. The fourier transform is something that analysts (including our team at Strong) have used in boosting, so I’m a bit curious why they don’t do Boosting + embeddings for comparison for that or both embedding types. These embeddings can be seen as a pre-processing step, so nothing would keep someone from using them for any model.\nAnother interesting aspect was how little difference there was in model performance. It seemed half the datasets showed extremely small differences between any model type."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training",
    "title": "Deep Learning for Tabular Data",
    "section": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training",
    "text": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training\n\nAuthors: Somepalli, Goldblum, Schwarzschild, Bayan-Bruss, & Goldstein\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper applies BERT-style attention over rows and columns, along with embedding/data augmentation. They distinguish the standard attention over features, with intersample attention of rows. In addition, they use CutMix for data augmentation (originally devised for images), which basically combines pairs of observations to create a new observation4. Their model is called SAINT, the Self-Attention and Intersample Attention Transformer.\n\n\nData\n\n16 data sets\nAll classification, 2 multiclass\n6 are heterogeneous, 2 notably so\nSizes 200 to almost 500K\n\n\n\nModels Explored\n\nLogistic Regression (!)\nRandom Forest\nBoosting\n\nCatBoost\nXGBoost\nLightGBM\n\nMLP\nTabNet\nVIME\nTabTransformer\nSAINT\n\n\n\nQuick Summary\n\nIt seems the SAINT does quite well on some of the data, and average AUROC across all datasets is higher than XGB.\nMain table shows only 9 datasets though, which they call ‘representative’ but it’s not clear what that means when you only have 16 to start. One dataset showed near perfect classification for all models so will not be considered. Of the 15 total remaining:\n\nSAINT wins 10 (including 3 heterogeneous)\nBoosting wins 5 (including 2 heterogeneous)\n\nSAINT benefits from data augmentation. This could have been applied to any of the other models, but doesn’t appear to have been done.\nAt least they also used some form of logistic regression as a baseline, though I couldn’t find details on its implementation (e.g. regularization, including interactions). I don’t think this sort of simple baseline is utilized enough.\n\nThis is an interesting result, but somewhat dampened by lack of including numeric targets and more heterogeneous data. The authors include small data settings which is great, and are careful to not generalize despite some good results, which I can appreciate.\nI really like the fact they also compare a simple logistic regression to these models, because if you’re not able to perform notably better relative to the simplest model one could do, then why would we care? The fact that logistic regression is at times competitive and even beats boosting/SAINT methods occasionally gives me pause though. Perhaps some of these data are not sufficiently complex to be useful in distinguishing these methods? It is realistic though. While it’s best not to assume as such, sometimes a linear model is appropriate given the features and target at hand."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning",
    "title": "Deep Learning for Tabular Data",
    "section": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning",
    "text": "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning\n\nAuthors: Kossen, Band, Lyle, Gomez, Rainforth, & Gal\nYear: 2021\nArxiv Link\n\n\nOverview\nThis paper introduces Non-Parametric Transformers, which focus on holistic processing of multiple inputs, and attempts to consider an entire dataset as input as opposed to a single row. Their model attempts to learn relations between data points to aid prediction. They use a mask to identify prediction points from the non-masked data, i.e. the entire \\(X_{\\textrm{not masked}}\\text{ }\\) data used to predict \\(X_{\\textrm{masked}}\\text{ }\\). The X matrix actually includes the target (also masked vs. not). At prediction, the model is able to make use of the correlations of inputs of training to ultimately make a prediction.\n\n\nData\n\n10 datasets from UCI, 2 are image (CIFAR MNIST)\n4 binary, 2 multiclass, 4 numeric targets\n\n\n\nModels Explored\n\nNPT\nBoosting\n\nGB\nXGB\nCatBoost\nLightGBM\n\nRandom Forest\nTabNet\nKnn\n\n\n\nQuick Summary\n\nGood performance of these models, but not too different from best boosting model for any type of data.\n\nNPT best on binary classification, but similar to CatBoost\nSame as XGB and similar to MLP on multiclass\nBoosting slightly better on numeric targets, but NPT similar\n\nAs seen several times now, TabNet continues to underperform\nk-nn regression worst (not surprising)\n\nWhen I first read the abstract where they say “We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input.”, I immediately was like ‘What about this, that, and those?’. The key phrase was ‘deep learning’, because the authors note later that this has a very long history in the statistical modeling realm. I was glad to see in their background of the research that they explicitly noted the models that came to my mind, like gaussian processes, kernel regression, etc. Beyond that, many are familiar with techniques like knn-regression and predictive mean matching, so it’s definitely not new to consider more than a single data point for prediction. I thought it was good of them to add k-nn regression to the model mix, even though it was not going to do well compared to the other approaches.\nThough the author’s acknowledge a clear thread/history here, I’m not sure this result is the fundamental shift they claim, versus a further extension/expansion into the DL domain. Even techniques that may work on a single input at a time may ultimately be taking advantage of correlations among the inputs (e.g. spatial correlations in images). Also, automatic learning of feature interactions is standard even in basic regularized regression settings, but here their focus is on observation interactions (but see k-nn regression)."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#conclusion",
    "title": "Deep Learning for Tabular Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn the two reviews on DL for tabular data that I’ve done, it appears there is more work in store for DL methods applied to tabular data. While it’d be nice to have any technique that would substantially improve prediction for such settings, I do have a suspicion results are likely rosier than they are, since that is just about the case for any newly touted technique, and at least in some cases, I don’t think we’re even making apple to apple comparisons.\nThat said, I do feel like some ground has been made for DL applications for tabular data, in that architectures can now more consistently performing as well as boosting methods in certain settings, especially if we include MLP. In the end though, results don’t appear strong enough to warrant a switch from boosting for truly heterogeneous data, or even tabular data in general. I feel like someday we’ll maybe have a breakthrough, but in the meantime, we can just agree that messy data is hard stuff to model, and the best tool is whichever one works for your specific situation."
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#guidelines-for-future-research",
    "title": "Deep Learning for Tabular Data",
    "section": "Guidelines for future research",
    "text": "Guidelines for future research\nI was thinking about what would be a convincing result, the type of setting and setup where if a DL technique was consistently performing statistically better than boosting methods, I’d be impressed. So I’ve made a list of things I’d like to see more of, and which would make for a better story if the DL method were to beat out other techniques.\n\nAlways use heterogeneous data. For giggles let’s say 20%+ of the minority feature type.\nFeatures should at least be minimally correlated, if not notably so.\nImage data results are not interesting (why would we use boosting on this in practice?).\nNumeric targets should at least be as much of focus as categorical targets.\nInclude ‘small’ datasets.\nInclude very structured data (e.g. clustered with repeated observations, geographical points, time series).\nUse a flexible generalized additive or similar penalized regression with interactions as a baseline statistical model.\nMaybe add survival targets to the mix.\nIf using a pre-processing step that is done outside of modeling, this likely should be applied to non-DL methods for better comparison, especially, if we’re only considering predictive accuracy and don’t care too much about interpretation.\nNote your model variants before analyzing any data. Tweaking/torturing model architecture after results don’t pan out is akin to p-hacking in the statistical realm, and likewise wastes both researcher and reader’s time.\nRegarding results…\n\nDon’t claim differences that you don’t have precision to do so, or at least back them up with an actual statistical test.\nIf margin of error in the metrics is overlapping, while statistically they could be different, practically they probably aren’t to most readers. Don’t make a big deal about it.\nIt is unlikely anyone will be interested in three decimal place differences for rmse/acc type metrics, and statistically, results often don’t even support two decimal precision.\nReport how you are obtaining uncertainty in any error estimates.\nIf straightforward, try to give an estimate of total tuning/run times.\n\nWith the datasets\n\nName datasets exactly how they are named at the source you obtained them from, provide direct links\nProvide a breakdown for both feature and target types\nProvide clear delineation of total/training/validation/test sizes"
  },
  {
    "objectID": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "href": "posts/2022-04-01-more-dl-for-tabular/index.html#footnotes",
    "title": "Deep Learning for Tabular Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t refer to numeric targets as ‘regression’ because that’s silly for so many reasons. 😄↩︎\nA quick look suggests it’s not too dissimilar from a b-spline.↩︎\nI’ll let you go ahead and make your own prediction about which method was best on that data set.↩︎\nIt’s not clear to me how well this CutUp approach would actually preserve feature correlations. My gut tells me the feature correlations of this approach would be reduced relative to the observed, since the variability of the new observations is likely reduced. This ultimately may not matter for predictive purposes or their ultimate use in embeddings. However, I wonder if something like SMOTE, random (bootstrap) sampling, other DL methods like autoencoders, or similar approaches might do the same or better.↩︎"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html",
    "href": "posts/2022-09-deep-linear-models/index.html",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#introduction",
    "href": "posts/2022-09-deep-linear-models/index.html#introduction",
    "title": "Deep Linear Models",
    "section": "",
    "text": "NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you’ve found an issue, please post it at GitHub.\n\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started",
    "title": "Deep Linear Models",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s get the primary packages loaded first.\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport torch\n\nNext, we’ll use the well-known titanic dataset, and to start things off, we’ll need to get a sense of what we’re dealing with. The basic idea is that we’d like to predict survival based on key features like sex, age, ticket class and more.\n\n# non-kaggle-requiring url here: https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/data/dl-linear-regression/titanic/train.csv\ndf_titanic_train = pd.read_csv('data/dl-linear-regression/titanic/train.csv')\n# df_titanic_train\n\n\ndf_titanic_train.describe()\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "href": "posts/2022-09-deep-linear-models/index.html#initial-data-processing",
    "title": "Deep Linear Models",
    "section": "Initial Data Processing",
    "text": "Initial Data Processing\nThe data is not ready for modeling as is, so we’ll do some additional processing to get it ready. We’ll check out the missing values and replace them with modes3.\n\ndf_titanic_train.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\nmodes = df_titanic_train.mode().iloc[0]\n\ndf_titanic_train.fillna(modes, inplace = True)\n\ndf_titanic_train.describe(include = (np.number))\n\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]\n\n\nWith features, sometimes it is worthwhile to log transform data for potentially more efficient optimization search. Since we have zeros, we add 1 before taking the log.\n\ndf_titanic_train['Fare'].hist()\n\n\nNow the transformed data looks a little more manageable. More to the point, we won’t potentially have huge coefficients relative to other covariates because of the range of the data.\n\ndf_titanic_train['LogFare'] = np.log1p(df_titanic_train['Fare'])\n\n# df_titanic_train['LogFare'].hist()\n\n\nThe Pclass (passenger class) feature is actually categorical.\n\npclasses = sorted(df_titanic_train.Pclass.unique())\npclasses\n\n[np.int64(1), np.int64(2), np.int64(3)]\n\n\nHere are the other categorical features.\n\ndf_titanic_train.describe(include = [object])\n\n                           Name   Sex  Ticket    Cabin Embarked\ncount                       891   891     891      891      891\nunique                      891     2     681      147        3\ntop     Braund, Mr. Owen Harris  male  347082  B96 B98        S\nfreq                          1   577       7      691      646\n\n\nIn order to use categorical variables, they need to be changed to numbers4, so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use embeddings5, particularly for things that have lots of unique categories.\n\ndf_titanic_train = pd.get_dummies(df_titanic_train, columns = [\"Sex\", \"Pclass\", \"Embarked\"])\n\nLet’s take a look at our data now.\n\ndf_titanic_train.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\ndf_titanic_train.head()\n\n   PassengerId  Survived  ... Embarked_Q  Embarked_S\n0            1         0  ...      False        True\n1            2         1  ...      False       False\n2            3         1  ...      False        True\n3            4         1  ...      False        True\n4            5         0  ...      False        True\n\n[5 rows x 18 columns]"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "href": "posts/2022-09-deep-linear-models/index.html#getting-started-with-pytorch",
    "title": "Deep Linear Models",
    "section": "Getting Started with pytorch",
    "text": "Getting Started with pytorch\n\nSetup\nNow we are ready to prep things for specific use with pytorch. I will not use the same terminology as in Jeremy’s original post, so for us, target = ‘dependent variable’ and X is our feature matrix6. Both of these will be pytorch tensors, which for our purposes is just another word for an array of arbitrary size.\n\nfrom torch import tensor\ndevice = torch.device('cpu')\ntarget = tensor(df_titanic_train.Survived)\n\n\ndummies = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nall_features = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummies \n\nX = df_titanic_train[all_features].apply(pd.to_numeric).astype(float)\nX = tensor(X.values, dtype = torch.float)\n\nX.shape\n\ntorch.Size([891, 12])"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "href": "posts/2022-09-deep-linear-models/index.html#setting-up-a-linear-model",
    "title": "Deep Linear Models",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nWe have our feature matrix and target variable prepped. The next step is to map the features to the target by means of predicted values. In linear regression, we typically call the weights that produce the predictions coefficients, but in standard deep/machine learning terminology, they are usually called weights, or more generally, parameters. Here, we generate some random values between -.5 and .5 to get started7:.\n\ntorch.manual_seed(442)\n\n&lt;torch._C.Generator object at 0x162e724b0&gt;\n\nn_coeff = X.shape[1]\ncoeffs = torch.rand(n_coeff) - 0.5  # default would produce values from 0 to 1\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625])\n\n\nThe original post did a form of min-max scaling to the features, basically putting everything on a potentially [0, 1] scale. Here we’ll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.\n\n# vals,indices = X.max(dim=0)\n# X = X / vals\nX_means = X.mean(dim = 0, keepdim = True)\nX_sds   = X.std(dim = 0)\n\nX_sc = (X - X_means) / X_sds\n\n# X_sc.mean(dim = 0)  # all means = 0 \n# X_sc.std(dim = 0)   # all sd = 1\n\nAs noted in the original post and worth iterating here for our statistical modeling crowd, we don’t estimate an intercept for this model and keep all the dummy coded features. The following takes our coefficients, multiplies them by their respective feature, and sums them.\n\npreds = (X_sc * coeffs).sum(axis = 1)\npreds[:10]\n\ntensor([ 0.6000, -1.9341,  0.2080,  0.1723, -0.0032,  0.3088, -0.5066,  1.6219,\n         0.6990, -1.2584])\n\n\nWe can calculate our loss, the difference in our predictions versus the target values, in many ways. Here we get the mean squared error.\n\nloss = torch.square(preds - target).mean()\nloss\n\ntensor(1.3960)\n\n\nNow we’ll create functions that do the previous steps, and finally, give it a test run! In the original fastai formulation, they use mean absolute error for the loss, which actually is just the L1loss that is available in torch. For a change of pace, we’ll keep our mean squared error, which is sometimes called L2 loss (this will create different results from the original notebook). I create the option within the function for you to do either. Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.\n\ndef calc_preds(X, weights):\n    return((X * weights).sum(axis = 1))\n\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n    \n    # torch.abs(calc_preds(X, coeffs)-target).mean()  # original\n\n    if which == 'l2':\n      loss = torch.nn.MSELoss()\n    else: \n      loss = torch.nn.L1Loss()\n      \n    L = loss(preds, target.float())\n      \n    return(L)\n\ncalc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which = 'l1')\n\n(tensor(1.3960), tensor(0.8891))\n\n\n\nDoing a Gradient Descent Step\nWe can continue our journey onward to actually estimating the weights rather than specifying them directly, since we definitely don’t want to just keep guessing! This is an iterative process where we still start with an initial (random) guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps epochs, and getting our next guess requires calculating what’s called a gradient. Here are some resources for more detail:\n\nHow Does a Neural Net Really Work?: great intro by Jeremy Howard\nSome by-hand code using gradient descent for linear regression, R, Python: By yours truly\n\nIn any case, this is basic functionality within pytorch, and it will keep track of each step taken.\n\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\n\nloss = calc_loss(X_sc, coeffs, target)\nloss\n\ntensor(1.3960, grad_fn=&lt;MseLossBackward0&gt;)\n\n\nWe use backward to calculate the gradients and inspect them.\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-0.9311,  0.6245,  0.4957, -0.7423,  0.6008, -0.6008, -0.9158,  0.0938,\n         0.7127, -1.7183,  0.1715,  1.3974])\n\n\nEach time backward is called, the gradients are added to the previous values. We can see here that they’ve now doubled.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\ncoeffs.grad\n\ntensor([-1.8621,  1.2491,  0.9914, -1.4847,  1.2015, -1.2015, -1.8317,  0.1877,\n         1.4254, -3.4366,  0.3431,  2.7947])\n\n\nWhat we want instead is to set them back to zero after they are used for our estimation step. The following does this.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)     # sub subtracts in place\n    coeffs.grad.zero_()                # zeros out in place\n    print(calc_loss(X, coeffs, target))\n\ntensor([-0.1836, -0.0488,  0.0922, -0.0035, -0.4435, -0.1345,  0.7624,  0.2854,\n         0.0661,  0.0763,  0.1588, -0.0567], requires_grad=True)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\ntensor(37.9424)\n\n\n\n\nTraining the Linear Model\nWe typically would split our data into training and test. We can do so here, or keep this data as training and import test.csv for the test set. The latter is actually used for the Kaggle submission, but that’s not a goal here. We’ll use scikit-learn for the splitting.\n\nfrom sklearn.model_selection import train_test_split\n\n# test size .2 in keeping with fastai RandomSplitter default\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n  X_sc, \n  target.float(), \n  test_size = 0.2, \n  random_state = 808\n)\n  \n\nlen(train_x), len(valid_x) # might be one off of the original notebook\n\n(712, 179)\n\n\nAs before, we’ll create functions to help automate our steps:\n\none to initialize the weights\na function to update weights\none to do a full epoch (using weights to calculate loss, updating weights)\none to train the entire model (run multiple times/epochs)\n\nAs mentioned, the approach here is to create functions that are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each verbose value epoch (e.g. verbose = 10 means you’ll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (possibly not too big a deal depending on your IDE).\n\ndef init_weights(n_wts): \n    return (torch.rand(n_wts) - 0.5).requires_grad_()\n\ndef update_weights(weights, lr):\n    weights.sub_(weights.grad * lr)\n    weights.grad.zero_()\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\ndef train_model(X, target, epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1])\n    \n    for i in range(epochs): \n        one_epoch(X, coeffs, target, lr = lr, i = i, verbose = verbose)\n    return coeffs\n\nTry out the functions if you like (not shown).\n\ncalc_loss(X_sc, init_weights(X_sc.shape[1]), target).backward()\n\n\none_epoch(train_x, init_weights(train_x.shape[1]), train_y, .01)\n\nNow train the model for multiple epochs. The loss drops very quickly before becoming more steady.\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, verbose = 5, lr = .2)\n\n 1.375618\n  0.296216\n  0.284019\n  0.281221\n  0.280271\n  0.279923\n  0.279794\n  0.279746\n  0.279728\n  0.279721\n \n\n\nLet’s create a function to show our estimated parameters/weights/coefficients in a pretty fashion.\n\ndef show_coeffs(estimates): \n  coef_dict = dict(zip(all_features, estimates.requires_grad_(False).numpy()))\n  return pd.DataFrame(coef_dict, index = ['value']).T\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.090825\nSibSp      -0.054449\nParch      -0.016111\nLogFare     0.046320\nSex_male   -0.406538\nSex_female -0.171426\nPclass_1    0.408707\nPclass_2    0.335766\nPclass_3    0.329800\nEmbarked_C  0.057091\nEmbarked_Q  0.032813\nEmbarked_S  0.039464\n\n\n\n\nMeasuring Accuracy\nIt’s one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data. The following function will convert our estimates to a binary value like our target, and compares them to the target. Depending on how you did your training setup, it might be pretty bad or at least better than guessing.\n\ndef acc(X, weights, target): \n    return (target.bool() == (calc_preds(X, weights) &gt; 0.5)).float().mean()\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7051), tensor(0.6425))\n\n\n\n\nUsing sigmoid\nNothing about the previous setup actually produces a result on the probability scale, so basing a cutoff of .5 is meaningless. you can inspect them and might see values are above 1 or below zero, which we generally don’t want8. However we do have a solution. The sigmoid function9 allows us to transform our predictions to values between 0 and 1, i.e. probabilities in this context, and in particular, the probability of survival. Then our acc function will be more appropriate, where any probability &gt; .5 will be given a value of 1 (or True technically), while others will be 0/False.\n\ndef calc_preds(X, weights):\n    return torch.sigmoid((X*weights).sum(axis = 1))\n\nWe also will do more iterations, and fiddle with the learning rate (a.k.a. step size)\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  epochs = 500,\n  lr = 1,\n  verbose = 100\n)\n\n 0.314158\n  0.154329\n  0.154237\n  0.154232\n  0.154232\n \n\n\nNot too shabby!\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n\n(tensor(0.7823), tensor(0.7989))\n\n\n\nshow_coeffs(coeffs_est)\n\n               value\nAge        -0.516476\nSibSp      -0.423656\nParch      -0.179623\nLogFare     0.396468\nSex_male   -0.927410\nSex_female  0.349448\nPclass_1    0.713895\nPclass_2    0.320935\nPclass_3    0.078919\nEmbarked_C  0.107378\nEmbarked_Q  0.082943\nEmbarked_S -0.036137\n\n\nIn implementing the sigmoid, let’s go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster)10. To do this, the coefficients will need to be a column vector, so we change our init_coeffs function slightly11.\n\ndef calc_preds(X, weights): \n    return torch.sigmoid(X@weights)\n\ndef init_coeffs(n_wts): \n    return (torch.rand(n_wts, 1) * 0.1).requires_grad_()\n\nNow our functions are more like the mathematical notation we’d usually see for linear regression.\n\\[\\hat{y} = X\\beta\\]\n\n\nCompare to Linear/Logistic Regression\nBefore getting too excited, let’s compare our results to basic linear and logistic regression. The linear regression is more like our model before using the sigmoid transformation, while the logistic is more like when we used it. Depending on your settings, the logistic regression is probably doing better at this point.\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\n\n\nreg = linear_model.LinearRegression()\nreg.fit(train_x, train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\nacc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)\n\n(tensor(0.7989), tensor(0.7821))\n\n\n\nreg = linear_model.LogisticRegression()\nreg.fit(train_x, train_y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n0.7821229050279329\n\n\nIt looks like our coefficient estimates are similar to the logistic regression ones.\n\nshow_coeffs(coeffs_est).assign(logreg = np.round(reg.coef_.T, 4))\n\n               value  logreg\nAge        -0.516476 -0.4799\nSibSp      -0.423656 -0.4191\nParch      -0.179623 -0.1265\nLogFare     0.396468  0.3441\nSex_male   -0.927410 -0.6262\nSex_female  0.349448  0.6262\nPclass_1    0.713895  0.3941\nPclass_2    0.320935  0.0675\nPclass_3    0.078919 -0.3945\nEmbarked_C  0.107378  0.0546\nEmbarked_Q  0.082943  0.0655\nEmbarked_S -0.036137 -0.0890"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "href": "posts/2022-09-deep-linear-models/index.html#a-neural-network",
    "title": "Deep Linear Models",
    "section": "A Neural Network",
    "text": "A Neural Network\n\nAt this point we’ve basically reproduced a general linear model. A neural network, on the other hand, has from one to many hidden layers of varying types in between input and output. Let’s say we have a single layer with two nodes. For a fully connected or dense network, we’d need weights to map our features to each node of the hidden layer (n_wts * n_hidden parameters total), and then another set of weights to map the hidden nodes to our next layer, which is our output, basically creating the predicted values. You can think of it as a second hidden layer with a single output node. With additional hidden nodes we add more complexity, but also flexibility, to the model. But this may come at a price, e.g. more difficulty with training due to the additional parameters that have to be estimated.\nSo basically we need matrices of weights, and the following function allows us to create those. We also add a bias/intercept/constant for the hidden-to-output processing. In the first layer, we divide the weights by n_hidden to create sums at the next layer that are of similar magnitude as the inputs. In general though, there are many ways to initialize weights.\n\ndef init_weights(n_wts, n_hidden = 20):\n    layer1 = (torch.rand(n_wts, n_hidden) - 0.5) / n_hidden # n_wts x n_hidden matrix of weights\n    layer2 = torch.rand(n_hidden, 1) - 0.3                  # n_hidden weights\n    const  = torch.rand(1)[0]                               # constant\n    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\nNow we revise our calc_preds function to incorporate all the weights. Initially, we extract the different sets of weights that are estimated by the model. For the original inputs, we multiply them by the layer 1 weights and sum. Then we apply a transformation to them to induce nonlinearity. Typical approaches are the sigmoid function we used before, hyperbolic tangent, and, probably the most common, the relu. The original notebook used relu, while I use a more recent one called Mish, which is a variant of relu. The hidden layer nodes then get multiplied by their respective weights and summed with the constant added. We then use our sigmoid function to get the probability scale as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(X, weights):\n    l1, l2, const = weights\n    res = F.mish(X@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res).flatten()\n\nWith additional sets of weights, we use an update loop.\n\ndef update_weights(weights, lr):\n    for layer in weights:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, lr = 1, verbose = 10)\n\n 0.325837\n  0.155810\n  0.141485\n  0.137652\n  0.136034\n \n\n\nAt this point we’re doing a little bit better in general, and even better than standard logistic regression on the test set!\n\nacc(train_x, coeffs_est, train_y), \\\nacc(valid_x, coeffs_est, valid_y), \\\naccuracy_score(valid_y.numpy(), reg.predict(valid_x))\n\n(tensor(0.8160), tensor(0.8045), 0.7821229050279329)"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "href": "posts/2022-09-deep-linear-models/index.html#deep-learning",
    "title": "Deep Linear Models",
    "section": "Deep Learning",
    "text": "Deep Learning\nWe previously used a single hidden layer, but we want to go deeper! That’s the whole point of deep learning right? The following modifies our previous functions to allow for an arbitrary number of layers. You’ll note there are some hacks to get the weights in a good way for each layer12, but you normally wouldn’t have to do that on your own, since most tools will provide sensible modifications.\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\n# change loss to binary\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n\n    loss = torch.nn.BCELoss()\n\n    L = loss(preds, target.float())\n\n    return(L)\n\n\ndef init_weights(n_wts, hiddens):  \n    sizes = [n_wts] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]\n    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers, consts\n\n\ndef calc_preds(X, weights):\n    layers, consts = weights\n    n = len(layers)\n    res = X\n    \n    for i, l in enumerate(layers):\n        res = res@l + consts[i]\n    \n    if i != n-1: \n      res = F.mish(res)\n      \n    \n    return torch.sigmoid(res).flatten()\n\ndef update_weights(weights, lr):\n    layers, consts = weights\n    for layer in layers + consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1], hiddens)\n    \n    for i in range(epochs): \n        if verbose != 0:\n            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)\n    \n    return coeffs\n\nWith everything set up, let’s do some deep linear modeling! You can play around with the number of hidden layers, number of nodes and other settings. Feel free to explore!\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  hiddens = [500, 250, 100],\n  epochs  = 500,\n  lr      = 1e-4,\n  verbose = 10\n)\n\n 5.123790\n  0.666971\n  0.653124\n  0.640325\n  0.628476\n  0.617496\n  0.607313\n  0.597861\n  0.589081\n  0.580918\n  0.573322\n  0.566249\n  0.559658\n  0.553510\n  0.547772\n  0.542413\n  0.537403\n  0.532715\n  0.528326\n  0.524212\n  0.520354\n  0.516733\n  0.513330\n  0.510130\n  0.507118\n  0.504281\n  0.501605\n  0.499080\n  0.496695\n  0.494439\n  0.492305\n  0.490283\n  0.488366\n  0.486547\n  0.484820\n  0.483178\n  0.481616\n  0.480129\n  0.478712\n  0.477361\n  0.476072\n  0.474840\n  0.473663\n  0.472538\n  0.471461\n  0.470429\n  0.469440\n  0.468493\n  0.467583\n  0.466710\n \n\n\nHooray! Our best model yet (at least tied).\n\npd.DataFrame({\n    'acc_train': acc(train_x, coeffs_est, train_y).flatten(), \n    'acc_test': acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm': accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int))\n}, index = ['value'])\n\n       acc_train  acc_test  acc_test_glm\nvalue    0.77809  0.804469      0.782123"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "href": "posts/2022-09-deep-linear-models/index.html#the-elephant-in-the-room",
    "title": "Deep Linear Models",
    "section": "The Elephant in the Room",
    "text": "The Elephant in the Room\nAs noted in my previous posts [1, 2], probably your biggest challenge in implementing a deep learning model for tabular data, one with mixed data types and other complexities, is beating an off the shelf boosting model. Here is a quick demo with lightgbm.\n\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(\n  # n_estimators = 500,  # the sorts of parameters you can play with (many more!)\n  # max_depth    = 4,\n  # reg_alpha    = .1\n)\n\nmodel.fit(train_x, train_y)\n\nLGBMClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifieriFittedLGBMClassifier() \n\nmodel.score(valid_x, valid_y.numpy())\n\n0.8491620111731844\n\n\n\n# sklearn example\n# from sklearn.ensemble import HistGradientBoostingClassifier\n# \n# res = HistGradientBoostingClassifier().fit(train_x.numpy(), train_y.numpy())\n# \n# res.score(valid_x.numpy(), valid_y.numpy())\n\nNo tuning at all, and we’re already doing significantly better. Granted, if you use a packaged DL model for tabular data like the one in fastai, you should be doing better than our little demo. Even then though, you may still find the boosting results tough to beat.\n\ndf_accs = pd.DataFrame({ \n    'acc_test_dl':   acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm':  accuracy_score(valid_y.numpy(), (reg.predict(valid_x) &gt; .5).astype(int)),\n    'acc_test_lgbm': model.score(valid_x, valid_y.numpy())\n}, index = ['value']).round(4)\n\ndf_accs\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue       0.8045        0.7821         0.8492\n\n\n\ndf_perc_improvement = 100 * (df_accs / df_accs.iloc[0,1] - 1)  # % improvement\ndf_perc_improvement\n\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue     2.864075           0.0       8.579466"
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#summary",
    "href": "posts/2022-09-deep-linear-models/index.html#summary",
    "title": "Deep Linear Models",
    "section": "Summary",
    "text": "Summary\nThis was a lot of work to do slightly better than a logistic regression! However, there is a lot going on with a typical DL model that would likely prove even better. But it also serves as a reminder to have a suitable baseline, and as we saw with the lightgbm model, it can take little effort to do very well without deep learning. Hopefully though, the peek behind the scenes to do some ‘deep’ linear modeling can make it more accessible for you."
  },
  {
    "objectID": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "href": "posts/2022-09-deep-linear-models/index.html#footnotes",
    "title": "Deep Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI won’t actually use fastai, since they aren’t up to supporting M1/2 Macs very well. I think it was only used for the train/test data split anyway. I would rant a bit about this, but a lot of fastai is geared toward non-local computing, and the fault is really with Apple and NVidia as near as I can tell.↩︎\nI’m also not going to go into broadcasting, submitting to Kaggle, and other things that I don’t think are necessary for our purposes here.↩︎\nJust as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here, and sometimes that’s enough.↩︎\nEven though every modeling tool requires this, strangely very few in the Python world offer options for automatic handling of such things, but it’s getting better.↩︎\nWe actually aren’t too far removed from this in our model coming up, the main difference is that we don’t treat the categorical feature part of the model separately.↩︎\nI’ll not perpetuate calling features/predictor variables that are clearly not independent as independent. That nomenclature really only works for randomized experiments, and that is definitely not the case here.↩︎\nYou could use torch.randn to get standard normal values, and often times we’ll even start with just zeros if we really are just doing a standard linear model.↩︎\nUnless you are an economist, in which case you call it a linear probability model and ignore the ridiculous predictions because you have very fine standard errors.↩︎\nA lot of R folks seem unaware that the base R plogis function accomplishes this.↩︎\nThe @ operator is essentially the dot product, so x@y is np.dot(x, y)↩︎\nThe fastai demo also changes the target to a column vector, but this doesn’t seem necessary.↩︎\nAnd they probably aren’t as good for the changes I’ve made.↩︎"
  },
  {
    "objectID": "posts/2024-05-20/index.html",
    "href": "posts/2024-05-20/index.html",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  },
  {
    "objectID": "posts/2024-05-20/index.html#book-in-progess",
    "href": "posts/2024-05-20/index.html#book-in-progess",
    "title": "Long time no see…",
    "section": "",
    "text": "TLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!"
  }
]