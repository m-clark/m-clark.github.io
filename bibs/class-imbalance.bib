
@article{shwartz-ziv_simplifying_2022,
	title = {Simplifying {Neural} {Network} {Training} {Under} {Class} {Imbalance}},
	url = {https://arxiv.org/abs/2312.02517},
	abstract = {Real-world datasets are often highly class-imbalanced, which can adversely impact the performance of deep learning models. The majority of research on training neural networks under class imbalance has focused on specialized loss functions, sampling techniques, or two-stage training procedures. Notably, we demonstrate that simply tuning existing components of standard deep learning pipelines, such as the batch size, data augmentation, optimizer, and label smoothing, can achieve state-of-the-art performance without any such specialized class imbalance methods. We also provide key prescriptions and considerations for training under class imbalance, and an understanding of why imbalance methods succeed or fail.},
	language = {en},
	author = {Shwartz-Ziv, Ravid and Goldblum, Micah and Li, Yucen Lily and Bruss, C Bayan and Wilson, Andrew Gordon},
	year = {2022},
	file = {Shwartz-Ziv et al. - Simplifying Neural Network Training Under Class Im.pdf:/Users/micl/Zotero/storage/QZ536GFT/Shwartz-Ziv et al. - Simplifying Neural Network Training Under Class Im.pdf:application/pdf},
}

@misc{foret_sharpness-aware_2021,
	title = {Sharpness-{Aware} {Minimization} for {Efficiently} {Improving} {Generalization}},
	url = {http://arxiv.org/abs/2010.01412},
	doi = {10.48550/arXiv.2010.01412},
	abstract = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by prior work connecting the geometry of the loss landscape and generalization, we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels. We open source our code at {\textbackslash}url\{https://github.com/google-research/sam\}.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
	month = apr,
	year = {2021},
	note = {arXiv:2010.01412 [cs]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/micl/Zotero/storage/8TMUNHG2/Foret et al. - 2021 - Sharpness-Aware Minimization for Efficiently Impro.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/49J5QJ5N/2010.html:text/html},
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	doi = {10.48550/arXiv.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/micl/Zotero/storage/NBPHXNJP/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/84V5VDWQ/1708.html:text/html},
}

@article{van_den_goorbergh_harm_2022,
	title = {The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression},
	volume = {29},
	issn = {1527-974X},
	shorttitle = {The harm of class imbalance corrections for risk prediction models},
	doi = {10.1093/jamia/ocac093},
	abstract = {OBJECTIVE: Methods to correct class imbalance (imbalance between the frequency of outcome events and nonevents) are receiving increasing interest for developing prediction models. We examined the effect of imbalance correction on the performance of logistic regression models.
MATERIAL AND METHODS: Prediction models were developed using standard and penalized (ridge) logistic regression under 4 methods to address class imbalance: no correction, random undersampling, random oversampling, and SMOTE. Model performance was evaluated in terms of discrimination, calibration, and classification. Using Monte Carlo simulations, we studied the impact of training set size, number of predictors, and the outcome event fraction. A case study on prediction modeling for ovarian cancer diagnosis is presented.
RESULTS: The use of random undersampling, random oversampling, or SMOTE yielded poorly calibrated models: the probability to belong to the minority class was strongly overestimated. These methods did not result in higher areas under the ROC curve when compared with models developed without correction for class imbalance. Although imbalance correction improved the balance between sensitivity and specificity, similar results were obtained by shifting the probability threshold instead.
DISCUSSION: Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed.
CONCLUSION: Outcome imbalance is not a problem in itself, imbalance correction may even worsen model performance.},
	language = {eng},
	number = {9},
	journal = {Journal of the American Medical Informatics Association: JAMIA},
	author = {van den Goorbergh, Ruben and van Smeden, Maarten and Timmerman, Dirk and Van Calster, Ben},
	month = aug,
	year = {2022},
	pmid = {35686364},
	pmcid = {PMC9382395},
	keywords = {Computer Simulation, Humans, calibration, class imbalance, Logistic Models, logistic regression, Probability, ROC Curve, Sensitivity and Specificity, synthetic minority oversampling technique, undersampling},
	pages = {1525--1534},
	file = {Full Text:/Users/micl/Zotero/storage/K4ARG9NI/van den Goorbergh et al. - 2022 - The harm of class imbalance corrections for risk p.pdf:application/pdf},
}

@misc{aprilliant_optimal_2021,
	title = {Optimal {Threshold} for {Imbalanced} {Classification}},
	url = {https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293},
	abstract = {How to choose the optimal threshold using a ROC curve and Precision-Recall curve},
	language = {en},
	urldate = {2025-01-19},
	journal = {Medium},
	author = {Aprilliant, Audhi},
	month = jul,
	year = {2021},
	file = {Snapshot:/Users/micl/Zotero/storage/UXGNDTA4/optimal-threshold-for-imbalanced-classification-5884e870c293.html:text/html},
}

@misc{brownlee_how_2018,
	title = {How to {Use} {ROC} {Curves} and {Precision}-{Recall} {Curves} for {Classification} in {Python}},
	url = {https://www.machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/},
	abstract = {It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly. This flexibility comes from the way that probabilities may be interpreted using different thresholds that allow the operator of the model to trade-off concerns in the errors made by the model, […]},
	language = {en-US},
	urldate = {2025-01-19},
	journal = {MachineLearningMastery.com},
	author = {Brownlee, Jason},
	month = aug,
	year = {2018},
}

@misc{kuhn_optimizing_2014,
	title = {Optimizing {Probability} {Thresholds} for {Class} {Imbalances}},
	url = {http://appliedpredictivemodeling.com/blog/2014/2/1/lw6har9oewknvus176q4o41alqw2ow},
	abstract = {Now let's run our random forest model and see what it comes up with for the best possible threshold:},
	language = {en-US},
	urldate = {2025-01-19},
	journal = {Applied Predictive Modeling},
	author = {Kuhn, Max},
	month = feb,
	year = {2014},
}

@misc{steen_understanding_2020,
	title = {Understanding the {ROC} {Curve} and {AUC}},
	url = {https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb},
	abstract = {These binary classification performance measures go hand-in-hand — let’s explore.},
	language = {en},
	urldate = {2025-01-19},
	journal = {Medium},
	author = {Steen, Doug},
	month = sep,
	year = {2020},
	file = {Snapshot:/Users/micl/Zotero/storage/YG6HUZ9B/understanding-the-roc-curve-and-auc-dd4f9a192ecb.html:text/html},
}

@misc{thomas_problem_2020,
	title = {The {Problem} with {Metrics} is a {Fundamental} {Problem} for {AI}},
	url = {http://arxiv.org/abs/2002.08512},
	doi = {10.48550/arXiv.2002.08512},
	abstract = {Optimizing a given metric is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, a myopic focus on short-term goals, and other unexpected negative consequences. This poses a fundamental contradiction for AI development. Through a series of real-world case studies, we look at various aspects of where metrics go wrong in practice and aspects of how our online environment and current business practices are exacerbating these failures. Finally, we propose a framework towards mitigating the harms caused by overemphasis of metrics within AI by: (1) using a slate of metrics to get a fuller and more nuanced picture, (2) combining metrics with qualitative accounts, and (3) involving a range of stakeholders, including those who will be most impacted.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Thomas, Rachel and Uminsky, David},
	month = feb,
	year = {2020},
	note = {arXiv:2002.08512 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to EDSC (Ethics of Data Science Conference) 2020},
	file = {Preprint PDF:/Users/micl/Zotero/storage/69RG4WA8/Thomas and Uminsky - 2020 - The Problem with Metrics is a Fundamental Problem .pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/UUXRXTAD/2002.html:text/html},
}

@misc{elor_smote_2022,
	title = {To {SMOTE}, or not to {SMOTE}?},
	url = {http://arxiv.org/abs/2201.08528},
	doi = {10.48550/arXiv.2201.08528},
	abstract = {Balancing the data before training a classifier is a popular technique to address the challenges of imbalanced binary classification in tabular data. Balancing is commonly achieved by duplication of minority samples or by generation of synthetic minority samples. While it is well known that balancing affects each classifier differently, most prior empirical studies did not include strong state-of-the-art (SOTA) classifiers as baselines. In this work, we are interested in understanding whether balancing is beneficial, particularly in the context of SOTA classifiers. Thus, we conduct extensive experiments considering three SOTA classifiers along the weaker learners used in previous investigations. Additionally, we carefully discern proper metrics, consistent and non-consistent algorithms and hyper-parameter selection methods and show that these have a significant impact on prediction quality and on the effectiveness of balancing. Our results support the known utility of balancing for weak classifiers. However, we find that balancing does not improve prediction performance for the strong ones. We further identify several other scenarios for which balancing is effective and observe that prior studies demonstrated the utility of balancing by focusing on these settings.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Elor, Yotam and Averbuch-Elor, Hadar},
	month = may,
	year = {2022},
	note = {arXiv:2201.08528 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/micl/Zotero/storage/U9ECG8UC/Elor and Averbuch-Elor - 2022 - To SMOTE, or not to SMOTE.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/63KE7YDD/2201.html:text/html},
}

@article{nixon_measuring_2020,
	title = {Measuring {Calibration} in {Deep} {Learning}},
	url = {https://arxiv.org/abs/1904.01685},
	abstract = {The reliability of a machine learning model’s conﬁdence in its predictions is critical for high-risk applications. Calibration—the idea that a model’s predicted probabilities of outcomes reﬂect true probabilities of those outcomes—formalizes this notion. Current calibration metrics fail to consider all of the predictions made by machine learning models, and are inefﬁcient in their estimation of the calibration error. We design the Adaptive Calibration Error (ACE) metric to resolve these pathologies and show that it outperforms other metrics, especially in settings where predictions beyond the maximum prediction that is chosen as the output class matter.},
	language = {en},
	author = {Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
	year = {2020},
	file = {Nixon et al. - Measuring Calibration in Deep Learning.pdf:/Users/micl/Zotero/storage/DPHRSMFH/Nixon et al. - Measuring Calibration in Deep Learning.pdf:application/pdf},
}

@misc{google_classification_2025,
	title = {Classification: {ROC} and {AUC} {\textbar} {Machine} {Learning}},
	shorttitle = {Classification},
	url = {https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc},
	abstract = {Learn how to interpret an ROC curve and its AUC value to evaluate a binary classification model over all possible classification thresholds.},
	language = {en},
	urldate = {2025-01-22},
	journal = {Google for Developers},
	author = {Google},
	month = jan,
	year = {2025},
	file = {Snapshot:/Users/micl/Zotero/storage/7VMX2RYN/roc-and-auc.html:text/html},
}

@misc{harrell_damage_2017,
	title = {Damage {Caused} by {Classification} {Accuracy} and {Other} {Discontinuous} {Improper} {Accuracy} {Scoring} {Rules}},
	url = {https://www.fharrell.com/post/class-damage/},
	abstract = {Estimating tendencies is usually a more appropriate goal than classification, and classification leads to the use of discontinuous accuracy scores which give rise to misleading results.},
	language = {en},
	urldate = {2025-01-22},
	journal = {Statistical Thinking},
	author = {Harrell, Frank},
	month = mar,
	year = {2017},
}

@misc{molnar_dont_2023,
	type = {Substack newsletter},
	title = {Don't "fix" your imbalanced data},
	url = {https://mindfulmodeler.substack.com/p/dont-fix-your-imbalanced-data},
	abstract = {Why SMOTE doesn't work for most cases.},
	urldate = {2025-01-22},
	journal = {Mindful Modeler},
	author = {Molnar, Christoph},
	month = sep,
	year = {2023},
}
