
@article{beyerlein_alternative_2008,
	title = {Alternative regression models to assess increase in childhood {BMI}},
	volume = {8},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/8/59},
	doi = {10.1186/1471-2288-8-59},
	number = {1},
	urldate = {2012-05-16},
	journal = {BMC Medical Research Methodology},
	author = {Beyerlein, Andreas and Fahrmeir, Ludwig and Mansmann, Ulrich and Toschke, André M},
	year = {2008},
	pages = {59},
	file = {BMC Medical Research Methodology | Full text | Alternative regression models to assess increase in childhood BMI:/Users/micl/Zotero/storage/KADXBB3S/59.html:text/html}
}

@book{wood_generalized_2006,
	title = {Generalized additive models: an introduction with {R}},
	volume = {66},
	shorttitle = {Generalized additive models},
	publisher = {CRC Press},
	author = {Wood, S. N},
	year = {2006},
	file = {[PDF] from bath.ac.uk:/Users/micl/Zotero/storage/N99I9S57/Wood - 2006 - Generalized additive models an introduction with .pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/AVVZHAIN/Wood - 2006 - Generalized additive models an introduction with .html:text/html}
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass.},
	title = {Gaussian processes for machine learning},
	isbn = {0-262-18253-X 978-0-262-18253-9},
	abstract = {"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics."--Jacket.},
	language = {English},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I},
	year = {2006}
}

@book{ruppert_semiparametric_2003,
	title = {Semiparametric {Regression}},
	isbn = {978-0-521-78516-7},
	abstract = {Semiparametric regression is concerned with the flexible incorporation of non-linear functional relationships in regression analyses. Any application area that benefits from regression analysis can also benefit from semiparametric regression. Assuming only a basic familiarity with ordinary parametric regression, this user-friendly book explains the techniques and benefits of semiparametric regression in a concise and modular fashion. The authors make liberal use of graphics and examples plus case studies taken from environmental, financial, and other applications. They include practical advice on implementation and pointers to relevant software. The book is suitable as a textbook for students with little background in regression as well as a reference book for statistically oriented scientists such as biostatisticians, econometricians, quantitative social scientists, epidemiologists, with a good working knowledge of regression and the desire to begin using more flexible semiparametric models. Even experts on semiparametric regression should find something new here.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Ruppert, David and Wand, Matt P. and Carroll, Raymond J.},
	month = jul,
	year = {2003},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / General, Regression analysis, Mathematics / Probability \& Statistics / Regression Analysis, Medical / Epidemiology, Nonparametric statistics}
}

@book{fox_nonparametric_2000,
	title = {Nonparametric {Simple} {Regression}: {Smoothing} {Scatterplots}},
	isbn = {978-0-7619-1585-0},
	shorttitle = {Nonparametric {Simple} {Regression}},
	abstract = {John Fox introduces readers to the techniques of kernel estimation, additive nonparametric regression, and the ways nonparametric regression can be employed to select transformations of the data preceding a linear least-squares fit.},
	language = {en},
	publisher = {SAGE},
	author = {Fox, John},
	month = jan,
	year = {2000},
	keywords = {Mathematics / Probability \& Statistics / General, Social Science / Research, Regression analysis, Nonparametric statistics, Medical / General, Social Science / General, Social Science / Statistics, Social sciences}
}

@book{fox_multiple_2000,
	title = {Multiple and {Generalized} {Nonparametric} {Regression}},
	isbn = {978-0-7619-2189-9},
	abstract = {This book builds on John Fox's previous volume in the QASS Series, Non Parametric Simple Regression. In this book, the reader learns how to estimate and plot smooth functions when there are multiple independent variables.},
	language = {en},
	publisher = {SAGE},
	author = {Fox, John},
	month = may,
	year = {2000},
	keywords = {Mathematics / Probability \& Statistics / General, Social Science / Research, Regression analysis, Mathematics / Probability \& Statistics / Regression Analysis, Nonparametric statistics, Social Science / General, Social Science / Statistics, Social sciences, Social Science / Methodology, Social sciences - Statistical methods, Social sciences/ Statistical methods}
}

@book{wasserman_all_2006,
	title = {All of {Nonparametric} {Statistics}},
	isbn = {978-0-387-25145-5},
	abstract = {The goal of this text is to provide the reader with a single book where they can find a brief account of many, modern topics in nonparametric inference. The book is aimed at Master's level or Ph.D. level students in statistics, computer science, and engineering. It is also suitable for researchers who want to get up to speed quickly on modern nonparametric methods. This text covers a wide range of topics including: the bootstrap, the nonparametric delta method, nonparametric regression, density estimation, orthogonal function methods, minimax estimation, nonparametric confidence sets, and wavelets. The book has a mixture of methods and theory. From the reviews: "...The book is excellent." (Short Book Reviews of the ISI, June 2006) "Now we have All of Nonparametric Statistics a?{\textbar} . the writing is excellent and the author is to be congratulated on the clarity achieved. a?{\textbar} the book is excellent." (N.R. Draper, Short Book Reviews, Vol. 26 (1), 2006) "Overall, I enjoyed reading this book very much. I like Wasserman's intuitive explanations and careful insights into why one path or approach is taken over another. Most of all, I am impressed with the wealth of information on the subject of asymptotic nonparametric inferences." (Stergios B. Fotopoulos for Technometrics, Vol. 49, No. 1., February 2007)},
	language = {en},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2006},
	keywords = {statistics, Mathematics / Probability \& Statistics / General, Mathematics / General, Nonparametric statistics, Artificial intelligence, Computers / Intelligence (AI) \& Semantics, Mathematical statistics}
}

@book{venables_modern_2002,
	title = {Modern {Applied} {Statistics} {With} {S}},
	isbn = {978-0-387-95457-8},
	abstract = {S-PLUS is a powerful environment for the statistical and graphical analysis of data. It provides the tools to implement many statistical ideas which have been made possible by the widespread availability of workstations having good graphics and computational capabilities. This book is a guide to using S-PLUS to perform statistical analyses and provides both an introduction to the use of S-PLUS and a course in modern statistical methods. S-PLUS is available for both Windows and UNIX workstations, and both versions are covered in depth.The aim of the book is to show how to use S-PLUS as a powerful and graphical data analysis system. Readers are assumed to have a basic grounding in statistics, and so the book in intended for would-be users of S-PLUS and both students and researchers using statistics. Throughout, the emphasis is on presenting practical problems and full analyses of real data sets. Many of the methods discussed are state-of-the-art approaches to topics such as linear, nonlinear, and smooth regression models, tree-based methods, multivariate analysis and pattern recognition, survival analysis, time series and spatial statistics. Throughout, modern techniques such as robust methods, non-parametric smoothing, and bootstrapping are used where appropriate.This third edition is intended for users of S-PLUS 4.5, 5.0, 2000 or later, although S-PLUS 3.3/4 are also considered. The major change from the second edition is coverage of the current versions of S-PLUS. The material has been extensively rewritten using new examples and the latest computationally intensive methods. The companion volume on S Programming will provide an in-depth guide for those writing software in the S language.The authors have written several software libraries that enhance S-PLUS; these and all the datasets used are available on the Internet in versions for Windows and UNIX. There are extensive on-line complements covering advanced material, user-contributed extensions, further exercises, and new features of S-PLUS as they are introduced.Dr. Venables is now Statistician with CSRIO in Queensland, having been at the Department of Statistics, University of Adelaide, for many years previously. He has given many short courses on S-PLUS in Australia, Europe, and the USA. Professor Ripley holds the Chair of Applied Statistics at the University of Oxford, and is the author of four other books on spatial statistics, simulation, pattern recognition, and neural networks.},
	language = {en},
	publisher = {Birkhäuser},
	author = {Venables, William N. and Ripley, Brian D.},
	month = aug,
	year = {2002},
	keywords = {statistics, Mathematics / Probability \& Statistics / General, Computers / Mathematical \& Statistical Software, Mathematical statistics, Business \& Economics / Statistics, Mathematical statistics - Data processing, Mathematical statistics/ Data processing, S, S (Computer program language), S (Computer system), S-PLUS (Computer program language), Statistics - Data processing, Statistics/ Data processing}
}

@book{hastie_generalized_1990,
	title = {Generalized {Additive} {Models}},
	isbn = {978-0-412-34390-2},
	language = {en},
	publisher = {CRC Press},
	author = {Hastie, T.J. and Tibshirani, R.J.},
	month = jun,
	year = {1990},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@book{hastie_elements_2009,
	edition = {2nd ed. 2009. Corr. 3rd printing 5th Printing.},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {0-387-84857-6},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	month = feb,
	year = {2009}
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a           rejoinder by the author)},
	volume = {16},
	issn = {0883-4237},
	shorttitle = {Statistical {Modeling}},
	url = {http://projecteuclid.org/euclid.ss/1009213726},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach
             conclusions from data. One assumes that the data are generated by a given
             stochastic data model. The other uses algorithmic models and treats the data
             mechanism as unknown. The statistical community has been committed to the
             almost exclusive use of data models. This commitment has led to irrelevant
             theory, questionable conclusions, and has kept statisticians from working on a
             large range of interesting current problems. Algorithmic modeling, both in
             theory and practice, has developed rapidly in fields outside statistics. It can
             be used both on large complex data sets and as a more accurate and informative
             alternative to data modeling on smaller data sets. If our goal as a field is to
             use data to solve problems, then we need to move away from exclusive dependence
             on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2012-07-22},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	note = {Mathematical Reviews number (MathSciNet): MR1874152},
	pages = {199--231}
}

@article{rigby_generalized_2005,
	title = {Generalized additive models for location, scale and shape},
	volume = {54},
	issn = {1467-9876},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2005.00510.x/abstract},
	doi = {10.1111/j.1467-9876.2005.00510.x},
	abstract = {Summary. A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton–Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
	language = {en},
	number = {3},
	urldate = {2012-07-13},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Rigby, R. A. and Stasinopoulos, D. M.},
	year = {2005},
	keywords = {Beta–binomial distribution, Box–Cox transformation, Centile estimation, Cubic smoothing splines, Generalized linear mixed model, LMS method, Negative binomial distribution, Non-normality, Nonparametric models, Overdispersion, Penalized likelihood, Random effects, Skewness and kurtosis},
	pages = {507--554},
	file = {Full Text PDF:/Users/micl/Zotero/storage/D6PZADQB/Rigby and Stasinopoulos - 2005 - Generalized additive models for location, scale an.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/W752GFV4/full.html:text/html}
}

@book{hardin_generalized_2012,
	edition = {3},
	title = {Generalized {Linear} {Models} and {Extensions}, {Third} {Edition}},
	isbn = {1-59718-105-6},
	publisher = {Stata Press},
	author = {Hardin, James W. and Hilbe, Joseph M.},
	month = jun,
	year = {2012}
}

@article{friedman_projection_1981,
	title = {Projection {Pursuit} {Regression}},
	volume = {76},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2287576},
	doi = {10.2307/2287576},
	abstract = {A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation.},
	number = {376},
	urldate = {2012-06-26},
	journal = {Journal of the American Statistical Association},
	author = {Friedman, Jerome H. and Stuetzle, Werner},
	month = dec,
	year = {1981},
	note = {ArticleType: research-article / Full publication date: Dec., 1981 / Copyright © 1981 American Statistical Association},
	pages = {817--823}
}

@book{bybee_pisa_2009,
	title = {Pisa {Science} 2006: {Implications} for {Science} {Teachers} and {Teaching}},
	isbn = {978-1-933531-31-1},
	shorttitle = {Pisa {Science} 2006},
	language = {en},
	publisher = {NSTA Press},
	author = {Bybee, Rodger W. and McCrae, Barry},
	month = may,
	year = {2009},
	keywords = {Education / Testing \& Measurement, Education / General, Education / Student Life \& Student Affairs, Education / Teaching Methods \& Materials / Science \& Technology, Educational tests and measurements, High school students, High school students - Rating of, High school students/ Rating of, Programme for International Student Assessment, Science, Science - Study and teaching - United States, Science - Study and teaching (Secondary), Science / Study \& Teaching, Science/ Study and teaching (Secondary)}
}

@book{hardin_generalized_2007,
	title = {Generalized linear models and extensions},
	publisher = {Stata Corp},
	author = {Hardin, J. W and Hilbe, J.},
	year = {2007},
	file = {Snapshot:/Users/micl/Zotero/storage/9T3DGIWI/Hardin and Hilbe - 2007 - Generalized linear models and extensions.html:text/html}
}

@article{simpson_modelling_2018,
	title = {Modelling {Palaeoecological} {Time} {Series} {Using} {Generalised} {Additive} {Models}},
	volume = {6},
	issn = {2296-701X},
	url = {https://www.frontiersin.org/articles/10.3389/fevo.2018.00149/full},
	doi = {10.3389/fevo.2018.00149},
	abstract = {In the absence of annual laminations, time series generated from lake sediments or other similar stratigraphic sequences are irregularly spaced in time, which complicates formal analysis using classical statistical time series models. In lieu, statistical analyses of trends in palaeoenvironmental time series, if done at all, have typically used simpler linear regressions or (non-) parametric correlations with little regard for the violation of assumptions that almost surely occurs due to temporal dependencies in the data or that correlations do not provide estimates of the magnitude of change, just whether or not there is a linear or monotonic trend. Alternative approaches have used LOESS-estimated trends to justify data interpretations or test hypotheses as to the causal factors without considering the inherent subjectivity of the choice of parameters used to achieve the LOESS fit (e.g. span width, degree of polynomial). Generalized additive models (GAMs) are statistical models that can be used to estimate trends as smooth functions of time. Unlike LOESS, GAMs use automatic smoothness selection methods to objectively determine the complexity of the fitted trend, and as formal statistical models, GAMs, allow for potentially complex, non-linear trends, a proper accounting of model uncertainty, and the identification of periods of significant temporal change. Here, I present a consistent and modern approach to the estimation of trends in palaeoenvironmental time series using GAMs, illustrating features of the methodology with two example time series of contrasting complexity; a 150-year bulk organic matter δ15N time series from Small Water, UK, and a 3000-year alkenone record from Braya-Sø, Greenland. I discuss the underlying mechanics of GAMs that allow them to learn the shape of the trend from the data themselves and how simultaneous confidence intervals and the first derivatives of the trend are used to properly account for model uncertainty and identify periods of change. It is hoped that by using GAMs greater attention is paid to the statistical estimation of trends in palaeoenvironmental time series leading to more a robust and reproducible palaeoscience.},
	language = {English},
	urldate = {2019-02-10},
	journal = {Frontiers in Ecology and Evolution},
	author = {Simpson, Gavin L.},
	year = {2018},
	keywords = {environmental change, generalized additive models, simultaneous interval, Spline, time series},
	file = {Full Text PDF:/Users/micl/Zotero/storage/8RLFG7ZV/Simpson - 2018 - Modelling Palaeoecological Time Series Using Gener.pdf:application/pdf}
}

@article{friedman_additive_2000,
	title = {Additive logistic regression: a statistical view of boosting ({With} discussion and a rejoinder by the authors)},
	volume = {28},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Additive logistic regression},
	url = {https://projecteuclid.org/euclid.aos/1016218223},
	doi = {10.1214/aos/1016218223},
	abstract = {Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.},
	language = {EN},
	number = {2},
	urldate = {2019-02-10},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {2000},
	mrnumber = {MR1790002},
	zmnumber = {1106.62323},
	keywords = {classification, machine learning, nonparametric estimation, stagewise fitting, tree},
	pages = {337--407},
	file = {Full Text PDF:/Users/micl/Zotero/storage/DRP4S38J/Friedman et al. - 2000 - Additive logistic regression a statistical view o.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/IU9EH95A/1016218223.html:text/html}
}

@article{wood_mgcv:_2012,
	title = {mgcv: {Mixed} {GAM} {Computation} {Vehicle} with {GCV}/{AIC}/{REML} smoothness estimation},
	shorttitle = {mgcv},
	url = {https://researchportal.bath.ac.uk/en/publications/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est},
	language = {English},
	urldate = {2019-09-29},
	author = {Wood, Simon},
	month = oct,
	year = {2012},
	file = {Snapshot:/Users/micl/Zotero/storage/V7NYNZR8/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est.html:text/html}
}

@book{wood_generalized_2017,
	title = {Generalized Additive Models : An Introduction with R, Second Edition},
	isbn = {978-1-315-37027-9},
	shorttitle = {Generalized {Additive} {Models}},
	url = {https://www.taylorfrancis.com/books/9781315370279},
	abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to},
	language = {en},
	urldate = {2019-09-29},
	publisher = {Chapman and Hall/CRC},
	author = {Wood, Simon N.},
	month = may,
	year = {2017},
	doi = {10.1201/9781315370279},
	file = {Full Text PDF:/Users/micl/Zotero/storage/CUH9EAKX/Wood - 2017 - Generalized Additive Models  An Introduction with.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/VQ2HCD9B/9781315370279.html:text/html}
}

@article{li_faster_2019,
	title = {Faster model matrix crossproducts for large generalized linear models with discretized covariates},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-019-09864-2},
	doi = {10.1007/s11222-019-09864-2},
	abstract = {Wood et al. (J Am Stat Assoc 112(519):1199–1210, 2017) developed methods for fitting penalized regression spline based generalized additive models, with of the order of 10410410{\textasciicircum}4 coefficients, to up to 10810810{\textasciicircum}8 data. The methods offered two to three orders of magnitude reduction in computational cost relative to the most efficient previous methods. Part of the gain resulted from the development of a set of methods for efficiently computing model matrix products when model covariates each take only a discrete set of values substantially smaller than the sample size [generalizing an idea first appearing in Lang et al. (Stat Comput 24(2):223–238, 2014)]. Covariates can always be rounded to achieve such discretization, and it should be noted that the covariate discretization is marginal. That is we do not rely on discretizing covariates jointly, which would typically require the use of very coarse discretization. The most expensive computation in model estimation is the formation of the matrix cross product 𝐗𝖳𝐖𝐗XTWX{\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\} where 𝐗X{\textbackslash}mathbf\{X\} is a model matrix and 𝐖W\{{\textbackslash}mathbf\{W\}\} a diagonal or tri-diagonal matrix. The purpose of this paper is to present a simple, novel and substantially more efficient approach to the computation of this cross product. The new method offers, for example, a 30 fold reduction in cross product computation time for the Black Smoke model dataset motivating Wood et al. (2017). Given this reduction in computational cost, the subsequent Cholesky decomposition of 𝐗𝖳𝐖𝐗XTWX{\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\} and follow on computation of (𝐗𝖳𝐖𝐗)−1(XTWX)−1({\textbackslash}mathbf\{X\}{\textasciicircum}\{{\textbackslash}mathsf\{T\}\}\{{\textbackslash}mathbf\{WX\}\}){\textasciicircum}\{-1\} become a more significant part of the computational burden, and we also discuss the choice of methods for improving their speed.},
	language = {en},
	urldate = {2019-09-29},
	journal = {Statistics and Computing},
	author = {Li, Zheyuan and Wood, Simon N.},
	month = mar,
	year = {2019},
	keywords = {BLAS, Fast regression, Generalized additive model},
	file = {Springer Full Text PDF:/Users/micl/Zotero/storage/7KC4ZXMP/Li and Wood - 2019 - Faster model matrix crossproducts for large genera.pdf:application/pdf}
}

@article{fasiolo_scalable_2019,
	title = {Scalable Visualization Methods for Modern Generalized Additive Models},
	volume = {0},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2019.1629942},
	doi = {10.1080/10618600.2019.1629942},
	abstract = {In the last two decades, the growth of computational resources has made it possible to handle generalized additive models (GAMs) that formerly were too costly for serious applications. However, the growth in model complexity has not been matched by improved visualizations for model development and results presentation. Motivated by an industrial application in electricity load forecasting, we identify the areas where the lack of modern visualization tools for GAMs is particularly severe, and we address the shortcomings of existing methods by proposing a set of visual tools that (a) are fast enough for interactive use, (b) exploit the additive structure of GAMs, (c) scale to large data sets, and (d) can be used in conjunction with a wide range of response distributions. The new visual methods proposed here are implemented by the mgcViz R package, available on the Comprehensive R Archive Network. Supplementary materials for this article are available online.},
	number = {0},
	urldate = {2019-09-29},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Fasiolo, Matteo and Nedellec, Raphaël and Goude, Yannig and Wood, Simon N.},
	month = jun,
	year = {2019},
	keywords = {Electricity load forecasting, Generalized additive models, Interactive model building, Regression modeling, Residuals checking, Visualization},
	pages = {1--9},
	file = {Snapshot:/Users/micl/Zotero/storage/V6LHAVE7/weblogin.umich.edu.html:text/html;Submitted Version:/Users/micl/Zotero/storage/4K2JDUWT/Fasiolo et al. - 2019 - Scalable Visualization Methods for Modern Generali.pdf:application/pdf}
}

@article{wood_generalized_2015,
	title = {Generalized additive models for large data sets},
	volume = {64},
	copyright = {© 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12068},
	doi = {10.1111/rssc.12068},
	abstract = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
	language = {en},
	number = {1},
	urldate = {2019-09-29},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
	year = {2015},
	keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
	pages = {139--155},
	file = {Snapshot:/Users/micl/Zotero/storage/8LYQID9I/rssc.html:text/html}
}

@article{wood_generalized_2017-1,
	title = {Generalized Additive Models for Gigadata: Modeling the U.K. Black Smoke Network Daily Data},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Generalized {Additive} {Models} for {Gigadata}},
	url = {https://amstat.tandfonline.com/doi/full/10.1080/01621459.2016.1195744},
	doi = {10.1080/01621459.2016.1195744},
	abstract = {We develop scalable methods for fitting penalized regression spline based generalized additive models with of the order of 104 coefficients to up to 108 data. Computational feasibility rests on: (i) a new iteration scheme for estimation of model coefficients and smoothing parameters, avoiding poorly scaling matrix operations; (ii) parallelization of the iteration’s pivoted block Cholesky and basic matrix operations; (iii) the marginal discretization of model covariates to reduce memory footprint, with efficient scalable methods for computing required crossproducts directly from the discrete representation. Marginal discretization enables much finer discretization than joint discretization would permit. We were motivated by the need to model four decades worth of daily particulate data from the U.K. Black Smoke and Sulphur Dioxide Monitoring Network. Although reduced in size recently, over 2000 stations have at some time been part of the network, resulting in some 10 million measurements. Modeling at a daily scale is desirable for accurate trend estimation and mapping, and to provide daily exposure estimates for epidemiological cohort studies. Because of the dataset size, previous work has focused on modeling time or space averaged pollution levels, but this is unsatisfactory from a health perspective, since it is often acute exposure locally and on the time scale of days that is of most importance in driving adverse health outcomes. If computed by conventional means our black smoke model would require a half terabyte of storage just for the model matrix, whereas we are able to compute with it on a desktop workstation. The best previously available reduced memory footprint method would have required three orders of magnitude more computing time than our new method. Supplementary materials for this article are available online.},
	number = {519},
	urldate = {2019-09-29},
	journal = {Journal of the American Statistical Association},
	author = {Wood, Simon N. and Li, Zheyuan and Shaddick, Gavin and Augustin, Nicole H.},
	month = jul,
	year = {2017},
	pages = {1199--1210},
	file = {Full Text:/Users/micl/Zotero/storage/2N4Z5LJ3/Wood et al. - 2017 - Generalized Additive Models for Gigadata Modeling.pdf:application/pdf;Snapshot:/Users/micl/Zotero/storage/73X423H7/01621459.2016.html:text/html}
}

@article{wood_generalized_2015-1,
	title = {Generalized additive models for large data sets},
	volume = {64},
	copyright = {© 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12068},
	doi = {10.1111/rssc.12068},
	abstract = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
	language = {en},
	number = {1},
	urldate = {2019-09-29},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
	year = {2015},
	keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
	pages = {139--155},
	file = {Snapshot:/Users/micl/Zotero/storage/66YDF5ZT/rssc.html:text/html}
}

@article{brooks_glmmtmb_2017,
	title = {glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling},
	volume = {9},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2017/RJ-2017-066},
	language = {en},
	number = {2},
	urldate = {2019-10-13},
	journal = {The R Journal},
	author = {Brooks, Mollie E. and Kristensen, Kasper and Benthem, Koen J. van and Magnusson, Arni and Berg, Casper W. and Nielsen, Anders and Skaug, Hans J. and Mächler, Martin and Bolker, Benjamin M.},
	year = {2017},
	pages = {378--400},
	file = {Snapshot:/Users/micl/Zotero/storage/4ZBA8UHP/RJ-2017-066.html:text/html}
}

@article{mccoach2018,
  title={Does the package matter? A comparison of five common multilevel modeling software packages},
  author={McCoach, D Betsy and Rifenbark, Graham G and Newton, Sarah D and Li, Xiaoran and Kooken, Janice and Yomtov, Dani and Gambino, Anthony J and Bellara, Aarti},
  journal={Journal of Educational and Behavioral Statistics},
  url={https://journals.sagepub.com/doi/10.3102/1076998618776348},
  volume={43},
  number={5},
  pages={594--627},
  year={2018},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
