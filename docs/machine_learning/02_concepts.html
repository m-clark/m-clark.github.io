<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="An Introduction to Machine Learning" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://m-clark.github.io/docs/" />
<meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
<meta property="og:description" content="An introduction to machine learning for applied researchers." />
<meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />


<meta name="date" content="2017-05-01" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to machine learning for applied researchers.">

<title>An Introduction to Machine Learning</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<script src="libs/d3-3.5.3/./d3.min.js"></script>
<link href="libs/d3heatmapcore-0.0.0/heatmapcore.css" rel="stylesheet" />
<script src="libs/d3heatmapcore-0.0.0/heatmapcore.js"></script>
<script src="libs/d3-tip-0.6.6/index.js"></script>
<script src="libs/d3heatmap-binding-0.6.1.1/d3heatmap.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-0.9.0/grViz.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="standard_html.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#section"></a></li>
<li><a href="00_preface.html#preface">Preface</a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#explanation-prediction">Explanation &amp; Prediction</a></li>
<li><a href="01_intro.html#some-terminology">Some Terminology</a></li>
<li><a href="01_intro.html#supervised-vs.unsupervised">Supervised vs. Unsupervised</a></li>
<li class="has-sub"><a href="01_intro.html#tools-you-already-have">Tools you already have</a><ul>
<li><a href="01_intro.html#the-standard-linear-model">The Standard Linear Model</a></li>
<li><a href="01_intro.html#logistic-regression">Logistic Regression</a></li>
<li><a href="01_intro.html#expansions-of-those-tools">Expansions of Those Tools</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#concepts">Concepts</a><ul>
<li class="has-sub"><a href="02_concepts.html#loss-functions">Loss Functions</a><ul>
<li><a href="02_concepts.html#continuous-outcomes">Continuous Outcomes</a></li>
<li><a href="02_concepts.html#categorical-outcomes">Categorical Outcomes</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#regularization">Regularization</a><ul>
<li><a href="02_concepts.html#r-example-1">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul>
<li><a href="02_concepts.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li><a href="02_concepts.html#the-tradeoff">The Tradeoff</a></li>
<li><a href="02_concepts.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a></li>
<li><a href="02_concepts.html#bias-variance-summary">Bias-Variance Summary</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#cross-validation">Cross-Validation</a><ul>
<li><a href="02_concepts.html#adding-another-validation-set">Adding Another Validation Set</a></li>
<li><a href="02_concepts.html#k-fold-cross-validation">K-fold Cross-Validation</a></li>
<li><a href="02_concepts.html#bootstrap">Bootstrap</a></li>
<li><a href="02_concepts.html#other-stuff">Other Stuff</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#opening-the-black-box">Opening the Black Box</a><ul>
<li class="has-sub"><a href="03_blackbox.html#process-overview">Process Overview</a><ul>
<li><a href="03_blackbox.html#data-preparation">Data Preparation</a></li>
<li><a href="03_blackbox.html#model-selection">Model Selection</a></li>
<li><a href="03_blackbox.html#model-assessment">Model Assessment</a></li>
</ul></li>
<li><a href="03_blackbox.html#the-dataset">The Dataset</a></li>
<li><a href="03_blackbox.html#r-implementation">R Implementation</a></li>
<li><a href="03_blackbox.html#feature-selection-the-data-partition">Feature Selection <em>&amp;</em> The Data Partition</a></li>
<li class="has-sub"><a href="03_blackbox.html#k-nearest-neighbors"><span class="math inline">\(k\)</span>-nearest Neighbors</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#neural-networks">Neural Networks</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-1">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-1">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#trees-forests">Trees <em>&amp;</em> Forests</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-2">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-2">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#support-vector-machines">Support Vector Machines</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-3">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-3">Final Thoughts</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_other.html#wrap-up">Wrap-up</a><ul>
<li class="has-sub"><a href="04_other.html#unsupervised-learning">Unsupervised Learning</a><ul>
<li><a href="04_other.html#clustering">Clustering</a></li>
<li><a href="04_other.html#latent-variable-models">Latent Variable Models</a></li>
<li><a href="04_other.html#graphical-structure">Graphical Structure</a></li>
<li><a href="04_other.html#imputation">Imputation</a></li>
</ul></li>
<li class="has-sub"><a href="04_other.html#ensembles">Ensembles</a><ul>
<li><a href="04_other.html#bagging">Bagging</a></li>
<li><a href="04_other.html#boosting">Boosting</a></li>
<li><a href="04_other.html#stacking">Stacking</a></li>
</ul></li>
<li><a href="04_other.html#feature-selection-importance">Feature Selection <em>&amp;</em> Importance</a></li>
<li><a href="04_other.html#natural-language-processingtext-analysis">Natural language processing/Text Analysis</a></li>
<li><a href="04_other.html#bayesian-approaches">Bayesian Approaches</a></li>
<li><a href="04_other.html#more-stuff">More Stuff</a></li>
<li class="has-sub"><a href="04_other.html#summary">Summary</a><ul>
<li><a href="04_other.html#cautionary-notes">Cautionary Notes</a></li>
<li><a href="04_other.html#some-guidelines">Some Guidelines</a></li>
<li><a href="04_other.html#conclusion">Conclusion</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="1000_appendix.html#appendix">Appendix</a><ul>
<li><a href="1000_appendix.html#bias-variance-demo">Bias Variance Demo</a></li>
<li class="has-sub"><a href="1000_appendix.html#programming-languages">Programming Languages</a><ul>
<li><a href="1000_appendix.html#r">R</a></li>
<li><a href="1000_appendix.html#python">Python</a></li>
<li><a href="1000_appendix.html#other">Other</a></li>
</ul></li>
<li><a href="1000_appendix.html#brief-glossary-of-common-terms">Brief Glossary of Common Terms</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="concepts" class="section level1">
<h1>Concepts</h1>
<p><span class="newthought">Given a set of predictor variables</span> <span class="math inline">\(X\)</span> and some target <span class="math inline">\(y\)</span>, we look for some function <span class="math inline">\(f(X)\)</span> to make predictions of y from those input variables. We also need a function to penalize errors in prediction, i.e. a <span class="emph">loss function</span>. With a chosen loss function, we then find the model which will minimize loss, generally speaking. We will start with the familiar and note a couple others that might be used.</p>
<div id="loss-functions" class="section level2">
<h2>Loss Functions</h2>
<div id="continuous-outcomes" class="section level3">
<h3>Continuous Outcomes</h3>
<div id="squared-error" class="section level4">
<h4>Squared Error</h4>
<p>The classic loss function for linear models with a continuous numeric response is the squared error loss function, or the residual sum of squares.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum(y-f(X))^2\]</span></p>
<p>Everyone who has taken a statistics course is familiar with this ‘least-squares’ approach on some level. Often they are not taught that it is one of many possible approaches. However, the average, or <span class="emph">mean squared error</span> is commonly used as a metric of performance (or it’s square root).</p>
</div>
<div id="absolute-error" class="section level4">
<h4>Absolute Error</h4>
<p>For an approach that is more robust to extreme observations, we might choose absolute rather than squared error. In this case, predictions are a conditional median rather than a conditional mean.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum|(y-f(X))|\]</span></p>
</div>
<div id="negative-log-likelihood" class="section level4">
<h4>Negative Log-likelihood</h4>
<p>We can also think of our usual likelihood methods learned in a standard applied statistics course<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> Well some of you. Many applied methods courses don’t teach the basic maximum likelihood approach, even thought it’s the most widely used of all techniques.</span> as incorporating a loss function that is the negative log-likelihood pertaining to the model of interest. If we assume a normal distribution for the response we can note the loss function as:</p>
<p><span class="math display">\[L(Y, f(X)) = n\ln{\sigma} + \sum \frac{1}{2\sigma^2}(y-f(X))^2\]</span></p>
<p>In this case it would converge to the same answer as the squared error/least squares solution.</p>
</div>
<div id="r-example" class="section level4">
<h4>R Example</h4>
<p>The following provides code that one could use with the <span class="func">optim</span> function in R to find estimates of regression coefficients (beta) based on minimizing the squared error. <code>X</code> is a design matrix of our predictor variables with the first column a vector of 1s in order to estimate the intercept. <code>y</code> is the continuous variable to be modeled. We can then compare the results with the <span class="func">lm</span> function from base R<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> Type <code>?optim</code> at the console for more detail.</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sqerrloss =<span class="st"> </span>function(beta, X, y){
  mu =<span class="st"> </span>X%*%beta
  <span class="kw">sum</span>((y-mu)^<span class="dv">2</span>)
}

<span class="co"># data setup</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
N =<span class="st"> </span><span class="dv">100</span>
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">rnorm</span>(N), <span class="kw">rnorm</span>(N))
beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, -.<span class="dv">5</span>, .<span class="dv">5</span>)
y =<span class="st">  </span><span class="kw">rnorm</span>(N, X%*%beta, <span class="dt">sd=</span><span class="dv">1</span>)

<span class="co"># results</span>
our_func =<span class="st"> </span><span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">fn=</span>sqerrloss, <span class="dt">X=</span>X, <span class="dt">y=</span>y, <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>)
lm_result =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>., <span class="kw">data.frame</span>(X[,-<span class="dv">1</span>]))  <span class="co"># check with lm </span>
<span class="kw">rbind</span>(<span class="kw">c</span>(our_func$par, our_func$value), <span class="kw">c</span>(<span class="kw">coef</span>(lm_result), <span class="kw">sum</span>(<span class="kw">resid</span>(lm_result)^<span class="dv">2</span>)))</code></pre></div>
<pre><code>     (Intercept)         X1        X2         
[1,]   0.1350654 -0.6331715 0.5238113 87.78187
[2,]   0.1350654 -0.6331715 0.5238113 87.78187</code></pre>
<p>While <span class="func">lm</span> uses a different approach, they are both going to result in the ‘least-squares’ estimates.</p>
</div>
</div>
<div id="categorical-outcomes" class="section level3">
<h3>Categorical Outcomes</h3>
<p>Here we’ll also look at some loss functions useful in classification problems. Note that there is not necessary exclusion in loss functions for continuous vs. categorical outcomes<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> For example, if dealing with probabilities, we technically could use minimize squared errors in the case of classification also. We could use a maximum likelihood for either setting (or minimize the negative log likelihood to turn it into a loss function).</span>. Generally though we’ll have different options.</p>
<div id="misclassification" class="section level4">
<h4>Misclassification</h4>
<p>Probably the most straightforward is misclassification, or 0-1 loss. If we note <span class="math inline">\(f\)</span> as the prediction, and for convenience we assume a [-1,1] response instead of a [0,1] response:</p>
<p><span class="math display">\[L(Y, f(X)) = \sum I(y\neq \mathrm{sign}(f))\]</span></p>
<p>In the above, <span class="math inline">\(I\)</span> is the indicator function, and so we are simply summing misclassifications.</p>
</div>
<div id="binomial-log-likelihood" class="section level4">
<h4>Binomial log-likelihood</h4>
<p><span class="math display">\[L(Y, f(X)) = \sum log(1 + e^{-2yf})\]</span></p>
<p>The above is in deviance form<span class="marginnote">Deviance can conceptually be thought of as the GLM version of residual variance.</span>, but is equivalent to binomial log likelihood if <span class="math inline">\(y\)</span> is on the 0-1 scale.</p>
</div>
<div id="exponential" class="section level4">
<h4>Exponential</h4>
<p>Exponential loss is yet another loss function at our disposal.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum e^{-yf}\]</span></p>
</div>
<div id="hinge-loss" class="section level4">
<h4>Hinge Loss</h4>
<p>A final loss function to consider, typically used with support vector machines, is the hinge loss function.</p>
<p><span class="math display">\[L(Y, f(X)) = \max(1-yf, 0)\]</span></p>
<p>Here negative values of <span class="math inline">\(yf\)</span> are misclassifications, and so correct classifications do not contribute to the loss. We could also note it as <span class="math inline">\(\sum (1-yf)_+\)</span> , i.e. summing only those positive values of <span class="math inline">\(1-yf\)</span>.</p>
<p><span class="marginnote"><img src="img/lossfuncs.png" style="display:block; margin: 0 auto;"></span></p>
<p>Which of these might work best may be specific to the situation, but the gist is that they penalize negative values (misclassifications) more heavily and increasingly so the worse the misclassification (except for misclassification error, which penalizes all misclassifications equally), with their primary difference in how heavy that penalty is. At right is a depiction of the loss as a functions above, taken from <span class="citation">Hastie, Tibshirani, and Friedman (<label for="tufte-mn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle">2009<span class="marginnote">Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</em>. 2nd ed. 2009. Corr. 10th Printing. Springer.</span>)</span>.</p>
</div>
</div>
</div>
<div id="regularization" class="section level2">
<h2>Regularization</h2>
<p><span class="newthought">It is important to note</span> that a model fit to a single data set might do very well with the data at hand, but then suffer when predicting independent data<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> In terminology we will discuss further later, such models might have low bias but notable variance.</span>. Also, oftentimes we are interested in a ‘best’ subset of predictors among a great many, and in this scenario the estimated coefficients are overly optimistic. This general issue can be improved by shrinking estimates toward zero, such that some of the performance in the initial fit is sacrificed for improvement with regard to prediction.</p>
<p>Penalized estimation will provide estimates with some shrinkage, and we can use it with little additional effort with our common procedures. Concretely, let’s apply this to the standard linear model, where we are finding estimates of <span class="math inline">\(\beta\)</span> that minimize the squared error loss.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2}\]</span></p>
<p>In words, we’re finding the coefficients that minimize the sum of the squared residuals. With the approach to regression here we just add a penalty component to the procedure as follows.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2} + \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>In the above equation, <span class="math inline">\(\lambda\)</span> is our penalty term<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> This can be set explicitly or also estimated via a validation approach. As we do not know it beforehand, we can estimate it on a validation data set (not the test set) and then use the estimated value when estimating coefficients via cross-validation with the test set. We will talk more about validation later.</span> for which larger values will result in more shrinkage. It’s applied to the <span class="math inline">\(L_1\)</span> or Manhattan norm of the coefficients, <span class="math inline">\(\beta_1,\beta_2...\beta_p\)</span>, i.e. <em>not including the intercept</em> <span class="math inline">\(\beta_0\)</span>, and is the sum of their absolute values (commonly referred to as the <span class="emph">lasso</span><label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> See Tibshirani (1996) Regression shrinkage and selection via the lasso.</span>). For generalized linear and additive models, we can conceptually express a penalized likelihood as follows:</p>
<p><span class="math display">\[l_p(\beta) = l(\beta) - \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>As we are maximizing the likelihood the penalty is a subtraction, but nothing inherently different is shown. This basic idea of adding a penalty term will be applied to all machine learning approaches, but as shown, we can apply such a tool to classical methods to boost prediction performance.</p>
<p><span class="marginnote">Interestingly, the lasso and ridge regression results can be seen as a Bayesian approach using a zero mean Laplace and Normal prior distribution respectively for the <span class="math inline">\(\beta_j\)</span>.</span>It should be noted that we can go about the regularization in different ways. For example, using the squared <span class="math inline">\(L_2\)</span> norm results in what is called <span class="emph"></span> (a.k.a. Tikhonov regularization), and using a weighted combination of the lasso and ridge penalties gives us <span class="emph">elastic net</span> regularization.</p>
<div id="r-example-1" class="section level3">
<h3>R Example</h3>
<p>In the following example, we take a look at the lasso approach for a standard linear model. We add the regularization component, with a fixed penalty <span class="math inline">\(\lambda\)</span> for demonstration purposes<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> As noted previously, in practice <span class="math inline">\(\lambda\)</span> would be estimated via some validation procedure.</span>. However you should insert your own values for <span class="math inline">\(\lambda\)</span> in the <span class="func">optim</span> line to see how the results are affected. I’ve also increased the number of predictors to 10.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># data setup</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
N =<span class="st"> </span><span class="dv">100</span>
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">matrix</span>(<span class="kw">rnorm</span>(N*<span class="dv">10</span>), <span class="dt">ncol=</span><span class="dv">10</span>))
beta =<span class="st"> </span><span class="kw">runif</span>(<span class="kw">ncol</span>(X))
y =<span class="st">  </span><span class="kw">rnorm</span>(N, X%*%beta, <span class="dt">sd=</span><span class="dv">2</span>)

sqerrloss_reg =<span class="st"> </span>function(beta, X, y, <span class="dt">lambda=</span><span class="dv">1</span>){
  mu =<span class="st"> </span>X%*%beta
  <span class="kw">sum</span>((y-mu)^<span class="dv">2</span>) +<span class="st"> </span>lambda*<span class="kw">sum</span>(<span class="kw">abs</span>(beta[-<span class="dv">1</span>]))
}

lm_result =<span class="st"> </span><span class="kw">lm</span>(y~., <span class="kw">data.frame</span>(X[,-<span class="dv">1</span>]) )
regularized_result =<span class="st"> </span><span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), <span class="dt">fn=</span>sqerrloss_reg, <span class="dt">X=</span>X, <span class="dt">y=</span>y, <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>)</code></pre></div>
<p><br></p>
<p><div id="htmlwidget-ac291b881a17a3904e44" style="width:35%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ac291b881a17a3904e44">{"x":{"filter":"none","data":[["(Intercept)","X1","X2","X3","X4","X5","X6","X7","X8","X9","X10","Squared Error Loss"],[0.145,-0.27,0.586,0.385,0.844,0.511,0.423,0.304,-0.076,0.547,0.625,402.205],[0.144,-0.268,0.581,0.379,0.838,0.506,0.418,0.298,-0.071,0.54,0.621,406.75]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Standard LM<\/th>\n      <th>Regularized Model<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"tp","pageLength":6,"ordering":false,"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[6,10,25,50,100],"rowCallback":"function(row, data) {\nvar value=data[0]; if (value!==null) $(this.api().cell(row, 0).node()).css({'background-color':'#fffff8'});\nvar value=data[1]; if (value!==null) $(this.api().cell(row, 1).node()).css({'background-color':'#fffff8'});\nvar value=data[2]; if (value!==null) $(this.api().cell(row, 2).node()).css({'background-color':'#fffff8'});\nvar value=data[3]; if (value!==null) $(this.api().cell(row, 3).node()).css({'background-color':'#fffff8'});\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script> <br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create test data</span>
N_test =<span class="st"> </span><span class="dv">50</span>
X_test =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">rnorm</span>(N_test), <span class="kw">rnorm</span>(N_test))
X_test =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">matrix</span>(<span class="kw">rnorm</span>(N_test*<span class="dv">10</span>), <span class="dt">ncol=</span><span class="dv">10</span>))
y_test =<span class="st"> </span><span class="kw">rnorm</span>(N_test, X_test%*%beta, <span class="dt">sd=</span><span class="dv">2</span>)

<span class="co"># squared error loss</span>
<span class="kw">crossprod</span>(y_test -<span class="st"> </span><span class="kw">predict</span>(lm_result, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(X_test[,-<span class="dv">1</span>])))  </code></pre></div>
<pre><code>         [,1]
[1,] 218.2474</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">crossprod</span>(y_test -<span class="st"> </span>X_test%*%regularized_result$par)</code></pre></div>
<pre><code>         [,1]
[1,] 216.9351</code></pre>
<p>From the above, we can see in this case that the penalized coefficients have indeed shrunk toward zero slightly, while the residual sum of squares has increased just a tad. On the test data however, the squared error loss is lower.</p>
<p>In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural net models, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data. Note that the estimates produced are in fact biased, but we have decreased the variance with new predictions as a counterbalance, and this brings us to the topic of the next section.</p>
</div>
</div>
<div id="bias-variance-tradeoff" class="section level2">
<h2>Bias-Variance Tradeoff</h2>
<p>In most of science, we are concerned with reducing uncertainty in our knowledge of some phenomenon. The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information. The initial step is to take the data at hand, and determine how well a model or set of models fit the data in various fashions. In many applications however, this part is also more or less the end of the game as well<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> I should note that I do not make any particular claim about the quality of such analysis. In many situations the cost of data collection is very high, and for all the current enamorment with ‘big’ data, a lot of folks will never have access to big data for their situation (e.g. certain clinical populations). In these situations getting new data for which one might make predictions is extremely difficult.</span>.</p>
<p>Unfortunately, such an approach in which we only fit models to one data set does not give a very good sense of <span class="emph">generalization</span> performance, i.e. the performance we would see with new data. While typically not reported, most researchers, if they are spending appropriate time with the data, are actually testing a great many models, for which the ‘best’ is then provided in detail in the end report. Without some generalization performance check however, such performance is overstated when it comes to new data.</p>
<p>In the following consider a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set in which we split the data in some random fashion into a <span class="emph">training set</span> , for initial model fit, and a<span class="emph">test set</span>, which is a separate and independent data set, to measure generalization performance<label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> In typical situations there are parameters specific to some analytical technique for which one would have no knowledge, and which must be estimated along with the usual parameters of the standard models. The <span class="math inline">\(\lambda\)</span> penalty parameter in regularized regression is one example of such a <span class="emph">tuning parameter</span>. In the best case scenario, we would also have a <span class="emph">validation set</span>, where we could determine appropriate values for such parameters based on performance with the validation data set, and then assess generalization performance on the test set when the final model has been chosen. However, methods are available to us in which we can approximate the validation step in other ways.</span>. We note <span class="emph">training error</span> as the (average) loss over the training set, and <span class="emph">test error</span> as the (average) prediction error obtained when a model resulting from the training data is fit to the test data. So in addition to the previously noted goal of finding the ‘best’ model (<span class="emph">model selection</span>), we are interested further in estimating the prediction error with new data (<span class="emph">model performance</span>).</p>
<div id="bias-variance" class="section level3">
<h3>Bias <em>&amp;</em> Variance</h3>
<p><span class="marginnote">Much of the following is essentially a paraphrase of parts of <span class="citation">Hastie, Tibshirani, and Friedman (<label for="tufte-mn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle">2009<span class="marginnote">Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</em>. 2nd ed. 2009. Corr. 10th Printing. Springer.</span>)</span> (chapters 2 and 7).</span>We start with a true data generating process for some target <span class="math inline">\(y\)</span>, expressed as a function of features <span class="math inline">\(X\)</span>. We can specify the true model as</p>
<p><span class="math display">\[y = f(X) + \epsilon\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is the expected value of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span>, i.e. <span class="math inline">\(f(x) = E(y|X)\)</span>, and the expected value of the error 0, <span class="math inline">\(E(\epsilon)=0\)</span>, with some variance, <span class="math inline">\(\textrm{Var}(\epsilon) = \sigma^2_\epsilon\)</span>. In other words, we are talking about the standard regression model we all know and love. Now we can conceptually think of the <em>expected prediction error</em> at a specific input <span class="math inline">\(X = x_0\)</span> as:</p>
<p><span class="math display">\[\text{Error}_{x_0} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}\]</span></p>
<p>To better understand this, think of training models over and over, each time with new training data, but testing each model at input <span class="math inline">\(x_0\)</span>. The <span class="math inline">\(\text{Error}_{x_0}\)</span> is the average, or expected value of the prediction error in this scenario, or <span class="math inline">\(E[(y - \hat f(x))^2|X=x_0]\)</span>, with <span class="math inline">\(\hat f\)</span> our current estimate of the true underlying data generating function <span class="math inline">\(f\)</span>. In other words, we have three components to our general notion of prediction error:</p>
<p><strong>Irreducible error</strong>: The variance of the (new test) target (<span class="math inline">\(\sigma^2_\epsilon\)</span>). This is unavoidable, since our <span class="math inline">\(y\)</span> is measured with error.</p>
<p><strong><span class="math inline">\(\textrm{Bias}^2\)</span></strong>: the amount the <em>average</em> of our estimate varies from the true (but unknown) value (<span class="math inline">\(E(\hat f) - f\)</span>). This is typically the result of trying to model the complexity of nature with something much simpler that the human brain can understand. While the simpler might make us feel good, it may not work very well.</p>
<p><strong>Variance</strong>: the amount by which our prediction would change if we had estimated it using a different training data set (<span class="math inline">\(Var(\hat f)\)</span>). Even with unbiased estimates, we could still see a high mean squared error due to high variance.</p>
<p>Slightly more formally, we can present this as follows, with <span class="math inline">\(h_0\)</span> our estimated (hypothesized) value at <span class="math inline">\(x_0\)</span>:</p>
<p><span class="math display">\[\text{Error}_{x_0} = Var(\epsilon) + (\text{E}[h_0] - f(x_0))^2 + Var(h_0)\]</span></p>
<p>The latter two components make up the mean squared error in our previous demonstration. While they are under our control, they compete with one another such that oftentimes we improve one at the detriment of the other. In other words, <em>bias and variance are not independent</em>.</p>
</div>
<div id="the-tradeoff" class="section level3">
<h3>The Tradeoff</h3>
<p>Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables, adding polynomial terms, including interactions), and then assess the performance of the chosen models in terms of prediction error on the test set. We then perform the same activity for a total of 100 simulated data sets, for each level of complexity.</p>
<p><span class="marginnote"><img src="img/biasvar2.svg" style="display:block; margin: 0 auto;" width=100%></span>The results from this process might look like the image to the right taken from <span class="citation">Hastie, Tibshirani, and Friedman (<label for="tufte-mn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">2009<span class="marginnote">Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</em>. 2nd ed. 2009. Corr. 10th Printing. Springer.</span>)</span>. With regard to the training data, we have <span class="math inline">\(\mathrm{error}_{\mathrm{train}}\)</span> for one hundred training sets for each level of model complexity. The bold blue line notes this average error over the 100 sets by model complexity, and we can see that more complex models fit the data better. The bold red line the average test error (<span class="math inline">\(\mathrm{error}_{\mathrm{test}}\)</span>) across the 100 test data sets, and it tells a different story.</p>
<p>Ideally we’d like to see low bias and (relatively) low variance, but things are not so easy. One thing we can see clearly is that <span class="math inline">\(\mathrm{error}_{\mathrm{train}}\)</span> is not a good estimate of <span class="math inline">\(\mathrm{error}_{\mathrm{test}}\)</span>, which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error. As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets. We can think of this as a problem of overfitting to the training data. Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented.</p>
<p>Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic. Specifically however, the situation is more nuanced, where the type of problem (classification with 0-1 loss vs. continuous response with squared error loss<label for="tufte-sn-18" class="margin-toggle sidenote-number">18</label><input type="checkbox" id="tufte-sn-18" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">18</span> See Friedman (1996) <em>On Bias, Variance, 0/1 Loss and the Curse of Dimensionality</em> for the unusual situations that can arise in dealing with classification error with regard to bias and variance.</span>) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships.</p>
<p><span class="marginnote"><img src="img/biasvartarget.svg" style="display:block; margin: 0 auto;"> <br> Figure adapted from <span class="citation">Domingos (<label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">2012<span class="marginnote">Domingos, Pedro. 2012. “A Few Useful Things to Know About Machine Learning.” <em>Commun. ACM</em> 55 (10). doi:<a href="https://doi.org/10.1145/2347736.2347755">10.1145/2347736.2347755</a>.</span>)</span>.</span></p>
</div>
<div id="diagnosing-bias-variance-issues-possible-solutions" class="section level3">
<h3>Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</h3>
<p>Let’s assume a regularized linear model with a standard data split into training and test sets. We will describe different scenarios with possible solutions.</p>
<div id="worst-case-scenario" class="section level4">
<h4>Worst Case Scenario</h4>
<p>Starting with the worst case scenario, poor models may exhibit high bias and high variance. One thing that will not help this situation (perhaps contrary to intuition) is adding more data. You can’t make a silk purse out of a sow’s ear (<a href="https://libraries.mit.edu/archives/exhibits/purse/"><em>usually</em></a>), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data.<span class="marginnote"><img class='imgbigger' src="img/biasvar_gp.svg" style="display:block; margin: 0 auto;"> <br> Figure inspired by <span class="citation">Murphy (<label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">2012<span class="marginnote">Murphy, Kevin P. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. The MIT Press.</span>)</span> (figure 6.5) showing the bias-variance tradeoff. Sample (left) and average (right) fits of linear regression using a Gaussian radial basis function expansion. The green line represents the true relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off. Compare to the less regularized (high variance, low bias) situation of the bottom row. See the <span class="pack">kernlab</span> package for the fitting function used, and the <a href="1000_appendix.html#appendix">appendix</a> for the code used to produce the graph.</span></p>
</div>
<div id="high-variance" class="section level4">
<h4>High Variance</h4>
<p>When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue. In this case more data may help as well.</p>
</div>
<div id="high-bias" class="section level4">
<h4>High Bias</h4>
<p>With bias issues, our training error is high and test error is not too different from training error (underfitting problem). Adding new predictors/features, e.g. interaction terms, polynomials etc., can help here. Additionally reducing the penalty parameter <span class="math inline">\(\lambda\)</span> would also work with even less effort, though generally it should be estimated rather than explicitly set.</p>
</div>
</div>
<div id="bias-variance-summary" class="section level3">
<h3>Bias-Variance Summary</h3>
<p>One of key ideas any applied researcher can take from machine learning concerns the bias-variance tradeoff and issues of overfitting in particular. Typical applied practice involves potentially dozens of models fit to the same data set without any validation whatsoever, yet only one or two are actually presented in publication. Many disciplines report nothing but the statistical significance, and yet one can have statistically significant predictors and have predictive capability that is no different from guessing. Furthermore very complex models are often fit to small data sets, compounding the problem.</p>
<p>It is very easy to describe <strong><em>science</em></strong> without ever talking about statistical significance. It is impossible to talk about science without talking about prediction. The bias-variance tradeoff is one way to bring the concerns of prediction to the forefront, and any applied researcher can benefit from thinking about its implications.</p>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross-Validation</h2>
<p>As noted in the previous section, in machine learning approaches we are particularly concerned with prediction error on new data. The simplest validation approach would be to split the data available into a training and test set as discussed previously. We estimate the model on the training data, and apply the model to the test data, get the predictions and measure our test error, selecting whichever model results in the least test error. <span class="marginnote"><img src="img/learningcurve.svg" style="display:block; margin: 0 auto;"></span> A hypothetical learning curve display the results of such a process is shown to the right. While fairly simple, other approaches are more commonly used and result in better estimates of performance<label for="tufte-sn-19" class="margin-toggle sidenote-number">19</label><input type="checkbox" id="tufte-sn-19" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">19</span> Along with some of the other works cited, see <span class="citation">Harrell (<label for="tufte-mn-8" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">2015<span class="marginnote">Harrell, F. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. Springer Series in Statistics. Springer International Publishing.</span>)</span> for a good discussion of model validation.</span>.</p>
<div id="adding-another-validation-set" class="section level3">
<h3>Adding Another Validation Set</h3>
<p>One technique that might be utilized for larger data sets, is to split the data into training, validation and final test sets. For example, one might take the original data and create something like a 60-20-20% split to create the needed data sets. The purpose of the initial validation set is to select the optimal model and determine the values of tuning parameters. These are parameters which generally deal with how complex a model one will allow, but for which one would have little inkling as to what they should be set at before hand (e.g. our <span class="math inline">\(\lambda\)</span> shrinkage parameter in regularized regression). We select models/tuning parameters that minimize the validation set error, and once the model is chosen examine test set error performance. In this way performance assessment is still independent of the model development process.</p>
<p><span class="marginnote"><img src="img/kfold.svg" style="display:block; margin: 0 auto;" width=200%> <br> An illustration of 3-fold classification.</span></p>
</div>
<div id="k-fold-cross-validation" class="section level3">
<h3>K-fold Cross-Validation</h3>
<p>In many cases we don’t have enough data for such a split, and the split percentages are arbitrary anyway, with results that would be specific to the split chosen. Instead we can take a typical data set and randomly split it into <span class="math inline">\(\kappa=10\)</span> equal-sized (or close to it) parts. Next, we take the first nine partitions, combine them, and use them as the training set. With chosen model from the training data, make predictions on the held out partition. Now we do it again, but this time use the 9<sup>th</sup> partition as the holdout set. Repeat the process until each of the initial 10 partitions of data have been used as the test set. Average the error across all procedures for our estimate of prediction error. With enough data, this (and the following methods) could be used as the validation procedure before eventual performance assessment on an independent test set with the final chosen model.</p>
<div id="leave-one-out-cross-validation" class="section level4">
<h4>Leave-one-out Cross-Validation</h4>
<p>Leave-one-out (LOO) cross-validation is the same thing but where <span class="math inline">\(\kappa=N\)</span>. In other words, we train a model for all observations except the <span class="math inline">\(\kappa^{th}\)</span> one, assessing fit on the observation that was left out. We then cycle through until all observations have been left out once to obtain an average accuracy.</p>
<p>Of the two, K-fold may have relatively higher bias but less variance, while LOO would have the converse problem, as well as possible computational issues<label for="tufte-sn-20" class="margin-toggle sidenote-number">20</label><input type="checkbox" id="tufte-sn-20" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">20</span> For squared-error loss situations, there is a Generalized cross-validation (GCV) that can be estimated more directly without actually going to the entire LOO procedure, and functions similarly to AIC.</span>. K-fold’s additional bias would be diminished would with increasing sample sizes, and generally 5 or 10-fold cross-validation is recommended. However, many model selection techniques (e.g. via AIC) have a leave-one-out interpretation.</p>
</div>
</div>
<div id="bootstrap" class="section level3">
<h3>Bootstrap</h3>
<p>With a bootstrap approach, we draw <span class="math inline">\(B\)</span> random samples with replacement from our original data set, creating <span class="math inline">\(B\)</span> bootstrapped data sets of the same size as the original data. We use the <span class="math inline">\(B\)</span> data sets as training sets and, using the original data as the test set, average the prediction error across the models.</p>
</div>
<div id="other-stuff" class="section level3">
<h3>Other Stuff</h3>
<p>Along with the above there are variations such as repeated cross validation, the ‘.632’ bootstrap and so forth. One would want to do a bit of investigating, but <span class="math inline">\(\kappa\)</span>-fold and bootstrap approaches generally perform well. If variable selection is part of the goal, one should be selecting subsets of predictors as part of the cross-validation process, not at some initial data step.</p>

</div>
</div>
</div>
<p style="text-align: center;">
<a href="01_intro.html"><button class="btn btn-default">Previous</button></a>
<a href="03_blackbox.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
