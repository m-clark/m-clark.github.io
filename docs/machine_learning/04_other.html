<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="An Introduction to Machine Learning" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://m-clark.github.io/docs/" />
<meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
<meta property="og:description" content="An introduction to machine learning for applied researchers." />
<meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />


<meta name="date" content="2017-05-01" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to machine learning for applied researchers.">

<title>An Introduction to Machine Learning</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<script src="libs/d3-3.5.3/./d3.min.js"></script>
<link href="libs/d3heatmapcore-0.0.0/heatmapcore.css" rel="stylesheet" />
<script src="libs/d3heatmapcore-0.0.0/heatmapcore.js"></script>
<script src="libs/d3-tip-0.6.6/index.js"></script>
<script src="libs/d3heatmap-binding-0.6.1.1/d3heatmap.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-0.9.0/grViz.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="standard_html.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#section"></a></li>
<li><a href="00_preface.html#preface">Preface</a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#explanation-prediction">Explanation &amp; Prediction</a></li>
<li><a href="01_intro.html#some-terminology">Some Terminology</a></li>
<li><a href="01_intro.html#supervised-vs.unsupervised">Supervised vs. Unsupervised</a></li>
<li class="has-sub"><a href="01_intro.html#tools-you-already-have">Tools you already have</a><ul>
<li><a href="01_intro.html#the-standard-linear-model">The Standard Linear Model</a></li>
<li><a href="01_intro.html#logistic-regression">Logistic Regression</a></li>
<li><a href="01_intro.html#expansions-of-those-tools">Expansions of Those Tools</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#concepts">Concepts</a><ul>
<li class="has-sub"><a href="02_concepts.html#loss-functions">Loss Functions</a><ul>
<li><a href="02_concepts.html#continuous-outcomes">Continuous Outcomes</a></li>
<li><a href="02_concepts.html#categorical-outcomes">Categorical Outcomes</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#regularization">Regularization</a><ul>
<li><a href="02_concepts.html#r-example-1">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul>
<li><a href="02_concepts.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li><a href="02_concepts.html#the-tradeoff">The Tradeoff</a></li>
<li><a href="02_concepts.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a></li>
<li><a href="02_concepts.html#bias-variance-summary">Bias-Variance Summary</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#cross-validation">Cross-Validation</a><ul>
<li><a href="02_concepts.html#adding-another-validation-set">Adding Another Validation Set</a></li>
<li><a href="02_concepts.html#k-fold-cross-validation">K-fold Cross-Validation</a></li>
<li><a href="02_concepts.html#bootstrap">Bootstrap</a></li>
<li><a href="02_concepts.html#other-stuff">Other Stuff</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#opening-the-black-box">Opening the Black Box</a><ul>
<li class="has-sub"><a href="03_blackbox.html#process-overview">Process Overview</a><ul>
<li><a href="03_blackbox.html#data-preparation">Data Preparation</a></li>
<li><a href="03_blackbox.html#model-selection">Model Selection</a></li>
<li><a href="03_blackbox.html#model-assessment">Model Assessment</a></li>
</ul></li>
<li><a href="03_blackbox.html#the-dataset">The Dataset</a></li>
<li><a href="03_blackbox.html#r-implementation">R Implementation</a></li>
<li><a href="03_blackbox.html#feature-selection-the-data-partition">Feature Selection <em>&amp;</em> The Data Partition</a></li>
<li class="has-sub"><a href="03_blackbox.html#k-nearest-neighbors"><span class="math inline">\(k\)</span>-nearest Neighbors</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#neural-networks">Neural Networks</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-1">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-1">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#trees-forests">Trees <em>&amp;</em> Forests</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-2">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-2">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#support-vector-machines">Support Vector Machines</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-3">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-3">Final Thoughts</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_other.html#wrap-up">Wrap-up</a><ul>
<li class="has-sub"><a href="04_other.html#unsupervised-learning">Unsupervised Learning</a><ul>
<li><a href="04_other.html#clustering">Clustering</a></li>
<li><a href="04_other.html#latent-variable-models">Latent Variable Models</a></li>
<li><a href="04_other.html#graphical-structure">Graphical Structure</a></li>
<li><a href="04_other.html#imputation">Imputation</a></li>
</ul></li>
<li class="has-sub"><a href="04_other.html#ensembles">Ensembles</a><ul>
<li><a href="04_other.html#bagging">Bagging</a></li>
<li><a href="04_other.html#boosting">Boosting</a></li>
<li><a href="04_other.html#stacking">Stacking</a></li>
</ul></li>
<li><a href="04_other.html#feature-selection-importance">Feature Selection <em>&amp;</em> Importance</a></li>
<li><a href="04_other.html#natural-language-processingtext-analysis">Natural language processing/Text Analysis</a></li>
<li><a href="04_other.html#bayesian-approaches">Bayesian Approaches</a></li>
<li><a href="04_other.html#more-stuff">More Stuff</a></li>
<li class="has-sub"><a href="04_other.html#summary">Summary</a><ul>
<li><a href="04_other.html#cautionary-notes">Cautionary Notes</a></li>
<li><a href="04_other.html#some-guidelines">Some Guidelines</a></li>
<li><a href="04_other.html#conclusion">Conclusion</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="1000_appendix.html#appendix">Appendix</a><ul>
<li><a href="1000_appendix.html#bias-variance-demo">Bias Variance Demo</a></li>
<li class="has-sub"><a href="1000_appendix.html#programming-languages">Programming Languages</a><ul>
<li><a href="1000_appendix.html#r">R</a></li>
<li><a href="1000_appendix.html#python">Python</a></li>
<li><a href="1000_appendix.html#other">Other</a></li>
</ul></li>
<li><a href="1000_appendix.html#brief-glossary-of-common-terms">Brief Glossary of Common Terms</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="wrap-up" class="section level1">
<h1>Wrap-up</h1>
<p><span class="newthought">In this section, I note some other techniques</span> one may come across, and others that will provide additional insight into machine learning applications.</p>
<div id="unsupervised-learning" class="section level2">
<h2>Unsupervised Learning</h2>
<p><span class="emph">Unsupervised learning</span> generally speaking involves techniques in which we are utilizing unlabeled data. In this case we have our typical set of features we are interested in, but no particular response to map them to. In this situation we are more interested in the discovery of structure within the data.</p>
<div id="clustering" class="section level3">
<h3>Clustering</h3>
<p>Many of the techniques used in unsupervised are commonly taught in various applied disciplines as various forms of “cluster” analysis. The gist is that we are seeking an unknown class structure rather than seeing how various inputs relate to a known class structure. Common techniques include k-means, hierarchical clustering, and model based approaches (e.g. mixture models).</p>
</div>
<div id="latent-variable-models" class="section level3">
<h3>Latent Variable Models</h3>
<div style="width:25%; margin: 0 12.5%; bgcolor=&#39;transparent&#39;">
<div id="htmlwidget-dc7620ae0529e527423e" style="width:100%;height:100%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-dc7620ae0529e527423e">{"x":{"diagram":"\ndigraph DAG2 {\n # Intialization of graph attributes\n graph [overlap = false rankdir=TB bgcolor=transparent]\n \n # Initialization of node attributes\n node [shape = circle,\n fontname = Helvetica,\n color = gray80,\n type = box,\n fixedsize = true]\n \n # Node statements\n node [width=.75, height=.75, shape=circle, color=gray80];\n LV1 [style=filled, fillcolor=lightblue, color=gray75,  fontcolor=gray50, label=<LV<sub>1<\/sub>>];\n LV2 [style=filled, fillcolor=lightblue, color=gray75,  fontcolor=gray50, label=<LV<sub>2<\/sub>>];\n \n node [width=1, shape=square, color=gray50, fontcolor=gray25]\n subgraph {\n    rank=same;\n    X1 [width=.5 height=.5 label=<X<sub>1<\/sub>>]; \n    X2 [width=.5 height=.5 label=<X<sub>2<\/sub>>]; \n    X3 [width=.5 height=.5 label=<X<sub>3<\/sub>>]; \n    X4 [width=.5 height=.5 label=<X<sub>4<\/sub>>]; \n    X5 [width=.5 height=.5 label=<X<sub>5<\/sub>>]; \n    X6 [width=.5 height=.5 label=<X<sub>6<\/sub>>]; \n }\n # Initialization of edge attributes\n edge [color = gray50, rel = yields minlen=2]\n \n # Edge statements\n LV1 -> {X1 X2 X3 X4};\n LV2 -> {X3 X4} [style=dashed] ;\n LV2 -> {X5 X6};\n}\n ","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<!-- <span class="marginnote"><img src="img/lvmodel.png" style="display:block; margin: 0 auto;"></span> -->
<p>Sometimes the desire is to reduce the dimensionality of the inputs to a more manageable set of information. In this manner we are thinking that much of the data can be seen as having only a few sources of variability, often called latent variables or factors. Again, this takes familiar forms such as principal components and (“exploratory”) factor analysis, but would also include independence components analysis and partial least squares techniques. Note also that these can be part of a supervised technique (e.g. principal components regression) or the main focus of analysis (as with latent variable models in structural equation modeling).</p>
</div>
<div id="graphical-structure" class="section level3">
<h3>Graphical Structure</h3>
<!-- <span class="marginnote"><img src="img/senate.png" style="display:block; margin: 0 auto;" width=50%></span> -->
<p><span class="marginnote"> <img src="machine_learning_files/figure-html/unnamed-chunk-2-1.svg" width="768"  style="display: block; margin: auto;" /> An example network graph of U.S. senators in 2006. Node size is based on the <em>betweeness</em> centrality measure, edge size the percent agreement (graph filtered to edges &gt;= 65%). Color is based on the clustering discovered within the graph <a href="http://support.google.com/fusiontables/answer/2566732?hl=en&amp;ref_topic=2572801">(link to data)</a>. </span></p>
<p>Other techniques are available to understand structure among observations or features. Among the many approaches is the popular <span class="emph">network analysis</span>, where we can obtain similarities among observations and examine visually the structure of those data points, where observations are placed closer together that are more similar in nature. In still other situations, we aren’t so interested in the structure as we are in modeling the relationships and making predictions from the correlations of inputs.</p>
</div>
<div id="imputation" class="section level3">
<h3>Imputation</h3>
<p>We can also use ML techniques when we are missing data, as a means to impute the missing values. While many are familiar with this problem and standard techniques for dealing with it, it may not be obvious that ML techniques may also be used. For example, both k-nearest neighbors and random forest techniques have been applied to imputation.</p>
<p><span class="marginnote">Imputation and related techniques may fall under the broad heading of <span class="emph">matrix completion</span>. </span> Beyond this we can infer values that are otherwise unavailable in a different sense. Consider Netflix, Amazon and other sites that suggest various products based on what you already like or are interested in. In this case the suggested products have missing values for the user which are imputed or inferred based on their available data and other consumers similar to them who have rated the product in question. Such <span class="emph">recommender systems</span> are widely used these days.</p>
</div>
</div>
<div id="ensembles" class="section level2">
<h2>Ensembles</h2>
<p>In many situations we can combine the information of multiple models to enhance prediction. This can take place within a specific technique, e.g. random forests, or between models that utilize different techniques. I will discuss some standard techniques, but there are a great variety of forms in which model combination might take place.</p>
<div id="bagging" class="section level3">
<h3>Bagging</h3>
<p><span class="emph">Bagging</span>, or <em>bootstrap aggregation</em>, uses bootstrap sampling to create many data sets on which a procedure is then performed. The final prediction is based on an average of all the predictions made for each observation. In general, bagging helps reduce the variance while leaving bias unaffected. A conceptual outline of the procedure is provided.</p>
<p><em>Model Generation</em></p>
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(N\)</span> observations with replacement <span class="math inline">\(B\)</span> times to create <span class="math inline">\(B\)</span> data sets of size <span class="math inline">\(N\)</span>.</li>
<li>Apply the learning technique to each of <span class="math inline">\(B\)</span> data sets to create <span class="math inline">\(t\)</span> models.</li>
<li>Store the <span class="math inline">\(t\)</span> results.</li>
</ol>
<p><em>Classification</em></p>
<p>For each of <span class="math inline">\(t\)</span> number of models:</p>
<ol style="list-style-type: decimal">
<li>Predict the class of <span class="math inline">\(N\)</span> observations of the original data set.</li>
<li>Return the class predicted most often across the <span class="math inline">\(t\)</span> number of models (or alternatively, the proportion <span class="math inline">\(t =1\)</span> as a probability).</li>
</ol>
<p>The approach would be identical for the continuous target domain, where the final prediction would be the average across all models.</p>
</div>
<div id="boosting" class="section level3">
<h3>Boosting</h3>
<p>With <span class="emph">boosting</span> we take a different approach to refitting models. Consider a classification task in which we start with a basic learner and apply it to the data of interest. Next, the learner is refit, but with more weight (importance) given to <em>misclassified</em> observations. This process is repeated until some stopping rule is reached (e.g. reaching some <span class="math inline">\(M\)</span> iterations). An example of the AdaBoost algorithm is provided (in the following <span class="math inline">\(\mathbb{I}\)</span> is the indicator function).</p>
<p>Set initial weights <span class="math inline">\(w_i\)</span> to <span class="math inline">\(1/N\)</span>.</p>
<p>for <span class="math inline">\(m=1:M\)</span> {</p>
<ul>
<li><p>Fit a classifier <span class="math inline">\(m\)</span> with given weights to the data resulting in predictions <span class="math inline">\(f^{(m)}_i\)</span> that minimizes some loss function.</p></li>
<li><p>Compute the error rate <span class="math inline">\(\text{err}_m = \frac{{\sum_{i=1}^N}\mathbb{I}(y_i\ne f^{(m)}_i)}{\sum^N_{i=1}w^{(m)}_i}\)</span></p></li>
<li><p>Compute <span class="math inline">\(\alpha_m = \log[(1-err_m)/err_m]\)</span></p></li>
<li><p>Set <span class="math inline">\(w_i \leftarrow w_i\exp[\alpha_m \mathbb{I}(y_i\ne f^{(m)}_i)]\)</span></p></li>
</ul>
<p>}</p>
<p>Return <span class="math inline">\(\textrm{sign} [\sum^M_{m=1}\alpha_m f^{(m)}]\)</span></p>
<p>Boosting can be applied to a variety of tasks and loss functions, and in general is highly resistant to overfitting.</p>
</div>
<div id="stacking" class="section level3">
<h3>Stacking</h3>
<p><span class="emph">Stacking</span> is a method that can generalize beyond a single fitting technique, though it can be applied in a fashion similar to boosting for a single technique. Here we will use it broadly to mean any method to combine models of different forms. Consider the four approaches we demonstrated earlier: k-nearest neighbors, neural net, random forest, and the support vector machine. We saw that they do not have the same predictive accuracy, though they weren’t bad in general. Perhaps by combining their respective efforts, we could get even better prediction than using any particular one.</p>
<p>The issue then is how we might combine them. We really don’t have to get too fancy with it, and can even use a simple voting scheme as in bagging. For each observation, note the predicted class on new data across models. The final prediction is the class that receives the most votes. Another approach would be to use a weighted vote, where the votes are weighted by the accuracy of their respective models.</p>
<p>Another approach would use the predictions on the test set to create a data set of just the predicted probabilities from each learning scheme. We can then use this data to train a meta-learner using the test labels as the response. With the final meta-learner chosen, we then retrain the original models on the entire data set (i.e. including the test data). In this manner the initial models and the meta-learner are trained separately and you get to eventually use the entire data set to train the original models. Now when new data becomes available, you feed them to the base level learners, get the predictions, and then feed the predictions to the meta-learner for the final prediction.</p>
</div>
</div>
<div id="feature-selection-importance" class="section level2">
<h2>Feature Selection <em>&amp;</em> Importance</h2>
<p>We hit on this topic some before, but much like there are a variety of ways to gauge performance, there are different approaches to select features and/or determine their importance. Invariably feature selection takes place from the outset when we choose what data to collect in the first place. Hopefully guided by theory, in other cases it may be restricted by user input, privacy issues, time constraints and so forth. But once we obtain the initial data set however, we may still want to trim the models under consideration.</p>
<p>In standard approaches we might have in the past used forward or other selection procedure, or perhaps some more explicit model comparison approach. Concerning the content here, take for instance the lasso regularization procedure we spoke of earlier. ‘Less important’ variables may be shrunk entirely to zero, and thus feature selection is an inherent part of the process, and is useful in the face of many, many predictors, sometimes outnumbering our sample points. As another example, consider any particular approach where the importance metric might be something like the drop in accuracy when the variable is excluded.</p>
<p>Variable importance was given almost full weight in the discussion of typical applied research in the past, based on statistical significance results from a one-shot analysis, and virtually ignorant of prediction on new data. We still have the ability to focus on feature performance with ML techniques, while shifting more of the focus toward prediction at the same time. For the uninitiated, it might require new ways of thinking about how one measures importance though.</p>
</div>
<div id="natural-language-processingtext-analysis" class="section level2">
<h2>Natural language processing/Text Analysis</h2>
<p>In some situations the data of interest is not in a typical matrix form but in the form of textual content, e.g. a corpus of documents (loosely defined). In this case, much of the work (like in most analyses but perhaps even more so) will be in the data preparation, as text is rarely if ever in a ready-to-analyze state. The eventual goals may include using the discovery of latent topics, parts-of-speech tagging, sentiment analysis, language identification, word usage in the prediction of an outcome, or examining the structure of the term usage graphically as in a network model. In addition, machine learning processes might be applied to sounds (acoustic data) to discern the speech characteristics and other information.</p>
</div>
<div id="bayesian-approaches" class="section level2">
<h2>Bayesian Approaches</h2>
<p>It should be noted that the approaches outlined in this document are couched in the frequentist tradition. But one should be aware that many of the concepts and techniques would carry over into the Bayesian perspective, and even some machine learning techniques might only be feasible or make more sense within the Bayesian framework (e.g. online learning).</p>
</div>
<div id="more-stuff" class="section level2">
<h2>More Stuff</h2>
<p>Aside from what has already been noted, there still exists a great many applications for ML such as data set shift<label for="tufte-sn-43" class="margin-toggle sidenote-number">43</label><input type="checkbox" id="tufte-sn-43" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">43</span> Used when fundamental changes occur between the data a learner is trained on and the data coming in for further analysis.</span>, deep learning<label for="tufte-sn-44" class="margin-toggle sidenote-number">44</label><input type="checkbox" id="tufte-sn-44" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">44</span> This includes learning at different levels of representation, e.g. from an image regarding a scene to the concepts used to describe it.</span>, semi-supervised learning<label for="tufte-sn-45" class="margin-toggle sidenote-number">45</label><input type="checkbox" id="tufte-sn-45" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">45</span> Learning with both labeled and unlabeled data.</span>, online learning<label for="tufte-sn-46" class="margin-toggle sidenote-number">46</label><input type="checkbox" id="tufte-sn-46" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">46</span> Learning from a continuous stream of data.</span>, and many more.</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<div id="cautionary-notes" class="section level3">
<h3>Cautionary Notes</h3>
<p>A standard mantra in machine learning and statistics generally is that there is <a href="http://www.no-free-lunch.org/">no free lunch</a>. All methods have certain assumptions, and if those don’t hold the results will be problematic at best. Also, even if in truth learner A is better than B, B can often outperform A in the finite situations we actually deal with in practice.</p>
<p>In general, without context, no algorithm can be said to be any better than another on average. Furthermore, being more complicated doesn’t mean a technique is better. As previously noted, simply incorporating regularization and cross-validation goes a long way toward to improving standard techniques, and may perform quite well in many situations. The basic conclusion is that</p>
<blockquote>
<p><em>Machine learning <strong>is not</strong> magic!</em></p>
</blockquote>
<p>ML does not prove your theories, it does not make your data better, and the days of impressing someone simply because you’re using it have long since past. Like any statistical technique, the reason to use ML is that is well-suited to the research or other problem at hand.</p>
</div>
<div id="some-guidelines" class="section level3">
<h3>Some Guidelines</h3>
<p>Here are some thoughts to keep in mind, though these may be applicable to applied statistical practice generally.</p>
<p>More data beats a cleverer algorithm, but a lot of data is not enough by itself <span class="citation">Domingos (<label for="tufte-mn-12" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle">2012<span class="marginnote">Domingos, Pedro. 2012. “A Few Useful Things to Know About Machine Learning.” <em>Commun. ACM</em> 55 (10). doi:<a href="https://doi.org/10.1145/2347736.2347755">10.1145/2347736.2347755</a>.</span>)</span>.</p>
<p>Avoid overfitting.</p>
<p>Let the data speak for itself.</p>
<p>“Nothing is more practical than a good theory.”<label for="tufte-sn-47" class="margin-toggle sidenote-number">47</label><input type="checkbox" id="tufte-sn-47" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">47</span> Kurt Lewin, and iterated by V. Vapnik for the machine learning context.</span></p>
<p>While getting used to ML, it might be best to start from simpler approaches and then work towards more black box ones that require more tuning. For example, naive Bayes <span class="math inline">\(\rightarrow\)</span> logistic regression <span class="math inline">\(\rightarrow\)</span> knn <span class="math inline">\(\rightarrow\)</span> svm.</p>
<p>Drawing up a visual path of your process is a good way to keep your analysis on the path to your goal. Some programs can even make this explicit.</p>
<p>Keep the tuning parameter/feature selection process separate from the final test process for assessing error.</p>
<p>Learn multiple models, selecting the best or possibly combining them.</p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>It is hoped that this document sheds some light on some areas that might otherwise be unfamiliar to some applied researchers. The field of statistics has rapidly evolved over the past couple decades. The tools available are myriad, and expanding all the time. Rather than feeling intimidated or overwhelmed, one should embrace the choice available, and have some fun with your data!</p>

</div>
</div>
</div>
<p style="text-align: center;">
<a href="03_blackbox.html"><button class="btn btn-default">Previous</button></a>
<a href="1000_appendix.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
