<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="An Introduction to Machine Learning" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://m-clark.github.io/docs/" />
<meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
<meta property="og:description" content="An introduction to machine learning for applied researchers." />
<meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />


<meta name="date" content="2017-04-21" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to machine learning for applied researchers.">

<title>An Introduction to Machine Learning</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="standard_html.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#section"></a></li>
<li><a href="00_preface.html#preface">Preface</a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#explanation-prediction">Explanation &amp; Prediction</a></li>
<li><a href="01_intro.html#some-terminology">Some Terminology</a></li>
</ul></li>
<li class="has-sub"><a href="02_tools.html#tools">Tools</a><ul>
<li><a href="02_tools.html#the-standard-linear-model">The Standard Linear Model</a></li>
<li><a href="02_tools.html#logistic-regression">Logistic Regression</a></li>
<li class="has-sub"><a href="02_tools.html#expansions-of-those-tools">Expansions of Those Tools</a><ul>
<li><a href="02_tools.html#generalized-linear-models">Generalized Linear Models</a></li>
<li><a href="02_tools.html#generalized-additive-models">Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#the-loss-function">The Loss Function</a><ul>
<li class="has-sub"><a href="03_loss.html#continuous-outcomes">Continuous Outcomes</a><ul>
<li><a href="03_loss.html#squared-error">Squared Error</a></li>
<li><a href="03_loss.html#absolute-error">Absolute Error</a></li>
<li><a href="03_loss.html#negative-log-likelihood">Negative Log-likelihood</a></li>
<li><a href="03_loss.html#r-example">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="03_loss.html#categorical-outcomes">Categorical Outcomes</a><ul>
<li><a href="03_loss.html#misclassification">Misclassification</a></li>
<li><a href="03_loss.html#binomial-log-likelihood">Binomial log-likelihood</a></li>
<li><a href="03_loss.html#exponential">Exponential</a></li>
<li><a href="03_loss.html#hinge-loss">Hinge Loss</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_regularization.html#regularization">Regularization</a><ul>
<li><a href="04_regularization.html#r-example-1">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="05_biasvar.html#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul>
<li><a href="05_biasvar.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li><a href="05_biasvar.html#the-tradeoff">The Tradeoff</a></li>
<li class="has-sub"><a href="05_biasvar.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a><ul>
<li><a href="05_biasvar.html#worst-case-scenario">Worst Case Scenario</a></li>
<li><a href="05_biasvar.html#high-variance">High Variance</a></li>
<li><a href="05_biasvar.html#high-bias">High Bias</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="06_crossval.html#cross-validation">Cross-Validation</a><ul>
<li><a href="06_crossval.html#adding-another-validation-set">Adding Another Validation Set</a></li>
<li class="has-sub"><a href="06_crossval.html#k-fold-cross-validation">K-fold Cross-Validation</a><ul>
<li><a href="06_crossval.html#leave-one-out-cross-validation">Leave-one-out Cross-Validation</a></li>
</ul></li>
<li><a href="06_crossval.html#bootstrap">Bootstrap</a></li>
<li><a href="06_crossval.html#other-stuff">Other Stuff</a></li>
</ul></li>
<li class="has-sub"><a href="07_models.html#model-assessment-selection">Model Assessment &amp; Selection</a><ul>
<li><a href="07_models.html#beyond-classification-accuracy-other-measures-of-performance">Beyond Classification Accuracy: Other Measures of Performance</a></li>
</ul></li>
<li class="has-sub"><a href="08_overview.html#process-overview">Process Overview</a><ul>
<li class="has-sub"><a href="08_overview.html#data-preparation">Data Preparation</a><ul>
<li><a href="08_overview.html#define-data-and-data-partitions">Define Data and Data Partitions</a></li>
<li><a href="08_overview.html#feature-scaling">Feature Scaling</a></li>
<li><a href="08_overview.html#feature-engineering">Feature Engineering</a></li>
<li><a href="08_overview.html#discretization">Discretization</a></li>
</ul></li>
<li><a href="08_overview.html#model-selection">Model Selection</a></li>
<li><a href="08_overview.html#model-assessment">Model Assessment</a></li>
</ul></li>
<li class="has-sub"><a href="1000_appendix.html#appendix">Appendix</a><ul>
<li><a href="1000_appendix.html#bias-variance-demo">Bias Variance Demo</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div id="explanation-prediction" class="section level2">
<h2>Explanation &amp; Prediction</h2>
<p><span class="newthought">For any particular analysis conducted</span>, emphasis can be placed on understanding the underlying mechanisms which have specific theoretical underpinnings, versus a focus that dwells more on performance and, more to the point, future performance. These are not mutually exclusive goals in the least, and probably most studies contain a little of both in some form or fashion. I will refer to the former emphasis as that of <span class="emph">explanation</span>, and the latter that of <span class="emph">prediction</span>.</p>
<p>In studies with a more explanatory focus, traditionally analysis concerns a single data set. For example, one assumes a data generating distribution for the response, and one evaluates the overall fit of a single model to the data at hand, e.g. in terms of R-squared, and statistical significance for the various predictors in the model. One assesses how well the model lines up with the theory that led to the analysis, and modifies it accordingly, if need be, for future studies to consider. Some studies may look at predictions for specific, possibly hypothetical values of the predictors, or examine the particular nature of individual predictors effects. In many cases, only a single model is considered. In general though, little attempt is made to explicitly understand how well the model will do with future data, but we hope to have gained greater insight as to the underlying mechanisms guiding the response of interest. Following <span class="citation">Breiman (<label for="tufte-mn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle">2001<span class="marginnote">Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statistical Science</em> 16 (3): 199–231. doi:<a href="https://doi.org/10.1214/ss/1009213726">10.1214/ss/1009213726</a>.</span>)</span>, this would be more akin to the <span class="emph">data modeling culture</span>.</p>
<p>For the other type of study focused on prediction, newer techniques are available that are far more focused on performance, not only for the current data under examination but for future data the selected model might be applied to. While still possible, relative predictor importance is less of an issue, and oftentimes there may be no particular theory to drive the analysis. There may be thousands of input variables, such that no simple summary would likely be possible anyway. However, many of the techniques applied in such analyses are quite powerful, and steps are taken to ensure better results for new data. Again referencing <span class="citation">Breiman (<label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">2001<span class="marginnote">Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statistical Science</em> 16 (3): 199–231. doi:<a href="https://doi.org/10.1214/ss/1009213726">10.1214/ss/1009213726</a>.</span>)</span>, this perspective is more of the <span class="emph">algorithmic modeling culture</span>.</p>
<p>While the two approaches are not exclusive, I present two extreme (though thankfully dated) views of the situation<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> A favorite around our office is ‘Machine learning is magic!’ attributed to our director Kerby Shedden, in response to a client’s over enthusiasm to something they did not understand well.</span>:</p>
<blockquote>
<p>To paraphrase provocatively, ‘machine learning is statistics minus any checking of models and assumptions’. <br> <span class="math inline">\(\sim\)</span> Brian Ripley, 2004</p>
</blockquote>
<blockquote>
<p>… the focus in the statistical community on data models has: <br> - Led to irrelevant theory and questionable scientific conclusions. <br> - Kept statisticians from using more suitable algorithmic models. <br> - Prevented statisticians from working on exciting new problems. <br> <span class="math inline">\(\sim\)</span> Leo Brieman, 2001</p>
</blockquote>
<p>Respective departments of computer science and statistics now overlap more than ever, as more relaxed views prevail today, but there are potential drawbacks to placing too much emphasis on either approach historically associated with them. Models that ‘just work’ have the potential to be dangerous if they are little understood. Situations for which much time is spent sorting out details for an ill-fitting model suffers the converse problem- some (though often perhaps very little actually) understanding with little pragmatism. While this document will focus on more algorithmic approaches, guidance will be provided with an eye toward their use in situations where the typical data modeling approach would be applied, thereby hopefully shedding some light on a path toward obtaining the best of both worlds.</p>
</div>
<div id="some-terminology" class="section level2">
<h2>Some Terminology</h2>
<p>For those used to statistical concepts such as dependent variables, clustering, and predictors, etc. you will have to get used to some differences in terminology<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> See <a href="https://statweb.stanford.edu/~tibs/stat315a/glossary.pdf">this</a> for a comparison.</span> such as targets, unsupervised learning, and features etc. It doesn’t take too much to get acclimated, even if it is somewhat annoying when one is first starting out. I won’t be too beholden to either in this paper, and it should be clear from the context what’s being referred to. Initially I will start off mostly with non-ML terms and note in brackets it’s ML version to help the orientation along.</p>

</div>
</div>
<p style="text-align: center;">
<a href="00_preface.html"><button class="btn btn-default">Previous</button></a>
<a href="02_tools.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
