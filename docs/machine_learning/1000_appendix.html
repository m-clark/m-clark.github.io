<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="An Introduction to Machine Learning" />
<meta property="og:type" content="book" />
<meta property="og:url" content="https://m-clark.github.io/docs/" />
<meta property="og:image" content="https://m-clark.github.io/docs/img/nineteeneightyR.png" />
<meta property="og:description" content="An introduction to machine learning for applied researchers." />
<meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />


<<<<<<< Updated upstream
<meta name="date" content="2017-05-01" />
=======
<meta name="date" content="2017-05-02" />
>>>>>>> Stashed changes

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="An introduction to machine learning for applied researchers.">

<title>An Introduction to Machine Learning</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<script src="libs/d3-3.5.3/./d3.min.js"></script>
<link href="libs/d3heatmapcore-0.0.0/heatmapcore.css" rel="stylesheet" />
<script src="libs/d3heatmapcore-0.0.0/heatmapcore.js"></script>
<script src="libs/d3-tip-0.6.6/index.js"></script>
<script src="libs/d3heatmap-binding-0.6.1.1/d3heatmap.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-0.9.0/grViz.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="standard_html.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="mytufte.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#section"></a></li>
<li><a href="00_preface.html#preface">Preface</a></li>
<li class="has-sub"><a href="01_intro.html#introduction">Introduction</a><ul>
<li><a href="01_intro.html#explanation-prediction">Explanation &amp; Prediction</a></li>
<li><a href="01_intro.html#some-terminology">Some Terminology</a></li>
<li><a href="01_intro.html#supervised-vs.unsupervised">Supervised vs. Unsupervised</a></li>
<li class="has-sub"><a href="01_intro.html#tools-you-already-have">Tools you already have</a><ul>
<li><a href="01_intro.html#the-standard-linear-model">The Standard Linear Model</a></li>
<li><a href="01_intro.html#logistic-regression">Logistic Regression</a></li>
<li><a href="01_intro.html#expansions-of-those-tools">Expansions of Those Tools</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#concepts">Concepts</a><ul>
<li class="has-sub"><a href="02_concepts.html#loss-functions">Loss Functions</a><ul>
<li><a href="02_concepts.html#continuous-outcomes">Continuous Outcomes</a></li>
<li><a href="02_concepts.html#categorical-outcomes">Categorical Outcomes</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#regularization">Regularization</a><ul>
<li><a href="02_concepts.html#r-example-1">R Example</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul>
<li><a href="02_concepts.html#bias-variance">Bias <em>&amp;</em> Variance</a></li>
<li><a href="02_concepts.html#the-tradeoff">The Tradeoff</a></li>
<li><a href="02_concepts.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a></li>
<li><a href="02_concepts.html#bias-variance-summary">Bias-Variance Summary</a></li>
</ul></li>
<li class="has-sub"><a href="02_concepts.html#cross-validation">Cross-Validation</a><ul>
<li><a href="02_concepts.html#adding-another-validation-set">Adding Another Validation Set</a></li>
<li><a href="02_concepts.html#k-fold-cross-validation">K-fold Cross-Validation</a></li>
<li><a href="02_concepts.html#bootstrap">Bootstrap</a></li>
<li><a href="02_concepts.html#other-stuff">Other Stuff</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#opening-the-black-box">Opening the Black Box</a><ul>
<li class="has-sub"><a href="03_blackbox.html#process-overview">Process Overview</a><ul>
<li><a href="03_blackbox.html#data-preparation">Data Preparation</a></li>
<li><a href="03_blackbox.html#model-selection">Model Selection</a></li>
<li><a href="03_blackbox.html#model-assessment">Model Assessment</a></li>
</ul></li>
<li><a href="03_blackbox.html#the-dataset">The Dataset</a></li>
<li><a href="03_blackbox.html#r-implementation">R Implementation</a></li>
<li><a href="03_blackbox.html#feature-selection-the-data-partition">Feature Selection <em>&amp;</em> The Data Partition</a></li>
<li class="has-sub"><a href="03_blackbox.html#k-nearest-neighbors"><span class="math inline">\(k\)</span>-nearest Neighbors</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#neural-networks">Neural Networks</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-1">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-1">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#trees-forests">Trees <em>&amp;</em> Forests</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-2">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-2">Final Thoughts</a></li>
</ul></li>
<li class="has-sub"><a href="03_blackbox.html#support-vector-machines">Support Vector Machines</a><ul>
<li><a href="03_blackbox.html#strengths-weaknesses-3">Strengths <em>&amp;</em> Weaknesses</a></li>
<li><a href="03_blackbox.html#final-thoughts-3">Final Thoughts</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="04_other.html#wrap-up">Wrap-up</a><ul>
<li class="has-sub"><a href="04_other.html#unsupervised-learning">Unsupervised Learning</a><ul>
<li><a href="04_other.html#clustering">Clustering</a></li>
<li><a href="04_other.html#latent-variable-models">Latent Variable Models</a></li>
<li><a href="04_other.html#graphical-structure">Graphical Structure</a></li>
<li><a href="04_other.html#imputation">Imputation</a></li>
</ul></li>
<li class="has-sub"><a href="04_other.html#ensembles">Ensembles</a><ul>
<li><a href="04_other.html#bagging">Bagging</a></li>
<li><a href="04_other.html#boosting">Boosting</a></li>
<li><a href="04_other.html#stacking">Stacking</a></li>
</ul></li>
<li><a href="04_other.html#feature-selection-importance">Feature Selection <em>&amp;</em> Importance</a></li>
<li><a href="04_other.html#natural-language-processingtext-analysis">Natural language processing/Text Analysis</a></li>
<li><a href="04_other.html#bayesian-approaches">Bayesian Approaches</a></li>
<li><a href="04_other.html#more-stuff">More Stuff</a></li>
<li class="has-sub"><a href="04_other.html#summary">Summary</a><ul>
<li><a href="04_other.html#cautionary-notes">Cautionary Notes</a></li>
<li><a href="04_other.html#some-guidelines">Some Guidelines</a></li>
<li><a href="04_other.html#conclusion">Conclusion</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="1000_appendix.html#appendix">Appendix</a><ul>
<li><a href="1000_appendix.html#bias-variance-demo">Bias Variance Demo</a></li>
<li class="has-sub"><a href="1000_appendix.html#programming-languages">Programming Languages</a><ul>
<li><a href="1000_appendix.html#r">R</a></li>
<li><a href="1000_appendix.html#python">Python</a></li>
<li><a href="1000_appendix.html#other">Other</a></li>
</ul></li>
<li><a href="1000_appendix.html#brief-glossary-of-common-terms">Brief Glossary of Common Terms</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div id="bias-variance-demo" class="section level2">
<h2>Bias Variance Demo</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
x =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>)
ytrue =<span class="st"> </span><span class="kw">sin</span>(<span class="dv">3</span>*pi*x)
basedat =<span class="st"> </span><span class="kw">cbind</span>(x,ytrue)[<span class="kw">order</span>(x),]

gendatfunc =<span class="st"> </span>function(<span class="dt">noise=</span>.<span class="dv">5</span>, <span class="dt">n=</span><span class="dv">1000</span>){
  x =<span class="st"> </span><span class="kw">runif</span>(n)
  y =<span class="st"> </span><span class="kw">sin</span>(<span class="dv">3</span>*pi*x) +<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">sd=</span>noise) <span class="co"># truth</span>
  d =<span class="st"> </span><span class="kw">cbind</span>(x, y)
  d
}

gendat =<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">100</span>, <span class="kw">gendatfunc</span>(<span class="dt">n=</span><span class="dv">100</span>))
<span class="kw">str</span>(gendat)

<span class="kw">library</span>(kernlab)

rbf1 =<span class="st"> </span><span class="kw">apply</span>(gendat, <span class="dv">3</span>, 
             function(d) <span class="kw">predict</span>(<span class="kw">gausspr</span>(y~x, <span class="dt">data=</span><span class="kw">data.frame</span>(d), <span class="dt">kpar=</span><span class="kw">list</span>(<span class="dt">sigma=</span>.<span class="dv">5</span>)), 
                                 <span class="dt">newdata =</span> <span class="kw">data.frame</span>(x), <span class="dt">type=</span><span class="st">&#39;response&#39;</span>))
rbf2 =<span class="st"> </span><span class="kw">apply</span>(gendat, <span class="dv">3</span>, 
             function(d) <span class="kw">predict</span>(<span class="kw">gausspr</span>(y~x, <span class="dt">data=</span><span class="kw">data.frame</span>(d)), 
                                 <span class="dt">newdata =</span> <span class="kw">data.frame</span>(x), <span class="dt">type=</span><span class="st">&#39;response&#39;</span>) )

<span class="kw">library</span>(ggplot2); <span class="kw">library</span>(tidyverse); <span class="kw">library</span>(gridExtra)

rbf1_samp =<span class="st"> </span>rbf1 %&gt;%<span class="st"> </span>
<span class="st">  </span>data.frame %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">cbind</span>(x, .) %&gt;%
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">100</span>, <span class="dv">25</span>)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key=</span>sample, <span class="dt">value=</span>yhat, -x)

rbf2_samp =<span class="st"> </span>rbf2 %&gt;%<span class="st"> </span>
<span class="st">  </span>data.frame %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">cbind</span>(x, .) %&gt;%
<span class="st">  </span><span class="kw">slice</span>(<span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">100</span>, <span class="dv">25</span>)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key=</span>sample, <span class="dt">value=</span>yhat, -x)

g1 =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span><span class="kw">data.frame</span>(basedat)) +
<span class="st">  </span><span class="kw">geom_blank</span>() +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat, <span class="dt">group=</span>sample), <span class="dt">color=</span><span class="st">&#39;#ff5503&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">25</span>, <span class="dt">data=</span>rbf1_samp) +
<span class="st">  </span><span class="kw">ylim</span>(<span class="kw">c</span>(-<span class="fl">1.5</span>, <span class="fl">1.5</span>)) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;&#39;</span>, <span class="dt">title=</span><span class="st">&#39;Low Variance&#39;</span>) +<span class="st"> </span>
<span class="st">  </span>lazerhawk::<span class="kw">theme_trueMinimal</span>() +
<span class="st">    </span><span class="kw">theme</span>(
    <span class="dt">legend.key =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">legend.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">panel.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">panel.grid =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">strip.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">plot.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;#fffff8&quot;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>)
  )

g2 =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span><span class="kw">data.frame</span>(basedat)) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>ytrue), <span class="dt">color=</span><span class="st">&#39;#03b3ff&#39;</span>) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat), <span class="dt">color=</span><span class="st">&#39;#ff5503&#39;</span>, <span class="kw">data.frame</span>(<span class="dt">yhat=</span><span class="kw">rowMeans</span>(rbf1))) +
<span class="st">  </span><span class="kw">ylim</span>(<span class="kw">c</span>(-<span class="fl">1.5</span>, <span class="fl">1.5</span>)) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;&#39;</span>, <span class="dt">title=</span><span class="st">&#39;High Bias&#39;</span>) +<span class="st"> </span>
<span class="st">  </span>lazerhawk::<span class="kw">theme_trueMinimal</span>() +
<span class="st">    </span><span class="kw">theme</span>(
    <span class="dt">legend.key =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">legend.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">panel.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">panel.grid =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">strip.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">plot.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;#fffff8&quot;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>)
  )

g3 =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span><span class="kw">data.frame</span>(basedat)) +
<span class="st">  </span><span class="kw">geom_blank</span>() +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat, <span class="dt">group=</span>sample), <span class="dt">color=</span><span class="st">&#39;#ff5503&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">25</span>, <span class="dt">data=</span>rbf2_samp) +
<span class="st">  </span><span class="kw">ylim</span>(<span class="kw">c</span>(-<span class="fl">1.5</span>, <span class="fl">1.5</span>)) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;&#39;</span>, <span class="dt">title=</span><span class="st">&#39;High Variance&#39;</span>) +<span class="st"> </span>
<span class="st">  </span>lazerhawk::<span class="kw">theme_trueMinimal</span>() +
<span class="st">    </span><span class="kw">theme</span>(
    <span class="dt">legend.key =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">legend.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">panel.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">panel.grid =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">strip.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">plot.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;#fffff8&quot;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>)
  )

g4 =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span><span class="kw">data.frame</span>(basedat)) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>ytrue), <span class="dt">color=</span><span class="st">&#39;#03b3ff&#39;</span>) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat), <span class="dt">color=</span><span class="st">&#39;#ff5503&#39;</span>, <span class="kw">data.frame</span>(<span class="dt">yhat=</span><span class="kw">rowMeans</span>(rbf2))) +
<span class="st">  </span><span class="kw">ylim</span>(<span class="kw">c</span>(-<span class="fl">1.5</span>, <span class="fl">1.5</span>)) +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&#39;&#39;</span>, <span class="dt">title=</span><span class="st">&#39;Low Bias&#39;</span>) +<span class="st"> </span>
<span class="st">  </span>lazerhawk::<span class="kw">theme_trueMinimal</span>() +
<span class="st">    </span><span class="kw">theme</span>(
    <span class="dt">legend.key =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">legend.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill=</span><span class="st">&#39;#fffff8&#39;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>),
    <span class="dt">panel.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">panel.grid =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">strip.background =</span> ggplot2::<span class="kw">element_blank</span>(),
    <span class="dt">plot.background =</span> ggplot2::<span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;#fffff8&quot;</span>, <span class="dt">colour =</span> <span class="ot">NA</span>)
  )

<span class="kw">grid.arrange</span>(g1, g2, g3, g4, <span class="dt">ncol=</span><span class="dv">2</span>)</code></pre></div>
</div>
<div id="programming-languages" class="section level2">
<h2>Programming Languages</h2>
<div id="r" class="section level3">
<h3>R</h3>
<p>Demonstrations for this document were done with R, and specifically the <span class="pack">caret</span> package. I would highly recommend using it for your own needs, as it makes a lot the ML process simpler, while providing access to whatever technique you want to use, even while it comes with the ability to use hundreds of approaches out of the box.</p>
</div>
<div id="python" class="section level3">
<h3>Python</h3>
<p>If your data fits on your machine and/or your analysis time is less than a couple hours, R is hands down the easiest to use to go from data to document, including if that document is an interactive website. That said, R probably isn’t even the most popular ML tool, because in many situations we have a lot more data, or simply need the predictions without frills and as fast as possible<label for="tufte-sn-48" class="margin-toggle sidenote-number">48</label><input type="checkbox" id="tufte-sn-48" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">48</span> Actually for this <span class="func">rf.fit</span> was slower than the default <span class="func">randomForest</span> function in R by about a second under similar settings.</span>. As such Python is the de facto standard in such situations, and probably the most popular development environment for machine learning.</p>
<p>One can start with the <span class="pack">scikit-learn</span> module, using it much in the same way as was demonstrated with caret. It will get you very far, but for some situations, you may need more heavy duty options like <span class="pack">tensorflow</span>, <span class="pack">Theano</span>, etc.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> sklearn <span class="im">as</span> sk
<span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split
<span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler
<span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier

<span class="co"># import data</span>
wine <span class="op">=</span> pd.read_csv(<span class="st">&#39;data/wine.csv&#39;</span>)

<span class="co"># data preprocessing</span>
np.random.seed(<span class="dv">1234</span>)
X <span class="op">=</span> wine.drop([<span class="st">&#39;free.sulfur.dioxide&#39;</span>, <span class="st">&#39;density&#39;</span>, <span class="st">&#39;quality&#39;</span>, <span class="st">&#39;color&#39;</span>, <span class="st">&#39;white&#39;</span>,<span class="st">&#39;good&#39;</span>], axis<span class="op">=</span><span class="dv">1</span>)
X <span class="op">=</span> MinMaxScaler().fit_transform(X)  <span class="co"># by default on 0, 1 scale</span>
y <span class="op">=</span> wine[<span class="st">&#39;good&#39;</span>]
X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)

<span class="co"># train model</span>
rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">1000</span>)
rf_train <span class="op">=</span> rf.fit(X_train, y_train)

<span class="co"># get test predictions</span>
rf_predict <span class="op">=</span> rf_train.predict(X_test)

<span class="co"># create confusion matrix, and accuracy</span>
cm <span class="op">=</span> sk.metrics.confusion_matrix(y_test,rf_predict)
cm_prob <span class="op">=</span> cm <span class="op">/</span> np.<span class="bu">sum</span>(cm)  <span class="co"># as probs</span>
<span class="bu">print</span>(cm_prob)

acc <span class="op">=</span> sk.metrics.accuracy_score(y_test, rf_predict)
acc <span class="op">=</span> pd.DataFrame(np.array([acc]), columns<span class="op">=</span>[<span class="st">&#39;Accuracy&#39;</span>])
<span class="bu">print</span>(acc)</code></pre></div>
<pre><code>[[ 0.26692308  0.09076923]
 [ 0.07615385  0.56615385]]
   Accuracy
0  0.833077</code></pre>
</div>
<div id="other" class="section level3">
<h3>Other</h3>
<p>I wouldn’t recommend a proprietary tool when better open source tools are available, but I will say <strong>Matlab</strong> is also very popular in machine learning, and specific areas like image processing. <strong>Julia</strong> has been coming along as well. Of course for maximum speed people still prefer lower level languages like <strong>C++</strong>, <strong>Java</strong>, and <strong>Fortran</strong>. And for whatever reason, people are reinventing ML wheels in languages like <strong>Javascript</strong> and others. See the <a href="https://github.com/josephmisiti/awesome-machine-learning">awesome list</a> for more.</p>
<p>What I can’t recommend is a traditional statistics package like SPSS, SAS, or Stata. Not only did they miss this boat by over a decade, their offerings are slim and less capable. It seems SAS is probably the only one that’s made serious effort here, and it has some audience in the business world due to its long entrenchment there. And you don’t have to take my word for it- here’s a comparison of trends at <a href="https://www.indeed.com/jobtrends/q-R-and-%28%22machine-learning%22-or-%22data-science%22%29-q-python-and-%28%22machine-learning%22-or-%22data-science%22%29-q-SAS-and-%28%22machine-learning%22-or-%22data-science%22%29-q-SPSS-and-%28%22machine-learning%22-or-%22data-science%22%29-q-Stata-and-%28%22machine-learning%22-or-%22data-science%22%29-q-Matlab-and-%28%22machine-learning%22-or-%22data-science%22%29.html">indeed.com</a>.</p>
</div>
</div>
<div id="brief-glossary-of-common-terms" class="section level2">
<h2>Brief Glossary of Common Terms</h2>
<<<<<<< Updated upstream
=======
<p>The following lists some common ML terms, and their more traditional statistical counterpart.</p>
>>>>>>> Stashed changes
<p><span class="emph">bias</span>: could mean the intercept (e.g. in neural nets), typically refers to the bias in bias-variance decomposition</p>
<p><span class="emph">regularization, penalization, shrinkage</span>: The process of adding a penalty to the size of coefficients, thus shrinking them towards zero but resulting in less overfitting (at an increase to bias)</p>
<p><span class="emph">classifier</span>: specific model or technique (i.e. function) that maps observations to classes</p>
<p><span class="emph">confusion matrix</span>: a table of predicted class membership vs. true class membership</p>
<p><span class="emph">hypothesis</span>: a specific model <span class="math inline">\(h(x)\)</span> of all possible in the hypothesis space <span class="math inline">\(\mathcal{H}\)</span></p>
<p><span class="emph">input, feature, attribute</span>: independent variable, explanatory variable, covariate, predictor variable, column</p>
<p><span class="emph">instance, example</span>: observation, row</p>
<p><span class="emph">learning</span>: model fitting</p>
<p><span class="emph">machine learning</span>: a form of statistics utilizing various algorithms with a goal to generalize to new data situations</p>
<<<<<<< Updated upstream
<p><span class="emph">supervised</span>: has a dependent variable</p>
<p><span class="emph">target, label</span>: dependent variable, response, the outcome of interest</p>
<p><span class="emph">unsupervised</span>: no dependent variable; think clustering, PCA etc.</p>
<p><span class="emph">weights</span>: coefficients, parameters</p>
=======
<p><span class="emph">supervised</span>: has a target variable</p>
<p><span class="emph">target, label</span>: dependent variable, response, the outcome of interest</p>
<p><span class="emph">unsupervised</span>: no target variable; think clustering, PCA etc.</p>
<p><span class="emph">weights</span>: coefficients, parameters</p>
<p><span class="emph"><code>NULL</code></span>: statistical significance (you will not be concerning yourself with p-values in ML)</p>
>>>>>>> Stashed changes

</div>
</div>
<p style="text-align: center;">
<a href="04_other.html"><button class="btn btn-default">Previous</button></a>
</p>
</div>
</div>



</body>
</html>
