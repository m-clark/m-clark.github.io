[
["index.html", "Graphical and Latent Variable Modeling", " Graphical and Latent Variable Modeling Michael Clark 2017-08-07 --> "],
["preface.html", "Preface Prerequisites", " Preface This document’s original purpose was to serve as the basis for a workshop on structural equation modeling held over a couple of afternoons. However, it is now more or less a document on graphical and latent variable modeling more generally. Most of it will not be covered in the workshop in detail, and will continue to be expanded upon over time. One of the goals of this set of notes is to not impart a false sense of comfort and/or familiarity with the techniques. For example, no one is going to be an expert after a couple of afternoons with SEM. SEM and related methods are typically taught over the course of a few weeks in a traditional applied statistics course, or even given their own course outright. Instead, one of the primary goals here is to instill a firm conceptual foundation starting with common approaches (e.g. standard regression), while exposing the participant to a wide family of related techniques, any of which might be useful to one’s modeling and data situation, but may or may not traditionally fall under the heading of traditional SEM topics. While the focus is still on SEM, it is just as important in my opinion to expand the range of techniques beyond SEM for those who might not otherwise be exposed to them. At this point I can more or less break down the focus of the sections of this document in a couple of ways. Mediation/path analysis: graphical models SEM: graphical models, latent variables, SEM, latent growth curves, IRT Measurement models/Reliability: latent variables, IRT Factor Analysis/Latent variables: latent variables, mixture models, Bayesian nonparametric models Prerequisites The following prerequisites are more for those attending the workshop where time and resources are limited. While they might apply to anyone attempting to learn SEM, and should give you a sense of what knowledge/skill is assumed, I encourage anyone to read through the notes on their own regardless of background if they are of interest. Statistical One should at least have a firm understanding of standard regression modeling techniques. If you are new to statistical analysis in general, I’ll be blunt and say you are probably not ready for SEM. SEM employs knowledge of maximum likelihood, multivariate analysis, measurement error, indirect effects etc., and none of this is typically covered in a first semester of statistics in many applied disciplines. SEM builds upon that basic regression foundation, and if that is not solid, SEM will probably be confusing and/or magical at best. The first two sections on graphical modeling and latent variables are more or less stand alone and have no prerequisite beyond that just described. The SEM section assumes the knowledge within those two. The sections beyond section are more technical and/or code-oriented, but still can simply be read to get the gist of the content. Programming SEM requires its own modeling language approach. As such, the syntax for Mplus and SEM specific programs, as well as SEM models within other languages or programs (e.g. R or Stata) are going to require you to learn something new. If you are not familiar with R, you’ll want to go through the R introduction to get oriented, but there are many intros on the web to get you started with more detail. Color coding in text: emphasis package function object/class link "],
["introduction.html", "Introduction Outline Programming Language Choice Setup", " Introduction Outline Graphical Models The document will start with the familiar, a standard regression model. It will then be presented as a graphical model, and extended to include indirect effects (e.g. \\(\\mathcal{A} \\rightarrow \\mathcal{B} \\rightarrow \\mathcal{C}\\)) and multiple outcomes. At this point we will discuss directed graphs, and demonstrate a more theoretically motivated approach (sometimes called path analysis), and compare it to more flexible approaches that do not require prespecification of paths nor specific directional relations. We’ll briefly discuss undirected graphs, with an example utilizing ‘network analysis’. Latent Variables We will then discuss the notion of latent variables in the context of underlying causes or constructs and measurement error. We will begin by noting that latent variable models are actually even more broadly utilized when one includes other dimension reduction, or data compression, techniques, several of which fall under the heading of factor analysis. A few common techniques will be demonstrated such as ‘factor analysis’ and principal components analysis, and an overview will be provided for others. In addition, we will note other places one might find latent variable models. SEM Next we turn to structural equation modeling, where previously covered models come together under one modeling approach. We will spend a good deal of time with measurement models first, comparing them to our previous efforts, and then extend those models to the case of regression with latent variables. There are many issues to consider when developing such models, and an attempt will be made to cover quite a bit of ground in that regard. Others The three sections just noted are the most developed, and serve as the basis for workshops. Other topics are touched upon for reference, and may also be added to in the future. For example latent growth curve models, are an alternative to a standard mixed model. Also, there are situations where the latent variable might be considered categorical, commonly called mixture models or cluster analysis, but in some specific contexts might go by other names (e.g. latent class analysis). Finally, an overview of other types of latent variable or structural equation models, such as item response theory, collaborative filtering etc. may also be provided. Programming Language Choice We will use R for a variety of reasons. One is that all of the techniques mentioned thus far are fully developed within various R packages, often taking just a line or two of code to implement after the data has been prepped. Furthermore, it is freely available and will work on Windows, Mac and Linux. R is well-known as a powerful statistical modeling environment, and its flexible programming language allows for efficient data manipulation and exploration. Furthermore, it has a vast array of visualization capabilities. In short, it provides everything one might need in a single environment for standard SEM, and nothing comes close in SEM-specific offerings. Among alternatives, Mplus is the most fully developed structural equation modeling package, and has been for years. However, it is a poor tool for data management, few universities have a campus-wide license for it, and most of its functionality (and all we will need for this our purposes) is implemented within the lavaan family of R packages. Stata has relatively recently provided SEM capabilities, but it is less well-developed (something that might change in time), and it still requires a license, making non-campus usage difficult or costly. Other alternatives exist, but are not as commonly used as lavaan, Mplus, and Stata, at least in my experience across two universities and dozens of academic disciplines. For more on what’s out there, see the appendix. Setup For those wishing to follow along, for things to go smoothly you’ll need to complete the following steps precisely. If you are not present or are bringing your laptop, you’ll need to have both R and RStudio installed on whatever machine you’ll be using. If this will be a new experience for you, install R first, then RStudio. For either you’ll need to choose the version appropriate to your operating system. As you go through the installation, for both just accept all defaults when prompted until the installation process begins. Once both are installed, you will only need to work with RStudio, and it will at all times be assumed you will be using RStudio during the workshop. Once those are installed, proceed through the following steps. Download this zipfile, and unzip its contents to an area on your machine that you have write access to. It contains the course contents, data, etc. in a folder that will serve as an RStudio project folder. Open RStudio. File/Open Project/ then navigate to the folder contents you just unzipped. Click on the SEM file (should look like a blue icon, but otherwise is the SEM.Rproj file). Open the R script, install_script.R inside. Run the one line of code there and you’re set. The lab for the workshop has Windows machines, and so the above is enough to proceed. For *nix systems, it should be mostly the same. "],
["introduction-to-r.html", "Introduction to R Getting Started Key things to know about R Moving forward", " Introduction to R This introduction to R is very brief and only geared toward providing some basics so that one can understand and run the code associated with the content. It is geared toward an audience that likely has no programming experience whatsoever, but may have had some exposure to traditional statistics packages. If you have some basic familiarity with R you may skip this chapter, though it might serve as a refresher for some. Getting Started Installation As mentioned previously, to begin with R for your own machine, you just need to go to the R website, download it for your operating system, and install. Then go to RStudio, download and install it. From there on you only need RStudio to use R. Updates occur every few months, and you should update R whenever a new version is released. Packages As soon as you install it, R is already the most powerful statistical environment within which to work. However, its real strength comes from the community, which has added thousands of packages that provide additional or enhanced functionality. You will regularly find packages that specifically do something you want, and will need to install them in order to use them. RStudio provides a Packages tab, but it is usually just as or more efficient to use the install.packages function. install.packages(&#39;mynewfavorite&#39;) At this point there are over 7000 packages available through standard sources, and many more through unofficial ones. To start getting some ideas of what you want to use, you will want to spend time at places like CRAN Task Views, Rdocumentation, or with a list like this one. The main thing to note is that if you want to use a package, you have to load it with the library function. library(lazerhawk) Sometimes, you only need the package for one thing and there is no reason to keep it loaded, in which case you can use the following approach. packagename::packagefunction(args) I suggest using the library function for newbies. You’ll get the hang of finding, installing, and using packages quite quickly. However, note that the increasing popularity and ease of using R means that packages can vary quite a bit in terms of quality, so you may need to try out a couple packages with seemingly similar functionality to find the best for your situation. RStudio RStudio is an integrated development environment (IDE) specifically geared toward R (though it works for other languages too). At the very least it will make your programming far easier and more efficient, at best you can create publish-ready documents, manage projects, create interactive website regarding your research, use version control, and much more. I have an overview here. See Emacs Speaks Statistics for an alternative. The point is, base R is not an efficient way to use R, and you have at least two very powerful options to make your coding experience easier and more efficient. Importing Data It is as easy to import data into R from other programs and text files as it is any other statistical program. As a first step, feel free to use the menu-based approach via the Environment tab. Note that you have easy access to the code that actually does the job. You should quickly get used to a code-based approach as soon as possible, that way you don’t waste time clicking and hunting for files, and instead can quickly run a single line of code. It is easier to select specific options as well. Some key packages to note: base: Base R already can read in tab-delimited, comma-separated etc. text files foreign: Base R comes with the foreign package that can read in files from other statistical packages. However support for Stata ended with version 12. Newer packages readr: enhanced versions of Base R read functionality haven: replacement for foreign that can also read current Stata files readxl: for Excel specifically There are many other packages that would be of use for special situations and other less common file types and structures. Key things to know about R R is a programming language, not a ‘stats package’ The first thing to note for those new to R is that R is a language that is oriented toward, but not specific to, dealing with statistics. This means it’s highly flexible, but does take some getting used to. If you go in with a spreadsheet style mindset, you’ll have difficulty, as well as miss out on the power it has to offer. Never ask if R can do what you want. It can. The only limitation to R is the user’s programming sophistication. The better you get at statistical programming the further you can explore your data and take your research. This holds for any statistical endeavor whether using R or not. Main components: script, console, graphics device With R, the script is where you write your R code. While you could do everything at the console, this would be difficult at best and unreproducible. The console is where the results are produced from running the script. Again you can do one-liner stuff there, such as getting help for a function. The graphics device is where visualizations are produced, and in RStudio you have two, one for static plots, and a viewer for potentially interactive ones. R is easy to use, but difficult to master. If you only want to use R in an applied fashion, as we will do here, R can be very easy to use. As an example the following code would read in data and run a standard linear regression with the lm (i.e. linear model) function. mydata = read.csv(file=&#39;location/myfile.csv&#39;) # some file with an &#39;x&#39; and &#39;y&#39; variable regModel = lm(y ~ x, data=mydata) # standard regression summary(regModel) # summary of results The above code demonstrates that R can be as easy to use as anything else, and in my opinion, it is for any standard analysis. In addition, the formula syntax is utilized the vast majority of modeling packages (including lavaan), as is the summary function for printing model results. The nice part is that R can still be easier to use with more complex analyses you either won’t find elsewhere or will only get with crippled functionality. Object-oriented This section may be a little more than a newcomer to R wants or even needs to know to get started with R. However, most of the newbie’s errors will be due not knowing a little about this aspect of R programming, and I’ve seen some make the same mistakes even after using R for a long time as a result. R is object-oriented. For our purposes this means that we create what are called objects and use functions to manipulate those objects in some fashion. In the above code we created two objects, an object that held the data and an object that held the regression results. Objects can be anything, a character string, a list of 1000 data sets, the results of an analysis, anything. Assignment In order to create an object, we must assign something to it. You’ll come across two ways to do this. myObject = something myObject &lt;- something These result in the same thing, an object called myObject that contains ‘something’ in it. For all appropriate practical use, whether you use = or &lt;-is a matter of personal preference. The point is that typical practice in R entails that one assigns something to an object and then uses functions on that object to get something more from it. Functions Functions are objects that take input of various kinds and produce some output (simply called a value). In the above code we used three functions: read.csv, lm, and summary. Functions (usually) come with named arguments, that note what types of inputs the function can take. With read.csv, we merely gave it one argument, the file name, but there are actually a couple dozen, each with some default. Type ?read.csv at the console to see the help file. Classes All objects have a certain class, which means that some functions will work in certain ways depending on the class. Consider the following code. x = rnorm(100) y = x + rnorm(100) mod = lm(y ~ x) summary(x) Min. 1st Qu. Median Mean 3rd Qu. Max. -1.96538 -0.80547 -0.05393 -0.05846 0.57086 2.56750 summary(mod) Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -2.48417 -0.75768 0.07618 0.68006 2.63588 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.1214 0.1034 -1.175 0.243 x 1.0985 0.1067 10.291 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.032 on 98 degrees of freedom Multiple R-squared: 0.5194, Adjusted R-squared: 0.5145 F-statistic: 105.9 on 1 and 98 DF, p-value: &lt; 2.2e-16 In both cases we used the summary function, but get different results. There are two methods for summary at work here: summary.default and summary.lm. Which is used depends on the class of the object given as an argument to the summary function. Most of the time however, there are unique functions that will only work on one class of object. class(x) [1] &quot;numeric&quot; class(mod) [1] &quot;lm&quot; One of the most common classes of objects used is the data.frame. Data frames are matrices that contain multiple types of vectors that are the variables and observations of interest. The contents can be numeric, factors (categorical variables), character strings and other types of objects. nams = c(&#39;Bob Ross&#39;, &#39;Bob Marley&#39;, &#39;Bob Odenkirk&#39;) places = c(&#39;Dirt&#39;, &#39;Dirt&#39;, &#39;New Mexico&#39;) yod = c(1995, 1981, 2028) bobs = data.frame(nams, places, yod) bobs nams places yod 1 Bob Ross Dirt 1995 2 Bob Marley Dirt 1981 3 Bob Odenkirk New Mexico 2028 Case sensitive A great deal of the errors you will get when you start learning R will result from not typing something correctly. For example, what if we try summary(X)? summary(X) Error in summary(X): object &#39;X&#39; not found Errors are merely messages or calls for help from R. It can’t do what you ask and needs something else in order to perform the task. In this case, R is telling you it can’t find anything called X, which makes sense because the object name is lowercase \\(x\\). This error message is somewhat straightforward, but error messages for all programming languages typically aren’t, and depending on what you’re trying to do, it may be fairly vague. If you get an error, your first thought as you start out with R should be to check the names of things. The lavaan package You’ll see more later, but in order to use lavaan for structural equation modeling, you’ll need two things primarily: an object that represents the data, and an object that represents the model. It will look something like the following. modelCode = &quot; y ~ x1 + x2 + lv # structural/regression model lv =~ z1 + z2 + z3 + z4 # measurement model with latent variable lv &quot; library(lavaan) mymodel = sem(modelCode, data=mydata) The character string is our model, and it can actually be kept as a separate file (without the assignment) if desired. Using the lavaan library, we can then use one of its specific functions like cfa, sem, or growth to use the model code object (modelCode above) and the data object (mydata above). The way to define things in lavaan can be summarized as follows. formulaType operator mnemonic latent variable definition =~ is measured by regression ~ is regressed on (residual) (co)variance ~~ is correlated with intercept ~ 1 intercept fixed parameter _* is fixed at the value of _ Getting help Use ? to get the help file for a specific function, or ?? to do a search, possibly on a quoted phrase. Examples: ?lm ??regression ??&#39;nonlinear regression&#39; Moving forward Hopefully you’ll get the hang of things as we run the code in later chapters. You’ll essentially only be asked to tweak code that’s already provided. This document (and associated workshop) is not the place to learn R. However, here are some exercises to make sure we start to get comfortable with it. Exercises Interactive version here Create an object that consists of the numbers one through five. using c(1,2,3,4,5) or 1:5 Create a different object, that is the same as that object, but plus 1 (i.e. the numbers two through six). Without creating an object, use cbind and rbind, feeding the objects you just created as arguments. For example cbind(obj1, obj2). Create a new object using data.frame just as you did rbind or cbind in #3. Inspect the class of the object, and use the summary function on it. Summary There’s a lot to learn with R, and the more time you spend with R the easier your research process will likely go, and the more you will learn about statistical analysis. "],
["graphical-models-1.html", "Graphical Models Directed Graphs Bayesian Networks Undirected Graphs Summary R packages used", " Graphical Models A graphical model can be seen as a mathematical or statistical construct connecting nodes (or vertices) via edges (or links). When pertaining to statistical models, the nodes might represent variables of interest in our data set, and edges specify the relationships among them. Visually they are depicted in the style of the following examples. Any statistical model you’ve conducted can be expressed as a graphical model. As an example, the first graph with nodes X, Y, and Z might represent a regression model in which X and Z predict Y. The emoticon graph shows an indirect effect, and the 123 graph might represent a correlation matrix. A key idea of a graphical model is that of conditional independence, something one should keep in mind when constructing their models. The concept can be demonstrated with the following graph. In this graph, X is conditionally independent of Z given Y- there is no correlation between X and Z once Y is accounted for1. We will revisit this concept when discussing path analysis and latent variable models. Graphs can be directed, undirected, or mixed. Directed graphs have arrows, sometimes implying a causal flow (a difficult endeavor to demonstrate explicitly) or noting a time component. Undirected graphs merely denote relations among the nodes, while mixed graphs might contain both directional and symmetric relationships. Most of the models discussed in this document will be directed or mixed. Directed Graphs As noted previously, we can represent standard models as graphical models. In most of these cases we’d be dealing with directed or mixed graphs. Almost always we are specifically dealing with directed acyclic graphs, where there are no feedback loops. Standard linear model Let’s start with the standard linear model (SLiM), i.e. a basic regression we might estimate via ordinary least squares (but not necessarily). In this setting, we want to examine the effect of each potential predictor (x* in the graph) on the target variable (y). The following shows what the graphical model might look like. --> While we start simpler, we can use much of the same sort of thinking with more complex models later. In what follows, we’ll show that whether we use a standard R modeling approach (via the lm function), or an SEM approach (via the sem function in lavaan), the results are identical aside from the fact that sem is using maximum likelihood (so the variances will be slightly different). First, we’ll with the standard lm approach. mcclelland = haven::read_dta(&#39;data/path_analysis_data.dta&#39;) lmModel = lm(math21 ~ male + math7 + read7 + momed, data=mcclelland) Now we can do the same model using the lavaan package, and while the input form will change a bit, and the output will be presented in a manner consistent with SEM, the estimated parameters are identical. Note that the square of the residual standard error in the lm output is compared to the variance estimate in the lavaan output. library(lavaan) model = &quot; math21 ~ male + math7 + read7 + momed &quot; semModel = sem(model, data=mcclelland, meanstructure = TRUE) summary(lmModel) Call: lm(formula = math21 ~ male + math7 + read7 + momed, data = mcclelland) Residuals: Min 1Q Median 3Q Max -6.9801 -1.2571 0.1376 1.4544 5.7471 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.84310 1.16418 4.160 4.08e-05 *** male 1.20609 0.25831 4.669 4.44e-06 *** math7 0.31306 0.04749 6.592 1.76e-10 *** read7 0.08176 0.01638 4.991 9.81e-07 *** momed -0.01684 0.06651 -0.253 0.8 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.329 on 324 degrees of freedom (101 observations deleted due to missingness) Multiple R-squared: 0.258, Adjusted R-squared: 0.2488 F-statistic: 28.16 on 4 and 324 DF, p-value: &lt; 2.2e-16 summary(semModel, rsq=T) lavaan (0.5-23.1097) converged normally after 21 iterations Used Total Number of observations 329 430 Estimator ML Minimum Function Test Statistic 0.000 Degrees of freedom 0 Minimum Function Value 0.0000000000000 Parameter Estimates: Information Expected Standard Errors Standard Regressions: Estimate Std.Err z-value P(&gt;|z|) math21 ~ male 1.206 0.256 4.705 0.000 math7 0.313 0.047 6.642 0.000 read7 0.082 0.016 5.030 0.000 momed -0.017 0.066 -0.255 0.799 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .math21 4.843 1.155 4.192 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .math21 5.341 0.416 12.826 0.000 R-Square: Estimate math21 0.258 As we will see in more detail later, SEM incorporates more complicated regression models, but at this point the parameters have the exact same understanding as our standard regression, because there is no difference between the two models except conceptually, in that SEM explicitly brings a causal aspect to bear on the interpretation. As we go along, we can see the structural equation models statistically as generalizations of those we are already well acquainted with, and so one can use that prior knowledge as a basis for understanding the newer ones. Path Analysis Path Analysis, and thus SEM, while new to some, is in fact a very, very old technique, statistically speaking2. It can be seen as a generalization of the SLiM approach that can allow for indirect effects and multiple target variables. Path analysis also has a long history in the econometrics literature though under different names (e.g. instrumental variable regression, 2-stage least squares etc.), and through the computer science realm through the use of graphical models more generally. As such, there are many tools at your disposal for examining such models, and I’ll iterate that much of the SEM perspective on modeling comes largely from specific disciplines (especially psychology and education), while other approaches that are very common in other disciplines may be better for your situation. Types of relationships The types of potential relationships examined by path analysis can be seen below. Most models deal only with direct effects. In this case there are no intervening variables we wish to consider. If such variables do exist, we are then dealing with what is often called a mediation model, and must interpret both indirect and (potentially) direct effects. When dealing with multiple outcomes, some may have predictors in common, such that the outcomes’ correlation can be explained by those common causes (this will come up in factor analysis later). Often there are unanalyzed correlations. As an example, every time you run a regression, the correlations among predictor variables are left ‘unanalyzed’. Multiple Targets While relatively seldom used, multivariate linear regression3 is actually very straightforward in some programming environments such as R, and conducting models with multivariate outcomes does not require anything specific to SEM, but that is the realm we’ll stick with. Using the McClelland data, let’s try it for ourselves. First, let’s look at the data to get a sense of things. vars n mean sd median trimmed mad min max range skew kurtosis se attention4 1 430 17.93 3.05 18 18.03 2.97 9 25 16 -0.31 -0.17 0.15 math21 2 364 11.21 2.69 11 11.30 2.97 3 17 14 -0.26 -0.18 0.14 college 3 286 0.37 0.48 0 0.34 0.00 0 1 1 0.52 -1.74 0.03 vocab4 4 386 10.18 2.53 10 10.23 2.97 4 17 13 -0.18 -0.37 0.13 math7 5 397 10.73 2.76 11 10.69 2.97 4 19 15 0.13 -0.47 0.14 read7 6 390 31.57 8.05 30 30.87 8.90 18 61 43 0.79 0.41 0.41 read21 7 360 73.67 8.52 76 74.69 7.41 35 84 49 -1.60 3.88 0.45 adopted 8 430 0.49 0.50 0 0.48 0.00 0 1 1 0.06 -2.00 0.02 male 9 430 0.55 0.50 1 0.56 0.00 0 1 1 -0.19 -1.97 0.02 momed 10 419 14.83 2.03 15 14.77 1.48 10 21 11 0.11 -0.45 0.10 college_missing 11 430 0.33 0.47 0 0.29 0.00 0 1 1 0.70 -1.52 0.02 While these are only weakly correlated variables to begin with, one plausible model might try to predict math and reading at age 21 with measures taken at prior years. model = &quot; read21 ~ attention4 + vocab4 + read7 math21 ~ attention4 + math7 read21 ~~ 0*math21 &quot; mvregModel = sem(model, data=mcclelland, missing=&#39;listwise&#39;, meanstructure = T) coef(mvregModel) read21~attention4 read21~vocab4 read21~read7 math21~attention4 math21~math7 read21~~read21 0.128 0.377 0.537 0.091 0.347 51.837 math21~~math21 read21~1 math21~1 5.965 50.290 5.856 The last line of the model code clarifies that we are treating math21 and read21 as independent, and as such we are simply running two separate regressions simultaneously. Note also that the coefficients in the output with ~1 are the intercepts. We can compare this to standard R regression. A first step is taken to make the data equal to what was used in lavaan. For that we can use the dplyr package to select the necessary variables for the model, and then omit rows that have any missing. mcclellandComplete = select(mcclelland, read21, math21, attention4, vocab4, read7, math7) %&gt;% na.omit lm(read21 ~ attention4 + vocab4 + read7, data=mcclellandComplete) Call: lm(formula = read21 ~ attention4 + vocab4 + read7, data = mcclellandComplete) Coefficients: (Intercept) attention4 vocab4 read7 50.2904 0.1275 0.3770 0.5368 lm(math21 ~ attention4 + math7, data=mcclellandComplete) Call: lm(formula = math21 ~ attention4 + math7, data = mcclellandComplete) Coefficients: (Intercept) attention4 math7 5.85633 0.09103 0.34732 Note that had the models for both outcomes been identical, we could have run both outcomes simultaneously using cbind on the dependent variables4. However, we can and probably should estimate the covariance of math and reading skill at age 21. Let’s rerun the path analysis removing that constraint. model = &quot; read21 ~ attention4 + vocab4 + read7 math21 ~ attention4 + math7 &quot; mvregModel = sem(model, data=mcclelland, missing=&#39;listwise&#39;, meanstructure = T) coef(mvregModel) read21~attention4 read21~vocab4 read21~read7 math21~attention4 math21~math7 read21~~read21 0.140 0.388 0.494 0.092 0.330 51.958 math21~~math21 read21~~math21 read21~1 math21~1 5.968 3.202 51.316 6.020 We can see that the coefficients are now slightly different from the SLiM approach. The read21~~math21 value represents the residual covariance between math and reading at age 21, i.e. after accounting for the other covariate relationships modeled, it tells us how correlated those skills are. Using summary will show it to be statistically significant. I additionally ask for standardized output to see the correlation along with the covariance. It actually is not very strong even if it is statistically significant. summary(mvregModel, standardized=T) lavaan (0.5-23.1097) converged normally after 38 iterations Used Total Number of observations 304 430 Estimator ML Minimum Function Test Statistic 26.965 Degrees of freedom 3 P-value (Chi-square) 0.000 Parameter Estimates: Information Expected Standard Errors Standard Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all read21 ~ attention4 0.140 0.134 1.043 0.297 0.140 0.051 vocab4 0.388 0.163 2.378 0.017 0.388 0.116 read7 0.494 0.050 9.942 0.000 0.494 0.486 math21 ~ attention4 0.092 0.045 2.041 0.041 0.092 0.109 math7 0.330 0.049 6.674 0.000 0.330 0.351 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .read21 ~~ .math21 3.202 1.026 3.119 0.002 3.202 0.182 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .read21 51.316 3.045 16.854 0.000 51.316 6.056 .math21 6.020 0.951 6.327 0.000 6.020 2.285 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .read21 51.958 4.214 12.329 0.000 51.958 0.724 .math21 5.968 0.484 12.329 0.000 5.968 0.860 Whether or not to take a multivariate/path-analytic approach vs. separate regressions is left to the researcher. It’s perhaps easier to explain univariate models in some circumstances. But as the above shows, it doesn’t take much to take into account correlated target variables. Indirect Effects So path analysis allows for multiple target variables, with the same or a mix of covariates for each target. What about indirect effects? Standard regression models examine direct effects only, and the regression coefficients reflect that direct effect. However, perhaps we think a particular covariate causes some change in another, which then causes some change in the target variable. This is especially true when some measures are collected at different time points. Note that in SEM, any variable in which an arrow is pointing to it in the graphical depiction is often called an endogenous variable, while those that only have arrows going out from them are exogenous. Exogenous variables may still have (unanalyzed) correlations among them. As we will see later, both observed and latent variables may be endogenous or exogenous. Consider the following model. Here we posit attention span and vocabulary at age 4 as indicative of what to expect for reading skill at age 7, and that is ultimately seen as a precursor to adult reading ability. In this model, attention span and vocabulary at 4 only have an indirect effect on adult reading ability through earlier reading skill. At least temporally it makes sense, so let’s code this up. model = &quot; read21 ~ read7 read7 ~ attention4 + vocab4 &quot; mediationModel = sem(model, data=mcclelland) summary(mediationModel, rsquare=TRUE) lavaan (0.5-23.1097) converged normally after 21 iterations Used Total Number of observations 305 430 Estimator ML Minimum Function Test Statistic 6.513 Degrees of freedom 2 P-value (Chi-square) 0.039 Parameter Estimates: Information Expected Standard Errors Standard Regressions: Estimate Std.Err z-value P(&gt;|z|) read21 ~ read7 0.559 0.050 11.152 0.000 read7 ~ attention4 0.270 0.151 1.791 0.073 vocab4 0.488 0.186 2.629 0.009 Variances: Estimate Std.Err z-value P(&gt;|z|) .read21 53.047 4.296 12.349 0.000 .read7 66.779 5.408 12.349 0.000 R-Square: Estimate read21 0.290 read7 0.035 What does this tell us? As before, we interpret the results as we would any other regression model, though conceptually there are two sets of models to consider (though they are estimated simultaneously5), one for reading at age 7 and one for reading at age 21. And indeed, one can think of path analysis as a series of linked regression models6. Here we have positive relationships between both attention and vocabulary at age 4 and reading at age 7, and a positive effect of reading at age 7 on reading at age 21. Statistically speaking, our model appears to be viable, as there appear to be statistically significant estimates for each path. However, look at the R2 value for reading at age 7. We now see that there is little to no practical effect of the age 4 variables at all, as all we are accounting for is &lt; 4% of the variance, and all that we have really discovered is that prior reading ability affects later reading ability. We can test the indirect effect itself by labeling the paths in the model code. In the following code, I label them based on the first letter of the variables involved (e.g. vr refers to the vocab to reading path), but note that these are arbitrary names. I also add the direct effects of the early age variable. While the indirect effect for vocab is statistically significant, as we already know there is not a strong correlation between these two variables, it’s is largely driven by the strong relationship between reading at age 7 and reading at age 21, which is probably not all that interesting. A comparison of AIC values, something we’ll talk more about later, would favor a model with only direct effects7. model = &quot; read21 ~ rr*read7 + attention4 + vocab4 read7 ~ ar*attention4 + vr*vocab4 # Indirect effects att4_read21 := ar*rr vocab4_read21 := vr*rr &quot; mediationModel = sem(model, data=mcclelland) summary(mediationModel, rsquare=TRUE, fit=T, std=T) lavaan (0.5-23.1097) converged normally after 27 iterations Used Total Number of observations 305 430 Estimator ML Minimum Function Test Statistic 0.000 Degrees of freedom 0 Minimum Function Value 0.0000000000000 Model test baseline model: Minimum Function Test Statistic 121.544 Degrees of freedom 5 P-value 0.000 User model versus baseline model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -3602.058 Loglikelihood unrestricted model (H1) -3602.058 Number of free parameters 7 Akaike (AIC) 7218.115 Bayesian (BIC) 7244.157 Sample-size adjusted Bayesian (BIC) 7221.957 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent Confidence Interval 0.000 0.000 P-value RMSEA &lt;= 0.05 NA Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Information Expected Standard Errors Standard Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all read21 ~ read7 (rr) 0.536 0.050 10.607 0.000 0.536 0.515 attentin4 0.134 0.134 0.998 0.318 0.134 0.015 vocab4 0.381 0.166 2.299 0.021 0.381 0.044 read7 ~ attentin4 (ar) 0.270 0.151 1.791 0.073 0.270 0.033 vocab4 (vr) 0.488 0.186 2.629 0.009 0.488 0.059 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .read21 51.926 4.205 12.349 0.000 51.926 0.695 .read7 66.779 5.408 12.349 0.000 66.779 0.965 R-Square: Estimate read21 0.305 read7 0.035 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all att4_read21 0.145 0.082 1.766 0.077 0.145 0.017 vocab4_read21 0.261 0.102 2.552 0.011 0.261 0.030 In the original article, I did not find their description or diagrams of the models detailed enough to know precisely what the model was in the actual study8, but here is at least one interpretation if you’d like to examine it further. modReading = &quot; read21 ~ read7 + attention4 + vocab4 + male + adopted + momed read7 ~ attention4 + vocab4 &quot; reading = sem(modReading, data=mcclelland, missing=&#39;fiml&#39;, mimic = &#39;Mplus&#39;, std.ov=TRUE) summary(reading, rsquare=TRUE) More mediation Here I would like to demonstrate a powerful approach to mediation models using Imai’s mediation package. It is based on a theoretical framework that allows for notably complex models for mediator, outcome or both, including different types of variables for the mediator and outcome. While it is simple to conduct, the results suggest there are more things to think about when it comes to mediation. To begin, we will run two separate models, one for the mediator and one for the outcome, using the mediate function to use them to estimate the mediation effect. In this case we’ll focus on the indirect effect of attention. library(mediation) mediator_model = lm(read7 ~ attention4 + vocab4, data=mcclellandComplete) outcome_model = lm(read21 ~ read7 + attention4 + vocab4, data=mcclellandComplete) results = mediate(mediator_model, outcome_model, treat=&quot;attention4&quot;, mediator = &#39;read7&#39;) summary(results) Causal Mediation Analysis Quasi-Bayesian Confidence Intervals Estimate 95% CI Lower 95% CI Upper p-value ACME 0.1451 -0.0102 0.31 0.068 . ADE 0.1281 -0.1213 0.39 0.348 Total Effect 0.2732 -0.0193 0.58 0.060 . Prop. Mediated 0.5232 -0.6462 2.72 0.092 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Sample Size Used: 304 Simulations: 1000 The first thing to note is that there are multiple results to consider, we have the average causal mediated effect, the average direct effect, the total effect and the proportion mediated. What is an average causal mediated effect? First, think about a standard experimental design in which participants are assigned to control and treatment groups. Ideally, if we really wanted to know about true effects, we’d see the outcome when an individual was a control and when they were were in the treatment. The true causal effect for an individual would be the difference between their scores when in the control and when in the treatment. Thus there are two hypothetical situations of different conditions, or counterfactuals, of interest- what would be the potential outcome of a control case if they treated, and vice versa for those in the treated group9. This potential outcomes framework is the essence of the mediate function. In the presence of a mediator variable, we can start by defining the average direct effect (ζ) as follows, where \\(Y_i\\) is the potential outcome for unit \\(i\\) (e.g. a person), \\(M_i\\) the potential outcome of the mediator, and \\(T_i\\) the treatment status. From the mediate function help file10: The expected difference in the potential outcome when the treatment is changed but the mediator is held constant at the value that would realize if the treatment equals \\(t\\) \\[ζ(t) = E[Y(t_1, M(t)) - Y(t_0, M(t))]\\] The ACME meanwhile is: The expected difference (δ) in the potential outcome when the mediator took the value that would realize under the treatment condition as opposed to the control condition, while the treatment status itself is held constant. \\[δ(t) = E[Y(t, M(t_1)) - Y(t, M(t_0))]\\] where \\(t\\), \\(t_1\\), \\(t_0\\) are particular values of the treatment T such that \\(t_1 \\neq t_0\\), M(t) is the potential mediator, and Y(t,m) is the potential outcome variable. Adding these two gives us the total effect. In the case of continuous variables, the values for ‘control’ and ‘treatment’ are fixed to specific values of interest, but by default represent a unit difference in the values of the variable. The results of the mediate function are based on a simulation approach, and averaged over those simulations. You’ll note there is also a ‘proportion of effect mediated’. This is the size of the ACME to the the total effect. It too is averaged over the simulations (and thus is not simply the ACME/Total Effect reported values). It’s not clear to me how useful of a notion this is, and as noted by the boundaries, it’s actually a ratio as opposed to a true proportion (as calculated), such that under some simulations the value can be negative or greater than 1. You may compare the result with the previous lavaan results. I’ve rerun it with bootstrap estimates (set se='boot') to be more in keeping with the simulation approach of Imai11. You could also use the blavaan package to get true Bayesian estimates. lhs op rhs label est se z pvalue ci.lower ci.upper 1 att4_read21 := ar*rr att4_read21 0.145 0.080 1.813 0.070 -0.018 0.301 2 read21 ~ attention4 0.134 0.139 0.960 0.337 -0.130 0.414 As expected, the results of lavaan and mediation packages largely agree with one another. However, the potential outcomes framework has the product-of-paths approach as a special case, and thus generalizes to different models and types of variables for the mediator and outcome. Explicit Example While we have shown how to use the tool and compared the outputs, let’s make things more simple and explicit to be sure that we understand what’s going on. The following closely follows Pearl (section 4.4)12. Consider the following three variable setup with estimated parameters. We have a treatment variable that is dummy coded with 0 = ‘Control group’ and 1 = ‘Treatment group’, a continuous Mediator and Target variable. In addition, we’ll consider just a single individual, whose observed values are ‘control’, i.e. 0, 3 for the mediator, and 2 for the target. We define the unique values \\(\\epsilon_*\\) that represent invariant characteristics that exist regardless of changes to interventions or counterfactual hypotheses and set them to the following: \\(\\epsilon_1 = 0\\) \\(\\epsilon_2 = 3 - 0*.3 = 3\\) \\(\\epsilon_3 = 2 - 0*.5 - 3*.2 = 1.4\\) If a person is in the control group, this setup suggests the predicted value for the Target would be 3*.2 + 1.4 = 2. Let’s now figure out what the value would be if this person was in the treatment. The Mediator value would now be 3 + 1*.3 = 3.3. This would in turn result in a predicted Target value as follows: 1.4 + 3.3*.2 + 1*.5 = 2.56. And we could take this approach for other situations. We could, for example, ask what the expected value be if the Treatment level is 0, but the Mediator was at whatever value it would have been, had the person received the treatment? The value is 3.3*.2 + 1.4 = 2.06, which if we subtract from when they are are fully in the control group gives us the mediated effect 2.06 - 2 = .06. In this linear model it is equivalent to indirect effect via the product-of-paths approach, .3*.2 = .06. But now with the estimate based on the target variable rather than coefficients, we don’t have to be restricted to linear models. One thing to note about the above, despite not having a data set or statistical result, we engaged in attempting to understand causal relations among the variables under theoretical consideration. Even though we may use the tools of statistical analysis, the key things that separates SEM and common regression approaches is the explicit assumption of causal relations and the ability to test some causal claims13. Cavets about indirect effects One should think very hard about positing an indirect effect, especially if there is no time component or obvious pseudo-causal path. If the effect isn’t immediately obvious, or you are dealing with cross-sectional data, then you should probably just examine the direct effects. Unlike other standard explorations one might do with models (e.g. look for nonlinear relationships), the underlying causal connection and associated assumptions are more explicit in this context. Many models I’ve seen in consulting struck me as arbitrary as far as which covariate served as the mediator, required a fairly convoluted explanation for its justification, or ignored other relevant variables because the reasoning would have to include a plethora of indirect effects if it were to be logically consistent. Furthermore, I can often ask one or two questions and will discover that researchers are actually interested in interactions (i.e. moderation), rather than indirect effects. This document will not get into models that have moderated mediation and mediated moderation. In my experience these are often associated with models that are difficult to interpret at best, or are otherwise not grounded very strongly in substantive concerns14. However, there are times, e.g. in experimental settings, which surprisingly few SEM are applied to for reasons unknown, where perhaps it would be very much appropriate. It is left to the reader to investigate those particular complexities when the time comes. Too often investigators will merely throw in these indirect effects and simply report whether the effect is significant or not. If you hide their results and ask if the indirect effect is positive or negative, I suspect they might not even know. Even if not used, I think the perspective of the potential outcomes framework can help one think appropriately about the situation, and avoid unnecessary complications. If one wants to explore possible indirect paths, there are better approaches which are still rooted within the structural causal model framework, which we’ll see shortly. In addition, a mediation model should always be compared to a simpler model with no mediation (e.g. via AIC). Terminology issues Some refer to models with indirect effects as mediation models15, and that terminology appears commonly in the SEM (and esp. psychology) literature16. Many applied researchers just starting out with SEM often confuse the term with moderation, which is called an interaction in every other statistical modeling context. As you begin your SEM journey, referring to indirect effects and interactions will likely keep you clear on what you’re modeling, and perhaps be clearer to those who may not be as familiar with SEM when you present your work. In models with moderation, one will often see a specific variable denoted as the moderator. However, this is completely arbitrary in the vast majority of situations in which it is used. In a typical model that includes an interaction, it makes just as much sense to say that the \\(\\mathcal{A} \\rightarrow \\mathcal{Y}\\) relationship varies as a function of \\(\\mathcal{B}\\) as it does the \\(\\mathcal{B} \\rightarrow \\mathcal{Y}\\) relationship varies as a function of \\(\\mathcal{A}\\). Some suggest using the term moderation when only when there is a potential causal relation, almost always temporal in nature, such that the moderator variable precedes the variable in question. In addition, the moderator and the variable moderated must be uncorrelated17. While such restrictions would render almost every model I’ve ever seen that uses the term moderation as an ordinary interaction instead, the idea is that the moderator has a causal effect on the path between the variable in question and its target. This is why you see graphs like the one to the left below, which has no tie to any statistical model and generally confuses people more often than it illuminates (the correct graph is to the right). Again though, the actual model is statistically indistinguishable from a standard model including an interaction, so if you want to understand moderation, you simply need to understand what an interaction is. --> Like SEM relative to standard modeling techniques, so too is mediation and moderation to indirect effects and interactions. Statistically they are indistinguishable models. Theoretically however, there is an underlying causal framework, and in some cases specific causal claims can be tested (e.g. via simulation). Recently I have also heard the phrase ‘spotlight analysis’ to refer to a categorical by continuous variable interaction. Please don’t apply new terms to very old techniques in an attempt to make a model sound more interesting18. The work should speak for itself and be as clearly expressed as possible. The gist is that if you want to talk about your relations using the terms ‘mediation’ and ‘moderation’ you’ll want to have specific and explicit causal relations among the variables. If you are unsure, then the answer is clearly that you don’t have that situation. However, you can still incorporate interactions as usual, and explore potential indirect effects by modeling the correlations, e.g. leaving the indirect path undirected. Aside: Instrumental variables Path analysis of a different sort has a long history in econometrics. A very common approach one may see utilized is instrumental variable analysis. The model can be depicted graphically as follows: In the graph, \\(\\mathcal{Y}\\) is our target variable, \\(\\mathcal{X}\\) is the causal variable of interest, and \\(\\mathcal{Z}\\) the instrument. The instrumental variable must not be correlated with disturbance of the target variable (later depicted as \\(\\mathcal{U}\\)), and only has an indirect relation to \\(\\mathcal{Y}\\) through \\(\\mathcal{X}\\). Neither target nor causal variable are allowed to effect the instrument directly or indirectly. In graph theory, this involves the notion of conditional independence we’ve mentioned before, where \\(\\mathcal{Z}\\) is conditionally independent of Y given X, or in other words, if we remove the \\(\\mathcal{X} \\rightarrow \\mathcal{Y}\\) path, \\(\\mathcal{Z}\\) and \\(\\mathcal{Y}\\) are independent. Furthermore, \\(\\mathcal{Z}\\) is not conditionally independent of \\(\\mathcal{X}\\) with the removal of that path19. The motivating factor for such a model is that the simple \\(\\mathcal{X} \\rightarrow \\mathcal{Y}\\) path may not provide a means for determining a causal relationship, particularly if \\(\\mathcal{X}\\) is correlated with the error. As an example20, if we look at the relationship of earnings with education, the unspecified causes of earnings might include things, e.g. some general ‘ability’, that would be correlated with the potential causal variable of education attained. With error \\(\\mathcal{U}\\) we can show the following diagram that depicts the problem. When \\(\\mathcal{X}\\) is endogenous, that is not the case. Changes in \\(\\mathcal{X}\\) are associated with \\(\\mathcal{Y}\\), but also \\(\\mathcal{U}\\). There are now multiple effects of \\(\\mathcal{X}\\) on \\(\\mathcal{Y}\\), both directly from \\(\\mathcal{X}\\) and indirectly from \\(\\mathcal{U}\\). We can measure the association between \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\), but not the causal relationship. An attempt can be made to remedy the problem by using an instrument, leading the following graph. The instrument \\(\\mathcal{Z}\\) affects \\(\\mathcal{X}\\) and thus is correlated with \\(\\mathcal{Y}\\) indirectly. However, unlike \\(\\mathcal{X}\\), it is uncorrelated with \\(\\mathcal{U}\\). This assumption excludes \\(\\mathcal{Z}\\) from having a direct effect in the model for \\(\\mathcal{Y}\\). If instead \\(\\mathcal{Y}\\) depended on both \\(\\mathcal{X}\\) and \\(\\mathcal{Z}\\) but only \\(\\mathcal{X}\\) has a direct path to \\(\\mathcal{Y}\\), then \\(\\mathcal{Z}\\) is basically absorbed into the error so would be correlated with it. With the direct path from \\(\\mathcal{Z}\\) to \\(\\mathcal{Y}\\) not allowed, we are left with only its influence on \\(\\mathcal{X}\\) . Such models were commonly fit with a technique known two-stage least squares, which is demonstrated here. I use a subset of the CigarettesSW data set and ivreg function from the AER package for comparison. The data concerns cigarette consumption for the 48 continental US States from 1985–1995, though only the last year is used for our purposes. Here we examine the effect on packs per capita (packs) of price (rprice), using state level income (rincome) as the instrumental variable. See the help file for the CigarettesSW for more on this data. head(Cigarettes) # raw, logged values actually used in models state rprice rincome tdiff tax cpi packs year 1 AL 103.9182 12.91535 0.9216975 40.50000 1.524 101.08543 1995 2 AR 115.1854 12.16907 5.4850193 55.50000 1.524 111.04297 1995 3 AZ 130.3199 13.53964 6.2057067 65.33333 1.524 71.95417 1995 4 CA 138.1264 16.07359 9.0363074 61.00000 1.524 56.85931 1995 5 CO 109.8097 16.31556 0.0000000 44.00000 1.524 82.58292 1995 6 CT 143.2287 20.96236 8.1072834 74.00000 1.524 79.47219 1995 # explicit 2sls mod0 = lm(rprice ~ rincome , data = Cigarettes) lm(packs ~ fitted(mod0), data = Cigarettes) Call: lm(formula = packs ~ fitted(mod0), data = Cigarettes) Coefficients: (Intercept) fitted(mod0) 7.919 -0.707 ## using AER package ivmod = AER::ivreg(packs ~ rprice | rincome, data = Cigarettes) summary(ivmod) Call: AER::ivreg(formula = packs ~ rprice | rincome, data = Cigarettes) Residuals: Min 1Q Median 3Q Max -0.64455 -0.08910 0.01721 0.11352 0.45742 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 7.9191 2.0819 3.804 0.000419 *** rprice -0.7070 0.4354 -1.624 0.111265 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2006 on 46 degrees of freedom Multiple R-Squared: 0.3351, Adjusted R-squared: 0.3207 Wald test: 2.637 on 1 and 46 DF, p-value: 0.1113 ymod = lm(packs ~ rincome, data = Cigarettes) We can see that these both produce the same results, though actually doing the piecemeal regression would lead to problematic standard errors. However, by using the fitted values from the regression of \\(\\mathcal{X}\\) on \\(\\mathcal{Z}\\), we are now able to essentially get the effect of \\(\\mathcal{X}\\) untainted by \\(\\mathcal{U}\\). To see more clearly where the effect comes from, the coefficient of regressing packs on income is -0.347, and dividing that by the coefficient of price on income, 0.492, will provide the ultimate effect of price on the pack of cigarettes that we see above. We can run the path model in lavaan as well. Note the slight difference between what we’d normally code up for a path model. In order to get the same results as instrumental variable regression, we have to explicitly add an estimate of the packs disturbance with price. mod = &#39; packs ~ rprice rprice ~ rincome packs ~~ rprice &#39; modiv = sem(mod, data=Ciglog, meanstructure = T, se = &#39;robust&#39;) summary(modiv) lavaan (0.5-23.1097) converged normally after 50 iterations Number of observations 48 Estimator ML Minimum Function Test Statistic 0.000 Degrees of freedom 0 Parameter Estimates: Information Expected Standard Errors Robust.sem Regressions: Estimate Std.Err z-value P(&gt;|z|) packs ~ rprice -0.707 0.432 -1.638 0.101 rprice ~ rincome 0.492 0.123 3.991 0.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) .packs ~~ .rprice -0.008 0.006 -1.277 0.202 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .packs 7.919 2.075 3.816 0.000 .rprice 3.464 0.324 10.684 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .packs 0.039 0.015 2.515 0.012 .rprice 0.012 0.002 4.926 0.000 While instrumental variable regression is common, this is perhaps not a typical path model seen in SEM. With SEM, a model comparison approach might be taken to compare models of competing complexity. One issue not discussed is where instruments come from. Generally data is not collected with acquiring them in mind, so it might be very difficult to use such an analysis. Furthermore, the structured causal framework that includes SEM would generally be a more flexible approach. Heckman Selection Model A different kind of instrumental variable model comes into play with sample selection models. Very common in econometrics or those exposed to it, and practically nowhere else, in this situation, our dependent variable is censored in some fashion, such that we only observe some the population. The classic, and evidently only, textbook example you’ll find on this regards the wages of working women. Here is the description from the Stata manual. In one classic example, the first equation describes the wages of women. Women choose whether to work, and thus, from our point of view as researchers, whether we observe their wages in our data. If women made this decision randomly, we could ignore that not all wages are observed and use ordinary regression to fit a wage model. Such an assumption of random participation, however, is unlikely to be true; women who would have low wages may be unlikely to choose to work, and thus the sample of observed wages is biased upward. In the jargon of economics, women choose not to work when their personal reservation wage is greater than the wage offered by employers. Thus women who choose not to work might have even higher offer wages than those who do work—they may have high offer wages, but they have even higher reservation wages. We could tell a story that competency is related to wages, but competency is rewarded more at home than in the labor force. So the primary problem here again is bias. Now you may say- but I’m not interested in the generalizing to all women21, or, if I were really worried about exact inference there are a whole host of other things being ignored, but the economists will have none of it22. Conceptually, the model is as follows: \\[ y = X\\beta + \\epsilon\\] \\[\\mathrm{observed} = Z\\gamma + \\nu\\] \\[\\begin{pmatrix} \\epsilon \\\\ \\nu \\end{pmatrix} \\sim N\\begin{bmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} \\sigma &amp; \\rho\\\\ \\rho &amp; 1 \\end{pmatrix}\\end{bmatrix}\\] There are actually two models to consider- one that we care about, and one regarding whether we observe the target variable \\(y\\) or not. As before, we have the correlated errors issue, and, as with instrumental variable regression, we can take a two step approach here, albeit a bit differently. In the one model, we will predict whether the data is observed or not using a probit model. Then we’ll use a variable based on the fits from that model in the next step. Note these are not the fitted values, but the inverse Mills ratio, or in survival model lingo, the nonselection hazard, based on them. I show results using the sampleSelection package, and how to duplicate the process with standard R model functions. wages = haven::read_dta(&#39;data/heckman_women.dta&#39;) library(sampleSelection) wages$select_outcome = factor(!is.na(wages$wage), labels=c(&#39;not&#39;, &#39;observed&#39;)) selection_model = selection(select_outcome ~ married + children + education + age, wage ~ education + age, data=wages, method=&#39;2step&#39;) summary(selection_model) -------------------------------------------- Tobit 2 model (sample selection model) 2-step Heckman / heckit estimation 2000 observations (657 censored and 1343 observed) 11 free parameters (df = 1990) Probit selection equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -2.467365 0.192563 -12.813 &lt; 2e-16 *** married 0.430857 0.074208 5.806 7.43e-09 *** children 0.447325 0.028742 15.564 &lt; 2e-16 *** education 0.058365 0.010974 5.318 1.16e-07 *** age 0.034721 0.004229 8.210 3.94e-16 *** Outcome equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.73404 1.24833 0.588 0.557 education 0.98253 0.05388 18.235 &lt;2e-16 *** age 0.21187 0.02205 9.608 &lt;2e-16 *** Multiple R-Squared:0.2793, Adjusted R-Squared:0.2777 Error terms: Estimate Std. Error t value Pr(&gt;|t|) invMillsRatio 4.0016 0.6065 6.597 5.35e-11 *** sigma 5.9474 NA NA NA rho 0.6728 NA NA NA --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -------------------------------------------- # probit model probit = glm(select_outcome ~ married + children + education + age, data=wages, family=binomial(link = &#39;probit&#39;)) # setup for wage model probit_lp = predict(probit) mills0 = dnorm(probit_lp)/pnorm(probit_lp) imr = mills0[wages$select_outcome==&#39;observed&#39;] lm_select = lm(wage ~ education + age + imr, data=filter(wages, select_outcome==&#39;observed&#39;)) summary(probit) Call: glm(formula = select_outcome ~ married + children + education + age, family = binomial(link = &quot;probit&quot;), data = wages) Deviance Residuals: Min 1Q Median 3Q Max -2.7594 -0.9414 0.4552 0.8459 2.0427 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.467365 0.192291 -12.831 &lt; 2e-16 *** married 0.430857 0.074310 5.798 6.71e-09 *** children 0.447325 0.028642 15.618 &lt; 2e-16 *** education 0.058365 0.011018 5.297 1.18e-07 *** age 0.034721 0.004232 8.204 2.33e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2532.4 on 1999 degrees of freedom Residual deviance: 2054.1 on 1995 degrees of freedom AIC: 2064.1 Number of Fisher Scoring iterations: 5 summary(lm_select) Call: lm(formula = wage ~ education + age + imr, data = filter(wages, select_outcome == &quot;observed&quot;)) Residuals: Min 1Q Median 3Q Max -15.4167 -3.5143 -0.1737 3.5506 20.3762 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.73404 1.16621 0.629 0.529 education 0.98253 0.05050 19.457 &lt; 2e-16 *** age 0.21187 0.02066 10.253 &lt; 2e-16 *** imr 4.00162 0.57710 6.934 6.35e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.359 on 1339 degrees of freedom Multiple R-squared: 0.2793, Adjusted R-squared: 0.2777 F-statistic: 173 on 3 and 1339 DF, p-value: &lt; 2.2e-16 Dividing the coefficient for imr by \\(\\sigma\\) will give you a rough estimate of \\(\\rho\\), but the \\(\\sigma\\) from the package results is the lm sigma plus an extra component, so the rough estimate of \\(\\rho\\) from just the lm results will be a bit larger. Unfortunately there isn’t a real good way to formulate this in SEM. Conceptually it’s still an instrumental variable type of model, but the complete separation of the data, where part of the model only exists for one category means that standard approaches won’t estimate it. Probably a good thing in the end. You could however estimate the IMR, and do the second step only using robust standard errors. It still won’t be exactly the same as the sampleSelection result, but close. If you’d like to see more detail, check this out. Aside: Nonrecursive Models Recursive models have all unidirectional causal effects and disturbances are not correlated. A model is considered nonrecursive if there is a reciprocal relationship, feedback loop, or correlated disturbance in the model23. Nonrecursive models are potentially problematic when there is not enough information to estimate the model (unidentified model), which is a common concern with them. A classic example of a nonrecursive relationship is marital satisfaction: the more satisfied one partner is, the more satisfied the other, and vice versa (at least that’s the theory). This can be represented by a simple model (below). Such models are notoriously difficult to specify in terms of identification, which we will talk more about later. For now, we can simply say the above model would not even be estimated as there are more parameters to estimate (two paths, two variances) than there is information in the data (two variances and one covariance). To make this model identified, we use instrumental variables as we did before. In this scenario, instrumental variables directly influence one of the variables in a recursive relationship, but not the other. For example, a wife’s education can influence her satisfaction directly and a husband’s education can influence his satisfaction directly, but a husband’s education cannot directly impact a wife’s satisfaction and vice versa (at least for this demonstration). These instrumental variables can indirectly impact a spouses’ satisfaction. In the following model family income is also included. The dashed line represents an unanalyzed correlation. Many instances of nonrecursive models might better be represented by a correlation. One must have a very strong theoretical motivation for such models, which is probably why they aren’t seen as often in the SEM literature, though they are actually quite common in some areas such as economics, where theory is assumed to be stronger (by economists). Aside: Tracing rule In a recursive model, implied correlations between two variables, X1 and X2, can be found using tracing rules. Implied correlations between variables in a model are equal to the sum of the product of all standardized coefficients for the paths between them. Valid tracings are all routes between X1 and X2 that a) do not enter the same variable twice, and b) do not enter a variable through an arrowhead and leave through an arrowhead. The following examples assume the variables have been standardized (variance values equal to 1), if standardization has not occurred the variance of variables passed through should be included in the product of tracings. Consider the following variables, A, B, and C (in a data frame called abc) with a model seen in the below diagram. We are interested in identifying the implied correlation between x and z by decomposing the relationship into its different components and using tracing rules. cor(abc) A B C A 1.0 0.2 0.3 B 0.2 1.0 0.7 C 0.3 0.7 1.0 model = &quot; C ~ B + A B ~ A &quot; pathMod = sem(model, data=abc) coef(pathMod) C~B C~A B~A C~~C B~~B 0.667 0.167 0.200 0.478 0.950 To reproduce the correlation between A and C (sometimes referred to as a ‘total effect’): Corr = ac + ab * ac Corr = 0.667 + 0.033 Corr = 0.7 In SEM models, it’s important to consider how well our model-implied correlations correspond to the actual observed correlations. For over-identified models, the correlations will not be reproduced exactly, and thus can serve as a measure of how well our model fits the data. We’ll discuss this more later. In addition, the tracing of paths is important in understanding the structural causal models approach of Judea Pearl, of which SEM and the potential outcomes framework are part of. Bayesian Networks In many cases of path analysis, the path model is not strongly supported by prior research or intuition, and in other cases, people are willing to use modification indices after the fact to change the paths in their model. This is unfortunate, as their model is generally overfit to begin with, and more so if altered in such an ad hoc fashion. A more exploratory approach to graphical models is available however. Bayesian Networks, an approach within Pearl’s structural causal model framework and which led to its development, are an alternative to graphical modeling of the sort we’ve been doing. Though they can be used to produce exactly the same results that we obtain with path analysis via maximum likelihood estimation, they can also be used for constrained or wholly exploratory endeavors as well, potentially with regularization in place to help reduce overfitting. As an example, I use the McClelland data to explore potential paths via the bnlearn package. I make the constraints that variables later in time do not effect variables earlier in time24, no variables are directed toward background characteristics like sex, and at least for these purposes I keep math and reading at a particular time from having paths to each other. I show some of the so-called blacklist of constraints here. Note that we can also require certain paths be in the model via a whitelist. from to 1 read21 read7 2 read21 math7 3 read21 attention4 4 read21 vocab4 5 read21 adopted 6 read21 male Now we can run the model. In this case we’ll use the Grow-Shrink algorithm, which is one of the simpler available in the package, though others came to the same conclusion25. library(bnlearn) model = gs(mcclellandNoCollege, blacklist = blacklist, test=&#39;mi-g-sh&#39;) The plot of the model results shows that attention span at age 4 has no useful relationship to the other variables, something we’d already suspected based on previous models, and even could guess at the outset given its low correlations. As it has no connections, I’ve dropped it from the visualization. Furthermore, the remaining paths make conceptual sense. The parameters, fitted values, and residuals can be extracted with the bn.fit function, and other diagnostic plots, cross-validation and prediction on new data are also available. We won’t get into the details of these models except to say that one should have them in their tool box. And if one really is in a more exploratory situation, they would typically come with methods far better suited for the situation than the SEM tools, none of which I come across consider predictive ability very strongly. The discovery process with Bayesian networks can also be a lot of fun. Even if one has strong theory, nature is always more clever than we are, and you might find something interesting. See also the bnpa, autoSEM, and regsem packages for more. Undirected Graphs So far we have been discussing directed graphs in which the implied causal flow tends toward one direction and there are no feedback loops. However, sometimes the goal is not so much to estimate the paths as it is to find the structure. Undirected graphs simply specify the relations of nodes with edges, but without any directed arrows regarding the relationship. Such graphs are sometimes called Markov Random Fields. While we could have used the bnlearn package for an undirected graph by adding the argument undirected = T, there are a slew of techniques available for what is often called network analysis. Often the focus is on observations, rather than variables, and what influences whether one sees a tie/edge between nodes, with modeling techniques available for predicting ties (e.g. Exponential Random Graph models). Often these are undirected graphs, and that is our focus here, but they do not have to be. Network analysis Networks can be seen everywhere. Personal relationships, machines and devices, various business and academic units… we can analyze the connections among any number of things. A starting point for a very common form of network analysis is an adjacency matrix, which represents connections among items we wish to analyze. Often it contains just binary 0-1 values where 1 represents a connection between nodes. Any similarity matrix could potentially be used (e.g. a correlation matrix). Here is a simple example of an adjacency matrix: Bernadette David J’Sean Lagia Mancel Nancy Bernadette 1 0 1 1 1 1 David 0 1 1 0 0 1 J’Sean 1 1 1 0 1 0 Lagia 1 0 0 1 0 1 Mancel 1 0 1 0 1 0 Nancy 1 1 0 1 0 1 Visually, we can see the connections among the nodes. As an example of a network analysis, let’s look at how states might be more or less similar on a few variables. We’ll use the state.x77 data set in base R. It is readily available, no need for loading. To practice your R skills, use the function str on the state.x77 object to examine its structure, and head to see the first 6 rows, and ? to find out more about it. Here are the correlations of the variables. The following depicts a graph of the states based on the variables of Life Expectancy, Median Income, High School Graduation Rate, and Illiteracy. The colors represent the results of a community detection algorithm, and serve to show that the clustering is not merely geographical, though one cluster is clearly geographically oriented (‘the South’). Understanding Networks Networks can be investigated in an exploratory fashion or lead to more serious modeling approaches. The following is a brief list of common statistics or modeling techniques. Centrality Degree: how many links a node has (can also be ‘in-degree’ or ‘out-degree’ for directed graphs) Closeness: how close a node is to other nodes Betweenness: how often a node serves as a bridge between the shortest path between two other nodes PageRank: From Google, a measure of node ‘importance’ Hub: a measure of the value of a node’s links Authority: another measure of node importance Characteristics of the network as a whole may also be examined, e.g. degree distribution, ‘clusteriness’, average path length etc. Cohesion One may wish to investigate how network members create communities and cliques. This is similar to standard cluster analysis used in other situations. Some nodes may be isolated. Modeling ERGM: exponential random graph models, regression modeling for network data Other ‘link’ analysis Comparison A goal might be to compare multiple networks to see if they differ in significant ways. Dynamics While many networks are ‘static’, many others change over time. One might be interested in this structural change by itself, or modeling something like link loss. See the ndtv package for some nice visualization of such approaches. Summary Graphical models are a general way to formulate and visualize statistical models. All statistical models can be developed within this framework. Structured causal models provide a means to posit causal thinking with graphical models, and structural equation models may be seen as a subset of those. Path analysis within SEM is a form of theoretically motivated graphical model involving only observed variables. These models might include indirect effects and multiple outcomes of interest, and can be seen as an extension of more familiar regression models to such settings. However, path analysis is just one of a broad set of graphical modeling tools widely used in many disciplines, any of which might be useful for a particular data situation. R packages used lavaan mediation bnlearn network There are other assumptions at work also, e.g. that the model is correct and there are no other confounders.↩ Sewall Wright first used path analysis almost 100 years ago.↩ I use multivariate here to refer to multiple dependent variables, consistent with multivariate analysis generally. Some use it to mean multiple predictors, but since you’re not going to see single predictor regression outside of an introductory statistical text, there is no reason to distinguish it. Same goes for multiple regression.↩ Note this is just a syntax shortcut to running multiple models, not an actual ‘multivariate’ analysis.↩ In the past people would run separate OLS regressions to estimate mediation models, particularly for the most simple, three variable case. One paper that for whatever reason will not stop being cited is Baron &amp; Kenny 1986. It was 1986. Please do not do mediation models like this. You will always have more than three variables to consider, and always have access to R or other software that can estimate the model appropriately. While I think it is very helpful to estimate your models in piecemeal fashion for debugging purposes and to facilitate your understanding, use appropriate tools for the model you wish to estimate. Some packages, such as mediate, may still require separate models, but there is far more going on ‘under the hood’ even then. For more recent work in this area, see the efforts of Pearl and Imai especially.↩ I’ve seen some balk at this sort of description, but the fact remains that SEM does not have any more tie to causality than any other statistical tool, other than what applied researchers ascribe to their results. As mentioned before, SEM does not discover causality or allow one to make causal statements simply by conducting it. It’s just a statistical model.↩ I suspect this is likely the case for the majority of modeling scenarios in social sciences that are often dealing with fairly weak effects, where the additional complexity would typically not result in better predictive value.↩ There is a statement “All results controlled for background covariates of vocabulary at age 4, gender, adoption status, and maternal education level.” and a picture of only the three-variable mediation model. If you are surprised at this lack of information, you may not be familiar with the field of psychological research.↩ The historical reference for this approach is with Rubin and even earlier with Neyman. Some may make the distinction between potential outcomes, which are potentially observable for everyone, and counterfactuals that are inherently unobservable.↩ See the package vignette and Imai’s papers for more detail↩ In general, the recommendation is to use bootstrap estimates for the indirect effect interval estimate anyway.↩ Pearl devised a do operator, e.g. \\(p(y|\\textrm{do}(\\mathcal{X}))\\), where given some model, we can assess results as if ‘\\(\\mathcal{X}\\) had been \\(x_0\\)’, i.e. holding \\(\\mathcal{X}\\) at some value. The gist is that taking the appropriate modeling steps will allow us to investigate questions such as ‘if X had been something else’, i.e. a counterfactual.↩ Bollen &amp; Pearl (2013) list myth #2 about SEM is that SEM and regression are essentially equivalent. While I agree that we can think of them differently, I don’t find their exposition very persuasive. Their explanation of a structural model is no different than what you see for standard regression in econometrics and more rigorous statistics books. It’s not clear to me where they think a ‘residual’ in regression comes from if not from ‘unexplained causes’ not included in the model (I actually prefer stating regression models as \\(y \\sim \\mathcal{N}(X\\beta, \\sigma^2)\\) so as to make the data generating process explicit, a point mostly ignored in SEM depictions). Changing to ‘equals’ (=) to ‘assignment’ (:=) in the equation doesn’t work if you consider that most programming languages use = for assignment. Adding a picture certainly doesn’t change things, and as a consultant, I can say that expressing a model graphically first doesn’t keep people from positing models that are very problematic from a causal perspective (and may actually encourage it). The gist is that causality is something the researcher, not the data or the model, brings to the scientific table in an explicit fashion, something Bollen &amp; Pearl even state clearly in their discussion of the first myth (that SEM aims to establish causal relations from correlations). Practically speaking, an SEM will produce exactly the same output whether you have strong causal claims or not.↩ As an example, see if you can think of a situation in which an interaction between two variables on some outcome, which technically represents a difference in coefficient values, would only exist indirectly. Also realize that simply positing such a situation for two variables would mean you’d have to rule out several competing models of all possible direct and indirect paths between A, B, and their product term. Just for giggles you might also consider that either A or B might have more than two categories, and so must be represented by multiple dummy coded variables and multiple interaction terms. Have fun!↩ Kline distinguishes indirect effects vs. mediation. I don’t, because I don’t believe SEM to have any inherently greater ability to discern causality than any other framework, and it doesn’t. I do like his suggestion that mediation be reserved for effects involving time precedence, and defaulting to ‘indirect effects’ should keep you out of trouble. But even then, and as physics dictates, the arrow of time is inconsequential on a macro level. All else being equal, your model will fit as well with time 2 \\(\\rightarrow\\) time 1 as time 2 \\(\\leftarrow\\) time 1.↩ Along with the notion of buffering, which is a near useless term in my opinion.↩ See for example, Wu &amp; Zumbo (2008) Understanding and using mediators and moderators. (link)↩ Unless you want to signal to people who are statistically knowledgeable that they should stop paying attention to your work. In general, it’s probably best to avoid using the applied literature as a basis for how to conduct or report methods. While excellent examples may be found, many fields have typically ignored their own quantitative methodologists and statisticians for decades. Psychology is rather notorious in this regard, while I might point to political science as a counter example, in the sense that the latter has long had one of its most popular journals a methodological one.↩ In Pearl’s language, Z is d-separated from Y if the X -&gt; Y path is removed, but not d-separated from X. The d stands for directional.↩ Some of this example comes from Cameron and Trivedi’s Microeconometrics, which has a pretty clean (in my opinion) depiction of instrumental variable analysis.↩ I’m curious how common it is that you would have enough fully complete X, but no Y to even model the missingness. The usual situation I see is either complete missingness or, far more commonly, missingness all over the data, which means you’d have to model the missingness in X in order to model they Y. In other words, you’d have a general missingness problem to consider to even be able to do the initial probit model. Stuff like this seems endemic to instrumental variable analysis. In the standard IV model, it’s all well and good, if you have an instrument. But as previously mentioned, no one collects data thinking about getting some instrumental variables just in case, and if you don’t get it right, which is impossible to tell, you’ve likely opened up any number of back doors in a causal sense. And if you think economists have it all figured out, do a search on ‘rainfall’ ‘tv’ and ‘autism’, or perhaps just use common sense and know those three things should not go together in a model.↩ Economists are like Vogons. They do good work, efficient, consistent, and without bias, but in the end no one wants anything to do with their poetry.↩ No, ‘non’-recursive as a name for these models makes no sense to me either.↩ Note that if this approach were able to uncover causal relations via statistical means, a blacklist would not be necessary. If you actually tried it here, you’d find adoption status influenced by early adult scholastic abilities.↩ I have no bone to pick nor know the authors, but after seeing the models thus far, at this point I have to question the major conclusion of the McClelland et al. paper, which focuses on the effects of attention span on these outcomes, and concludes the indirect effects are notable and that there is a noteworthy direct effect on math at age 21. To begin, the attention span variable starts with no strong linear correlation with either outcome. They admit no direct effect on reading, but then suggest there is still an indirect effect even though it is practically zero and the bulk of it clearly is the read7 \\(\\rightarrow\\) read21 path. If demographics only predict read21 (again this isn’t clear from their paper, but such a model does duplicate the few values they actually report), then the R2 for read7 is ~ 1.6%. Thus I can’t think of any way to conclude a direct or indirect effect of attention span at age 4 on reading at 21. The math results are only marginally better, but even there the internal fit indices are poor (e.g. RMSEA etc.). Had a model comparison approach been used, AIC would not have chosen the mediation model in either case. In general, if the path from a variable to a potential mediator is essentially zero to begin with, save yourself some trouble- there is no reason to test for indirect effects (zero times any value is still zero).↩ "],
["latent-variables-1.html", "Latent Variables Dimension Reduction/Compression Constructs and Measurement Models Other issues in Factor Analysis Terminology Some Other Uses of Latent Variables Summary R packages used", " Latent Variables Not everything we want to measure comes with an obvious yardstick. If one wants to measure something like a person’s happiness, what would they have at their disposal? Are they smiling? Did they just get a pay raise? Are they interacting well with others? Are they relatively healthy? Any of these might be useful as an indicator of their current state of happiness, but of course none of them would tell us whether they truly are happy or not. At best, they can be considered imperfect measures. If we consider those and other indicators collectively, perhaps we can get an underlying measure of something we might call happiness, contentment, or some other arbitrary but descriptive name. Despite the above depiction of a latent variable, which is consistent with how they are typically used within psychology, education and related fields, the use of latent variable models is actually seen all over, and in ways that may have little to do with what we will be mostly focusing on here26. Broadly speaking, factor analysis can be seen as a dimension reduction technique, or as an approach to modeling measurement error and understanding underlying constructs. We will give some description of the former while focusing on the latter. Dimension Reduction/Compression Before getting into what we’ll call measurement models in the SEM context, we can first take a look at things from a more general perspective, especially in terms of dimension reduction. Many times we simply have the goal of taking a whole lot of variables, reducing them to much fewer, but while retaining as much information about the originals as possible. For example, this is an extremely common goal in areas of image and audio compression. Statistical techniques amenable to these approaches are commonly referred to as matrix factorization. Principal Components Analysis Probably the most commonly used factor-analytic technique is principal components analysis (PCA). It seeks to extract components from a set of variables, with each component containing as much of the original variance as possible. Equivalently, it can be seen as producing projections on a lower dimensional subspace that have a minimum distance from the original data. Components are linear combination of the original variables. PCA works on a covariance/correlation matrix, and it will return as many components as there are variables that go into the analysis. Each subsequent component accounts for less variance than the previous component, and summing all components results in 100% of the total variance in the original data accounted for. With appropriate steps, the components can completely reproduce the original data/correlation matrix. However, as the goal is dimension reduction, we only want to retain some of these components, and so the reproduced matrix will not be exact. This however gives us some sense of a goal to shoot for, and the same idea is also employed in factor analysis/SEM, where we also work with the covariance matrix and prefer models that can closely reproduce the original correlations seen with the observed data. Visually, we can display PCA as a graphical model. Here is one with four components/variables. The size of the components represents the amount of variance each accounts for. Let’s see an example. The following regards a very small data set27 5 socioeconomic indicators for 12 census tracts in Los Angeles (a classic example from Harman, 1967). We’ll use the psych package, and the principal function within it. To use the function we provide the data (available via the package), specify the number of components/factors we want to retain, and other options (in this case, a rotated solution might be a little more interpretable28, but is typically not employed in PCA, so we specify ‘none’). The psych package gives us more options and a little more output than standard PCA packages and functions, and one that is more consistent with the factor analysis technique we’ll spend time with later. While we will also use lavaan for factor analysis to be consistent with the SEM approach, the psych package is a great tool for standard factor analysis, assessing scale reliability, and other fun stuff. For the PCA, we’ll retain three components and use no rotation, and we’ll also focus on a standardized solution. Not doing so would result in components favoring variables with more variance relative to others. As such standardization is almost always conducted as a pre-processing step for PCA, though here it is an option to specify as part of the function (covar=F). library(psych) pc = principal(Harman.5, nfactors=3, rotate=&#39;none&#39;, covar = F) pc Principal Components Analysis Call: principal(r = Harman.5, nfactors = 3, rotate = &quot;none&quot;, covar = F) Standardized loadings (pattern matrix) based upon correlation matrix PC1 PC2 PC3 h2 u2 com population 0.58 0.81 0.03 0.99 0.0114 1.8 schooling 0.77 -0.54 0.32 0.99 0.0130 2.2 employment 0.67 0.73 0.11 0.99 0.0075 2.0 professional 0.93 -0.10 -0.31 0.97 0.0250 1.2 housevalue 0.79 -0.56 -0.06 0.94 0.0583 1.8 PC1 PC2 PC3 SS loadings 2.87 1.80 0.21 Proportion Var 0.57 0.36 0.04 Cumulative Var 0.57 0.93 0.98 Proportion Explained 0.59 0.37 0.04 Cumulative Proportion 0.59 0.96 1.00 Mean item complexity = 1.8 Test of the hypothesis that 3 components are sufficient. The root mean square of the residuals (RMSR) is 0.02 with the empirical chi square 0.07 with prob &lt; NA Fit based upon off diagonal values = 1 First focus on the portion of the output where it says SS loadings . The first line is the sum of the squared loadings29 for each component (in this case where we are using a correlation matrix, summing across all 5 possible components would equal the value of 5). The Proportion Var tells us how much of the overall variance the component accounts for out of all the variables (e.g. 2.87 / 5 = 0.57). The Cumulative Var tells us that all 3 components make up over 98% the variance. The others are the same thing just based on the 3 retained components rather than all 5 variables. We can see that each component accounts for a decreasing amount of variance. Loadings, also referred to as the pattern matrix, in this scenario represent the estimated correlation of an item with its component, and provide the key way in which we interpret the factors. As an example, we can reproduce the loadings by correlating the observed variables with the estimated component scores, something we’ll talk more about later. cor(Harman.5, pc$scores) %&gt;% round(2) PC1 PC2 PC3 population 0.58 0.81 0.03 schooling 0.77 -0.54 0.32 employment 0.67 0.73 0.11 professional 0.93 -0.10 -0.31 housevalue 0.79 -0.56 -0.06 It can be difficult to sort it out just by looking at the values, so we’ll look at it visually. In the following plot, stronger loadings are indicated by blue, and we can see the different variables associated with different components. Interpretation is the fun but commonly difficult part. As an example, PC1 looks to be mostly our socioeconomic status of the tract. Any tract with a high score on that component will have high values on all variables. It’s worth mentioning the naming fallacy at this point. Just because we associate a factor with some concept, doesn’t make it so. The underlying cause of the result could for example merely be due to population itself. Some explanation of the other parts of the output: h2: the amount of variance in the item/variable explained by the (retained) components. It is the sum of the squared loadings, a.k.a. communality. For example, population is almost completely explained by the three components u2: 1 - h2 com: A measure of complexity. A value of 1 might be seen for something that loaded on only one component, and zero otherwise (a.k.a. perfect simple structure). We can get a quick graphical model displayed as follows30: PCA may not be the best choice in this scenario, nor likely is this the most interpretable solution. One issue with PCA is that this graphical model assumes the observed variables are measured without error. In addition, the principal components do not correlate with one another by default, but it seems likely that we would want to allow the latent variables to do so in many situations, perhaps including this one (a different rotation would allow this). However, if our goal is merely to reduce the 24 items to a few that account for the most variance, this would be a standard technique. Factor Analysis Now let’s examine what is sometimes called common factor analysis, and also sometimes exploratory factor analysis in the social sciences, and even ‘classical’ or ‘traditional’, but typically just factor analysis (FA) everywhere else. While we can use both PCA and FA for similar reasons (dimension reduction) and even have similar interpretations (in terms of loadings), there are some underlying subtleties between the two that provide unique distinctions31. Noting these distinctions with some detail will require some matrix notation, but for readers not so keen on such presentation they may note the images and concluding points. First let’s revisit PCA, where we can depict it conceptually as an approach where we attempt to approximate the correlation matrix in terms of the product of components, represented by our loading matrix \\(L\\)32. \\[R \\approx LL&#39;\\] and \\[C = XW\\] In other words, each component score \\(C\\), i.e. the score for a particular observation with regard to the component, is a weighted combination of the \\(p\\) observed variables \\(X\\), the weights (with weight/loading matrix \\(W\\)) of which are determined by the loadings, but yet do not say anything about the correlations between the variables. We can use those component loadings to approximate the correlation matrix, or reproduce it exactly if we retain all the components possible. The following demonstrates this. pc_all = principal(Harman.5, nfactors=5, rotate=&#39;none&#39;, covar = F) all.equal(tcrossprod(loadings(pc_all)), cor(Harman.5)) [1] TRUE If we return to our causal thinking from before, the causal flow originates with the observed variables to the components. Perhaps now it is clearer as to the interpretation of the loadings we had before, as correlations with components. In fact, we see now that the loadings are used to create the estimated component scores in the first place. Things are different with factor analysis. Now the causal flow is in the other direction, originating with the latent variables. \\[X \\approx FW\\] Each observed variable \\(x\\) is a function of the latent variables that it is associated with. In addition, we also take into account the uniquenesses \\(\\Psi\\), or that part which the factors do not explain. And to reproduce the correlation matrix: \\[R \\approx LL&#39; + \\Psi\\] So if we just use the loadings from the FA, we cannot reproduce the correlation matrix exactly, we need to add the uniquenesses as well. If you look at the pc_all results, you’ll note that the uniquenesses are all zero, but this is not the case with factor analysis. fa_all = fa(Harman.5, nfactors=5, rotate=&#39;none&#39;, covar = F, fm=&#39;old.min&#39;) all.equal(tcrossprod(loadings(fa_all)), cor(Harman.5)) # doesn&#39;t reproduce cormat [1] &quot;Mean relative difference: 0.0186086&quot; # add uniquenesses all.equal(tcrossprod(loadings(fa_all)) + diag(fa_all$uniquenesses), cor(Harman.5)) [1] TRUE What this amounts to conceptually are a few key ideas: Factor analysis focuses on covariance. PCA focuses on variance. Factors are the cause of the observed variables, variables are the cause of components. Factor analysis does not assume perfect measurement of observed variables. Let’s now do the factor analysis for the same Harman 5 data. fac = fa(Harman.5, nfactors=3, rotate=&#39;none&#39;) fac Factor Analysis using method = minres Call: fa(r = Harman.5, nfactors = 3, rotate = &quot;none&quot;) Standardized loadings (pattern matrix) based upon correlation matrix MR1 MR2 MR3 h2 u2 com population 0.61 0.78 -0.05 0.99 0.014 1.9 schooling 0.74 -0.54 0.23 0.89 0.114 2.1 employment 0.70 0.71 0.10 0.99 0.005 2.0 professional 0.88 -0.13 -0.11 0.80 0.199 1.1 housevalue 0.78 -0.59 -0.14 0.97 0.025 1.9 MR1 MR2 MR3 SS loadings 2.78 1.77 0.09 Proportion Var 0.56 0.35 0.02 Cumulative Var 0.56 0.91 0.93 Proportion Explained 0.60 0.38 0.02 Cumulative Proportion 0.60 0.98 1.00 Mean item complexity = 1.8 Test of the hypothesis that 3 factors are sufficient. The degrees of freedom for the null model are 10 and the objective function was 6.38 with Chi Square of 54.25 The degrees of freedom for the model are -2 and the objective function was 0 The root mean square of the residuals (RMSR) is 0 The df corrected root mean square of the residuals is NA The harmonic number of observations is 12 with the empirical chi square 0 with prob &lt; NA The total number of observations was 12 with Likelihood Chi Square = 0 with prob &lt; NA Tucker Lewis Index of factoring reliability = 1.318 Fit based upon off diagonal values = 1 Measures of factor score adequacy MR1 MR2 MR3 Correlation of scores with factors 0.99 0.99 0.83 Multiple R square of scores with factors 0.99 0.99 0.69 Minimum correlation of possible factor scores 0.98 0.97 0.38 Once again, we’ll depict the loadings visually. We have a notably different interpretation with this approach. To begin, our first two factors account for roughly the same amount of variance. Secondly, none reflect all the variables simultaneously. For example, the first might reflect the wealth of census tracts, while the second size. The third latent variable has a murky interpretation at best, but is mostly reflective of education. However, for identifiability reasons, a topic we will address more later, technically 3 factors is too many for five variables without imposing constraints on some of the parameters. Correlated constructs Another thing to keep in mind with factor analysis is that the default setting33 allows correlated latent variables, unlike with PCA, which instead produces orthogonal components by default. This leads to an issue interpreting the loadings as ‘mere correlations’ of observed variables to the latent variable. If we had only one factor, or forced the factors to have zero correlation as is the case with PCA, we could still interpret the loadings as such. But now, even if an observed variable has a loading of 0 for Factor 1, it still may be correlated with that factor because it has a strong loading for Factor 2 and Factor 2 is correlated with Factor 1. As such, with factor analysis (or PCA with non-orthogonal rotations) we distinguish the following definitions that will carry over to the SEM setting: pattern coefficients: these are the standard loadings you see structure coefficients: the correlation of an observed variable with a factor that reflects any association, causal or otherwise. As noted these are identical if there is only one factor or the factors are orthogonal. To demonstrate the difference, let’s rerun the analysis with 2 factors and allow them to correlate. By default the fa function uses a oblimin rotation that will do this. Then we can examine the structure coefficients versus the loadings. Note that when you run this you will get a warning, which amounts to telling you 12 observations isn’t enough. I also add a little bit, [], to get rid of the psych package’s unfortunate default printing of these matrices, which includes information beyond the matrices themselves. fac2 = fa(Harman.5, nfactors=2) fac2$loadings[] %&gt;% round(2) MR1 MR2 population -0.07 1.01 schooling 0.88 -0.07 employment 0.06 0.97 professional 0.76 0.36 housevalue 1.00 -0.09 fac2$Structure[] %&gt;% round(2) MR1 MR2 population 0.11 1.00 schooling 0.87 0.09 employment 0.23 0.98 professional 0.82 0.49 housevalue 0.98 0.08 The number of factors A key question, even in the context of SEM, is the number of latent variables to posit for a particular set of variables. In SEM this is usually guided by theory, but in common factor analysis, all items load on whatever number of factors we choose. In both cases some solutions will work better than others for the data at hand, and different models should be compared. The same techniques used in other modeling situations, e.g. comparing AIC, are often available. Furthermore, one should inspect the residuals of the the observed vs. reproduced correlation matrix to further understand where a particular model fails, even if it seems to be a decent fit otherwise. As an example, comparing the BIC of a two factor model vs. a one factor model would show that the two factor model is preferable (lower BIC), and aside from a couple of the variances, the residuals are quite low. fac1 = fa(Harman.5, nfactors=1, rotate=&#39;none&#39;, fm=&#39;ml&#39;) fac2 = fa(Harman.5, nfactors=2, rotate=&#39;none&#39;, fm=&#39;ml&#39;) fac1$BIC [1] 12.13712 fac2$BIC [1] -0.2618109 The main thing you need to know is that every time you use a scree plot to determine the number of factors to retain, both a kitten and a puppy will die in some gruesome fashion. Estimation in Factor Analysis There are several methods available to estimate a factor analysis. Principal Axis: essentially PCA with communalities on the diagonal of the correlation matrix. Minimum residuals: attempts to minimize the residuals of the off-diagonal elements of the correlation matrix, i.e. minimized the difference in the observed versus model implied correlation matrix. This is the default in the psych package. Maximum likelihood: the most widely used approach, assumes a normal distribution for the data generating process, but can be extended to others. Click here for some by-hand R code with comparison to lavaan output. Bayesian: the basic method extends the maximum likelihood approach as with other analyses by adding priors to the estimation process, i.e. posterior \\(\\propto\\) likelihood + prior. A different tact would estimate the factor scores as additional parameters (along with the loadings) etc. Nonparametric Factor Analysis As noted previously, one of the hard problems in factor analysis is determining the number of factors to retain, a similar issue seen in cluster analysis. Although relatively rarely used, if even heard of, among many practitioners of factor analysis, Bayesian nonparametric approaches get around this by letting the number of clusters/factors grow with the data. Sticking with factor analysis, an underlying Dirichlet or other process is assumed, and depending on the prior, new factors will be added with more or less ease. Such approaches would also be applicable to a streaming data context. See the related section in this document. Other Techniques Before leaving PCA and common factor analysis for factor analysis of the sort we’ll mostly be concerned with in SEM, I’ll mention other matrix factorization techniques that might be of use depending on your data situation, or merely of interest otherwise. In general, the use of latent variables is wide, and a number of techniques may be more suitable for certain data situations or modeling goals. Just having PCA and Factor Analysis at your disposal may be too limiting. However, many of the techniques are similar to others. For example, many have ties to PCA in various ways, as noted below. SVD: singular value decomposition. Works on a raw data matrix rather than covariance matrix, and is still a very viable technique that may perform better in a lot of situations relative to fancier latent variable models, and other more recently developed techniques. Variations on SVD are/were behind some recommender systems of the sort you come across at Amazon, Netflix etc. There is a notable tie between SVD and PCA such that SVD is actually used for PCA for computational reasons in many scenarios. ICA: Independent components analysis. Extracts non-normal, independent components. The primary goal is to create independent latent variables. Generalized PCA: PCA techniques for different types of data, e.g. binary data situations. PC Regression: combining PCA with regression in one model. NMF: non-negative matrix factorization. Applied to positive valued matrices, produces positive valued factors. Useful, for example, when dealing with counts. LSI: Latent Semantic Indexing, an early form of topic modeling. It has an equivalence to NMF. LDA: Latent Dirichlet Allocation, the typical approach for topic modeling, generalizes LSI/NMF. Essentially can be seen as discrete PCA, i.e. a special case of exponential/generalized PCA. See the topic models section for an example. Canonical correlation: an early approach to correlating two sets of variables by constructing composite variables of the corresponding sets. Standard regression, MANOVA/discriminant function analysis, and the simple Pearson correlation are all special cases. Superseded by SEM. Many others. Check the ‘other uses’ section. Summary So to summarize, we have two techniques, very similar on the surface, but with key differences between them. Both allow us to reduce the data to fewer dimensions than we start with. However, both do so in different ways and with different goals in mind. Depending on the specifics of your situation you might prefer either for dimension reduction or construct exploration. Constructs and Measurement Models As we have seen, factor analysis is a general technique for uncovering latent variables within data. While initially one might think it similar to PCA, we saw a key difference in the underlying causal interpretation, which is more explicit in the SEM setting. Now we will move beyond factor analysis as a dimension reduction technique (and fully ‘exploratory’ technique, see below), and instead present it as an approach with a potentially strong theoretical underpinning, and one that can help us assess measurement error, ultimately even leading to regression models utilizing the latent variables themselves. So let us turn to what are typically called measurement models within SEM. The underlying model can be thought of as a case in which the observed variables, in some disciplines referred to as indicators (or manifest variables) of the latent construct, are caused by the latent variable. The degree to which the observed variables correlate with one another depends in part on how much of the underlying (presumed) latent variable they reliably measure34. For each indicator we can think of a regression model as follows, where \\(\\beta_0\\) is the intercept and \\(\\lambda\\) the regression coefficient that expresses the effect of the latent variable \\(F\\) on the observed variable \\(X\\). \\[X = \\beta_0 + \\lambda F + \\epsilon\\] We will almost always have multiple indicators, and often multiple latent variables. Some indicators may be associated with multiple factors. \\[\\begin{aligned} X_1 &amp;= \\beta_{01} + \\lambda_{11} F_1 + \\lambda_{21} F_2 + \\epsilon\\\\ X_2 &amp;= \\beta_{02} + \\lambda_{12} F_1 + \\lambda_{22} F_2 + \\epsilon\\\\ X_3 &amp;= \\beta_{03} + \\lambda_{13} F_1 + \\epsilon \\end{aligned}\\] It is important to understand this regression model, because many who engage in factor analysis seemingly do not, and often think of it the other way around, where the observed variables cause the latent. In factor analysis in the SEM settings, these \\(\\lambda\\) coefficients are typically called loadings as they were before, but are in fact interpreted as any other regression coefficient- a one unit change in the latent variable results in a \\(\\lambda\\) change in the observed variable. Most factor models assume that, controlling for the latent variable, the observed variables are independent (recall our previous discussion on conditional independence in graphical models), though this is sometimes relaxed. If only one factor is associated with an item and does not correlate with any other factors, then we have a simple regression setting where the standardized coefficient is equal to the correlation between the latent variable and the observed. In addition, as we would in regression we will have residual variance, \\(\\epsilon\\) above, i.e. what the latent variable does not account for in the observed variables. This is the SEM counterpart to the uniquenesses we discussed in the standard setting, and is the same as to the the disturbances from our SEM models with fully observed variables. As there, we can think of a latent variable representing all other causes that are not the construct(s) we are positing in the model. This is why they are also sometimes depicted as circles graphically, with an arrow pointing at the observed variable. Exploratory vs. Confirmatory An unfortunate and unhelpful distinction in some disciplines is that of exploratory vs. confirmatory factor analysis (and even exploratory SEM). In any regression analysis, there is a non-zero correlation between any variable and some target variable. We don’t include everything for theoretical (and even practical) reasons, which is akin to fixing a path coefficient to zero, and here it is no different. Furthermore, most modeling endeavors could be considered exploratory, regardless of how the model is specified. As such, this distinction doesn’t tell us anything about the model, and is thus unnecessary in my opinion. As an example, in the above equations \\(X_3\\) is not modeled by \\(F_2\\), which is the same as fixing the \\(\\lambda_{23}\\) coefficient for \\(F_2\\) to \\(0\\). However, that doesn’t tell me whether the model is exploratory or not, and yet that is all the distinction refers to in a practical sense, namely, whether we let all indicators load on all factors or not. An analysis doesn’t necessarily have more theoretical weight, validity, causal efficacy, etc. due to the paths specified, though as we noted before, not having paths does convey causal interpretation. Example Let’s now see a factor analysis from the SEM approach. The motivating example for this section comes from the National Longitudinal Survey of Youth (1997, NLSY97), which investigates the transition from youth to adulthood. For this example, we will investigate a series of questions asked to the participants in 2006 pertaining to the government’s role in promoting well-being. Questions regarded the government’s responsibility for the following: providing jobs for everyone, keeping prices under control, providing health care, providing for elderly, helping industry, providing for unemployed, reducing income differences, providing college financial aid, providing decent housing, protecting the environment. Each item has four values 1:4, which range from ‘definitely should be’ to ‘definitely should not be’35. We’ll save this for the exercise. There are also three items regarding their emotional well-being (depression)- how often the person felt down or blue, how often they’ve been a happy person, and how often they’ve been depressed in the last month. These are also four point scales and range from ‘all of the time’ to ‘none of the time’. We’ll use this here. depressed = read.csv(&#39;data/nlsy97_depressedNumeric.csv&#39;) library(lavaan) modelCode = &quot; depressed =~ FeltDown + BeenHappy + DepressedLastMonth &quot; famod = cfa(modelCode, data=depressed) summary(famod, standardized=T) lavaan (0.5-23.1097) converged normally after 19 iterations Used Total Number of observations 7183 8985 Estimator ML Minimum Function Test Statistic 0.000 Degrees of freedom 0 Minimum Function Value 0.0000000000000 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all depressed =~ FeltDown 1.000 0.541 0.813 BeenHappy -0.732 0.020 -37.329 0.000 -0.396 -0.609 DeprssdLstMnth 0.719 0.019 37.992 0.000 0.388 0.655 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .FeltDown 0.150 0.007 20.853 0.000 0.150 0.339 .BeenHappy 0.266 0.006 46.489 0.000 0.266 0.629 .DeprssdLstMnth 0.201 0.005 41.606 0.000 0.201 0.571 depressed 0.292 0.010 30.221 0.000 1.000 1.000 Raw results In a standard measurement model such as this we must scale the factor by fixing one of the indicator’s loadings to one. This is done for identification purposes, so that we can estimate the latent variable variance, which is otherwise arbitrary (as is the mean, though we usually, but not always, are not concerned about). Which variable is selected for scaling is arbitrary, but doing so means that the sum of the latent variable variance and the residual variance of the variable whose loading is fixed to one equals the variance of that observed variable36. var(depressed$FeltDown, na.rm=T) # .29 + .15 [1] 0.441856 Standardized latent variable An alternative way to scale the latent variable is to simply fix its variance to one (the std.lv=TRUE results). It does not need to be estimated, allowing us to obtain loadings for each observed variable. Again, think of the SLiM setting. The loadings would be standardized coefficients where the latent construct is the standardized covariate predicting the item of interest. Standardized latent and observed With both standardized (using the summary function, set standardized=T), these loadings represent correlations between the observed and latent variables. This is the default output in the factor analysis we’d get from non-SEM software (i.e. ‘exploratory’ FA). If one is just doing a factor-analytic model, these loadings are typically reported. Standardized coefficients in a CFA are computed by taking the unstandardized coefficient (loading) and multiplying it by the model implied standard deviation of the indicator then dividing by the latent variable’s standard deviation. Otherwise, one can simply use standardized variables in the analysis, or supply only the correlation matrix. Multiple factors in SEM When we move to multiple factors, not much changes relative to what we saw in the non-SEM setting and with the one factor case in the SEM. We still specify specific paths, and we are now interested in the correlation between the two latent variables. The following data example regards a random sample of items from a test of cognitive ability. Specifically we are looking at both verbal and spatial reasoning, and we assume them to be correlated. Note that by default, most if not all SEM programs assume you want the latent variables to be correlated. In general it would be useful to compare the fit of a model with and without the correlation regardless of what theory dictates. cog_ability = psych::iqitems modelCode = &quot; verbal =~ reason.4 + reason.16 + reason.17 + reason.19 spatial =~ rotate.3 + rotate.4 + rotate.6 + rotate.8 # not necessary # verbal ~~ spatial # if you don&#39;t want them correlated # verbal ~~ 0*spatial &quot; famod = cfa(modelCode, data=cog_ability) summary(famod, standardized=T, rsq=T, nd=2) lavaan (0.5-23.1097) converged normally after 49 iterations Used Total Number of observations 1523 1525 Estimator ML Minimum Function Test Statistic 336.043 Degrees of freedom 19 P-value (Chi-square) 0.000 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all verbal =~ reason.4 1.00 0.69 0.56 reason.16 1.04 0.08 13.89 0.00 0.72 0.63 reason.17 0.83 0.07 11.50 0.00 0.58 0.43 reason.19 1.53 0.11 13.55 0.00 1.05 0.57 spatial =~ rotate.3 1.00 0.94 0.43 rotate.4 0.60 0.09 6.46 0.00 0.56 0.23 rotate.6 1.51 0.13 11.30 0.00 1.42 0.55 rotate.8 1.79 0.16 11.19 0.00 1.68 0.70 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all verbal ~~ spatial 0.32 0.04 8.35 0.00 0.49 0.49 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .reason.4 1.05 0.05 21.07 0.00 1.05 0.69 .reason.16 0.79 0.04 18.11 0.00 0.79 0.60 .reason.17 1.46 0.06 24.40 0.00 1.46 0.82 .reason.19 2.28 0.11 20.53 0.00 2.28 0.67 .rotate.3 3.80 0.16 23.90 0.00 3.80 0.81 .rotate.4 5.79 0.22 26.79 0.00 5.79 0.95 .rotate.6 4.53 0.23 19.88 0.00 4.53 0.69 .rotate.8 2.94 0.23 12.54 0.00 2.94 0.51 verbal 0.47 0.05 9.23 0.00 1.00 1.00 spatial 0.88 0.13 6.94 0.00 1.00 1.00 R-Square: Estimate reason.4 0.31 reason.16 0.40 reason.17 0.18 reason.19 0.33 rotate.3 0.19 rotate.4 0.05 rotate.6 0.31 rotate.8 0.49 To begin, we do see that the factors are notably correlated at roughly 0.49, but in general these aren’t the most spectacular results, as some items are not loading too highly on their respective construct, and some R-squares reinforce that notion. Remember also our distinction between pattern and structure coefficients though. Just because we don’t have paths from some items to the other factor doesn’t mean they aren’t correlated with that factor. Other issues in Factor Analysis Some specific factor models in SEM Hierarchical/Second-Order In hierarchical models we posit that latent variables can also serve as indicators of other latent variables. As an example, one latent variable might represent verbal cognition, another mathematical strength, a third scientific acumen, and they might be indicators for a general ‘scholastic ability’ latent variable. Graphically they can be depicted as follows: The above is equivalent to a three factor model with correlated factors. This is demonstrated with the following. HS.model = &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; HS.model2 = &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 g =~ visual + textual + speed &#39; fit1 = cfa(HS.model, data = HolzingerSwineford1939) fit2 = cfa(HS.model2, data = HolzingerSwineford1939) ## summary(semTools::compareFit(fit1, fit2)) model chisq df pvalue cfi tli aic bic rmsea srmr fit1 85.31 24 8.503e-09 0.9306 0.8958 7517 7595 0.09212 0.06521 fit2 85.31 24 8.503e-09 0.9306 0.8958 7517 7595 0.09212 0.06521 However, theoretically we may have a specific latent cause in mind and want to express it in this hierarchical manner. In other cases, model complexity necessitates the hierarchical approach. Bifactor A bifactor model might seem similar to the hierarchical model at first blush. I’ll use the previous 3-factor model example, and see if you can come up with why these are different models specifically. The gist is that now the general factor has no correlation to the others, and the specific factors \\(F_1\\), \\(F_2\\) and \\(F_3\\) account for whatever it does not. Disturbances for the factors 1-3 in the hierarchical model are now the latent factors 1-3 in the bifactor model. For more on these models and reliability statistics based on them, I highly recommend taking a look at the omega function in the psych package. It may surprise some that there are actually measures of reliability that are not called Cronbach’s alpha that have been developed since 1950. Speaking of which, see the scale development section below. Measurement invariance Measurement invariance refers generally to consistency of a construct under different settings. Unfortunately there is not one single way we can define ‘invariance’, and testing for more strict settings would often posit implausible models (my opinion). One can think of it as taking a multi-group approach in which the factor analysis is conducted separately for each group, e.g. male vs. female, but where we constrain various parameters to be equal (or not). A starting point is simply to determine whether the same factor structure holds, e.g. a one factor model in each setting. This is referred to as configural invariance. The common types of measurement invariance include the following list from least restrictive to most. Each assumes the prior setting, so if a less restrictive setup does not hold, there is no point in doing further tests. Configural: The same FA model is specified for each group. Weak: Requires equal unstandardized pattern coefficients. Rejection implies the latent variables have different meanings across groups or perhaps there is a different style of response to the items. Strong: Requires equal unstandardized intercepts for indicators. Rejection suggestion other variables unrelated to the latent variable produce a different pattern of response (Kline refers to this as differential additive response style) Strict: Includes equal (co-)variances. Required for a claim of identical latent structure. Testing for invariance involves inclusion of mean structures in the model and specific constraints pertaining to the type of invariance tested. There are numerous issues with the approach. One problem is that this necessitates more data than most seem to have that want to do this. Further, it potentially assumes an interaction with the group variable for every parameter that might be estimated. This is implausible in my opinion, and we rarely assume this elsewhere in standard model settings. Conversely, I can’t think of a viable theoretical reason to assume, or even want, that every parameter would be the same across groups outside of a formal testing situation (e.g. scholastic exams). So one is left with some sort mix and match approach or similar to guess what may or may not vary. Yet one more issue is statistical. Let’s say we want to test loading equivalence across groups. You find a global test would suggest that they are not equal. However, inspection of the interval estimates for each loading might suggest considerable overlap. And finally, I’ve not seen any sort of regularization applied to testing all these parameters, which would typically be several dozen parameters even in the simplest of settings. If you want to examine group specific output, it’s no change for the lavaan approach, you simply add the argument group='x' to get results for multiple groups for grouping variable ‘x’. In the semTools package, the measurementInvariance function works in the exact same way, but returns the statistical test for each of the versions of invariance mentioned above, plus another that restricts the means as well. cfa(HS.model, data=HolzingerSwineford1939, group=&quot;school&quot;) %&gt;% summary(ci=T) semTools::measurementInvariance(HS.model, data=HolzingerSwineford1939, group=&quot;school&quot;, strict=T) Some also refer to stability or measurement invariance over time in the longitudinal setting. This in fact important to test, but difficult when there is drop out. For example, is it that the construct changes or simply you aren’t able to capture the same individuals (even with some sort of imputation approach)? Furthermore, if there is a large separation in time, it may be possible to get similar results but the constructs are in fact different, a form of the naming fallacy problem discussed previously. The semTools package also has a longInvariance for the longitudinal setting. Scale development A good use of factor analysis regards scale development. If we come up with 10 items that reflect some underlying construct, factor analysis can provide a sense of how well the scale is put together. Recall that in path analysis, residual variance, sometimes called disturbance37, reflects both omitted causes as well as measurement error. In this context, \\(1-R^2_{item}\\) provides a sense of how unreliable the item is. A perfectly reliable item would be perfectly explained by the construct it is a measure of. Strong loadings indicate a strong relationship between the item and the underlying construct. The following diagram shows how the variance breaks down (from Kline). Reliability Please repeat the following 10 times, possibly while in the lotus position. Reliability is not a constant. There is no single concept of reliability. As a motivating example, say we derive a scale for ‘stick-to-it-tiveness’ and did so based on a largely white, young adult, somewhat educated, middle class sample of Americans38. Let’s say the reliability for the scale was off the charts based on whatever statistic you want to use, and you are most pleased. Now answer the following… How reliable do you think the scale is on a sample of: native Chinese speakers? a sample of Londoners? a sample of older adults from the Texas panhandle? a sample of women only? a sample of second generation Hispanic Americans? a sample of teenagers? the same population 15 years later? the same exact people a second time? Asking it in this context makes it almost seem absurd to even think the scale would be equally effective for all these groups. While some scales are in fact applicable across a wide spectrum of individuals, assuming it to be reliable for all scenarios is at best a problematic stance. Yet common practice for utilizing scales is to simply cite the original study, possibly conducted decades ago, note that the scale was found to be ‘reliable’, possibly without even defining reliability or producing any statistic, and going on with using the scale. Furthermore, many adhere to sometimes very low, and utterly arbitrary, statistical cutoffs, largely on a single type of statistic that may not even be appropriate given the construct(s) under study. Let’s be more explicit with our example. The scale, which actually measures two aspects of stick-to-it-tiveness is developed on mostly freshman psychology students at a private school, and the Cronbach’s alpha for it is .7. To begin, even if we deem this an acceptable result, there is no a priori reason to think it will generalize to even a more equitable population of college students at the same school, much less adults out of this age range or any number of other specific populations we could think of. In addition, there is a notable portion of the variance of the items that isn’t related to the construct, and one wonders whether that is in fact a good result. And finally, our statistic of choice is actually a bit problematic if there is no general factor beyond the two aspects. Classical Test Theory The underlying idea behind many measures of reliability is the following simple equation: \\[O = T + E\\] In words, the observed score equals the true score plus measurement error. One of the key consequences of measurement error is attenuation of correlation. If \\(\\mathcal{X^*}\\) and \\(\\mathcal{Y^*}\\) are the true scores, and \\(X\\) and \\(Y\\) are the ones we actually observe and contain measurement error, \\(\\mathrm{cor}_{\\mathcal{X^*Y^*}}\\) will always be greater than \\(\\mathrm{cor}_{XY}\\), and using less reliable measures can only hurt any analysis that seeks to find correlations among the data (i.e. every analysis). Examining a scale’s reliability is an attempt to understand the measurement error involved. Even if you aren’t working with multi-item scales, the problem doesn’t go away, and you should think hard about how accurately your variables represent what they intend to. Measures of Reliability There are many, many measures of reliability. Here are a couple. Parallel Measures/Split-half: Many reliability measures rely on the notion of parallel tests, such that reliability can be seen the correlation between two parallel forms of a test. More explicitly, consider a single test with items randomly split into two sets. The correlation between those two tests can be seen as a measure of reliability. Guttman’s \\(\\lambda_6\\): is based on the R2 of each item regressed on the others, and this squared multiple correlation can be seen as a lower bound on the communality. The \\(\\lambda_6\\) can be seen as a lower bound on split-half reliability. Greatest lower bound*: Multiple versions. One is the greatest split half reliability (essentially Guttman’s \\(\\lambda_4\\)). Another is conceptually 1 - error/total variance, similar to \\(\\omega_{\\mathrm{t}}\\) but will actually not necessarily be as ‘great’ as \\(\\omega_{\\mathrm{t}}\\). \\(\\beta\\): based on the average correlation among worst split that produces the least inter-item correlations. Somewhat improves on \\(\\alpha\\) as a measure of factor saturation but others do better in that regard. Cronbach’s \\(\\alpha\\): the most widely used/abused measure of reliability. Equivalent to the earlier measure by Guttman (\\(\\lambda_3\\)), and it is a generalization of measure for dichotomous items developed in 1937 (\\(\\mathrm{KR}_{20}\\)). Issues: underestimates reliability and overestimates factor saturation. Not so useful for multidimensional scales. \\(\\omega\\)*: from McDonald, a measure of ‘factor saturation’. \\(\\omega_{\\mathrm{t}}\\), or omega total, provides a proportion of variance that is not unique in the factor-analytic sense. \\(\\omega_{\\mathrm{h}}\\), or omega-hierarchical, is the measure associated with the ‘general’ factor in the bifactor model noted above. It assumes enough items for a potential hierarchical structure to be tested in the first place. Composite Reliability*: described in Kline, reference Raykov (2004), see \\(\\omega\\) Average Variance Extracted*: For single factor indicator settings, this is the average squared (standardized) loading. It’s simple and has a straightforward interpretation as R2 does in regression. Both Guttman’s and Cronbach’s measures are a function of the number of items and average correlation of items, and will increase if either of those increase all else being equal. Neither should be seen as indicating a single factor structure (they assume it). My own opinion is that the most sensical notion of reliability is a function of some underlying model that can actually be tested, i.e. a latent variable model. Furthermore, a good item is infused with the construct it purports to measure. McDonald’s \\(\\omega_t\\) gets at that notion explicitly. Practically speaking, if loadings are equal and the one is dealing with a unidimensional construct \\(\\alpha=\\omega_t=\\omega_h\\). Otherwise, i.e. the usual case, one should consider which measure might be best. The ones with a * are more explicitly driven by an underlying factor model. Some suggest AVE can be used as a measure of convergent validity (if it’s above some arbitrary value) or discriminant validity (if it’s square root is higher than the factor’s correlation with some other latent variable). I find this interpretation somewhat problematic. Convergent and discriminant validity have typically always been explicitly defined by a construct’s correlations with other constructs/measures that are similar/different, not individual items, which are theoretically just a random sample of all available items that measure all facets of the construct. If your items, say for a newly proposed depression scale, correlate with one another, that’s all well and good, but it doesn’t mean your scale as a whole automatically measures what you think it does, and would have the proper correlations with other measures, e.g. positive with another established depression measure or anxiety measure, and negative with one measuring self-esteem. You actually have to collect these other measures to establish it. As another example, a poor AVE, i.e. one that would not suggest even construct validity (say ave = .1, sqrt = .32), could signify discriminant validity if using the proposed yardstick (&gt;.30 correlation with another factor). It also works in the opposite direction, your very high AVE may be higher than something else the construct strongly correlates with, but shouldn’t. However, the AVE is clearly related to the notion of construct validity in general, i.e. whether the items measure what they purport to measure, and would be a useful first step in that regard. If one collects other measures they might then test for convergent and discriminant validity, further establishing construct validity. In the psych package you can use functions splitHalf, alpha, and omega to get these and other measures, though some are only of historical interest. If you are going to report With semTools, use the reliability function on a lavaan class object. See example below. Because of the amount of output, I only show the results for the last line. ## omega(cog_ability, nfactors=4) ## alpha(scale(Harman.5)) ## ## library(semTools) ## reliability(famod) # omega1 is the omega total, omega 3 is omega hierarchical Scale development as software development Scale development is hard and a scale can always be improved, always. There is practically no chance that the first time one comes up with variables to measure a construct that those indicators will be perfect. For example, in a scale that measures some internal construct, such as some aspect of personality, things like item wording, difficulty, context etc. all factor into how the variables will be perceived by the individual, whose own context and background will also affect how items are answered. After analysis, items might be dropped, altered, or new items added. The same goes for non-human constructs, like a democracy scale for countries or a diversity index for species. For example, even economic indicators are not going to have the same reliability from country to country, as the raw data may be harder to come by or not even applicable. Given this, it would be best to think of scale development the same as we do software development. In software development, bugs are discovered, applications to other or new versions of operating systems are constructed and so forth. The development cycle is continuous and ongoing, or else the app dies as people move on to better products. With scales, items are discovered to be problematic, applications to new segments of the population are to be developed etc. As such, it would be nice if scales were developed more regularly and came with version numbers, same as software, and not so broadly applied to very different populations. At least this way, perhaps people wouldn’t treat the scales so precious as to not be amenable to any change, and expect more from the development process39. Factor Scores In factor analysis, we can obtain estimated factor scores for each observation, possibly to be used in additional analyses or examined in their own right. One common way is to simply use the loadings as one would regression weights/coefficients (actually scaled versions of them), and create a ‘predicted’ factor score as the linear combination of the indicator variables, just as we would to produce predicted values in regression. There is no correct way to do this in standard approaches, as an infinite number of factor scores could be related to the same factor analysis results, a problem known as factor indeterminancy. However, this doesn’t mean that they aren’t useful, especially if we are employing factor analysis primarily as a data compression technique, where the scores might be used in other analyses (e.g. regression). A common reason to do so is that we might have some SEM model in mind, but too small a sample. Recent work suggests that different types of factor scores are better used in different roles, e.g. if the factor is to be used as a predictor, the regression/Thurstone method works best. If it is to be used as a dependent variable, then Bartlett factor scores will be unbiased. If using them as both, then adjustments will have to be made to avoid bias. See Hypothesis Testing Using Factor Score Regression for details (one of the authors is affiliated with lavaan). Results are compared to SEM. vs. Means/Sums On many occasions, people reduce the number of variables in a model by using a mean or sum score. These actually can be seen to reflect an underlying factor analysis where all loadings are fixed to be equal and the residual variance of the observed variables is fixed to zero, i.e. perfect measurement. If you really think the items reflect a particular construct, you’d probably be better off using a score that comes from a model that doesn’t assume perfect measurement. I did some simulations that provide more to think about regarding this point, the results of which are here. vs. Composites Composites scores are what we’d have if we turned the arrows around, and allowed different weights for the different variables, which may not be similar too similar in nature or necessarily correlated (e.g. think of how one might construct a measure of socioeconomic status). Unlike a simple mean, these would have different weights associated with the items, which might make them preferred in that comparison. As we noted previously, PCA is one way one could create such a composite. Sometimes people just make up weights to use based on what they think they should be (especially in the sporting world). This is silly in my opinion, as I can’t think of any reasonable justification for such an approach over the many available that would better represent the data. --> Some also note latent variables with causal indicators, such that the variable is latent, not a composite, but that the arrows go from the indicators as they do with the composite. However, these are not identified by default, and would still require effect indicators to be so (the so-called MIMIC model of multiple indicators and multiple causes). As such, thinking of them this way requires theory adjustment to even do, but there is no inherent reason to think of a variable with a causal effect on a latent variable as we do ‘effect’ indicators, i.e. the normal indicators that are caused by the latent variable, in the first place. Any particular latent variable might be caused any number of observed or other latent variables. That’s just standard SEM though, and doesn’t require a different name or manner of thinking, nor will it have fundamental identification problems. That’s just my opinion though. Terminology Factor Analysis: The non-PCA factor analytic technique known by various names, but not conducted as SEM, and usually estimated by maximum likelihood. Latent Variables, Factors, Constructs etc.: Terms I use interchangeably. Item, Indicator, Observed, Manifest, Variable: Terms I use interchangeably. Loadings: measures of the relationship between an indicator and the latent variable. For clarity, it’s probably better to use pattern or structure coefficient. Pattern coefficient: What is usually displayed in FA results. The path coefficient from a latent variable to some observed variable. In some cases it is a simple correlation coefficient. Structure coefficient: The correlation between an observed an latent variable. Communality: the amount of variance in the item/variable explained by the (retained) components. It is the sum of the squared loadings. Uniqueness: 1 - communality. The unexplained variance of a variable. Exploratory Factor Analysis: all factor analysis if done well. By default places no constraints on the loadings of observed variables. Confirmatory Factor Analysis: factor analysis done in the SEM context. May in fact not confirm anything. Typically places (perhaps unrealistic) constraints on the model such that some loadings/paths are set to zero. Some Other Uses of Latent Variables EM algorithm: A very common technique to estimate model parameters for a variety of model situations, it incorporates a latent variable approach where parameters of interest are treated as a latent variable (e.g. probability of belonging to some cluster). Item Response Theory: uses latent variables, especially in test situations (though is much broader), to assess things like item discrimination, student ability etc. See the related section in this document. Hidden Markov Model: A latent variable model approach commonly used for time series. Topic Model: In the analysis of text, one can discover latent ‘topics’ based on the frequency of words. See the related section in this document. Collaborative Filtering: For example, in recommender systems for movies or music, the latent variable might represent genre or demographic subsets. Gaussian Processes: A common covariance structure in GPs uses a factor analysis distance. I have Stan code demonstrating it. Multinomial Model In some more complicated multinomial regression models, a factor analytic structure is used to understand the correlations of coefficients, e.g. across category labels. Summary Latent variable approaches are a necessary tool to have in your statistical toolbox. Whether your goal is to compress data or explore underlying theoretically motivated constructs, ‘factor-analysis’ will serve you well. R packages used psych lavaan Some even use use a factor analytic approach to estimating correlations among parameters in models (e.g. I’ve seen this with gaussian processes and multinomial regression).↩ Principal components, standard factor analysis and SEM can work on covariance/correlation matrices even without the raw data, this will be perhaps demonstrated in a later version of this doc.↩ I don’t think it necessary to get into rotation here, though will likely add a bit in the future. If you’re doing PCA, you’re likely not really concerned about interpretation of loadings, as you are going to use the components for other means. It might help with standard factor analysis, but this workshop will spend time on more focused approaches where one would have some idea of the underlying structure rather than looking to uncover the structure. Rotation doesn’t change anything about the fundamental model result, so one just uses whatever leads to the clearest interpretation.↩ They are the eigenvalues of the correlation matrix. In addition, they are the diagonal of the crossproduct of the loading matrix.↩ Many often use what are called biplots for visualization of PCA, but I find the path diagram more readily understandable.↩ One version of factor analysis is nearly identical to PCA in terms of mechanics, save for what are on the diagonals of the correlation matrix (1s for PCA vs. ‘communalities’ for FA).↩ This part of the document borrows notably from the Revelle reference.↩ Unfortunately for users of SPSS, the default setting for ‘factor analysis’ in the menus is actually PCA, and this has lead to erroneous reporting of results for decades where people thought they had conducted factor analysis but in fact had not. I’ll bite my tongue and just say that this is one of many ridiculous defaults that SPSS has.↩ There are actually deep philosophical underpinnings to this approach, going at least as far back as the notion of the Platonic forms, and continuing on through philosophical debates about what mental aspects can be measured scientifically. However, even when it became a more quantitative discipline, the philosophy was not far behind. See, for example, The Vectors of Mind by L.L. Thurstone, one of the pioneers of measurement theory (1935). As a philosophy major from back in the day, latent variable modeling has always had great appeal to me.↩ For your own sake, if you develop a questionnaire, make higher numeric values correspond to meaning ‘more of’ something, rather than in this backward fashion. You’re basically begging for a less reliable measure otherwise.↩ Note that this is actually done for all disturbance/residual terms, as there is an underlying latent variable there which represents measurement error and the effect of unexplained causes. The path of that latent variable is fixed to 1, and its variance is the residual variance in the SEM output.↩ Kline distinguishes between the residuals in standard regression and disturbance in SEM (p. 131 4th ed.), but the distinction there appears to conflate the estimated variance as a parameter/construct and the actual residuals (\\(y - \\hat{y}\\)) you’d get after model estimation. A standard regression as typically estimated is no different than the same model in the graphical modeling context. Calling path analysis a causal model makes it no more causal than any other regression model, and the remaining variance is the effect of many things not in the model, and they are causal, regardless of estimation technique. I think we care more deeply about it in the SEM context, and perhaps that necessitates another name, and anything would be better than ‘error’.↩ Note that this is similar to the vast majority of psychological research, which is typically conducted on what isn’t even a random sample of college attendees.↩ The Minnesota Multiphasic Personality Inventory, or MMPI, was left unchanged for 45 years before a second version, the MMPI-2, came along in 1989. Since then one more has been released, the MMPI-2-RF. That’s like going from version 1 to 2 to 2.5 in 65 years (plus the almost 10 since the last version). Other versions have been constructed for adolescents (soon to be updated for the first time since 1992), and other languages. This slow development has occurred despite known issues for certain segments of the population, to which it continues to be applied.↩ "],
["structural-equation-modeling.html", "Structural Equation Modeling Measurement Model Structural Model The Process SEM Example Issues in SEM How to fool yourself with SEM Summary Terminology R Packages Used", " Structural Equation Modeling Structural equation modeling combines the path analytic and latent variable techniques together to allow for regression models among latent and observed variables. Any model, even the SLiM, can be seen as some form of SEM, or graphical model more generally. However, the term is typically reserved for the combination of latent and observed variables in a model, often with causal implications40. Measurement Model The measurement model refers to the latent variable models, i.e. factor analysis, and typical practice in SEM is to investigate these separately and first. The reason is that one wants to make sure that the measurement model holds before going any further with the underlying constructs. For example, for one’s sample of data one might detect two latent variables work better for a set of indicators, or might find that some indicators are performing poorly. Adjustments might then be made before moving on to the final model. Structural Model The structural model specifies the regression paths among latent and observed variables that do not serve as indicators. It can become quite complex, but at this stage one can lean on what they were exposed to with path analysis, as conceptually we’re in the same place, except now some variables may be latent. The Process Initial Considerations of Complexity Before even beginning a road to SEM, do you have enough data? SEM is a large sample technique You will even with simple models likely be estimating a couple dozen parameters, and it’s assumed that there are noisy measures and generally small effects when present. As a comparison, if you were running a standard regression in similar scenarios, how much data would you feel comfortable with if you were using a model with 20+ predictors? In SEM it can be even more difficult, where latent variables with less than ideal loadings on few items might require a couple hundred observations for just a single measurement model. In addition, as noted previously, less reliable measures will only hurt your ability to detect the relationships among variables. If you are conducting multigroup analysis, you must consider an SEM from the standpoint of the sample size for the smallest group. Because of the inherent complexity of SEM, many run SEM models that are overly simple, and actually misspecified, due to the fact that key variables, even ones that were collected and available in the data, are left out41. Sometimes models are so complex they are left out because it would be difficult to even know what all should be pointing to what. In other cases I’ve seen almost a hundred parameters estimated with maybe only 200 observations42. The gist is that SEM is not always the right tool, even if conceptually it may be considered for a model. As flexible as it is, it is also quite restrictive in other respects, and may simply be impractical to model key aspects of the data. Steps to Take Assuming SEM is the right tool for the job, the first analytical step is to look at all the observed variables on their own by examining their descriptive statistics. This also helps with debugging, such that you can find data entry errors and other issues before they get lost amidst the sea of output from an SEM result. And you certainly should know your data well enough to know that a value of 50 isn’t possible for a scale that goes 0-40. Next comes an examination of the correlations, possibly broken up to be more digestible or reflect item scales. SEM does not supernaturally produce correlations in your data, it works on the correlations that are there. If there are very low correlations, you will have a poor fit when you do the SEM that assumes stronger ones, and possibly even estimation problems. This should not in any way be surprising. Now consider your measurement models. This step itself is a two stage process (at least). First, consider every latent variable on its own, i.e. examine whether the single factor models hold up. Then you might specify correlations among those that are excepted to correlate in some fashion (whether it is a regression relationship or not doesn’t matter, don’t worry with the structural model), maybe even with observed variables. Again, if you are spotting potential issues here, they aren’t going to go away later, and in fact, the problems can actually get worse in terms of fit with the structural component. Note that if you’re using lavaan (or Mplus), you can simply write the syntax for a model with constraints specifying zero correlations, then just comment those lines out (recall that latent variables are allowed to correlate by default). Now you are ready for the SEM, though by this stage you should already be pretty knowledgeable about what to expect. You should always have multiple models, and I can think of at least three in any SEM setting. The first should be the simplest-yet-still-theoretically-plausible model. This would likely resemble a standard regression model, though including latent variables, and possibly multiple outcomes (but at this stage leave the outcomes uncorrelated and considered separately). The other model is the complex one that might include indirect effects and other posited correlations, i.e. the one that drove you to do the analysis in the first place. Ideally there would also be a model based on a competing theory, and if so, you should definitely run that. And finally, a data driven model. Perhaps your initial explorations of the data, or your interpretation of one of the other two models, suggested something else going on. Try it out, just be clear about it being more exploratory in any results summary. Theories are great and all, but nature typically proves more interesting, and you don’t want to ignore intriguing results just because they didn’t align with theory. In summary: Have a sense of complexity (vs. sample size) and consider other modeling techniques Develop a good understanding of all observed variables Inspect correlations Examine measurement models Conduct SEM with multiple competing models SEM works best with strong causal theory and settings where those claims can be tested. Such a setting is not nearly as common as SEM is used in practice. While SEM can be seen as an extension of other common modeling approaches and used primarily for that, one should think hard before using it, and know that there is no data situation that requires SEM. SEM Example The following model is a classic example from Wheaton et al. (1977), which used longitudinal data to develop a model of the stability of alienation from 1967 to 1971, accounting for socioeconomic status as a covariate. Each of the three factors have two indicator variables, SES in 1966 is measured by education and occupational status in 1966 and alienation in both years is measured by powerlessness and anomie (a feeling of being lost with regard to society). The structural component of the model hypothesizes that SES in 1966 influences both alienation in 1967 and 1971 and alienation in 1967 influences the same measure in 1971. We also let the disturbances correlate from one time point to the next. # In this example we import the covariance matrix wheaton.cov = as.matrix(read.csv(&#39;data/wheaton_cov.csv&#39;, row.names=1)) # the model wheaton.model = &#39; # measurement model ses =~ education + sei alien67 =~ anomia67 + powerless67 alien71 =~ anomia71 + powerless71 # structural model alien71 ~ aa*alien67 + ses alien67 ~ sa*ses # correlated residuals anomia67 ~~ anomia71 powerless67 ~~ powerless71 # Indirect effect IndirectEffect := sa*aa &#39; alienation &lt;- sem(wheaton.model, sample.cov=wheaton.cov, sample.nobs=932) The standardized results of the structural model are visualized below, and statistical results below that. In this case, the structural paths are statistically significant, as is the indirect effect specifically. Note that the disturbances for the endogenous latent variables reflect only unobserved causes, not measurement error, which is captured via the uniquenesses for the indicators. In other words, the path coefficients from one latent variable to another have been adjusted for measurement error. Here we see that higher socioeconomic status is affiliated with less alienation, while there is a notable positive relationship of prior alienation with later alienation. We are also accounting for roughly 50% of the variance in 1971 alienation. Colors represent positive vs. negative weights, and the closer to zero the more faded they are. lavaan (0.5-23.1097) converged normally after 73 iterations Number of observations 932 Estimator ML Minimum Function Test Statistic 4.735 Degrees of freedom 4 P-value (Chi-square) 0.316 Model test baseline model: Minimum Function Test Statistic 2133.722 Degrees of freedom 15 P-value 0.000 User model versus baseline model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 0.999 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -15213.274 Loglikelihood unrestricted model (H1) -15210.906 Number of free parameters 17 Akaike (AIC) 30460.548 Bayesian (BIC) 30542.783 Sample-size adjusted Bayesian (BIC) 30488.792 Root Mean Square Error of Approximation: RMSEA 0.014 90 Percent Confidence Interval 0.000 0.053 P-value RMSEA &lt;= 0.05 0.930 Standardized Root Mean Square Residual: SRMR 0.007 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ses =~ education 1.000 2.607 0.842 sei 5.219 0.422 12.364 0.000 13.609 0.642 alien67 =~ anomia67 1.000 2.663 0.774 powerless67 0.979 0.062 15.895 0.000 2.606 0.852 alien71 =~ anomia71 1.000 2.850 0.805 powerless71 0.922 0.059 15.498 0.000 2.628 0.832 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all alien71 ~ alien67 (aa) 0.607 0.051 11.898 0.000 0.567 0.567 ses -0.227 0.052 -4.334 0.000 -0.207 -0.207 alien67 ~ ses (sa) -0.575 0.056 -10.195 0.000 -0.563 -0.563 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .anomia67 ~~ .anomia71 1.623 0.314 5.176 0.000 1.623 0.356 .powerless67 ~~ .powerless71 0.339 0.261 1.298 0.194 0.339 0.121 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .education 2.801 0.507 5.525 0.000 2.801 0.292 .sei 264.597 18.126 14.597 0.000 264.597 0.588 .anomia67 4.731 0.453 10.441 0.000 4.731 0.400 .powerless67 2.563 0.403 6.359 0.000 2.563 0.274 .anomia71 4.399 0.515 8.542 0.000 4.399 0.351 .powerless71 3.070 0.434 7.070 0.000 3.070 0.308 ses 6.798 0.649 10.475 0.000 1.000 1.000 .alien67 4.841 0.467 10.359 0.000 0.683 0.683 .alien71 4.083 0.404 10.104 0.000 0.503 0.503 R-Square: Estimate education 0.708 sei 0.412 anomia67 0.600 powerless67 0.726 anomia71 0.649 powerless71 0.692 alien67 0.317 alien71 0.497 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all IndirectEffect -0.349 0.041 -8.538 0.000 -0.319 -0.319 To be clear, your interpretations based on standard regression and previous models with only observed variables still hold here, as the edge parameters are still regression coefficients just like they have always been. It will serve you well in the beginning to interpret each endogenous variable individually as its own model, then move toward the big picture. Regression with latent variables is the same as regression with observed variables which is the same as a mixture of them. Correlations, i.e. undirected paths, are still just correlations just like anywhere else. As for the model fit indices, I will discuss them momentarily and in turn. Issues in SEM Identification Identification generally refers to the problem of finding a unique estimate of the value for each parameter in the model. Consider the following: \\[ a + b = 2\\] There is no way for us to determine a unique solution for \\(a\\) and \\(b\\), e.g. the values of 1 and 1 work just as well as -1052 and 1054 and infinite other combinations. We can talk about 3 basic scenarios, and the problem generally regards how much information we have (in terms of (co)variances) vs. how many parameters we want to estimate in the model. A model which has an equal number of observations (again, in terms of (co)variances) and parameters to estimate would have zero degrees of freedom, and is known as a just identified model. In a just identified model there are no extra degrees of freedom leftover to test model fit. Underidentified models are models where it is not possible to find a unique estimate for each parameter. These models may have negative degrees of freedom or problematic model structures, as in the example above, and you’ll generally know right away there is a problem as whatever software package will note an error, warning, or not provide output. Overidentified models have positive degrees of freedom, meaning there is more than enough pieces of information to estimate each parameter. It is desirable to have overidentified models as it allows us to use other measures of model fit. Consider the following example in which we try to estimate a latent variable model with only two observed variables, as would be the case in the prior Alienation measurement models if they are examined in isolation. We have only two variances and one covariance to estimate two paths, the latent variable variance and the two residual variances. By convention, a path is fixed at 1 to scale the latent variable, but that still leaves us with four parameters to estimate with only three pieces of information, hence the \\(3 - 4 = -1\\) degrees of freedom and lack of standard errors in the output. modelUnder = &#39;LV =~ x1 + x2&#39; modelJust = &#39;LV =~ x1 + x2 + x3&#39; underModel = cfa(modelUnder, data=cbind(x1,x2,x3)) summary(underModel) lavaan (0.5-23.1097) converged normally after 10 iterations Number of observations 100 Estimator ML Minimum Function Test Statistic NA Degrees of freedom -1 Minimum Function Value 0.0000000000000 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) LV =~ x1 1.000 x2 0.679 NA Variances: Estimate Std.Err z-value P(&gt;|z|) .x1 0.404 NA .x2 0.740 NA LV 0.614 NA If we had a third manifest variable, we now have \\(N(N+1)/2 = 3*4/2 = 6\\) pieces of information, i.e. 3 variances and 3 covariances, to estimate the model, and still seven unknowns. Again though, we usually fix the first loading to 1 rather than estimate it, and so the model would be identified. An alternative approach would be to fix the factor variance to some value (typically 1 to create a standardized latent variable). This will allow us to estimate a unique value for each path. Even so, this is the just-identified situation, and so the model runs fine, but we won’t have any fit measures because we can perfectly reproduce the observed correlation matrix. justModel = cfa(modelJust, data=cbind(x1,x2,x3)) summary(justModel, fit=T) lavaan (0.5-23.1097) converged normally after 21 iterations Number of observations 100 Estimator ML Minimum Function Test Statistic 0.000 Degrees of freedom 0 Minimum Function Value 0.0000000000000 Model test baseline model: Minimum Function Test Statistic 90.969 Degrees of freedom 3 P-value 0.000 User model versus baseline model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.000 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -389.071 Loglikelihood unrestricted model (H1) -389.071 Number of free parameters 6 Akaike (AIC) 790.143 Bayesian (BIC) 805.774 Sample-size adjusted Bayesian (BIC) 786.824 Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent Confidence Interval 0.000 0.000 P-value RMSEA &lt;= 0.05 NA Standardized Root Mean Square Residual: SRMR 0.000 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) LV =~ x1 1.000 x2 1.417 0.281 5.033 0.000 x3 1.773 0.383 4.629 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .x1 0.724 0.113 6.424 0.000 .x2 0.432 0.113 3.840 0.000 .x3 0.220 0.151 1.457 0.145 LV 0.294 0.112 2.636 0.008 Note that in the alienation model above, we have 6*7/2 = 21 variances and covariances, which provides enough information to estimate the 17 parameters of the model. Thus had we run a two indicator measurement model on just one of those factors, it would not be identified without fixing some parameters. Allowing correlations among latent variables allows more information from the covariance matrix to be brought to bear. The important point to remember is that identification is based on the covariance matrix (and number of means for some models). You can have an N of 10,000, but still may not have enough information to estimate the model. Note that determining identification is difficult for any complex model. Necessary conditions include there being model degrees of freedom \\(\\geq 0\\), and scaling of all latent variables, but they are not sufficient. In general though, it is enough to know conceptually what the issue is and how the information you have relates to what you can estimate. Fit There are many, many measures of model fit for SEM, and none of them will give you a definitive answer as to how your model is doing. Your assessment, if you use them, should be based on a holistic approach to get a global sense of how your model is doing. Let’s look again at the alienation results. fitMeasures(alienation, c(&#39;chisq&#39;, &#39;df&#39;, &#39;pvalue&#39;, &#39;cfi&#39;, &#39;rmsea&#39;, &#39;srmr&#39;, &#39;AIC&#39;)) chisq df pvalue cfi rmsea srmr aic 4.735 4.000 0.316 1.000 0.014 0.007 30460.548 Chi-square test Estimator ML Minimum Function Test Statistic 4.735 Degrees of freedom 4 P-value (Chi-square) 0.316 Conceptually, the \\(\\chi^2\\) test measures the discrepancy between the observed correlations and those implied by the model. In the graphical model section, we actually gave an example of reproducing a correlation from a path analysis. In general, the model goal is to reproduce them as closely as we can. This test compares the fitted model with a (saturated) model that does not have any degrees of freedom. The degrees of freedom for this test are equal to the data (variances + covariances) minus the number of parameters estimated, and that is why when you have df=0 in a just-identified situation, you get no fit statistics. A non-significant \\(\\chi^2\\) suggests our model-implied correlations are not statistically different from those we observe, so yay! Or not. Those familiar with null-hypothesis testing know that one cannot accept a null hypothesis, and attempting to do so is fundamentally illogical. Other things that affect this measure specifically include multivariate non-normality, the size of the correlations (larger ones are typically related to larger predicted vs. observed discrepancies), unreliable measures (can actually make this test fail to reject), and sample size (same as with any other model scenario and statistical significance). So if it worries you that a core measure of model fit in SEM is fundamentally problematic, good. As has been said before, no single measure will be good enough on its own, so gather as much info as you can. Some suggest to pay more attention to the \\(\\chi^2\\) result, but to me, even if useful in some sense, the flawed logic is something that can’t really be overcome. If you use it with appropriate null hypothesis testing logic, a significant \\(\\chi^2\\) test can tell you that something is potentially wrong with the model. Note that lavaan also provides a Chi-square test which compares the current model to a model in which all paths are zero, and is essentially akin to the likelihood ratio test we might use in standard model settings (e.g. comparing against an intercept only model). For that test, we want a statistically significant result. However, one can specify a prior model conducted with lavaan to test against specifically (i.e. to compare nested models), but there is a specific function for that also, and it would provide more as well. CFI etc. User model versus baseline model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 0.999 The Comparative Fit Index compares the fitted model to a null model that assumes there is no relationship among the measured items. It ranges from 0 to 1, or rather it is rounded to fall between 0 and 1, and CFI values larger than .9 or especially .95 are typically desired, but like other completely arbitrary cutoffs those values guarantee nothing. Others include the Tucker-Lewis Fit Index, which is provided in standard lavaan output, but there are more incremental fit indices where those come from. Both of these are sensitive to the average correlations, to the point that adding meaningful covariates could actually worsen fit if they aren’t notably correlated with other variables in the model. You only need to report one, as they are generally notably correlated. RMSEA Root Mean Square Error of Approximation: RMSEA 0.014 90 Percent Confidence Interval 0.000 0.053 P-value RMSEA &lt;= 0.05 0.930 The root mean squared error of approximation is a measure that also centers on the model-implied vs. sample covariance matrix, and, all else being equal, is lower for simpler models and larger sample sizes. Maybe look for values less than .05, but again, don’t be rigid with this. Lavaan also provides a one-sided test that the RMSEA is \\(\\leq .05\\), or ‘test of close fit’, which ideally would be high. The confidence interval is enough for reporting purposes. SRMR Standardized Root Mean Square Residual: SRMR 0.007 The standardized root mean squared residual is the mean absolute correlation residual, i.e. the difference between the observed and model-implied correlations. Historical suggestions are to also look for values less than .05, but it is better to simply inspect the residuals and note where there are large discrepancies. residuals(alienation, type=&#39;cor&#39;)$cor eductn sei anom67 pwrl67 anom71 pwrl71 education 0.000 sei 0.000 0.000 anomia67 0.007 -0.020 0.000 powerless67 -0.006 0.018 0.000 0.000 anomia71 0.007 -0.017 0.000 0.001 0.000 powerless71 -0.001 0.001 -0.001 0.000 0.000 0.000 Like many matrices, it doesn’t take much to where it can be difficult to discern patterns unless one takes a visual approach. Perhaps consider d3heatmap (examples throughout this document) or corrplot packages. Fit Summarized A brief summary of these and other old/typical measures of fit are described here43. However they all have issues, and one should never use cutoffs as a basis for your ultimate thinking about model performance. Studies have been done and all the fit indices can potentially have problems in various scenarios, and the cutoffs commonly used by applied researchers do not hold up under scrutiny. While they can provide some assistance in the process, they are not meant to replace a global assessment of theory-result compatibility. Model Comparison All of the above, while rooted in model comparison approaches, are by themselves only providing information about the fit or lack thereof regarding the current model only. In any modeling situation, SEM or otherwise, a model comparison approach is to be preferred. Even when we don’t have the greatest model, being able to choose among viable, or at least competing, options can help science progress. AIC AIC is a good way to compare models in SEM just as it is elsewhere, where a penalty is imposed on the likelihood based on the number of parameters estimated, i.e. model complexity. The value by itself is not useful, but the ‘better’ model will have a lower value. A natural question arises as to how low is low enough to prefer one model over another, but this is impossible to answer because the value of AIC varies greatly with the data and model in question. However, this frees you to consider the relative merits of the models being considered, while knowing one has been deemed ‘better’, at least in terms of AIC. However, if the lower AIC is associated with the simpler model, you’d be hard-pressed to justify not taking it. Some suggest calculating \\(e^{\\frac{\\mathrm{AIC}_{min}-\\mathrm{AIC}_{other}}{2}}\\) to obtain the relative probability of a model vs the minimum AIC model. But that just puts leaves you with a different arbitrary cutoff to consider. A couple other things. Even if not nested, models must have the same variables for them to be compared, which may require you to constrain certain paths to be zero. Also, AIC can be used to compare just-identified models to over-identified ones. BIC While BIC is very popularly used in SEM, it has known issues relative to AIC if predictive capability is concerned. Most SEM programs will give you AIC as well. As such, one can probably ignore the BIC, but they will likely agree. Note that while BIC has a Bayesian interpretation, using it doesn’t actually put you in a Bayesian context, and if you were using a Bayesian approach, other measures would be used. If you aren’t using a Bayesian approach, then AIC would likely be preferable in most circumstances. The BIC has a different penalty than AIC, and is not a measure based on predictive performance, which is what we typically want in model selection. Model Comparison Example Let’s compare the previous model to one without the indirect effect and in which the SES and Alienation contributions are independent (i.e. just make the previous code change to alien67 ~ 0*ses). We’ll use the semTools package for easy side by side comparison via the compareFit function44. library(semTools) ## compareFit(alienation, alienationNoInd) ################### Nested Model Comparison ######################### chi df p delta.cfi alienation - alienationNoInd 200.28 1 &lt;.001 0.0941 DT::datatable(compfit, options=list(dom=&#39;t&#39;, scrollX=TRUE)) The first result is a likelihood ratio test. The model with no indirect path is nested within the model with a path and so this is a viable option. It tells us essentially that adding the indirect path results in a statistically significantly better model. AIC measures would suggest the indirect path would result in a better model as well. In terms of internal fit indices, the model including the indirect effect appears to fit the data well (note that the .000 is actually 1.00), while the other model does not. So now we can say that not only does our model appear to fit the data well, but is better than a plausible competitor. Prediction While the fitted correlation matrix is nice to be able to obtain, it has always struck me a bit odd that one can’t even predict the actual data with typical SEM software. Part of this is due to the fact that the models regard the covariance matrix as opposed to the raw data, and that is the focus in many SEM situations. In addition, most researchers using SEM seem only in explaining correlations. However, in path analysis, measurement models, and SEM where mean structures are of focus (e.g. growth curves), it stands to reason that one would like to get predicted values and/or be able to test a model on new data. Even in more complex models, predictions can be made by fixing parameters at estimated values and supplying new data. Lavaan at least does do this for you with some models, and its lavPredict function allows one to get predicted values for both latent and observed variables, for the current or new data. In addition, the semTools package is a great resource for comparing models generally, comparing models across groups, model simulation and so forth. Under active development is the capacity to create predictions in more complicated models with latent and observed variables. If you have only observed variables, you might consider the mediation and bnlearn packages. Observed covariates While I would hope it is by now, just to be clear, SEM doesn’t only have to be about structural relations among latent variables. At any point observed covariates can be introduced to the structural model as well, and this is common practice. As an example, the alienation model is fundamentally inadequate, as it doesn’t include many background or other characteristics we’d commonly collect on individuals and which might influence feelings of alienation. Interactions Interactions among both observed and latent variables can be included in SEM, and have the same interpretation as they would in any regression model. With latent variables, one can think of adding a latent variable whose indicator variables consists of product terms of the indicators for the latent variables we want to have an interaction. See indProd and probe2WayMC in the semTools package. As noted previously, a common term for this in SEM terminology is moderation. While many depictions in SEM suggest that one variable moderates another, statistically speaking, just like with standard interactions, it is arbitrary whether one says A interacts with/moderates B or vice versa, and this fact doesn’t change just because we are conducting an SEM. As described previously, one would need to have a specific causal setup to test for moderation. Estimation In everything demonstrated thus far, we have been using standard maximum likelihood to estimate the parameters. This may not be the best choice for various situations. As an example, some estimation procedures provide ‘robust’ standard errors, essentially the equivalent of Huber-White standard errors in the SEM context. One should probably ask for these robust or bootstrapped standard errors almost as a default. Other procedures may be better for categorical data situations. The following list comes from the Mplus manual, and most of these are available in lavaan. ML: maximum likelihood parameter estimates with conventional standard errors and chi-square test statistic MLM: maximum likelihood parameter estimates with standard errors and a mean-adjusted chi-square test statistic that are robust to non-normality. The chi-square test statistic is also referred to as the Satorra-Bentler chi-square. MLMV: maximum likelihood parameter estimates with standard errors and a mean- and variance-adjusted chi-square test statistic that are robust to non-normality MLR: maximum likelihood parameter estimates with standard errors and a chi-square test statistic (when applicable) that are robust to non-normality and non-independence of observations when used with TYPE=COMPLEX. The MLR standard errors are computed using a sandwich estimator. The MLR chi-square test statistic is asymptotically equivalent to the Yuan-Bentler T2* test statistic. MLF: maximum likelihood parameter estimates with standard errors approximated by first-order derivatives and a conventional chi-square test statistic MUML: Muthén’s limited information parameter estimates, standard errors, and chi-square test statistic WLS: weighted least square parameter estimates with conventional standard errors and chi-square test statistic that use a full weight matrix. The WLS chi-square test statistic is also referred to as ADF when all outcome variables are continuous. WLSM: weighted least square parameter estimates using a diagonal weight matrix with standard errors and mean-adjusted chi-square test statistic that use a full weight matrix WLSMV: weighted least square parameter estimates using a diagonal weight matrix with standard errors and mean- and variance-adjusted chi-square test statistic that use a full weight matrix ULS: unweighted least squares parameter estimates ULSMV: unweighted least squares parameter estimates with standard errors and a mean- and variance-adjusted chi-square test statistic that use a full weight matrix GLS: generalized least square parameter estimates with conventional standard errors and chi-square test statistic that use a normal-theory based weight matrix Bayes: Bayesian posterior parameter estimates with credibility intervals and posterior predictive checking45 Missing data A lot of data of interest in applications of SEM have missing values. Two common approaches to dealing with this are Full Information Maximum Likelihood (FIML) and Multiple Imputation (MI), and both are generally available in SEM packages. This is far too detailed an issue to treat adequately here, though we can take a moment to describe the approach generally. FIML uses the available information in the data (think pairwise correlations). MI uses a process to estimate the raw data values, and to adequately account for the uncertainty in those guesses, it creates multiple versions of complete data sets, each with different estimates for the missing values. The SEM model is conducted with all completed data sets and estimates combined across all models (e.g. the mean path parameter). The imputation models, i.e. those used to estimate the missing values, can be any sort of regression model, including using variables not in the SEM model. In addition, Bayesian approaches can estimate the missing values as additional parameters in the model (in fact, MI is essentially steeped within the Bayesian approach). Also there may additional concerns when data is missing over time, i.e. longitudinal dropout. Using the lavaan package is nice because it comes with FIML, and the semTools package adds MI and 2-stage maximum likelihood procedures. Other SEM approaches SEM is very flexible and applicable to a wide variety of modeling situations. Some of these will be covered in their own module (e.g. mixture models, growth curve modeling). How to fool yourself with SEM Kline’s third edition text listed over 50(!) ways in which one could fool themselves with SEM, which speaks to the difficulty in running SEM and dealing with all of its issues. I will note a handful of some of them to keep in mind in particular. Sample size If you don’t have at least a thousand observations, and your primary means of understanding the model is through statistically significant paths under the assumption of stable results, you will probably only be able to conduct (possibly unrealistically) simple SEM models, or just the measurement models for scale development, or only structural models with observed variables (path analysis). Even with simpler modeling situations and with truly ideal data circumstances (which rarely happens in practice), multiple studies have shown one should have several hundred observations for best results. In the simple alienation model above, we already are dealing with 17 parameters to estimate, and it doesn’t include any background covariates of the individuals, which is unrealistic. Furthermore, because it’s a mediation model, adding such variables might require additional direct and indirect paths, time-varying covariates that would have effects at both years, etc., and the number of parameters could balloon quite quickly. One will see many articles of published research with low sample sizes using SEM. This doesn’t make it acceptable practice, only perhaps that the journal editors and reviewers of the article are not methodologists. One should be highly cautious of the results suggested in those papers, as they are overfit and/or are possibly not including relevant variables. Poor data If the correlations among the data are low, one isn’t going to magically have strong effects by using SEM. I have seen many clients running these models and who are surprised that they don’t turn out well, when a quick glance at the correlation matrix would have suggested that there wasn’t much to work with in the first place. Naming a latent variable doesn’t mean it exists We discussed the naming fallacy in the section on latent variables, and it continues to exist when exploring structural relations among them. While everything may turn out well for one’s measurement model, and the results in keeping with theory, this doesn’t make it so. This is especially the case with less reliable measures. Latent constructs require operational definitions and other considerations in order to be useful, and rule out that one isn’t simply measuring something else, or that it makes sense that such a construct has real (physical ties). As an example, many diagnoses in the Diagnostic and Statistical Manual of Mental Disorders have not even been shown to exist via a statistical approach like SEM46, while others are simply assumed to exist, and even presumably (subsequently) supported by measurement models (often with low N), only to be shown to have no ties to any underlying physiology. Ignoring diagnostics Ignoring residuals, warning messages, etc. is a sure path to trying to interpret nonsensical results. Look at your residuals, fitted values etc. If your SEM software of choice is giving you messages, find out what they mean, because it may be very important. Ignoring performance As in our previous path analysis example, one can write a paper on a good fitting model with statistically significant results, and not explain the targets of interest very well on a practical level. When possible, check things like R-square (and accuracy if binary targets) when running your models, compare them with AIC etc. Summary SEM is a powerful modeling approach that generalizes many other statistical techniques, but it simply cannot be used lightly. Unfortunately, you cannot look much to how SEM is typically practiced to find out how to do SEM well47. The usual issues are poor underlying theory, small samples, poor samples, use of post-hoc ‘theory correction’, and lack of competition among plausible alternatives. The vast majority of the situations I’ve come across would have benefitted from the model search paradigm described in the section on [Bayesian networks][Baysian Networks]. On the other hand, strong theory, strong data, and a lot of data can potentially result in quite interesting models that have a lot to say about the underlying constructs of interest. While it doesn’t have to be restricted to such, SEM works best for models with explicit causal motivations. Go into it cautiously, prepare to spend a lot of time in model building and inspection, and allow for competing ideas to have their say, and you may find success. Terminology Measurement model: the factor analysis component of an SEM Structural model: the regression component of an SEM Fit: Model fit is something very difficult to ascertain in SEM, and notoriously problematic in this setting, where all proposed cutoffs for a good fit are ultimately arbitrary. Even if one had most fit indices suggesting a ‘good’ fit, there’s little indication the model has predictive capability. Identification: Refers to whether a unique solution can possibly be estimated for a given model. Models may be under-, over- or just-identified. It is a function of the model specification rather than the data. Exploratory SEM: a term about as useful as ‘exploratory’ factor analysis. Fully vs. Partially Latent SEM: see previous. MIMIC Model: see previous. Mixture Models: models using categorical latent variables. See the relevant chapter. Growth Curve Models: models for longitudinal data setting. See the relevant chapter. Growth Mixture Models: guess. Single indicator models: In some circumstances, and with the appropriate constraints, a latent variable can have a single indicator. Multilevel SEM: a model where your Mplus syntax looks confusing, and then you see the output and long for the good ol’ days. R Packages Used lavaan semTools semPlot Bollen &amp; Pearl (2013) put the causal aspect this way: ‘SEM is an inference engine that takes in two inputs, qualitative causal assumptions and empirical data, and derives two logical consequences of these inputs: quantitative causal conclusions and statistical measures of fit for the testable implications of the assumptions. Failure to fit the data casts doubt on the strong causal assumptions of zero coefficients or zero covariances and guides the researcher to diagnose, or repair the structural misspecifications. Fitting the data does not “prove” the causal assumptions, but it makes them tentatively more plausible. Any such positive results need to be replicated and to withstand the criticisms of researchers who suggest other models for the same data.’.↩ For example, practically every growth curve model I’ve ever come across. I cringe every time I see one that doesn’t include time-varying covariates, though I know why they aren’t there.↩ And not in a Bayesian approach where it might be more viable.↩ See also this site, which seems to somehow have escaped the dissolution of GeoCities webpages.↩ This function, while very useful, incorrectly notes which fit is ‘better’ in terms of fit indices that cannot be used to compare models, and furthermore provides no way to turn the flag off.↩ See the blavaan package.↩ Despite the name, there is nothing inherently ‘statistical’ about the DSM.↩ I’m not sure why this is, but usually it is due to a lack of communication. According to Google scholar, Rex Kline’s applied text on SEM, which did not even have a chapter on graphical models until the most recent edition (which came out 17 years after the first), has been cited 10000 times more than any specific work Pearl has put out, who has several works cited well over 1000 times each. I would hazard a guess that most of the people doing SEM in psychology and education are not citing Pearl, despite his having provided many non-technical summaries that most could follow. There is certainly a disconnect somewhere.↩ "],
["latent-growth-curves.html", "Latent Growth Curves Random effects Random Effects in SEM Simulating Random Effects Running a Growth Curve Model Thinking more generally about regression More on LGC Some Differences between Mixed Models and Growth Curves Other stuff Summary R Packages Used", " Latent Growth Curves Latent growth curve (LGC) models are in a sense, just a different form of the very commonly used mixed model framework. In some ways they are more flexible, mostly in the standard structural equation modeling framework that allows for indirect, and other complex covariate relationships. In other ways, they are less flexible (e.g. requiring balanced data, estimating nonlinear relationships, data with many time points, dealing with time-varying covariates). With appropriate tools there is little one can’t do with the normal mixed model approach relative to the SEM approach, and one would likely have easier interpretation. As such I’d recommend sticking with the standard mixed model framework unless you really need to. That said, growth curve models are a very popular SEM technique, so it makes sense to become familiar with them. To best understand a growth curve model, I still think it’s instructive to see it from the mixed model perspective, where things are mostly interpretable from what you know from a SLiM. Random effects Often data is clustered, e.g. students within schools or observations for individuals over time. The standard linear model assumes independent observations, and in these situations we definitely do not have that. One very popular way to deal with these are a class of models called mixed effects models, or simply mixed models. They are mixed, because there is a mixture of fixed effects and random effects. The fixed effects are the regression coefficients one has in standard modeling approaches. The random effects allow each cluster to have its own unique effect in addition to the overall fixed effect. This is simply a random deviation, almost always normally distributed in practice, from the overall intercept and slopes. Mixed models are a balanced approach between ignoring these unique contributions, and over-contextualizing by running separate regressions for every cluster. Model formality The following depicts a mixed model from a multilevel modeling context, which is just using a mixed model with possibly multiply nested grouping structures. The cluster that produce the random effects do not have to be hierarchical however, we just use the depiction here as it may be easier than the matrix approach. As an example, consider the case of repeated measures within an individual or individuals clustered within a geographic region. To model some target variable \\(y\\), within a group/cluster, each observation \\(i\\) in that cluster \\(c\\), will have a cluster specific intercept: \\[y_{i} = b_{0c} + b_1*X_i + e_i\\] At this point it looks like a standard regression, and is conceptually. In addition, the cluster specific intercepts have overall means (the fixed effect) plus some cluster specific deviation (random effect). These two can be thought of as additional regression models for the intercepts and slopes. \\[b_{0c} = b_0 + u_c\\] These random effects \\(u\\) are typically assumed normally distributed with some standard deviation, just like our ‘error’. \\[u_c \\sim N(0, \\tau)\\] \\[e_i \\sim N(0, \\sigma)\\] Plugging in the cluster level model to the initial model, we get the following: \\[y_{i} = [b_0 + u_c] + b1 * X_i + e_i\\] This is where the notion of ‘random coefficients’ comes from. We also might display the model as follows: \\[y_{i} = b_0 + b_1 * X_i + [u_c + e_i]\\] Now the focus is on a standard regression model with an additional source of variance. This is just one depiction, we might allow the slope to vary also, have random effects from multiple clustering sources, cluster level covariates, allow the random intercepts and slopes to correlate, and a host of other interesting approaches. The goal here is to keep things within the standard regression context as much as possible. Random Effects in SEM As we’ve seen with other models, the latent variables are assumed normally distributed, usually with zero mean, and some estimated variance. Well so are the random effects in mixed models, and it’s through this that we can maybe start to get a sense of random effects as latent variables (or vice versa). Indeed, mixed models have ties to many other kinds of models (e.g. spatial, additive), because they too add a ‘random’ component to the model in some fashion. Simulating Random Effects Through simulation we can demonstrate conceptual understanding of mixed models, and be well on our way toward better understanding LGC models. We’ll have balanced data, with scores across four time-points for 500 individuals (subjects). We will only investigate the trend (‘growth’), and allow subject-specific intercepts and slopes. set.seed(1234) n = 500 timepoints = 4 time = rep(0:3, times=n) subject = rep(1:n, each=4) We’ll have ‘fixed’ effects, i.e. our standard regression intercept and slope, set at .5 and .25 respectively. We’ll allow their associated subject-specific intercept and slope to have a slight correlation (.2), and as such we’ll draw them from a multivariate normal distribution (variance of 1 for both effects). intercept = .5 slope = .25 randomEffectsCorr = matrix(c(1,.2,.2,1), ncol=2) randomEffects = MASS::mvrnorm(n, mu=c(0,0), Sigma = randomEffectsCorr, empirical=T) %&gt;% data.frame() colnames(randomEffects) = c(&#39;Int&#39;, &#39;Slope&#39;) Let’s take a look at the data thus far. Note how I’m using subject as a row index. This will spread out the n random effects to n*timepoints total, while being constant within a subject. Now, to get a target variable, we simply add the random effects for the intercept to the overall intercept, and likewise for the slopes. We’ll throw in some noise at the end with standard deviation equal to \\(\\sigma\\). sigma = .5 y1 = (intercept + randomEffects$Int[subject]) + # random intercepts (slope + randomEffects$Slope[subject])*time + # random slopes rnorm(n*timepoints, mean=0, sd=sigma) d = data.frame(subject, time, y1) Let’s estimate this as a mixed model first48 using the lme4 package. See if you can match the parameters from our simulated data to the output. library(lme4) mixedModel = lmer(y1 ~ time + (1 + time|subject), data=d) # 1 represents the intercept ## summary(mixedModel) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: y1 ~ time + (1 + time | subject) Data: d REML criterion at convergence: 5833.3 Scaled residuals: Min 1Q Median 3Q Max -2.36211 -0.48276 0.02046 0.47515 2.84524 Random effects: Groups Name Variance Std.Dev. Corr subject (Intercept) 0.9999 0.9999 time 0.9898 0.9949 0.21 Residual 0.2382 0.4881 Number of obs: 2000, groups: subject, 500 Fixed effects: Estimate Std. Error t value (Intercept) 0.48986 0.04830 10.141 time 0.26400 0.04555 5.796 Our fixed effects are at the values we set for the overall intercept and slope. The estimated random effects variances are at 1, the correlation near .2, and finally, our residual standard deviation is near the .5 value we set. Running a Growth Curve Model As before, we’ll use lavaan, but now the syntax will look a bit strange compared to what we’re used to with our prior SEM, because we have to fix the factor loadings to specific values in order to make it work. This also leads to non-standard output relative to other SEM models, as there is nothing to estimate for the many fixed parameters. Specifically, we’ll have a latent variable representing the random intercepts, as well as one representing the random slopes. All loadings for the intercept factor are 149. The loadings for the effect of time are arbitrary, but should accurately reflect the time spacing, and typically it is good to start at zero, so that the zero has a meaningful interpretation. As can probably be guessed, additionally our data needs to be in wide format, where each row represents a person and we have separate columns for each time point of the target variable, as opposed to the long format we used in the previous mixed model. We can use the spread function from tidyr to help with that. subject time y1 1 1 0 -1.5801417 2 1 1 -0.1231067 3 1 2 -0.3397778 4 1 3 1.4511151 5 2 0 1.4904810 6 2 1 -0.5164371 # also change the names, as usually things don&#39;t work well if they start with a # number. yes, it is the 21st century. dWide = d %&gt;% spread(time, y1) %&gt;% rename_at(vars(-subject), function(x) paste0(&#39;y&#39;, x)) head(dWide) subject y0 y1 y2 y3 1 1 -1.58014168 -0.1231067 -0.3397778 1.451115 2 2 1.49048096 -0.5164371 0.2034644 -1.080743 3 3 0.93670868 2.8842359 4.6893758 6.219357 4 4 -2.29755097 -2.6728027 -3.1374417 -3.934300 5 5 0.08520448 1.2338897 4.3629780 6.008491 6 6 1.05332405 0.9019989 1.5461479 1.518288 Now we’re ready to run the model. Note that lavaan has a specific function, growth, to use for these models. It doesn’t spare us any effort for the model syntax, but does make it unnecessary to set various things with the sem function. model = &quot; # intercept and slope with fixed coefficients i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3 s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3 &quot; growthCurveModel = growth(model, data=dWide) summary(growthCurveModel) lavaan (0.5-23.1097) converged normally after 42 iterations Number of observations 500 Estimator ML Minimum Function Test Statistic 10.616 Degrees of freedom 5 P-value (Chi-square) 0.060 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) i =~ y0 1.000 y1 1.000 y2 1.000 y3 1.000 s =~ y0 0.000 y1 1.000 y2 2.000 y3 3.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) i ~~ s 0.226 0.050 4.512 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .y0 0.000 .y1 0.000 .y2 0.000 .y3 0.000 i 0.487 0.048 10.072 0.000 s 0.267 0.045 5.884 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .y0 0.287 0.041 6.924 0.000 .y1 0.219 0.021 10.501 0.000 .y2 0.185 0.027 6.748 0.000 .y3 0.357 0.065 5.485 0.000 i 0.977 0.076 12.882 0.000 s 0.969 0.065 14.841 0.000 Most of the output is blank, which is needless clutter, but we do get the same five parameter values we are interested in though. Start with the ‘intercepts’: Intercepts: Estimate Std.Err Z-value P(&gt;|z|) i 0.487 0.048 10.072 0.000 s 0.267 0.045 5.884 0.000 It might be odd to call your fixed effects ‘intercepts’, but it makes sense if we are thinking of it as a multilevel model as depicted previously, where we actually broke out the random effects as a separate model. The estimates here are pretty much spot on with our mixed model estimates, which are identical to just the standard regression estimates. fixef(mixedModel) (Intercept) time 0.4898598 0.2640034 lm(y1 ~ time, data=d) Call: lm(formula = y1 ~ time, data = d) Coefficients: (Intercept) time 0.4899 0.2640 Now let’s look at the variance estimates. The estimation of residual variance for each y in the LGC distinguishes the two approaches, but not necessarily so. We could fix them to be identical here, or conversely allow them to be estimated in the mixed model framework. Just know that’s why the results are not identical (to go along with their respective estimation approaches, which are also different by default). Again though, the variances are near one, and the correlation between the intercepts and slopes is around the .2 value. Covariances: Estimate Std.Err Z-value P(&gt;|z|) i ~~ s 0.226 0.050 4.512 0.000 Variances: Estimate Std.Err Z-value P(&gt;|z|) y0 0.287 0.041 6.924 0.000 y1 0.219 0.021 10.501 0.000 y2 0.185 0.027 6.748 0.000 y3 0.357 0.065 5.485 0.000 i 0.977 0.076 12.882 0.000 s 0.969 0.065 14.841 0.000 VarCorr(mixedModel) Groups Name Std.Dev. Corr subject (Intercept) 0.99994 time 0.99488 0.208 Residual 0.48806 The differences provide some insight. LGC by default assumes heterogeneous variance for each time point. Mixed models by default assume the same variance for each time point, but can allow them to be estimated separately in most modeling packages. As an example, if we fix the variances to be equal, the models are now identical. model = &quot; # intercept and slope with fixed coefficients i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3 s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3 y0 ~~ resvar*y0 y1 ~~ resvar*y1 y2 ~~ resvar*y2 y3 ~~ resvar*y3 &quot; growthCurveModel = growth(model, data=dWide) summary(growthCurveModel) lavaan (0.5-23.1097) converged normally after 27 iterations Number of observations 500 Estimator ML Minimum Function Test Statistic 17.105 Degrees of freedom 8 P-value (Chi-square) 0.029 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) i =~ y0 1.000 y1 1.000 y2 1.000 y3 1.000 s =~ y0 0.000 y1 1.000 y2 2.000 y3 3.000 Covariances: Estimate Std.Err z-value P(&gt;|z|) i ~~ s 0.207 0.050 4.170 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .y0 0.000 .y1 0.000 .y2 0.000 .y3 0.000 i 0.490 0.048 10.151 0.000 s 0.264 0.046 5.802 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .y0 (rsvr) 0.238 0.011 22.361 0.000 .y1 (rsvr) 0.238 0.011 22.361 0.000 .y2 (rsvr) 0.238 0.011 22.361 0.000 .y3 (rsvr) 0.238 0.011 22.361 0.000 i 0.998 0.074 13.478 0.000 s 0.988 0.066 15.076 0.000 Compare to the lme4 output. Groups Name Variance Corr subject (Intercept) 0.99989 time 0.98979 0.208 Residual 0.23820 In addition, the random coefficients estimates from the mixed model perfectly correlate with those of the latent variables. Int_mix Slope_mix Int_lgc Slope_lgc 1 -1.12 0.72 -1.12 0.72 2 0.90 -0.61 0.90 -0.61 3 1.08 1.72 1.08 1.72 4 -1.86 -0.68 -1.86 -0.68 5 0.06 1.94 0.06 1.94 6 0.87 0.24 0.87 0.24 Int_mix Slope_mix Int_lgc Slope_lgc Int_mix 1.00 0.29 1.00 0.29 Slope_mix 0.29 1.00 0.29 1.00 Int_lgc 1.00 0.29 1.00 0.29 Slope_lgc 0.29 1.00 0.29 1.00 Both approaches allow those residuals to covary, though it gets tedious in SEM syntax, while it is a natural extension in the mixed model framework. Here is the syntax for letting each time point covary with the next, at least, what it might be. It’s unclear if lavaan actually will do this, and the syntax here roughly follows Mplus’ manual, except that we can’t define new variables in lavaan as there. As such, the hope is that the a parameter should equal resvar*corr as in the Mplus syntax, but there’s not a clear way to fix it to be. It seems consistent here and with larger sample sizes. model = &quot; # intercept and slope with fixed coefficients i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3 s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3 # all of the following is needed for what are essentially only two parameters # to estimate- resvar and correlation (the latter defined explicitly here) y0 ~~ resvar*y0 y1 ~~ resvar*y1 y2 ~~ resvar*y2 y3 ~~ resvar*y3 # timepoints 1 step apart; technically the covariance is e.g. a*sqrt(y0)*sqrt(y1), # but since the variances are constrained to be equal, we don&#39;t have to be so verbose. y0 ~~ a*y1 y1 ~~ a*y2 y2 ~~ a*y3 # two steps apart y0 ~~ b*y2 y1 ~~ b*y3 # three steps apart y0 ~~ c*y3 # fix parameters according to ar1 b == a^2 c == a^3 &quot; lavaan (0.5-23.1097) converged normally after 288 iterations Number of observations 500 Estimator ML Minimum Function Test Statistic 14.881 Degrees of freedom 7 P-value (Chi-square) 0.038 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all i =~ y0 1.000 0.980 0.884 y1 1.000 0.980 0.603 y2 1.000 0.980 0.399 y3 1.000 0.980 0.291 s =~ y0 0.000 0.000 0.000 y1 1.000 0.991 0.610 y2 2.000 1.983 0.808 y3 3.000 2.974 0.882 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .y0 ~~ .y1 (a) 0.029 0.021 1.397 0.162 0.029 0.110 .y1 ~~ .y2 (a) 0.029 0.021 1.397 0.162 0.029 0.110 .y2 ~~ .y3 (a) 0.029 0.021 1.397 0.162 0.029 0.110 .y0 ~~ .y2 (b) 0.001 0.001 0.698 0.485 0.001 0.003 .y1 ~~ .y3 (b) 0.001 0.001 0.698 0.485 0.001 0.003 .y0 ~~ .y3 (c) 0.000 0.000 0.466 0.642 0.000 0.000 i ~~ s 0.217 0.051 4.293 0.000 0.223 0.223 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .y0 0.000 0.000 0.000 .y1 0.000 0.000 0.000 .y2 0.000 0.000 0.000 .y3 0.000 0.000 0.000 i 0.492 0.048 10.195 0.000 0.502 0.502 s 0.263 0.046 5.765 0.000 0.265 0.265 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .y0 (rsvr) 0.267 0.025 10.714 0.000 0.267 0.218 .y1 (rsvr) 0.267 0.025 10.714 0.000 0.267 0.101 .y2 (rsvr) 0.267 0.025 10.714 0.000 0.267 0.044 .y3 (rsvr) 0.267 0.025 10.714 0.000 0.267 0.024 i 0.960 0.079 12.155 0.000 1.000 1.000 s 0.983 0.066 14.883 0.000 1.000 1.000 Constraints: |Slack| b - (a^2) 0.000 c - (a^3) 0.000 Linear mixed-effects model fit by maximum likelihood Data: d AIC BIC logLik 5836.589 5875.795 -2911.295 Random effects: Formula: ~1 + time | subject Structure: General positive-definite, Log-Cholesky parametrization StdDev Corr (Intercept) 0.9768894 (Intr) time 0.9907666 0.225 Residual 0.5215327 Correlation Structure: AR(1) Formula: ~time | subject Parameter estimate(s): Phi 0.123578 Fixed effects: y1 ~ time Value Std.Error DF t-value p-value (Intercept) 0.4918554 0.0482736 1499 10.188910 0 time 0.2628093 0.0455996 1499 5.763412 0 Correlation: (Intr) time 0.121 Standardized Within-Group Residuals: Min Q1 Med Q3 Max -2.29560322 -0.45933037 0.01075091 0.45334444 2.76052132 Number of Observations: 2000 Number of Groups: 500 Thinking more generally about regression In fact, your standard regression is already equipped to handle heterogeneous variances and a specific correlation structure for the residuals. The linear model can be depicted as the following: \\[y \\sim N(X\\beta, \\Sigma)\\] \\(X\\beta\\) represents the linear predictor, i.e. the linear combination of your predictors, and a big, N by N covariance matrix \\(\\Sigma\\). Thus the target variable \\(y\\) is multivariate normal with mean vector \\(X\\beta\\) and covariance \\(\\Sigma\\). SLiMs assume that the covariance matrix is constant diagonal. A single value on the diagonal, \\(\\sigma^2\\), and zeros on the off-diagonals. Mixed models, and other approaches as well, can allow the covariance structure to be specified in myriad ways, and it ties them to still other models, which in the end produces a very flexible modeling framework. More on LGC LGC are non-standard SEM In no other SEM situation are you likely to fix so many parameters or think about your latent variables in this manner. This can make for difficult interpretations relative to the mixed model (unless you are aware of the parallels). Residual correlations Typical models that would be investigated with LGC have correlated residuals as depicted above. Nonlinear time effect A nonlinear time effect can be estimated if we don’t fix all the parameters for the slope factor. As an example, the following would actually estimate the loadings for times in between the first and last point. s =~ 0*y0 + y1 + y2 + 1*y3 It may be difficult to assess nonlinear relationships unless one has many time points50, and even then, one might get more with an additive mixed model approach. Growth Mixture Models Adding a latent categorical variable would allow for different trajectories across the latent groups. Most clients that I’ve seen typically did not have enough data to support it, as one essentially can be estimating a whole growth model for each group. Some might restrict certain parameters for certain groups, but given that the classes are a latent construct to be discovered, there would not be a theoretical justification to do so, and it would only complicate interpretation at best. Researchers rarely if ever predict test data, nor provide evidence that the clusters hold up with alternate data. In addition, it seems that typical interpretation of the classes takes on an ordered structure (e.g. low, medium, and high), which means they just have a coarsely measured continuous latent variable. In other cases, the groups actually reflect intact groups represented by covariates they have not included in the data (or perhaps are an interaction of those). Had they started under the assumption of a continuous latent variable, it might have made things easier to interpret and estimate. As of this writing, Mplus is perhaps the only SEM software used for these Growth Mixture Models, and it requires yet another syntax style, and, depending on the model you run, some of the most confusing output you’ll ever see in SEM. Alternatives in R include flexmix (demonstrated in the Mixture Models Module) for standard mixture modeling (including mixed effects models), as well as the R package OpenMx. Other covariates Cluster level To add a cluster-level covariate, for a mixed model, it looks something like this: standard random intercept \\[y = b_{0c} + b1*\\mathrm{time} + e \\] \\[b_{0c} = b_0 + u_c\\] Plugging in becomes: \\[y = b_0 + b1*\\mathrm{time} + u_c + e \\] subject level covariate added \\[b_{0c} = b_0 + c1*\\mathrm{sex} + u_c\\] But if we plug that into our level 1 model, it just becomes: \\[y = b_0 + c1*\\mathrm{sex} + b1*\\mathrm{time} + u_c + e\\] In our previous modeling syntax it would look like this: mixedModel = lmer(y1 ~ sex + time + (time|subject), data=d) We’d have a fixed effect for sex and interpret it just like in the standard setting. Similarly, if we had a time-varying covariate, say socioeconomic status, it’d look like the following: mixedModel = lmer(y1 ~ time + ses + (time|subject), data=d) Though we could have a random slope for SES if we wanted. You get the picture. Most of the model is still standard regression interpretation. With LGC, there is a tendency to interpret the model as an SEM, and certainly one can. But adding additional covariates typically causes confusion for those not familiar with mixed models. We literally do have to regress the intercept and slope latent variables on cluster level covariates as follows. model.syntax &lt;- &#39; # intercept and slope with fixed coefficients i =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 s =~ 0*y1 + 1*y2 + 2*y3 + 3*y4 # regressions i ~ x1 + x2 s ~ x1 + x2 &#39; Applied researchers commonly have difficulty on interpreting the model due to past experience with SEM. While these are latent variables, they aren’t just latent variables or underlying constructs. It doesn’t help that the output can be confusing, because now one has an ‘intercept for your intercepts’ and an ‘intercept for your slopes’. In the multilevel context it makes sense, but there you know ‘intercept’ is just ‘fixed effect’. Time-varying covariates With time varying covariates, i.e. those that can have a different value at each time point, the syntax starts to get tedious. Here we add just one such covariate, \\(c\\). model.syntax &lt;- &#39; # intercept and slope with fixed coefficients i =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 s =~ 0*y1 + 1*y2 + 2*y3 + 3*y4 # regressions i ~ x1 + x2 s ~ x1 + x2 # time-varying covariates y1 ~ c1 y2 ~ c2 y3 ~ c3 y4 ~ c4 &#39; fit &lt;- growth(model.syntax, data=Demo.growth) summary(fit) Now imagine having just a few of those kinds of variables as would be common in most longitudinal settings. In the mixed model framework one would add them in as any covariate in a regression model, and each covariate would be associated with a single fixed effect. In the LGC framework, one has to regress each time point for the target variable on its corresponding predictor time point. It might take a few paragraphs to explain the coefficients for just a handful of covariates. If you fix them to a single value, you would duplicate the mixed model, but the syntax requires even more tedium. Some Differences between Mixed Models and Growth Curves Random slopes One difference seen in comparing LGC models vs. mixed models is that in the former, random slopes are always assumed, whereas in the latter, one would typically see if it’s worth adding random slopes in the first place, or simply not assume them. There is currently a fad of ‘maximal’ mixed models in some disciplines, that would require testing every possible random effect. All I can say is good luck with that. Wide vs. long The SEM framework is inherently multivariate, and your data will need to be in wide format. This isn’t too big of a deal until you have many time-varying covariates, then the model syntax is tedious and you end up having the number of parameters to estimate climb rapidly. Sample size As we have noted before, SEM is inherently a large sample technique. The growth curve model does not require as much for standard approaches, but may require a lot more depending on the model one tries to estimate. In my own simulations, I haven’t seen too much difference compared to mixed models even for notably small sample sizes, but those were for very simple models. Number of time points A basic growth curve model requires four time points to incorporate the flexibility that would make it worthwhile. Mixed models don’t have the restriction (outside of the obvious need of two). Balance Mixed models can run even if some clusters have a single value. SEM requires balanced data and so one will always have to estimate missing values or drop them. Whether this missingness can be ignored in the standard mixed model framework is a matter of some debate in certain circles. Numbering the time points Numbering your time from zero makes sense in both worlds. This leads to the natural interpretation that the intercept is the mean for your first time point. In other cases having a centered value would make sense, or numbering from 0 to a final value of 1, which would mean the slope coefficient represents the change over the whole time span. Other stuff In the appendix I provide an example of a parallel process in which we posit two growth curves at the same time, with possible correlations among them. This could be accomplished quite easily with a standard mixed model in the Bayesian framework, with a multivariate response, though I’ll have to come back to that later. Summary Growth curve modeling is an alternative way to do what is very commonly accomplished through mixed models, and allow for more complex models than typically seen for standard mixed models. One’s default should probably be to use the more common, and probably more flexible (in most situations), mixed modeling tools, where there are packages in R that could handle nonlinear effects, mediation and multivariate outcomes for mixed models. I have other documents regarding mixed models on my website and code at GitHub. However, the latent variable approach may provide what you need, and at the very least gives you a fresh take on the standard mixed model perspective. R Packages Used lavaan lme4 nlme One can set REML=F so as to use standard maximum likelihood and make the results directly comparable to lavaan.↩ Those familiar with the model matrix for regression will recognize ‘intercept as 1’.↩ I personally cannot see bends with only four time points, at least such that I couldn’t just as easily posit a linear trend.↩ "],
["mixture-models.html", "Mixture Models A Motivating Example Create Clustered Data Mixture modeling with Old Faithful SEM and Latent Categorical Variables Summary R Packages Used", " Mixture Models Thus far we have understood latent variables as possessing an underlying continuum, i.e. normally distributed with a mean of zero and some variance. This does not have to be the case, and instead we can posit a categorical variable. Some approaches you may be already familiar with, as any modeling process under the heading of cluster analysis could be said to deal with latent categorical variables. The issue is that we may feel that there is some underlying structure to the data that is described as discrete, and based on perhaps multiple variables. We will approach this in the way that has been done from statistical and related motivations, rather than the SEM/psychometric specific approach. This will hopefully make clearer what it is we’re dealing with, as well as not get bogged down in terminology. Furthermore, mixture models are typically poorly implemented within SEM, as many of the typical issues found in such models can often be magnified. The goal here as before is clarity of thought over ‘being able to run it’. A common question in such analysis is how many clusters? There are many, many techniques for answering this question, and not a single one of them even remotely definitive. On the plus side, the good news is that we already know the answer, because the answer is always 1. However, that won’t stop us from trying to discover more than that, so here we go. A Motivating Example Take a look at the following data. It regards the waiting time between eruptions and the duration of the eruption (both in minutes) for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. Take a good look. This is probably the cleanest separation of clustered data you will likely ever see in practice51, at least where the categories wouldn’t be obvious without any statistical analysis. Even so there are still data points that might fall into either cluster. Create Clustered Data To get a sense of mixture models, let’s actually create some data that might look like the Old Faithful data above. In the following we start by creating something similar the eruptions variable in the faithful data. To do so, we draw one random sample from a normal distribution with a mean of 2, and the other with a mean of 4.5, and both get a standard deviation of .25. The first plot is based on the code below, the second on the actual data. set.seed(1234) erupt1 = rnorm(150, mean=2, sd=.25) erupt2 = rnorm(150, mean=4.5, sd=.25) erupt = sample(c(erupt1, erupt2)) What do we see here? The data is a mixture of two normals, but we can think of the observations as belonging to a latent class, and each class has its own mean and standard deviation (and is based on a normal distribution here, but doesn’t have to be). Each observation has some likelihood, however great or small, of coming from either cluster, and had we really wanted to do more appropriate simulation, we would incorporate that information. A basic approach for categorical latent variable analysis from a model based perspective52 could be seen as follows: Posit the number of clusters you believe there to be For each observation, estimate those probability of coming from either cluster Assign observations to the most likely class (i.e. the one with the highest probability) More advanced approaches might include: Predicting the latent classes with other covariates in a manner akin to logistic regression Allow your model coefficients to vary based on cluster membership For example, have separate regression models for each class Use more recent techniques that will allow the number of clusters to grow with the data (see the BNP section) Mixture modeling with Old Faithful The following uses the flexmix package and function to estimate a regression model for each latent class. In this case, our model includes only an intercept, and so is equivalent to estimating the mean and variance of each group. We posit k=2 groups. library(flexmix) mod = flexmix(eruptions~1, data=faithful, k = 2) summary(mod) Call: flexmix(formula = eruptions ~ 1, data = faithful, k = 2) prior size post&gt;0 ratio Comp.1 0.652 177 190 0.932 Comp.2 0.348 95 98 0.969 &#39;log Lik.&#39; -276.3611 (df=5) AIC: 562.7222 BIC: 580.7512 We can see from the summary about 2/3 are classified to one group. We also get the estimated means and standard deviations for each group, as well as note respective probabilities of each observation for each class. Note that the group labels are completely arbitrary. parameters(mod) # means (Intercept) and std dev (sigma) for each group Comp.1 Comp.2 coef.(Intercept) 4.2735128 2.0187913 sigma 0.4376169 0.2363539 [,1] [,2] [1,] 1.0000 0.0000 [2,] 0.0000 1.0000 [3,] 1.0000 0.0000 [4,] 0.0001 0.9999 [5,] 1.0000 0.0000 [6,] 0.8384 0.1616 [7,] 1.0000 0.0000 [8,] 1.0000 0.0000 [9,] 0.0000 1.0000 [10,] 1.0000 0.0000 The first of the next two plots shows the estimated probabilities for each observation for the clusters (with some jitter). Basically it shows that if that most are very highly probable of belonging to either class. Again, you will probably never see anything like this, but clarity is useful here. The second plot shows the original data with their classification and contours of the density for each group. SEM and Latent Categorical Variables Dealing with categorical latent variables can be somewhat problematic. Interpreting a single SEM model might be difficult enough, but then one might be allowing parts of it to change depending on which latent class observations belong to, while having to assess the latent class measurement model as well. It can be difficult to find a clarity of understanding from this process, as one is discovering classes then immediately assuming key differences among these classes they didn’t know existed beforehand53. In addition, one will need even more data than standard SEM to deal with all the additional parameters that are allowed to vary across the classes. Researchers also tend to find classes that represent ‘hi-lo’ or ‘hi-med-lo’ groups, which may suggest they should have left the latent construct as a continuous variable. When given the choice to discretize continuous variables in normal settings, it is rare in which the answer is anything but a resounding no. As such, one should think hard about the ultimate goals of such a model. See the IRT section for alternative approaches to similar data. Another issue I’ve seen in applied practice is that the the ‘latent’ classes uncovered are in fact intact classes representing an interaction of variables available in the data, or possibly some grouping variable that wasn’t in the model. It would be much more straightforward to use the standard interaction approach in a more guided fashion, as opposed to assuming every variable effect interacts with a latent variable. In typical modeling scenarios such an option (i.e. laying waste to our model with interactions) is relatively rarely considered, and even then we’d want some sort of regularizing approach54, which is non-existent in this SEM setting. All that said, let’s go ahead and look at a traditional latent class approach. We’ll use the poLCA package which is geared specifically to such analysis. The term ‘latent class analysis’ would assume binary outcomes, but as we’ve noted before, usage of a latent categorical variable does not require them (nor does this package). Our example will also not be binary data. % data.frame --> Recall the depression data from the section on latent variable models. We have three items regarding emotional well-being (depression)- how often the person felt down or blue, how often they’ve been a happy person, and how often they’ve been depressed in the last month. These are four point scales and range from ‘all of the time’ (1) to ‘none of the time’ (4) (i.e. reversed). The conceptual model is still the same, except that we are thinking of the latent variable as categorical, and thus people will fall under depression types rather than have a continuous score that reflects ‘depression’. We now run the model. As with mixture models before, and to be clear we are still conducting a finite mixture model, one of our primary goals is to assign individuals to a latent class. In addition, we can obtain probabilities of each item category according to latent class. If we had covariates, we could also look at their relationship to the latent classes library(poLCA) result = poLCA(cbind(FeltDown, BeenHappy, DepressedLastMonth) ~ 1, depressed) result Conditional item response (column) probabilities, by outcome variable, for each class (row) $FeltDown All of the time None of the time Rarely Some of the time class 1: 0.000 0.4997 0.5003 0.0000 class 2: 0.047 0.0085 0.6637 0.2809 $BeenHappy All of the time None of the time Rarely Some of the time class 1: 0.1418 0.0045 0.1069 0.7468 class 2: 0.0178 0.0518 0.7008 0.2296 $DepressedLastMonth All of the time None of the time Rarely Some of the time class 1: 0.0025 0.9290 0.0654 0.0031 class 2: 0.0235 0.2328 0.6388 0.1049 Estimated class population shares 0.6828 0.3172 Predicted class memberships (by modal posterior prob.) 0.6251 0.3749 ========================================================= Fit for 2 latent classes: ========================================================= number of observations: 7183 number of estimated parameters: 19 residual degrees of freedom: 44 maximum log-likelihood: -17515.44 AIC(2): 35068.88 BIC(2): 35199.59 G^2(2): 1254.267 (Likelihood ratio/deviance statistic) X^2(2): 4237.753 (Chi-square goodness of fit) In general, aside from poLCA reordering the variable labels alphabetically, this is a fairly clear result. The first class, which are about 2/3 of the individuals, are those that rarely or didn’t feel down nor were depressed in the past month, and felt happy some or all of the time. The other class represents our more depressed individuals. However, this is a good example where it would be notably more appropriate to assume depression on a continuous scale, rather than categorical. We have a lot more output from the LCA, but nothing is really gained in our understanding of the latent aspect underlying the scores. Furthermore, we actually haven’t moved away from a continuous latent variable, as the probabilities of class membership are continuous, and we are merely picking an arbitrary cutpoint to create the latent class. Latent Categories vs. Multi-group Analysis The primary difference between the latent class analysis and a multiple group approach is that in the latter, grouping structure explicitly exists in the data, for example, sex, race etc. In that case, a multi-group analysis, a.k.a. multi-sample analysis, would allow for separate SEM models per group. In the latent categorical variable situation, one must first discover the latent groups. By contrast, in multi-group analysis, a common goal is to test measurement invariance, a concept which has several definitions itself. An example would be to see if the latent structure holds for an American vs. non-U.S. sample, with the same items for the scale provided in the respective languages. This makes a lot of sense from a measurement model perspective and has some obvious use. If one wants to see a similar situation for a less practically driven model, e.g. to see if an SEM model is the same for males vs. females, this is equivalent to having an interaction with the sex variable for every path in the model. The same holds for ‘subgroup analysis’ in typical settings, where you won’t find any more than you would by including the interactions of interest with the whole sample, though you will certainly have less data to work with. In any case, whether the classes are latent or intact, we need a lot of data to estimate parameters that are allowed to vary by group vs. a model in which they are fixed, and many simply do not have enough data for this. Latent Trajectories As noted in the growth curve modeling section, these are growth curve models in which intercepts and slopes are allowed to vary across latent groups clusters. The flexmix package used previously as well as others would allow one to estimate such models from the mixed model perspective, and might be preferred. Estimation If you would like to see the conceptual innards of estimating mixture models using the EM Algorithm, see my GitHub page for some examples. Terminology in SEM Some unnecessary terminology comes into play from the SEM literature, such that it might be worth knowing about them so that one doesn’t get out of sorts. SEM literature often makes the following distinctions: Aside from noting whether the latent variable is categorical or not, these aren’t very enlightening55, and go back as far as the 60s. Back then perhaps it was useful, but in the end, it’s all just ‘latent variable analysis’. Some other terminology includes: Mixture models Refers generally to dealing with categorical latent variables as demonstrated above. Finite Mixture models Same thing. Cluster analysis Same thing. Infinite Mixture model A Bayesian version of it. Latent Class Analysis refers to dealing with categorical latent variables in the context of multivariate data, especially within the measurement model scenario. For example one might have a series of yes/no questions on a survey, and want to discover categories of collections of responses. Some use it for the specific case of categorical indicators, but this is not necessary. Latent Profile Analysis refers to dealing with categorical latent variables in the context of multivariate numerical/continuous data. Again, most people outside of SEM would simply call this a mixture model. Latent Trait Analysis refers to dealing with continuous latent variables in the context of multivariate categorical data. Again, most people outside of SEM would simply call this a mixture model, but it also serves as the setting for Item-Response Theory models where we, for example, find latent ‘ability’ among binary test items that are correct vs. not. Summary In general, humans are predisposed to think and label things in terms of categories because it presumably simplifies things. More often than not, it’s an oversimplification, and has a long history of causing more trouble than it’s worth. Furthermore, we know from our study of reliability that if we categorize a continuous variable, it is a less reliable measure, and potentially causes numerous issues statistically. One should think long and hard about positing types instead of allowing more nuance. Even so, it might be appropriate for a specific avenue of study, in which case mixture models may serve one well. R Packages Used flexmix Note that one shortcoming of lavaan relative to Mplus is the lack of estimating latent classes. However, along with flexmix where the latent classes are on the predictor side of the equation and serve to moderate the predictor effects, one might also consider PoLCA, which models latent classes for an outcome similar to traditional LCA within the SEM setting. This is essentially logistic regression where the classes to predict are latent. However, as of early 2017, while some have inquired about such models, I’ve not come across an R package that easily does both, i.e. simultaneously estimates latent classes on either side of the regression equation. Mplus can do so with Mplus simulated data, but it’s not clear how easily it’s been used in practice. The number of parameters to interpret would easily become overwhelming, not to mention ratios of odds ratios. Outside of iris, which is also used regularly to give people unrealistic expectations about what to expect from their cluster analysis.↩ Note that k-means and other common cluster analysis techniques are not model based as in this example. In model-based approaches we assume some probabilistic data generating process (e.g. normal distribution) rather than some heuristic.↩ By contrast, continuous latent variables are typically clearly theoretically motivated in usual SEM practice. In addition, SEM data set sizes would usually limit the number of groups to three or four at most, because the smallest latent group will be the limiting factor, but still needs enough data to run an SEM model confidently. However, there is no reason to believe there would only be three distinct latent classes in any reasonable amount of data.↩ See for example, Rendle (2010). Factorization machines. link↩ I can never keep profile vs. trait straight.↩ "],
["item-response-theory.html", "Item Response Theory Standard Models Other IRT Models Summary IRT Terminology R Packages Used", " Item Response Theory Item Response Theory (IRT) is a class of latent variable models with a long history in the testing environment (e.g. scholastic aptitude), but are actually a more general latent variable approach that might be applicable to a wide variety of settings. In the typical scenario, we might have a set of test items which are simply binary indicators for whether the item was answered correctly. The relationship between IRT and SEM comes in the form of a specific type of factor analysis depending on the type of IRT model being considered. Standard Models We can begin our understanding of IRT with an example with a logistic regression model. \\[g(\\mu) = X\\beta\\] \\[\\pi = g^{-1}(\\mu)\\] \\[y \\sim \\mathrm{Bernoulli}(\\pi)\\] The link function \\(g(.)\\) is the logit function, and its inverse, the logistic (or sigmoid) function, maps our linear predictor, the logit, or log odds (\\(\\ln(\\pi)/\\ln(1-\\pi)\\)), to the probability scale, \\(\\pi\\). Finally, our binary response is bernoulli distributed (i.e. binomial with size=1). Let’s see this for a single observation to remove any mystery. logit = -1 exp(logit)/(1+exp(logit)) # convert logit to probability via logistic function [1] 0.2689414 1/(1+exp(-logit)) # convert logit to probability (alternate) [1] 0.2689414 plogis(logit) [1] 0.2689414 prob = .75 logit = log(.75/(1-.75)) # convert probability to logit logit [1] 1.098612 plogis(logit) # and back [1] 0.75 Now let’s speak more generally, and say that with our response \\(y\\) we are concerned with the probability that a person answers correctly. In terms of a logistic regression model: \\[P(y=1) = f(X)\\] In other words, the probability of choosing the correct response (or simply endorsing an attitude or many other scenarios), \\(y=1\\), is some function of the X variables, which will at a minimum be the items and person scores for an IRT model. One Parameter Model We now turn to specific IRT models. The one-parameter, a.k.a. Rasch, model (1PM) can be expressed as follows: \\[P(y=1|\\theta, \\delta) = \\mathrm{logis}(\\theta_i-\\delta_j)\\] In this setting, the probability of endorsement (or getting an item correct), \\(\\pi_{ij}\\), is a function of the difficulty of item \\(j\\), \\(\\delta_j\\) above, and the latent trait (ability) of person \\(i\\), \\(\\theta_i\\). In other words, it’s a specific type of logistic regression model. In the testing context, a person with more ‘ability’ relative to the item difficulty will answer correctly. In terms of the logit: \\[\\mathrm{logit_{ij}} = \\mathrm{log}(\\frac{\\pi_{ij}}{1-\\pi_{ij}}) = \\theta_i-\\delta_j\\] IRT often utilizes a different parameterization, though the results are the same. There is an additional parameter, \\(\\alpha\\), item discrimination, which refers to the item’s ability to distinguish one person from another. In the Rasch model it is held constant, and in its original formulation it was fixed at 1. If we add it to the mix we have: \\[P(y=1|\\theta, \\delta) = \\mathrm{logis}(\\alpha(\\theta_i-\\delta_j))\\] As we will see later, the two parameter IRT model estimates the discrimination parameter for each item. Note also, the ltm package we will use doesn’t fix the discrimination parameter to be 1 in the 1PM, so you’ll actually have an estimate for it, but it’s still constant across items. To begin we’ll use the abortion data that comes with the ltm package. I provide this non-testing example so that one will be clear that IRT is not just for testing data, though I will often refer to the testing lingo for additional context. It regards 379 individuals who were asked if the law should allow abortion under the circumstances presented for each item: Item 1: The woman decides on her own that she does not. Item 2: The couple agree that they do not wish to have a child. Item 3: The woman is not married and does not wish to marry the man. Item 4: The couple cannot afford any more children. data(Abortion, package=&#39;ltm&#39;) # for later use in SEM, convert to ordered factors Abortion_asfactor = mutate_all(Abortion, ordered) colnames(Abortion_asfactor) = paste0(&#39;Item_&#39;, 1:4) The ltm package provides some nice descriptives via the descript function. Descriptive statistics for the &#39;Abortion&#39; data-set Sample: 4 items and 379 sample units; 0 missing values Proportions for each level of response: 0 1 logit Item 1 0.5620 0.4380 -0.2493 Item 2 0.4063 0.5937 0.3791 Item 3 0.3641 0.6359 0.5575 Item 4 0.3826 0.6174 0.4786 Frequencies of total scores: 0 1 2 3 4 Freq 103 33 37 65 141 Point Biserial correlation with Total Score: Included Excluded Item 1 0.8153 0.6664 Item 2 0.8663 0.7531 Item 3 0.8758 0.7726 Item 4 0.8344 0.7016 Cronbach&#39;s alpha: value All Items 0.8707 Excluding Item 1 0.8573 Excluding Item 2 0.8223 Excluding Item 3 0.8148 Excluding Item 4 0.8430 Pairwise Associations: Item i Item j p.value 1 1 4 &lt;2e-16 2 1 3 &lt;2e-16 3 2 4 &lt;2e-16 4 1 2 &lt;2e-16 5 2 3 &lt;2e-16 6 3 4 &lt;2e-16 Now we’ll start by examining the initial results from the 1PM by using the rasch function, for both IRT parameterizations. If you want to look at the original formulation with discrimination fixed to 1.0 I show the code for that, but not the results. library(ltm) irt_rasch_par1 = rasch(Abortion, IRT.param=F) irt_rasch_par2 = rasch(Abortion, IRT.param=T) # irt_rasch_original = rasch(Abortion, IRT.param=T, constraint = cbind(ncol(Abortion) + 1, 1)) irt_rasch_par1 irt_rasch_par2 Call: rasch(data = Abortion, IRT.param = F) Coefficients: Item 1 Item 2 Item 3 Item 4 z -0.729 1.054 1.596 1.354 4.457 Log.Lik: -708.55 Call: rasch(data = Abortion, IRT.param = T) Coefficients: Dffclt.Item 1 Dffclt.Item 2 Dffclt.Item 3 Dffclt.Item 4 Dscrmn 0.164 -0.237 -0.358 -0.304 4.457 Log.Lik: -708.55 Again, the parameterization used doesn’t really matter (note the identical log likelihoods and discrimination). Though setting IRT.param=T is perhaps more common in the IRT world, the other is more in keeping with standard logistic models elsewhere. The gist is, that the first item is ‘more difficult’, i.e. less likely to be endorsed by default, relative to the other items. In the second parameterization, we can think of it as requiring a latent trait score above average (i.e. 0) for endorsement. We can see this even by just looking at the proportion of endorsements via colMeans. Now let’s look at some of the individual latent trait scores. By default, ltm will only provide scores for the unique response patterns, and in fact for the standard estimation only the response patterns are required rather than all the observations. With only items and no other individual information, multiple response patterns of the same type are redundant in estimating the latent trait. These are obtained with the factor.scores function. Other information includes standard errors, and observed and expected frequencies. Item Analysis We can obtain some additional results to aid our understanding, as well as distinguish some of the different IRT models we’ll discuss. We’ll start with the item characteristic curve (ICC). It plots the probability of endorsement as a function of the latent person trait, and takes on the familiar sigmoid shape due to the underlying logistic function. In this case we can see that three of the items essentially behave identically, and in general distinguish (slightly less than average) individuals. The first item would however would take more ‘ability’ before endorsement, i.e. it is more ‘difficult’ in test taking terms, but even then it is not too different from the others. We can now start to think of the latent trait as representing a pro-choice stance, where at the average score the person would likely be endorsing all but the first item. Another way to look at this is in terms of item information56. The way one can interpret this is that it tells us how individuals, in terms of the latent trait, are distinguished best by the items. The item information curves (IIC) are the derivative of the item characteristic curve, and so tell us the rate of change in that probability. It is a maximum at the inflection point of the ICC, i.e. when the probability of endorsement/correct vs. not is equal. In addition, the peak of the IIC is at point of item difficulty on the latent trait scale. In other words, in the IRT parameterization, the estimate of an item’s difficulty is that point on the latent scale where half the subjects endorse (get correct) the item, or where the information for that item is at a maximum. Because we don’t estimate separate item discrimination, all items have the same information and the same distribution. In this case, items 2-4 have more information for those scoring below average on the latent trait, while item 1 has most for those slightly above. For further interpretation, consider a worst case scenario. Individuals would have the same chance of getting the answer correct regardless of ability. In other words the ICC would be flat, i.e. a constant. The derivative of a constant is zero, meaning the item has no information at all. One final interpretation of item information- had we done a standard factor analysis, it would be equivalent to the ratio of the communality, i.e. the squared loading (or sum of for multiple factors) for that item to its uniqueness. So item information can be seen as the reciprocal of the error of measurement for that item. Furthermore, we can get total test information by simply summing the item information scores. This allows us to take a specific strategies when designing a test or scale, e.g. to provide maximum information at particular points of difficulty or be more or less uniform across a wide range of ability. We can see that the bulk of the test’s information is for those individuals between -1 and 1 on the latent trait. Call: rasch(data = Abortion, IRT.param = T) Total Information = 17.83 Information in (-1, 1) = 17.08 (95.81%) Based on all the items And can get individual item information as the area under the IIC. Call: rasch(data = Abortion, IRT.param = T) Total Information = 4.46 Information in (-1, 1) = 4.33 (97.1%) Based on items 1 Finally we can look at the density plot of the latent scores. Dots reflect the difficulty estimates from the IRT parameterization. At this point we’ll take a moment to summarize things. IRT models can be seen as a specific type of logistic regression model. The 1PM assumes a latent individual score as well as item-specific difficulty, and from the model, we can gain information about person performance as well as item characteristics. With extensions we’ll gain even more information about how the items and individuals function. 1PM as a Mixed Model As an additional means of understanding, we can think of IRT from the perspective of a mixed model. In this approach, we can melt the data into long format such that multiple rows/observations pertain to an individual’s response for the items. We then run a mixed model predicting the binary response with a fixed effect for item and a random effect for person. The fixed effects for item represent item difficulty, while The latent trait in the IRT for the person is the random effect for that person in the mixed model. For easier presentation we’ll omit the intercept. Abortion_long = gather(data.frame(Subject=1:nrow(Abortion), Abortion), key=Item, value=Response, -Subject, factor_key=T) %&gt;% arrange(Subject, Item) head(Abortion_long, 8) Subject Item Response 1 1 Item.1 1 2 1 Item.2 1 3 1 Item.3 1 4 1 Item.4 1 5 2 Item.1 1 6 2 Item.2 1 7 2 Item.3 1 8 2 Item.4 1 ## See https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004668.html library(lme4) lme_rasch = glmer(Response ~ -1 + Item + (1|Subject), Abortion_long, family=binomial(link=&#39;logit&#39;)) summary(lme_rasch, cor=F) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] Family: binomial ( logit ) Formula: Response ~ -1 + Item + (1 | Subject) Data: Abortion_long AIC BIC logLik deviance df.resid 1471.7 1498.4 -730.9 1461.7 1511 Scaled residuals: Min 1Q Median 3Q Max -2.6205 -0.3029 0.1283 0.3790 3.5635 Random effects: Groups Name Variance Std.Dev. Subject (Intercept) 13.89 3.727 Number of obs: 1516, groups: Subject, 379 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) ItemItem.1 -0.6670 0.2768 -2.409 0.015982 * ItemItem.2 1.0165 0.2829 3.593 0.000327 *** ItemItem.3 1.4998 0.2918 5.140 2.75e-07 *** ItemItem.4 1.2851 0.2875 4.470 7.81e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Aside from the estimation approach, the only difference is that the IRT model assumes the latent ability of person is distributed as standard normal, and estimates the discrimination parameter as a multiplier of that ability, \\(\\alpha \\cdot N(0,1)\\). The mixed model on the other hand assumes that the random effects are distributed as normal with mean zero and standard deviation equal to the discrimination parameter, \\(N(0,\\alpha)\\). In this case lme4 estimates a slightly lower discrimination parameter57. Comparing the fixed effects of the mixed model to the first parameterization of the IRT, they are quite similar. ltm lme Item 1 -0.7293065 -0.6669831 Item 2 1.0543555 1.0165352 Item 3 1.5961332 1.4998087 Item 4 1.3543951 1.2850734 Same goes for the latent individual scores. Since the Abortion data is essentially ordered by pattern of response, I’ll mix it up a little bit by displaying a random ordering (idx). As the results are on different scales, we can alternate rescaling one or the other to put them on equal footing. The correlation of the scores is essentially 1.0. Note that I do not display the initial data processing. head(data.frame(ltm = discrimnation*iscore_rasch[idx], lmer = iscore_lme[idx])) ltm lmer 1 2.9014753 2.6075672 2 -0.8284266 -0.7685527 3 -4.0267853 -3.6739666 4 -4.0267853 -3.6739666 5 2.9014753 2.6075672 6 2.9014753 2.6075672 head(data.frame(ltm = iscore_rasch[idx], lmer = iscore_lme[idx]/RE_stddev)) ltm lmer 1 0.6509795 0.6996604 2 -0.1858671 -0.2062175 3 -0.9034558 -0.9857958 4 -0.9034558 -0.9857958 5 0.6509795 0.6996604 6 0.6509795 0.6996604 We see the same thing with probability of item endorsement. In the mixed effect model, these are the unconditional estimated probabilities, i.e. those that ignore the individual-specific effect. In the IRT model, these are the expected probabilities at the average latent trait score (i.e. 0), which amounts to the exact same thing. ltm lme Item 1 0.3253469 0.3391727 Item 2 0.7416104 0.7342971 Item 3 0.8314773 0.8175459 Item 4 0.7948473 0.7833121 And finally, we can look at probability of person endorsement. In the mixed effect model, these are the estimated probabilities conditional on the individual. In the IRT model, they include the latent score for the individual. ltm lme 115 0.9889870 0.9838154 305 0.8977223 0.8744163 178 0.9812168 0.9740199 166 0.9812168 0.9740199 30 0.9812168 0.9740199 140 0.9860175 0.9800161 The gist is that standard IRT is equivalent to a generalized linear mixed model where item responses are clustered by individual. Knowing this allows for forays into more flexible modeling situations, including structural equation modeling. 1PM as SEM Now let’s look at the model from a structural equation modeling perspective. We saw in the growth curve modeling section how a latent growth curve model is equivalent to a mixed model, though where the data are analyzed in wide format, and the latent variable is equivalent to the random effects in the mixed model. Given the connection between SEM and mixed models, it probably comes as no surprise that we can do IRT as SEM as well. The LGCM is unusual in the SEM framework in that most of the parameters are fixed. As we have seen the IRT has connections to a random effects model as well, in order to do a 1PM IRT in SEM, we’ll take a similar approach of fixing several parameters. An additional distinction here from our previous SEM examples is that we are now dealing categorical indicators. We’ll look at the SEM approach for each IRT parameterization. We’ll start with the first and compare the results to the mixed model as well. For the first approach, we fix all the loadings to be equal, and fix the factor variance to 1 (std.lv = T). For the binary case, the thresholds are essentially the intercept from a logistic regression model of each item with the latent trait \\(\\theta\\) as the covariate. The one issue with using lavaan is that it only uses a probit link58 (or at least will not do a logit link not without difficulty and slowness). Likewise the ltm package only uses the logit link. Interestingly, using the probit link in IRT is equivalent to a factor analysis based on the tetrachoric correlation matrix of the items. So to make things comparable, we will have to convert the ltm output by dividing by 1.759, or conversely, multiply the lavaan estimates by 1.7. We’ll also rerun the mixed model with a probit link, and this will put all the models in the same place. With the estimated loading and threshold, we can convert them to the IRT parameters60. library(lavaan) sem_irt_raschel = &#39; # loadings theta =~ l1*Item_1 + l1*Item_2 + l1*Item_3 + l1*Item_4 # thresholds Item_1 | th1*t1 Item_2 | th2*t1 Item_3 | th3*t1 Item_4 | th4*t1 # convert loading to discrimination discrm := l1 / sqrt(1-l1^2) # use thresholds to get difficulty diff_1 := -th1 / sqrt(1-l1^2) diff_2 := -th2 / sqrt(1-l1^2) diff_3 := -th3 / sqrt(1-l1^2) diff_4 := -th4 / sqrt(1-l1^2) &#39; sem_rasch &lt;- cfa(sem_irt_raschel, data=Abortion_asfactor, std.lv=T) summary(sem_rasch) lavaan (0.5-23.1097) converged normally after 7 iterations Number of observations 379 Estimator DWLS Robust Minimum Function Test Statistic 10.171 13.453 Degrees of freedom 5 5 P-value (Chi-square) 0.071 0.019 Scaling correction factor 0.777 Shift parameter 0.370 for simple second-order correction (Mplus variant) Parameter Estimates: Information Expected Standard Errors Robust.sem Latent Variables: Estimate Std.Err z-value P(&gt;|z|) theta =~ Item_1 (l1) 0.935 0.010 90.625 0.000 Item_2 (l1) 0.935 0.010 90.625 0.000 Item_3 (l1) 0.935 0.010 90.625 0.000 Item_4 (l1) 0.935 0.010 90.625 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .Item_1 0.000 .Item_2 0.000 .Item_3 0.000 .Item_4 0.000 theta 0.000 Thresholds: Estimate Std.Err z-value P(&gt;|z|) Itm_1|t1 (th1) 0.156 0.065 2.410 0.016 Itm_2|t1 (th2) -0.237 0.065 -3.639 0.000 Itm_3|t1 (th3) -0.347 0.066 -5.273 0.000 Itm_4|t1 (th4) -0.299 0.066 -4.559 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .Item_1 0.126 .Item_2 0.126 .Item_3 0.126 .Item_4 0.126 theta 1.000 Scales y*: Estimate Std.Err z-value P(&gt;|z|) Item_1 1.000 Item_2 1.000 Item_3 1.000 Item_4 1.000 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) discrm 2.635 0.231 11.412 0.000 diff_1 -0.440 0.182 -2.413 0.016 diff_2 0.668 0.185 3.611 0.000 diff_3 0.979 0.189 5.188 0.000 diff_4 0.842 0.188 4.478 0.000 ## lme_rasch_probit = glmer(Response~ -1 + Item + (1|Subject), Abortion_long, ## family=binomial(link=&#39;probit&#39;)) Logistic link comparison. Probit link comparison61. For the second IRT parameterization, IRT.param=T in the ltm function, we just use the initial results. This time I multiply the estimated loading, i.e. the discrimination, from lavaan by 1.7. sem_irt_raschel = &#39; # loadings theta =~ l1*Item_1 + l1*Item_2 + l1*Item_3 + l1*Item_4 # thresholds Item_1 | th1*t1 Item_2 | th2*t1 Item_3 | th3*t1 Item_4 | th4*t1 # # # convert loading to discrimination discrm := l1 / sqrt(1-l1^2) * 1.7 &#39; sem_rasch &lt;- cfa(sem_irt_raschel, data=Abortion_asfactor, std.lv=T) # summary(sem_rasch) # not shown as is identical to previous In both scenarios the IRT and SEM results are quite close, and the mixed model is not far off, though it estimates less variance for the latent trait, which results in the rest of the estimates being slightly different since the latent trait scores are slightly different. Again though, the correlation of the IRT latent trait and random effects from the mixed model are 1.0, so we are not coming to different conclusions. Two Parameter Model The 1PM suggests items only differ by difficulty. In the SEM approach, this led to the factor loadings being constrained to be equal, which in SEM is probably not a likely scenario. The two parameter IRT model (2PM) allows the discrimination parameter to vary by item. We noted the model before, where \\(\\alpha\\), the discrimination parameter was constant, so nothing else has changed besides that aspect, where now it is allowed to vary by item. \\[P(y_{ij}=1|\\theta, \\delta, \\alpha) = \\mathrm{logis}(\\alpha_j(\\theta_i-\\delta_j))\\] Let’s see it how this turns out for the abortion data. irt_2pm_par1 = ltm(Abortion ~ z1, IRT.param=F) irt_2pm_par2 = ltm(Abortion ~ z1, IRT.param=T) We start to see a lack of parallelism in the item characteristic curves, as well as differences in the item information curves. Above, we see that Item 3, ‘The woman is not married and does not wish to marry the man.’, has the most information, and as before distinguishes well those individuals lower than average score on the latent trait. In the testing interpretation, it is a relatively ‘easy’ item, though not too different from items, 2 and 4. Item 1 on the other hand, ‘The woman decides on her own that she does not.’, doesn’t discriminate well those who are low-scoring on the latent trait, but does for those on the high end. In the testing interpretation, this would be a relatively difficult item. 2PM as SEM The only change with the SEM approach62 is that we allow all the loadings to be estimated, much as we would with typical SEM models. The following shows the necessary model syntax. sem_2pm_model = &#39; # loadings theta =~ l1*Item_1 + l2*Item_2 + l3*Item_3 + l4*Item_4 # thresholds Item_1 | th1*t1 Item_2 | th2*t1 Item_3 | th3*t1 Item_4 | th4*t1 # use thresholds to get difficulty # or comment out and use thresholds from parameterization=&quot;theta&quot; diff_1 := - th1 / sqrt(1-l1^2) diff_2 := - th2 / sqrt(1-l2^2) diff_3 := - th3 / sqrt(1-l3^2) diff_4 := - th4 / sqrt(1-l4^2) # convert loadings to discrimination # or comment out and use loadings from parameterization=&quot;theta&quot; discrm_1 := l1 / sqrt(1-l1^2) discrm_2 := l2 / sqrt(1-l2^2) discrm_3 := l3 / sqrt(1-l3^2) discrm_4 := l4 / sqrt(1-l4^2) &#39; sem_2pm &lt;- cfa(sem_2pm_model, data=Abortion_asfactor, std.lv=T) # sem_2pm &lt;- cfa(sem_2pm_model, data=Abortion_asfactor, std.lv=T, parameterization=&#39;theta&#39;) summary(sem_2pm) lavaan (0.5-23.1097) converged normally after 13 iterations Number of observations 379 Estimator DWLS Robust Minimum Function Test Statistic 7.291 12.679 Degrees of freedom 2 2 P-value (Chi-square) 0.026 0.002 Scaling correction factor 0.586 Shift parameter 0.234 for simple second-order correction (Mplus variant) Parameter Estimates: Information Expected Standard Errors Robust.sem Latent Variables: Estimate Std.Err z-value P(&gt;|z|) theta =~ Item_1 (l1) 0.921 0.022 42.552 0.000 Item_2 (l2) 0.940 0.021 44.737 0.000 Item_3 (l3) 0.964 0.019 50.568 0.000 Item_4 (l4) 0.905 0.025 35.507 0.000 Intercepts: Estimate Std.Err z-value P(&gt;|z|) .Item_1 0.000 .Item_2 0.000 .Item_3 0.000 .Item_4 0.000 theta 0.000 Thresholds: Estimate Std.Err z-value P(&gt;|z|) Itm_1|t1 (th1) 0.156 0.065 2.410 0.016 Itm_2|t1 (th2) -0.237 0.065 -3.639 0.000 Itm_3|t1 (th3) -0.347 0.066 -5.273 0.000 Itm_4|t1 (th4) -0.299 0.066 -4.559 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) .Item_1 0.151 .Item_2 0.117 .Item_3 0.071 .Item_4 0.182 theta 1.000 Scales y*: Estimate Std.Err z-value P(&gt;|z|) Item_1 1.000 Item_2 1.000 Item_3 1.000 Item_4 1.000 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) diff_1 -0.402 0.171 -2.353 0.019 diff_2 0.694 0.227 3.051 0.002 diff_3 1.306 0.425 3.072 0.002 diff_4 0.701 0.177 3.965 0.000 discrm_1 2.371 0.369 6.425 0.000 discrm_2 2.752 0.527 5.220 0.000 discrm_3 3.623 1.012 3.580 0.000 discrm_4 2.122 0.329 6.454 0.000 Logistic link comparison. Probit link comparison. Three Parameter Model The 3PM will add a guessing parameter to the 2PM model. As an example, in most testing situations one can get a correct response on an item just by guessing. However, individuals do not necessarily guess randomly, such that if there are 4 choices, they’d not just have a .25 chance of getting something correct. \\[P(y_{ij}=1|\\theta, \\delta, \\alpha) = \\gamma_j + (1-\\gamma_j) \\cdot \\mathrm{logis}(\\alpha_j(\\theta_i-\\delta_j))\\] The model has the effect of including a lower bound on responding to the 2PM, and could vary by item as well. While it probably isn’t as applicable to the Abortion data, one can think of it as an offset or propensity/bias to endorse, and such a model in general might be suited to more imbalanced data response. We can use the tpm function in ltm to estimate such a model. Here we can see item 2, ‘The couple agree that they do not wish to have a child.’, does have an asymptote slightly above 0, where the others are estimated to be zero. This is perhaps not surprising as this is not a testing scenario, and less amenable to guessing. Four Parameter Model As one might have already been thinking, just as we could have a lower bound, we can also add an upper bound to the probability of endorsement. In the testing scenario, this would regard very difficult items, that even those high on the latent trait might not have a high probability of being correct. I’ve seen these ‘ceilings’ referred to 1 - slipping, where the slip parameter is the probability of providing an incorrect response despite knowing the associated skill. \\[P(y_{ij}=1|\\theta, \\delta, \\alpha) = \\gamma_j + (\\zeta_j-\\gamma_j) \\cdot \\mathrm{logis}(\\alpha_j(\\theta_i-\\delta_j))\\] See the sirt package and its function rasch.mml2 for a means to estimate such models. Other IRT Models Additional covariates If you think back to the Rasch model as a mixed model, it is straightforward to add person level characteristics to the model. One would think, and especially in the case of non-testing situations, that any number of demographic contexts might influence item endorsement. As such, one might consider adding them when doing IRT as well. Graded Response Model The graded response model allows us to move from a simple binary setting to one in which we have multiple, ordered response categories, as with Likert items. The first approach to analyze such data just switches to an ordinal model. If there are only two categories, it’s identical to the 2PM, just as ordinal logistic regression would be to binary logistic regression. Consider a response with four categories. The basic ordinal model assumes different, ordered thresholds as we move from category 1 to 2, 2 to 3 and so on. However, we only need \\(k-1\\) thresholds, where \\(k\\) is the number of categories, as any that are not classified into the k-1 categories would automatically be in the \\(k^{th}\\) category. Most ordinal regression models would assume that any fixed effects, for example, for items, would be constant as we consider 1 vs. 3:4, 1 or 2 vs. 3 or 4, 1:3 vs. 4. Given the multiple thresholds per item, the interpretation can no longer be thought of simply as ‘difficulty’, though the discrimination parameter would have the same interpretation as in the binary case. In general, any standard ordinal regression model would potentially be applicable (e.g. cumulative, continuation-ratio, adjacent-category, generalized etc.). IRT specific extensions include the partial credit model, which in the simplest setting is the Rasch for ordered items, and a special case of the PCM, the rating scale model63, which is used if response categories have the same meaning for all items (thresholds are thus fixed to be equal across items). To get started, one might examine the grm and gpcm functions in ltm, or RSM in the eRm package. If you move into the Rasch/1PM model setting, you might also consider the ordinal package for the mixed model approach with ordinal outcomes. Multidimensional IRT From the SEM perspective, multidimensional IRT is pretty straightforward, as we simply assume more than one latent variable pertaining to individuals. As in SEM, this should be driven by theoretical considerations as much as possible. See the mirt package. Other IRT There are many more complicated variants of the models explicitly discussed here, different estimation methods, ways to assess multidimensionality and so forth, and people have ascribed names to very similar models or slight tweaks. In general though, the IRT approach is highly flexible for a wide range of item situations. Summary Too many exposed to latent class analysis seem to think that’s the only way to deal with categorical sets of items. In fact, assuming distinct latent classes is likely less plausible than positing an underlying continuum, and many who find such classes often consider them ordered anyway. IRT supplies not only a rich way to understand potentially multiple traits, it provides a means for deep inspection of item performance, lending much to assessing reliability of a measure in a more comprehensive fashion than simply noting a single statistic like Cronbach’s \\(\\alpha\\). In general, IRT provides many tools for assessment and development of scales to measure any number of things, and should be in your SEM toolbox. IRT Terminology 1PM: only concerns the latent trait of a unit of observation and item endorsement 2PM: add item discrimination to the 1PM 3PM: adds a lower bound of response to the 2PM 4PM: adds an upper bound to the 3PM Polytomous IRT: IRT for ordinal response, including graded response, partial credit, response scale and other models. Multidimensional IRT: includes multiple latent traits for the units of observation Differential Item Functioning (DIF): items are responded to differently by different groups (bias). R Packages Used ltm lavaan lme4 Others noted but not demonstrated include mirt and sirt. This bit on item information is more or less a paraphrase of a section Revelle’s chapter on IRT which, though incomplete, provides still more detail.↩ I actually did a Bayesian Rasch model and a Bayesian mixed model approach, both with Stan (the latter with brms), and came up with around ~4.3 for the birt and duplicated ltm’s result with the mixed model.↩ The probit link uses the cumulative normal distribution to convert the latent variable (the logit from before) to the probability scale. In R we use pnorm instead of plogis.↩ The 1.7 is not arbitrary and has a long history in IRT. The basic idea is that the variance of the logistic is \\(\\pi^2/3\\), or in terms of standard deviation, 1.814. However, it turns out that 1.702 actually minimizes the difference between the two approaches. You can see it noted in the ltm vignette, but see this article for some historical context Origin of the Scaling Constant d = 1.7 in Item Response Theory, Gregory Camilli.↩ These transformations are standard and you will see them in most discussions of the connection between factor analysis and IRT. As a starting point, see the help file for the irt.fa function in the psych package. Also see the code here for the text on latent variable modeling in R- link.↩ Note that we can actually calculate the thresholds as follows: -qnorm(colMeans(Abortion)) = 0.156, -0.237, -0.347, -0.299↩ Note that the 2PM has no generalized linear mixed model complement as it uses products of parameters. Others extend the 2PM and so the story is the same for those. See Boeck et al. (2011) The Estimation of Item Response Models with the lmer Function from the lme4 Package in R.↩ The graded response model is akin to the cumulative probability model, while the partial credit and rating scale models go with the adjacent category approach, which itself can be seen a special case of the multinomial logistic model.↩ "],
["topic-models.html", "Topic Models Latent Dirichlet Allocation Analysis Summary of Topic Models", " Topic Models Anyone who has taken a literature class has been asked to discuss the various themes of a given text. How are such themes discovered? Upon close reading of the text, recurrent references to similar concepts might be bundled in our minds as some general theme hidden throughout the text. Discovery of such themes is part of the joy of reading in general. One can think of these hidden, or latent, themes much as we would latent variables in factor analysis. Different texts have both contrasting and common themes, but the author doesn’t usually come out and announce a theme explicitly. Now consider a situation in which you have a million books. You can’t read that many, but may still want to discover the themes in them. This is where topic modeling can be useful. We don’t actually analyze raw text, but what we can do is get word counts for every text64, and construct a document term matrix (DTM). In this matrix, rows represent the documents and columns represent terms that are found in all documents. The values represent the counts of how many times a term is found in a text. With many texts, one will easily have thousands of columns, but most texts do not use most of the words, resulting in a very sparse matrix that is mostly zeros. Still, now that we have a numeric matrix, we can perform analysis on it. The goal is just like the goal PCA and factor analysis- we want to reduce these thousands of columns of terms to a far fewer number of topics. Furthermore, like factor analysis, we will want to interpret the topics, in this case based on the terms associated with them. Latent Dirichlet Allocation The most common approach to topic modeling is latent dirichlet allocation (LDA), which one can think of as discrete PCA. In my opinion, this should be as much a part of your toolbox as PCA and Factor Analysis, as ‘compositional’ data, where we have counts of occurrences (out of some total), are quite common. In the past PCA was applied to such data, but it is essentially a less performant approach with less intuitive results. I have gone into far more detailed demonstration elsewhere, and have no desire to duplicate it. I have workshop notes devoted exclusively to it, as well as hands-on demos here and here. However we can discuss a couple things, starting with the Dirichlet distribution. A draw from the Dirichlet distribution can be seen as a probability distribution for a k category event. It has one parameter, we’ll call \\(\\alpha\\), which is often referred to as the concentration parameter. If the k \\(\\alpha\\) values are equal, the resulting k probabilities will be equal on average, and with larger \\(\\alpha\\), there will be less variance around that probability. When they are unequal, the larger values will result in larger probabilities assigned. Consider the following for k=5 topics. library(gtools) probs1 = rdirichlet(n=1000, alpha=rep(1,5)) probs2 = rdirichlet(n=1000, alpha=rep(100,5)) # less variance probs3 = rdirichlet(n=1000, alpha=(1:5)^2) map(list(probs1, probs2, probs3), colMeans) %&gt;% map(round, 2) [[1]] [1] 0.20 0.19 0.20 0.20 0.20 [[2]] [1] 0.2 0.2 0.2 0.2 0.2 [[3]] [1] 0.02 0.07 0.17 0.29 0.45 In topic modeling, the probabilities can represent the probability of various topics, or the probability of terms within topics. However, the thing to note is that LDA can be applied to any appropriate data, it doesn’t have to be a document-term matrix resulting from text. Any count-based data matrix might potentially be appropriate. Analysis When it comes to topic modeling, most of the time is spent on processing the text itself. Importing/scraping it, dealing with capitalization, punctuation, removing stopwords, dealing with encoding issues, removing other miscellaneous common words. It is a highly iterative process such that once you initially get to the document-term matrix, you’re just going to find the stuff that was missed before and repeat the process with new ‘cleaning parameters’ in place. So getting to the analysis stage is the hard part. The following image is from the tidytext book, and gives some sense of the process, as well as some R packages that might be of use to you. In what follows we’ll start at the point of having the DTM in place and ready for analysis. For our needs we’ll use the topicModels package for the analysis, and mostly others for post-processing. As mentioned, one of the primary results of such an analysis are the probabilities of terms within topics, which like factor loadings, can aid in interpreting the topics. The other result is the probability that a topic will be present in a given document. The texts we’ll analyze are Dante’s Divine Comedy. Each canto within the three texts of Inferno, Purgatory, and Paradise will be treated as a document. I have already created the DTM where stopwords have been removed, but plenty more cleaning could have been applied. To get a sense of what’s going into the analysis, I show some of the more common terms, but note also, as in most text analysis, most terms are not present in most documents, leading to a notably sparse DTM. load(&#39;data/topic/divine_comedy_dtm.RData&#39;) &lt;&lt;DocumentTermMatrix (documents: 101, terms: 10516)&gt;&gt; Non-/sparse entries: 36748/1025368 Sparsity : 97% Maximal term length: 16 Weighting : term frequency (tf) As with factor or cluster analysis, one must choose the number of topics to retain. There are various methods/statistics that can help with this, but simple interpretability could be used as well. We’ll go with three to see if the three books are uniquely expressive. library(topicmodels) chapters_lda = LDA(divine_comedy_chapters_dtm, k = 3, control = list(seed = 1234)) chapters_lda A LDA_VEM topic model with 3 topics. First, we can simply look at probable terms. The following shows the top 10 most probable terms for each topic. For the following I’ve collapsed across cantos to get an average topic probability for each book. For example, with the Inferno, the topic where god, heaven, and virtue are less probable, while the one with beneath and cried are more so. With Paradise you have the opposite situation, and are more likely to find the topics regarding virtue, heaven, sun, and god. Purgatory, perhaps not surprisingly is a fairly balanced mix of topics65. Summary of Topic Models This is just a snippet of the potential with topic modeling, but hopefully you get the idea. As in other latent variable approaches, one can see LDA as a dimension reduction technique, where thousands of terms are boiled down to a few topics. However, topic models can also possess a rich interpretive quality. Just remember that what you call ‘documents’, ‘terms’ and ‘topics’ is not limited to text, and is in fact highly flexible. Words don’t have to be just words, but also might be collections of terms.↩ This is just like in factor analysis where some variables load mostly on one factor while some have notable cross-loadings.↩ "],
["bayesian-nonparametric-models.html", "Bayesian Nonparametric Models Chinese Restaurant Process Indian Buffet Process Summary R packages used", " Bayesian Nonparametric Models In mixture models and factor analysis we must set the number of clusters or factors respectively in order to run the models. The natural question is obviously how many clusters or how many factors? There is no answer66. For standard cluster analyses like k-means and standard factor analysis, crude measures may be provided but they are not very satisfactory. Model based approaches like mixture models and SEM may allow for information-based model comparison or similar, but are still problematic. Bayesian nonparametric models allow for a different approach. Instead of predetermining the number of clusters or latent variables, their numbers are allowed to grow with the data itself, or rather, the number is assumed infinite and the addition of latent classes/variables grows with the data. The section provides a very brief overview of the techniques, and essentially summarizes the Gershman &amp; Blei tutorial (2011). Chinese Restaurant Process The Chinese Restaurant Process (CRP) regards the extension of (finite) mixture models previously discussed, but where we now consider infinite capacity models. The idea is to imagine a restaurant67 with an infinite number of tables, where the tables represent our clusters. Now come the customers. The first sits at the first table. The next sits at that table with probability \\(1/(1+\\alpha)\\), or the next table with \\(\\alpha/(1+\\alpha)\\). The process continues for the nth person, who sits at any occupied table with probability \\(m_k/(n-1+\\alpha)\\) where \\(m_k\\) is the number of people sitting at table \\(k\\), and at a new table \\(\\alpha/(n - 1+\\alpha)\\). In other words, a person sits at the occupied tables with some probability proportional to the number of people already sitting there, and the unoccupied table at some probability proportional to \\(\\alpha\\). The choice of the parameter determines how likely new customers are to join an occupied or new table. It is perhaps instructive, at least for those more familiar with R or other programming, to see some code for this process68. The following code is in no way optimized, and serves only for demonstration. One can set the alpha parameter and number of observations. crp &lt;- function(alpha, n) { table_assignments = 1 for (i in 2:n){ table_counts = table(table_assignments) # counts of table assignments nt = length(table_counts) # number of tables table_prob = table_counts/(i-1+alpha) # probabilities of previous table assignments # sample assignment based on probability of current tables and potential next table current_table_assignment = sample(1:(nt+1), 1, prob=c(table_prob, 1-sum(table_prob))) # concatenate new to previous table assignments table_assignments = c(table_assignments, current_table_assignment) } table_assignments } Setting a higher \\(\\alpha\\), sometimes called the concentration parameter, allows new table assignments to be made more easily. In the following visuals, rows represent data points, columns are the cluster to which they are assigned. n = 100 crp_1 = crp(alpha=1, n=n) crp_4 = crp(alpha=4, n=n) Going back to the finite mixture model setting, the CRP defines the prior for category membership in that context, but without assuming a number of classes beforehand. As in the standard mixture model, any number of parameters \\(\\theta_k\\) may serve to define each of the \\(k\\) categories. In addition, in the Bayesian context, a hyperprior may be placed on an unknown \\(\\alpha\\). Indian Buffet Process The so-called Indian Buffet Process (IBP) is the continuous latent variable counterpart to the CRP. The idea is that of sampling an infinite number dishes at an Indian restaurant buffet, where each dish represents a latent factor. Like sitting at tables in the CRP, customers are more likely to sample dishes that have already been sampled. One key difference is that the customers are the observed variables or measures, and may be associated with any number of dishes (latent factors), whereas in the CRP, data points are assigned to only one latent class. The following code demonstrates the IBP. ibp = function(alpha, N){ # preallocate assignments with upper bound of N*alpha number of latent factors assignments = matrix(NA, nrow=N, ncol=N*alpha) # start with some dishes/assigments dishes = rpois(1, alpha) zeroes = ncol(assignments) - dishes # fill in the rest of potential dishes assignments[1,] = c(rep(1, dishes), rep(0, zeroes)) for(i in 2:N){ prev = i-1 # esoteric line that gets the last dish sampled without a search for it last_previously_sampled_dish = sum(colSums(assignments[1:prev,,drop=F]) &gt; 0) # initialize dishes_previously_sampled = matrix(0, nrow=1, ncol=last_previously_sampled_dish) # calculate probability of sampling from previous dishes dish_prob = colSums(assignments[1:prev, 1:last_previously_sampled_dish, drop=F]) / i dishes_previously_sampled[1,] = rbinom(n=last_previously_sampled_dish, size=1, prob=dish_prob) # sample new dish and assign based on results new_dishes = rpois(1, alpha/i) zeroes = ncol(assignments) - (last_previously_sampled_dish + new_dishes) assignments[i,] = c(dishes_previously_sampled, rep(1,new_dishes), rep(0, zeroes)) } # return only the dimensions sampled last_sampled_dish = sum(colSums(assignments[1:prev,]) &gt; 0) return(assignments[, 1:last_sampled_dish]) } With the above code we can demonstrate the IBP, with one setting that would make the creation of more latent variables more difficult, and one less so. In the following visualization, the columns represent the latent factors and the rows are the dimensions assigned to them. ibp_1 = ibp(1, 100) ibp_4 = ibp(4, 100) In both cases, the result of the IBP is a binary matrix where rows represent the measures and columns the latent factors. However, we see that with the initial result, most stick only to one or two factors, while in the latter they are more likely to sample additional ‘dishes’. So now that we have the binary, on-off, switch, \\(Z\\), that results from the IBP, we can think of the usual factor loading matrix \\(\\Lambda\\) as decomposed into the \\(Z\\) and weight matrix \\(w\\), and then we are as we were before with observed variables X as a weighted combination of the latent factors69. \\[Z \\odot w = \\Lambda\\] \\[X_n = \\Lambda F_n + \\epsilon_n\\] The above assumes an observed data matrix \\(X\\), where each \\(n\\) observations is of some dimension \\(m\\). \\(\\Lambda\\) is our \\(m\\) by \\(k\\) loading matrix, where \\(k\\) is the number of latent factors, and is created by the elementwise product of \\(Z\\) and \\(w\\). \\(F\\) represents our factors, and \\(\\epsilon\\) the noise. The main take home point regarding the IBP is that, like with the CRP, we do not have to prespecify a number of latent factors. In addition, it has a regularizing effect to keep the number of factors low, depending on the prior setting (\\(\\alpha\\)). Summary Bayesian nonparametric analyses allow the number latent classes or factors to grow with the data, and without determining a (possibly arbitrary) number beforehand. There are many applications where such tools would have obvious appeal. There are very few cases where one knows the number of (hard) clusters beforehand, and the model comparison approaches for determining the ‘best’ number of clusters is problematic. In SEM we usually ‘know’ the number of factors based on theory. However, very often what theory suggests for a specific data setting may be less clear. In other cases we may be in the scale development stage, and not be to sure about the number of latent factors. Outside of SEM, in the purely dimension reduction scenario, we definitely do not know how many to retain. A method using IBP may suffice to help in this regard. Furthermore, one can combine clustering and latent variable approaches with known or nonparametric approaches, leading to a range of modeling choices- factor analysis, mixture models, mixtures of FA, mixtures of infinite FA, and infinite mixtures of infinite FA (IMIFA). See Murphy, Gormley, &amp; Viroli (2017) for more detail. R packages used None. There are numerous packages for the CRP if one looks for ‘Bayesian nonparametric’ or ‘dirichlet process’, though at this point there are quite a few variants of the underlying process possible too. In addition, many of them would allow usage as with the flexmix package, where model parameters can vary over the latent clusters. I can find very little in R for the IBP, even under ‘beta’ and ‘bernoulli process’ but see BCSub for starters, and there are Python and Matlab implementations. Just very recently, the IMIFA has been released on CRAN, for inifinite mixtures of infinite factor analysers, and as it appears to model everything from standard FA to IMIFA, looks to be quite promising. When time permits I may provide an example in the future. There is no answer because they aren’t real.↩ The name comes from an early paper whose authors were referencing restaurants in San Francisco.↩ The code the CRP and IBP above is based partly on some R code I found on GitHub, which has a nice shiny app you can explore.↩ This follows the presentation in Gershman &amp; Blei tutorial (2011) and Knowles &amp; Ghahramani (2011). In other instances, it is depicted similar to the CRP, where the binary mask matrix Z regards the latent factors (e.g. Knowles &amp; Ghahramani (2007)), i.e. it focuses on the data observations rather than the data dimensions \\(\\Lambda(Z\\odot F)\\).↩ "],
["exercises-1.html", "Exercises", " Exercises Exercises are in a separate document here. "],
["appendix.html", "Appendix Data Set Descriptions Terminology in SEM Lavaan Output Explained Code Examples Causal Bias Software Revisited Resources", " Appendix Data Set Descriptions McClelland Description McClelland et al. (2013) abstract This study examined relations between children’s attention span-persistence in preschool and later school achievement and college completion. Children were drawn from the Colorado Adoption Project using adopted and non-adopted children (N = 430). Results of structural equation modeling indicated that children’s age 4 attention span-persistence significantly predicted math and reading achievement at age 21 after controlling for achievement levels at age 7, adopted status, child vocabulary skills, gender, and maternal education level. Relations between attention span-persistence and later achievement were not fully mediated by age 7 achievement levels. Logistic regressions also revealed that age 4 attention span-persistence skills significantly predicted the odds of completing college by age 25. The majority of this relationship was direct and was not significantly mediated by math or reading skills at age 7 or age 21. Specifically, children who were rated one standard deviation higher on attention span-persistence at age 4 had 48.7% greater odds of completing college by age 25. Discussion focuses on the importance of children’s early attention span-persistence for later school achievement and educational attainment. Reference McClelland, Acock, Piccinin, Rheac, Stallings. (2013). Relations between preschool attention span-persistence and age 25 educational outcomes. link Note that there is only one age 25 outcome (college completion) and two age 21 outcomes. The following models duplicate the paper results. Additional non-mediation models are provided for comparison. modReading = &quot; read21 ~ rr*read7 + ar21*attention4 + vocab4 + male + adopted + momed read7 ~ ar7*attention4 # in att4_read21 := ar7*rr &quot; reading = sem(modReading, data=mcclelland, missing=&#39;fiml&#39;, mimic = &#39;Mplus&#39;, std.ov=TRUE, se =&#39;boot&#39;) summary(reading, rsquare=TRUE) modRead = &quot; read21 ~ read7 + attention4 + vocab4 + male + adopted + momed &quot; readnomed = sem(modread, data=mcclelland, missing=&#39;fiml&#39;, mimic = &#39;Mplus&#39;, std.ov=TRUE, se =&#39;boot&#39;) AIC(read, readnomed) modMath = &quot; math21 ~ mm*math7 + am21*attention4 + vocab4 + male + adopted + momed math7 ~ am7*attention4 # in att4_math21 := am7*mm &quot; math = sem(modMath, data=mcclelland, missing=&#39;fiml&#39;, mimic = &#39;Mplus&#39;, std.ov=TRUE, se =&#39;boot&#39;) summary(math, rsquare=TRUE, fit=T) modMath = &quot; math21 ~ math7 + attention4 + vocab4 + male + adopted + momed &quot; mathnomed = sem(modMath, data=mcclelland, missing=&#39;fiml&#39;, mimic = &#39;Mplus&#39;, std.ov=TRUE, se =&#39;boot&#39;) AIC(math, mathnomed) National Longitudinal Survey of Youth (1997, NLSY97) Description NLSY97 consists of a nationally representative sample of approximately 9,000 youths who were 12 to 16 years old as of December 31, 1996. Round 1 of the survey took place in 1997. In that round, both the eligible youth and one of that youth’s parents received hour-long personal interviews. In addition, during the screening process, an extensive two-part questionnaire was administered that listed and gathered demographic information on members of the youth’s household and on his or her immediate family members living elsewhere. Youths are interviewed on an annual basis. The NLSY97 is designed to document the transition from school to work and into adulthood. It collects extensive information about youths’ labor market behavior and educational experiences over time. Employment information focuses on two types of jobs, “employee” jobs where youths work for a particular employer, and “freelance” jobs such as lawn mowing and babysitting. These distinctions will enable researchers to study effects of very early employment among youths. Employment data include start and stop dates of jobs, occupation, industry, hours, earnings, job search, and benefits. Measures of work experience, tenure with an employer, and employer transitions can also be obtained. Educational data include youths’ schooling history, performance on standardized tests, course of study, the timing and types of degrees, and a detailed account of progression through post-secondary schooling. Reference Bureau of Labor Statistics Wheaton 1977 data Description Longitudinal data to develop a model of the stability of alienation from 1967 to 1971, accounting for socioeconomic status as a covariate. Each of the three factors have two indicator variables, SES in 1966 is measured by education and occupational status in 1966 and alienation in both years is measured by powerlessness and anomia. Reference Wheaton, B., Muthen B., Alwin, D., &amp; Summers, G., 1977, “Assessing reliability and stability in panel models”, in D. R. Heise (Ed.), Sociological Methodology 1977 (pp. 84-136), San Francisco: Jossey-Bass, Inc. Harman 5 Description “data…were taken (not entirely arbitrarily) from a study of the Los Angeles Standard Metropolitan Statistical Area. The twelve individuals are used in the examples are census tracts.” Included are: Total population Median school years Total employment Miscellaneous professional services Median house value Reference Harman, H. Modern Factor Analysis. Google Books link Big Five Description 25 personality self report items regarding five factors- Agreeableness, Conscientiousness, Extroversion, Neuroticism, and Openness - taken from the International Personality Item Pool (ipip.ori.org) were included as part of the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project. The data from 2800 subjects are included here as a demonstration set for scale construction, factor analysis, and Item Response Theory analysis. Three additional demographic variables (sex, education, and age) are also included. A1: Am indifferent to the feelings of others. (q_146) A2: Inquire about others’ well-being. (q_1162) A3: Know how to comfort others. (q_1206) A4: Love children. (q_1364) A5: Make people feel at ease. (q_1419) C1: Am exacting in my work. (q_124) C2: Continue until everything is perfect. (q_530) C3: Do things according to a plan. (q_619) C4: Do things in a half-way manner. (q_626) C5: Waste my time. (q_1949) E1: Don’t talk a lot. (q_712) E2: Find it difficult to approach others. (q_901) E3: Know how to captivate people. (q_1205) E4: Make friends easily. (q_1410) E5: Take charge. (q_1768) N1: Get angry easily. (q_952) N2: Get irritated easily. (q_974) N3: Have frequent mood swings. (q_1099) N4: Often feel blue. (q_1479) N5: Panic easily. (q_1505) O1: Am full of ideas. (q_128) O2: Avoid difficult reading material.(q_316) O3: Carry the conversation to a higher level. (q_492) O4: Spend time reflecting on things. (q_1738) O5: Will not probe deeply into a subject. (q_1964) gender: Males = 1, Females =2 education: 1 = HS, 2 = finished HS, 3 = some college, 4 = college graduate 5 = graduate degree age: age in years Reference Goldberg, L.R. (1999) A broad-bandwidth, public domain, personality inventory measuring the lower-level facets of several five-factor models. In Mervielde, I. and Deary, I. and De Fruyt, F. and Ostendorf, F. (eds) Personality psychology in Europe. 7. Tilburg University Press. Tilburg, The Netherlands. Revelle, W., Wilt, J., and Rosenthal, A. (2010) Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link In Gruszka, A. and Matthews, G. and Szymura, B. (Eds.) Handbook of Individual Differences in Cognition: Attention, Memory and Executive Control, Springer. Old Faithful From the R helpfile Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. A closer look at faithful$eruptions reveals that these are heavily rounded times originally in seconds, where multiples of 5 are more frequent than expected under non-human measurement. For a better version of the eruption times, see the example below. There are many versions of this dataset around: Azzalini and Bowman (1990) use a more complete version. Harman 1974 Description A correlation matrix of 24 psychological tests given to 145 seventh and eight-grade children in a Chicago suburb by Holzinger and Swineford. Reference Harman, H. H. (1976) Modern Factor Analysis, Third Edition Revised, University of Chicago Press, Table 7.4. Marsh &amp; Hocevar 1985 Description Data collected using the Self-Description Questionnaire and includes sixteen subscales designed to measure nonacademic status: four intended to measure self-perception of physical ability, four intended to measure self-perception of physical appearance, four intended to measure quality of relations with peers, and four intended to measure quality of relations with parents. The *.dta file has summary statistics based on 134 students in grade 4 and 251 students in grade 5 from Sydney, Australia. Group 1 is grade 4, group 2 is grade 5. It’s used as an example in the SEM reference manual for Stata. Reference Marsh, H. W. and Hocevar, D., (1985). “Application of confirmatory factor analysis to the study of self-concept: First- and higher order factor models and their invariance across groups”, Psychological Bulletin, 97: 562-582. Abortion Attitudes Description The data contain responses given by 410 individuals to four out of seven items concerning attitude to abortion. A small number of individual did not answer to some of the questions and this data set contains only the complete cases. 379 individuals answered to the following questions after being asked if the law should allow abortion under the circumstances presented under each item. Item 1: The woman decides on her own that she does not. Item 2: The couple agree that they do not wish to have a child. Item 3: The woman is not married and does not wish to marry the man. Item 4: The couple cannot afford any more children. Reference McGrath, K. and Waterton, J. (1986) British social attitudes, 1983-86 panel survey. Terminology in SEM This just puts the terminology sections of the previous chapters together in one place. Latent variable: an unobserved or hidden variable. It’s specific interpretation will depend on the modeling context. aka factor, construct, etc. Factor Analysis: in the SEM literature, this refers to a latent variable (measurement) model to assess the underlying construct behind the correlations among a set of observed variables. Elsewhere it may refer to a very broad family of matrix factorization techniques that would include things like principal components analysis, non-negative matrix factorization, etc. Item, Indicator, Observed, Manifest, Variable: Terms I use interchangeably within the context of factor analysis. Loadings: measures of the relationship between an indicator and the latent variable. For clarity, it’s probably better to use pattern or structure coefficient. Pattern coefficient: What is usually displayed in FA results. The path coefficient from a latent variable to some observed variable. In some cases it is a simple correlation coefficient. Structure coefficient: The correlation between an observed an latent variable. Communality: the amount of variance in the item/variable explained by the (retained) components. It is the sum of the squared loadings. Uniqueness: 1 - communality. The unexplained variance of a variable. See disturbance. Identification: Refers to whether a unique solution can possibly be estimated for a given model. Models may be under-, over- or just-identified. It is a function of the model specification rather than the data. Fit: Model fit is something very difficult to ascertain in SEM, and notoriously problematic in this setting, where all proposed cutoffs for a good fit are ultimately arbitrary. Even if one had most fit indices suggesting a ‘good’ fit, there’s little indication the model has predictive capability. Endo/Exogenous: Endogenous variables are determined by other variables, i.e. have an arrow pointing at their node. Exogenous variables have no analyzed causes. Disturbance: residual variance. Includes measurement error and unknown causes. Mixture Models: models using categorical latent variables. Growth Curve Models: models for longitudinal data setting. Growth Mixture Models: combines growth curves and mixture models. Sometimes called latent trajectory Multiple group models: I’ll let you guess. Usually involves tests of measurement invariance, or the ability for measurement models to hold up in different settings. Single indicator models: In some circumstances, and with the appropriate constraints, a latent variable can have a single indicator. Non-/Recursive models: have all unidirectional causal effects and disturbances are not correlated. A model is considered nonrecursive if there is a reciprocal relationship, feedback loop, or correlated disturbance in the model Modification Indices: A good way to further overfit the data without adding any explanatory power, since most will regard correlating residuals. Problematic and/or not very useful terms This section is based on my opinion, but also on what I see in consulting that confuses clients and muddies their thinking time and time again. Honestly, give researchers and methodologists enough time and they will invent a unique term for every model one can think of, and applied researchers are already taught statistics poorly enough to not be able to see the bigger picture that ties many models together under one framework. The graphical depiction of your model should be clear enough to note what types of variables and paths are involved, and models do not require a new name due to slight adjustments of previous ones. Exploratory vs. Confirmatory: This distinction is problematic. Science and data analysis is inherently exploratory, and most who use SEM do some model exploration as they would with any other model. Some SEM models have more constraints than others, but that does not require a separate name or way of thinking about the model. Mediation: an indirect effect, e.g. A-&gt;B-&gt;C, A has an indirect effect on C. A can have a direct effect on C too. The term mediation should be reserved for explicitly causal scenarios. Moderation: an interaction (the same ones utilized in a standard regression modeling) but with specific constraints on the relationships of the moderator and other variables. The term Moderation should be reserved for explicitly causal scenarios. See the terminology section in the graphical models chapter for more detail. Exploratory SEM: a term as useful as ‘exploratory’ factor analysis. Fully vs. Partially Latent SEM: see previous. MIMIC Model: see previous. Honestly, what’s the difference between a MIMIC and partially latent model except for calling one of the observed variables an indicator? With categorical data: Aside from noting whether the latent variable is categorical or not, these aren’t very enlightening, and in the end, it’s all just ‘latent variable analysis’. Lavaan Output Explained Here I provided the most detailed output from an SEM with lavaan with each component explained. lavaan (0.5-23.1097) converged normally after 73 iterations Number of observations 932 Estimator ML Minimum Function Test Statistic 4.735 Degrees of freedom 4 P-value (Chi-square) 0.316 Model test baseline model: Minimum Function Test Statistic 2133.722 Degrees of freedom 15 P-value 0.000 User model versus baseline model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 0.999 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -15213.274 Loglikelihood unrestricted model (H1) -15210.906 Number of free parameters 17 Akaike (AIC) 30460.548 Bayesian (BIC) 30542.783 Sample-size adjusted Bayesian (BIC) 30488.792 Root Mean Square Error of Approximation: RMSEA 0.014 90 Percent Confidence Interval 0.000 0.053 P-value RMSEA &lt;= 0.05 0.930 Standardized Root Mean Square Residual: SRMR 0.007 Parameter Estimates: Information Expected Standard Errors Standard Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ses =~ education 1.000 2.607 0.842 sei 5.219 0.422 12.364 0.000 13.609 0.642 alien67 =~ anomia67 1.000 2.663 0.774 powerless67 0.979 0.062 15.895 0.000 2.606 0.852 alien71 =~ anomia71 1.000 2.850 0.805 powerless71 0.922 0.059 15.498 0.000 2.628 0.832 Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all alien71 ~ alien67 (aa) 0.607 0.051 11.898 0.000 0.567 0.567 ses -0.227 0.052 -4.334 0.000 -0.207 -0.207 alien67 ~ ses (sa) -0.575 0.056 -10.195 0.000 -0.563 -0.563 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .anomia67 ~~ .anomia71 1.623 0.314 5.176 0.000 1.623 0.356 .powerless67 ~~ .powerless71 0.339 0.261 1.298 0.194 0.339 0.121 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .education 2.801 0.507 5.525 0.000 2.801 0.292 .sei 264.597 18.126 14.597 0.000 264.597 0.588 .anomia67 4.731 0.453 10.441 0.000 4.731 0.400 .powerless67 2.563 0.403 6.359 0.000 2.563 0.274 .anomia71 4.399 0.515 8.542 0.000 4.399 0.351 .powerless71 3.070 0.434 7.070 0.000 3.070 0.308 ses 6.798 0.649 10.475 0.000 1.000 1.000 .alien67 4.841 0.467 10.359 0.000 0.683 0.683 .alien71 4.083 0.404 10.104 0.000 0.503 0.503 R-Square: Estimate education 0.708 sei 0.412 anomia67 0.600 powerless67 0.726 anomia71 0.649 powerless71 0.692 alien67 0.317 alien71 0.497 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all IndirectEffect -0.349 0.041 -8.538 0.000 -0.319 -0.319 lavaan (0.5-22) converged normally after 73 iterations The first is the iterations, i.e. how many steps the estimation process took to arrive at the final conclusion for parameter estimates. It could potentially be a useful diagnostic if, for example, an alteration to the model resulted in many more iterations, but this is highly dependent on a number of things. To understand more about this you’d need to get more familiar with maximum likelihood estimation. Number of observations 932 Next is the sample size, which you should check against your expectations. You will get two values if you did some something to deal with missing values. One will regard the complete cases only, the other larger one will regard the full data. As with every other modeling setting, larger samples will result in smaller standard errors, all else being equal. Estimator ML Minimum Function Test Statistic 4.735 Degrees of freedom 4 P-value (Chi-square) 0.316 The next part regards the estimation technique along with the standard \\(\\mathcal{\\chi}^2\\) test that is reported in all SEM studies. This first line tells us we’re doing standard maximum likelihood estimation. Much of the output following will be duplicated if you run a robust version, as it will give you both the standard ML output and the robust results. The Minimum Function Test Statistic is the \\(\\mathcal{\\chi}^2\\) value. The degrees of freedom are essentially the number of observations in terms of covariances and variances minus the number of ‘free’ parameters, i.e. the number of parameters estimated. Recall that when these are equal, we the model is just-identified, and the fit is perfect. If it’s negative, the model is under-identified and you will need to go back to the drawing board. In general, this reflects how far away the model implied covariance matrix is from the observed covariance matrix. See the SEM chapter for more information. Model test baseline model: Minimum Function Test Statistic 2133.722 Degrees of freedom 15 P-value 0.000 The Model test baseline model is another \\(\\mathcal{\\chi}^2\\) test essentially comparing the model fit vs. a model that assumes no covariances among the data, i.e. the worst-fitting model. As such, this is similar to the model test result we get in standard regression settings, where the null model is an intercept-only model. It should be statistically significant, but one shouldn’t be doing cartwheels in the streets if it is. However, one can use it to test an another (nested) lavaan model result for statistical comparison. User model versus baseline model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 0.999 Next are two of many fit indices, several of which are based on the previous test. See the fit section for more detail. Loglikelihood and Information Criteria: Loglikelihood user model (H0) -15213.274 Loglikelihood unrestricted model (H1) -15210.906 Next is the log likelihood values that go into the initial \\(\\mathcal{\\chi}^2\\). If you multiply their values by two and take the difference, you’ll get the Minimum Function Test Statistic. Note that your model is the null model here, and is compared to an unrestricted, i.e. saturated model. Number of free parameters 17 Akaike (AIC) 30460.548 Bayesian (BIC) 30542.783 Sample-size adjusted Bayesian (BIC) 30488.792 Now we see how many parameters were estimated and information criteria. See if you can look at the results of the coefficients etc. further down to come up with the value of 17. The information criteria, which are not in any way specific to SEM and are widely used for model comparison, are based on the log-likelihood, but add a penalty for the number of parameters estimated. For example, \\(\\textrm{AIC} = -2\\mathcal{L} + 2k\\), i.e. 2 times the ‘user model’ log-likelihood plus the log of the number of parameters estimated. The others are variations on this theme. See the fit indices section. Root Mean Square Error of Approximation: RMSEA 0.014 90 Percent Confidence Interval 0.000 0.053 P-value RMSEA &lt;= 0.05 0.930 Standardized Root Mean Square Residual: SRMR 0.007 These are discussed in the fit indices section. Parameter Estimates: Information Expected Standard Errors Standard This bit of information that starts the parameter estimates section of output regards how the standard errors are calculated. As an example, one might desire bootstrapped standard errors, and this would be noted here, along with the number of bootstrap iterations. The rest regards the parameter estimates, i.e. the paths, variances, covariances, and possibly ‘intercepts’ if examination of mean structure is desired (e.g. as in growth curve models), and these may regard both latent and observed variables. We will also get any user-defined parameter estimates. See any of the previous sections for more detail. Code Examples Factor Analysis via Maximum Likelihood On GitHub I have some R code I’ve put together that will estimate a factor analysis via maximum likelihood, i.e. a ‘by-hand’ approach fully within R. It will simulate some data for two correlated factors with four indicators each, and produce raw (function shown below) and standardized loadings, the log-likelihood, AIC, and BIC. It will also compare these results to lavaan output for the same data, as well as provide the corresponding Mplus code. # measurement model, covariance approach cfa.cov = function (parms, data) { # Arguments- parms: initial values (named); data: raw data # Extract paramters by name require(psych) # for tr l1 = c(1, parms[grep(&#39;l1&#39;, names(parms))]) # loadings for factor 1 l2 = c(1, parms[grep(&#39;l2&#39;, names(parms))]) # loadings for factor 2 cov0 = parms[grep(&#39;cov&#39;, names(parms))] # factor covariance, variances # Covariance matrix S = cov(data)*((nrow(data)-1)/nrow(data)) # ML covariance div by N rather than N-1, the multiplier adjusts # loading estimates lambda = cbind(c(l1, rep(0, length(l2))), c(rep(0, length(l1)), l2)) # disturbances dist.init = parms[grep(&#39;dist&#39;, names(parms))] disturbs = diag(dist.init) # factor correlation phi.init = matrix(c(cov0[1], cov0[2], cov0[2], cov0[3]), 2, 2) #factor cov/correlation matrix # other calculations and log likelihood sigtheta = lambda%*%phi.init%*%t(lambda) + disturbs pq = dim(data)[2] #in Bollen p + q (but for the purposes of this just p) = tr(data) #out = -(log(det(sigtheta)) + tr(S%*%solve(sigtheta)) - log(det(S)) - pq) #a reduced version; Bollen 1989 p.107 ll = ((-nrow(data)*pq/2)*log(2*pi)) - (nrow(data)/2)*(log(det(sigtheta)) + tr(S%*%solve(sigtheta))) #should be same as Mplus H0 loglike ll } Parallel Process Example # parallel process -------------------------------------------------------- # See EXAMPLE 6.13 in Mplus User&#39;s Guide # let&#39;s simulate data with a negative slope average and positive correlation among intercepts and other process slopes set.seed(1234) n = 500 timepoints = 4 time = rep(0:3, times=n) subject = rep(1:n, each=4) # first we&#39;ll draw intercepts with overall mean .5 and -.5 for the two # processes, and let them have a slight correlation. Their variance is 1. intCorr = matrix(c(1,.2,.2,1), ncol=2) colnames(intCorr) = rownames(intCorr) = c(&#39;i1&#39;, &#39;i2&#39;) intCorr interceptP1 = .5 interceptP2 = -.5 ranInts = MASS::mvrnorm(n, mu=c(0,0), Sigma = intCorr, empirical=T) ranInts = data.frame(ranInts) head(ranInts) cor(ranInts) colMeans(ranInts) # now create slopes with intercept/mean .4, -.4, but the same positive # relationship with their respective intercept. Their variances are also 1. slopeP1 = .4 slopeP2 = -.4 s1 = .3*ranInts$i2 + rnorm(n) s2 = .3*ranInts$i1 + rnorm(n) ranef = data.frame(ranInts, s1, s2) head(ranef) # so we have slight positive correlations among all random intercepts and slopes y1 = (interceptP1 + ranef$i1[subject]) + (slopeP1+ranef$s1[subject])*time + rnorm(n*timepoints, sd=.5) d1 = data.frame(Subject=subject, time=time, y1) head(d1) library(ggplot2) ggplot(aes(x=time, y=y1), data=d1) + geom_line(aes(group=Subject), alpha=.1) + geom_smooth(method=&#39;lm&#39;,color=&#39;red&#39;) + lazerhawk::theme_trueMinimal() y2 = (interceptP2 + ranef$i2[subject]) + (slopeP2+ranef$s2[subject])*time + rnorm(n*timepoints, sd=.5) d2 = data.frame(Subject=subject, time=time, y2) # process 2 shows the downward overall trend as expected ggplot(aes(x=time, y=y2), data=d2) + geom_line(aes(group=Subject), alpha=.1) + geom_smooth(method=&#39;lm&#39;,color=&#39;red&#39;) + lazerhawk::theme_trueMinimal() # Widen from long form for lavaan library(tidyr) negslopepospath1 = d1 %&gt;% spread(time, y1) colnames(negslopepospath1) = c(&#39;Subject&#39;, paste0(&#39;y1&#39;, 1:4)) head(negslopepospath1) negslopepospath2 = d2 %&gt;% spread(time, y2) colnames(negslopepospath2) = c(&#39;Subject&#39;, paste0(&#39;y2&#39;, 1:4)) # combine dparallel = dplyr::left_join(negslopepospath1, negslopepospath2) head(dparallel) mainModel = &quot; i1 =~ 1*y11 + 1*y12 + 1*y13 + 1*y14 s1 =~ 0*y11 + 1*y12 + 2*y13 + 3*y14 i2 =~ 1*y21 + 1*y22 + 1*y23 + 1*y24 s2 =~ 0*y21 + 1*y22 + 2*y23 + 3*y24 s1 ~ i2 s2 ~ i1 &quot; library(lavaan) mainRes = growth(mainModel, data=dparallel) summary(mainRes) fscores = lavPredict(mainRes) broom::tidy(data.frame(fscores)) lm(s2~., fscores) heatR::corrheat(cor(fscores)) qplot(s1, i2, data=data.frame(fscores)) + geom_smooth(method=&#39;lm&#39;, se=F) fv = lavPredict(mainRes, &#39;ov&#39;) summary(mainRes, standardized=T) d3heatmap::d3heatmap(cor(fv, fscores)) d3heatmap::d3heatmap(cor(select(dparallel, -Subject), ranef), Rowv = F, Colv = F) process1Model = &quot; i1 =~ 1*y11 + 1*y12 + 1*y13 + 1*y14 s1 =~ 0*y11 + 1*y12 + 2*y13 + 3*y14 &quot; p1Res = growth(process1Model, data=dparallel) fscoresP1 = lavPredict(p1Res) process2Model = &quot; i2 =~ 1*y21 + 1*y22 + 1*y23 + 1*y24 s2 =~ 0*y21 + 1*y22 + 2*y23 + 3*y24 &quot; p2Res = growth(process2Model, data=dparallel) fscoresP2 = lavPredict(p2Res) fscoresSeparate = data.frame(fscoresP1, fscoresP2) pathMod = &quot; s1 ~ i2 s2 ~ i1 i1~~i2 &quot; pathModRes = sem(pathMod, data=fscoresSeparate, fixed.x = F) summary(pathModRes) # you&#39;d have come to the same conclusions summary(mainRes) Causal Bias Consider what effects, that might conceivably have practical bearings, we conceive the object of our conception to have. Then, our conception of these effects is the whole of our conception of the object. I figure I should note my stance on soi-disant causal modeling so that whatever I might say in this document is taken with the appropriate context. What follows is more or less a philosophical stance, perhaps a naïve and not very well developed one at that, despite my philosophy background from long ago, but one that I think is a safer perspective than others commonly held regarding causes and statistics. Mostly this is just me jotting down some things I’m thinking about while working on this document, and perhaps I’ll be able to spend more time with it later. To begin, no statistical model by itself will ever provide evidence of causal effects. This is a fact that cannot be disputed. Statistical models of the sort we are usually interested in are inherently probabilistic, atheoretical in and of themselves, and wholly dependent on the data collected. No amount of fancy analysis or double-blind randomization will change the fact that in the end you have a probabilistic result, and one that is highly dependent on many assumptions both statistical and theoretical, as well as nuances of the data. The data itself might suggest several statistical models that are essentially if not exactly equally adequate. If you are using SEM or other approach to discover causal effects you will thus be unsuccessful, and as such, you should not be using the technique if that is the primary reason for doing so. Philosophically speaking, I also don’t think the methods of scientific analysis, i.e. statistics, can be used to prove causal relations, and more so if one considers that we’ve been debating the nature of causality for thousands of years and have yet to come to a conclusion about its definition that everyone can agree on. Such bold ‘proof-like’ and ‘causal’ claims, consistently shown to be wrong because they must be in order to be a scientific claim in the first place, have much to do with undermining public faith in scientific results. However, if we can’t entirely agree on what constitutes a causal relation in the first place, we definitely can’t hope that there is some magical statistical technique that would help us determine it. As an example, available data should not convince you that smoking causes lung cancer, and that is because it doesn’t. If it did, everyone who ever smoked would have lung cancer. This is a deterministic notion of causality, and there are others, but it is the one I think is used in everyday parlance. Using a probabilistic interpretation instead, e.g. a smoker is more likely to have cancer, just serves to emphasize the uncertainty that exists in the underlying model. I’m perfectly okay with this, but many seem uncomfortable with any lack of certainty. I might even say that smoking has an ‘effect’ on the likelihood of getting lung cancer70. In the end, all we have in the data are those that have lung cancer or not, and there is no uncertainty about them having it, nor does our knowledge of their smoking habits change the fact of whether they do. As another example, you can randomly assign people who have cancer to two groups- in one they take an aspirin every day, in the other they drink orange juice every day. You may then find that they are equally effective in terms of remission rates, but it would be silly to think the ‘treatments’ had any causal effect at all, even though the effects could be non-zero statistically. Randomization, which is assumed by many to have the magical ability to confer causality on a scientific endeavor, doesn’t help the situation either. In fact, it could be said that control is the key element in determining causal relations (a claim that like others is not without issue), and the random nature of assignment actually undermines that. Modern health science is not removed from this issue, or even far removed from this example, and regularly finds ‘effects’ of drugs or behavior that have no causal relation to the phenomenon under study. This is especially the case with psychological phenomena, many of which to this day still have little or no tie to operational definitions based on physiology rather than behavior. People even go so far as to ascribe an actual causal effect to nothing at all (placebo effect). I personally think it’s somewhat misguided to think that a major goal of (at least most of working) science is to establish formal causal relations. Its primary tool for weighing evidence, i.e. statistical modeling, certainly cannot do that. As much as we want control, which is one thing that could potentially establish causality, it eludes us, and problematically seems to beg for a human element to causality besides. Ceteris peribus can only work for what is included in our models. Furthermore, if we actually knew the cause of something, we definitely would not need a statistical model. On the practical side, few seem to be engaging in science for reasons of determining causal effects (except perhaps in the discussion sections of papers), and rather do so to discover new potential explanations and predict something better than we did before. Prediction A well-worn data set on which to test classification algorithms is the MNIST hand written digits data. The methods use the pixel gray-scale information to classify an image of what are originally handwritten digits as being one of the values 0-9. Modern approaches can get accurate classification on test data with less than 1% error. If one’s goal is to understand why the digits are the way they are, the algorithm cannot help you. And yet, a statistical approach can be extremely successful while still having nothing to say about the causal mechanisms influencing the target variable we wish to speak about. If you can predict something with 99%+ accuracy, how much are you concerned about the underlying causal reality from a practical point of view? It depends on the nature of the research, but this is in fact what I think is the primary practical goal of scientific endeavor, i.e. accurate prediction. It definitely is not about finding ‘true’ parameters and p-values. In physical and related sciences there is often a confusion between deterministic models and causal understanding. But knowing the functional relationship between variables doesn’t in any way necessarily relate to the causal circumstances surrounding the situation under investigation. As an example, we might be able to predict the trajectory of an asteroid in our solar system with high accuracy based on deterministic model, but such a model tells us nothing about how the asteroid got there in the first place. That said, we care about that a lot less than whether the asteroid will impact the earth. In general our models are wrong, but they can work, and we’d prefer those that work better. That is something science can and does provide if it’s conducted well, almost by default. Models can even correspond to ‘reality’ as we currently understand it, but we all know that knowledge of reality changes as well, often due to scientific discoveries. Which model will be closer to being correct now versus later is hard to say (Peirce figured this out long ago). In a lot of situations, I’d personally rather have something that works than have one of the ‘true causes’ and a model that leaves me little better than guessing. Now let’s say you have a ‘causal’ model and another model that uses other covariates, and yet they both predict equally well. What would be the basis for preferring one over the other? I think we’d all prefer the causal model, but what would tangibly be gained in the preference in many circumstances, and how will such knowledge be acted upon? Outside of some obvious/more extreme instances, e.g. in disease prevention, it might be difficult to say. Chance Another related point, chance definitely does not cause anything. There are mechanisms you do not know about contributing to the variability in the subject matter under study, but nothing from your results are due to chance. Statistical models will always have uncertainty regarding them, and how much or how little there is depends on many things. But just because we don’t know what’s going on, we cannot put the unknown as due to some magical notion akin to coincidence. Other There are people, who have been taken seriously in other contexts, actively devoting time, money and energy to determine a. that our current existence is a simulation, and b. to find a way to ‘break out of it’ (I suspect this has to do with them taking their experience at Burning Man too seriously). However useless an endeavor this may be, if it was a simulation, where would any theory about causality reside then? An interesting point noted by Pearl is that causal assumptions are not encoded via the paths/edges, which only note possible causal relations, but in the missing ones. This reminds me of a chapter of the Tao Te Ching, that talks about the effectiveness of nothingness. An example being that it is not the walls or physical aspects of a house, but the rooms, i.e. spaces, that make it an effective home. If one adheres strictly to the strong causal assumptions for SEM, it is difficult for me to see where any discovery or exploration comes in. In that perspective, one uses statistical tools to merely uncover the parameter values that are assumed to be non-zero. I don’t even see why one would even reference statistical significance, interval estimates etc., as the causal aspects are assumed to be true. I’m not sure how the sampling variability, which could potentially result in conflicting causal understanding, is incorporated. If you go to the weak causal assumption, where the parameter can take on a potentially infinite range of values, we now have something that is more practical and amenable to statistical analysis as traditionally engaged. However, I don’t know what definition of causality it is consistent with. An example would be that for some model, one of the causal assumptions is that X causes Y, and that relationship may be weak or strong, positive or negative, possibly even indistinguishable from zero, but we won’t know until we look at the data. In other words, the effect could be A, its opposite, or not A. This is in fact how SEM is practiced the vast majority of the time. However, that doesn’t sound like any causal claim is being made at all to me, and instead merely posits some correlation to be explored. I’m fine with that, but I’d take issue that some causal assumption is given more credibility by the statistical result, which could have been anything and still have been consistent with the ‘prior knowledge’. As noted previously, the strong causal claims in SEM come from where we set paths equal to zero. However, a far more realistic assumption is that every effect is non-zero, however trivially small. Such models would be unidentifiable in SEM though. Some references Practically anything by Judea Pearl, even though some of the above might seem like I’m entirely missing many of the points he tries to make. On the contrary, I actually find it hard to disagree with Pearl, I just prefer to focus on the practical aspects of modeling, which in the end includes a statistical model that can exist independently of any causal notion, and on ways of improving such models. He is very clear however, that structural models rest on untestable assumptions and statistical results. The quote at the beginning is by C.S. Peirce. I note it because I am more interested in clear ideas, without which one cannot hope to even begin to understand causal relations, however defined. The notion of a thing intimately entails all of its practical effects. In other words, my definition of you must include how you are potentially able to act upon the universe, and in fact, my definition of you is what practical effects you have on the universe. The lack of clear ideas in scientific research is a fundamental problem too often ignored. Philosophical, such as Aristotle, Hume, non-Western as well. Software Revisited Mplus Mplus is more or less the gold standard for SEM programming, and there is no SEM-specific package or suite of commands that has all the functionality seen with Mplus. Pros Mplus can potentially do practically everything one would want to in the SEM setting Help forum: among the best you’ll come across in that the Muthens themselves are willing to answer practically any question quickly Relatively cheap to upgrade/maintain User manual has many, many working examples Cons Cost: very expensive for initial purchase License: Most universities will not have a campus-wide license Data processing: This is not what Mplus is for, so don’t Exploration: the lack of a proper programming language environment makes it difficult to do model exploration. It would be easier to use the Mplusautomation in R than do this with Mplus (even the Mplus website suggests this) User manual: None of the output is explained, which for some models is notably complicated. The simulated data examples are not very contextually helpful, and are overly optimistic relative to what researchers will actually face. Too much space is devoted to conceptually identical models with only minor syntax differences, which without explanation of the output differences, is not very helpful to users. Closed source: not only that, it’s only one programmer doing everything. Other It’s not clear to me where the future of Mplus lies. Bengt Muthén, the driving force behind Mplus has been emeritus faculty for some time now, and version 7 has been out without a full release longer than any previous version of Mplus. ‘Minor’ corrections are typically taking several months. Meanwhile, many things that users commonly find confusing or could be made automatic are left unchanged. R R does everything, SEM or otherwise. It even helps you use Mplus more efficiently. There’s no reason not to use it for SEM. Between the SEM-specific tools, and other packages that can fill in the remaining functionality, it can do any model Mplus can, and many of them better and more efficiently. What follows is a list of R packages for SEM on CRAN, put together by a simple search on ‘structural equation model’ at metacran. It is obviously not exhaustive, nor do I include packages that might do equivalent models (e.g. lme4 can do latent growth curve models as mixed models, flexmix for mixture models), nor are github-only packages noted. What should be clear is that R has well surpassed Mplus in what it can offer in the SEM world. autoSEM bnpa* ctsem CorReg dlsem dpa EffectLiteR faoutlier fSRM gesca gimme gof gSEM influence.SEM lavaan family: lavaan, blavaan, survey.lavaan, lavaan.shiny lsl lvnet metaSEM MIIVsem Mplusautomation nlsem OpenMx onyx piecewiseSEM plotSEMM psych* RAMpath* RMediation regsem* rsem sem semdiag semds semGOF SEMID semTools* semPlot* semPLS semtree sesem simsem sirt sparseSEM stablespec strum umx * Known connection to lavaan models. There may be many others. Pros Cost: free Development: source is freely available, issues are made known and tracked, and multiple people are involved in development. Help forum: possibly several outlets for different packages. Cons I would say learning curve, but no one knows SEM syntax until they do it, and it will always be different from the syntax typically employed by a statistical program or programming language. If your data is already processed and ready to go, it is no more difficult to run an SEM in R than Stata, and it will be easier than Mplus, Amos, etc. Stata Stata was very late to the SEM game, and it’s not going to do as much as Mplus, but it’s pretty easy to use, and is a nice statistical tool besides. Stata probably has more readily available functionality for instrumental variable regression and other econometrics oriented techniques, but that isn’t part of or specific to their SEM commands, and has been around a long time. Pros Cost: for the price of Mplus you get an entire statistical software package with vastly more functionality. License: research universities are far more likely to have a campus-wide license for Stata Help forum: the Stata community is generally very friendly and helpful Cons Cost: still costs more than free License: You still won’t be able to take it home. I have seen issues with SEM in Stata that otherwise run fine with lavaan or Mplus. No SEM specific forum If you are using Mplus and/or R, there is no reason to use Stata for SEM. Other Other SEM-specific software Lisrel, EQS, Amos, which were popular in the past, are no longer viable these days, either largely no longer developed or restricted to a single operating system, and yet they still want you to shell out hundreds of dollars to use them. SAS has some functionality but it’s rarely used for SEM relative to the other options. Other statistical programs have functionality, but they are relatively little used for standard models, and much less for SEM. Other Modeling Tools Within R one can do mixture/latent class models, growth mixture models, multilevel models, etc., and they will almost always provide more tools and output for understanding and exploring those models. Similarly, Stata has a lot more to offer for many other non-SEM tools relative to SEM-specific software. If you aren’t doing SEM, there is zero reason to use software that was designed specifically for it. The old adage of ‘just because you can doesn’t mean you should’ applies. The programming time alone would be reason not to, but a great deal of additional functionality would be lost too. Python, which is the most popular data science tool outside of R, has minimal SEM capabilities presently, but I will update if I come across anything. Bayesian Any of these analysis could be run with Bayesian programming languages such as Stan or BUGS, and you could feel more confident about understanding the underlying uncertainty when there are many parameters relative to the sample size. Resources This list serves only as a starting point, geared toward those that would be taking the workshop, though may be added to over time. Graphical Models Judea Pearl’s website: Includes papers and technical reports. Shalizi’s text Has a part on causal inference that summarizes a lot of work in Pearl, Spirtes et al., and others. It’s a work in progress (indefinitely), and parts are incomplete, but what’s there is useful. UseR Series: Contains texts on graphical models, Bayesian networks, and network analysis. Potential Outcomes Imai’s website: Papers and other info. Measurement Models (including IRT) Personality Project: William Revelle’s website and text on psychometric theory. Zinbarg et al. Cronbach’s α, Revelle’s β, &amp; McDonald’s ω_H_: Their realations with each other and two alternative conceptualizations of reliability. Jonathan Templin’s ICPSR workshop notes Applied SEM Kline, Rex. Principles and Practice of Structural Equation Modeling. A very applied introduction that covers a lot of ground. The latest edition finally includes explicit discussion of the more general graphical modeling framework within which SEM resides. Beaujean, A. A. (2014). Latent variable modeling using R: A step by step guide. New York, NY: Routledge/Taylor and Francis. Lavaan based guide to SEM SEMNET forum Nonparametric models Gershman &amp; Blei (2011). A Tutorial on Bayesian Nonparametric Models. Griffiths &amp; Ghahramani (2005). Infinite latent feature models and the Indian buffet process. lavaan lavaan website lavaan google group Tutorial Bayesian estimation with lavaan Complex surveys with lavaan Interactive lavaan Other SEM tools in R [MetaCran]: search results. Though it might be less of an effect than simply being African-American, something I’m sure we can agree is not a causal effect.↩ "],
["other-3.html", "Other", " Other These are other topics that may be covered in varying detail in the future. With possible packages noted. Collaborative filtering recommenderlab Other Mixture models (additional to what is already there) Discuss zero-inflated models, and possibly add other content. HMM, Linear Dynamical Systems HMM, HiddenMarkov depmixS4 Others Extensions to current content that might be added in the future: To SEM chapter: regression with latent variable scores comparison To Growth chapter: why not to do cross-lagged models. "],
["section-1.html", "", " "]
]
