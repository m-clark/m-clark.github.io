[
  {
    "path": "posts/2024-05-20/",
    "title": "Long time no see...",
    "description": "New modeling book under way!",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2024-05-20",
    "categories": [
      "miscellaneous"
    ],
    "contents": "\n\nContents\nBook in progess\n\nBook in progess\nTLDR: https://m-clark.github.io/book-of-models\nBeen a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.\nBut the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled Models Demystified. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing good enough modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can check it out now while it’s in progress. Hope you enjoy it!\n\n\n\n",
    "preview": "posts/2024-05-20/../../img/book_gp.svg",
    "last_modified": "2024-05-23T20:17:32-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-misc/",
    "title": "Stuff Going On",
    "description": "Penalty kicks, class imbalance, tabular deep learning, industry and acadmia",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2023-03-10",
    "categories": [
      "miscellaneous"
    ],
    "contents": "\n\nContents\nFootball players still don’t know penalty kick basics.\nTabular data post\nClass Imbalance\nTwo years at Strong\nComing up\n\nIt’s been a bit so thought I’d force myself to post a couple things I’ve played around with, or that aren’t ready yet for a full post, or won’t be one.\nFootball players still don’t know penalty kick basics.\nDid a quick and dirty Bayesian analysis to get posterior probabilities for location, controlling for various factors. As a side note, I won the office world cup challenge with a fancy model of which I will never reveal the details, but may or may not have included lots of guessing and luck.\n\nTabular data post\nI finally did my first post at the Strong blog! It’s a high-level overview of tabular data and deep learning that summarizes some of my previous posts here and here.\nClass Imbalance\nFor my next post at the Strong blog, Elizabeth Monroe and I are working on a similarly high-level overview of issues with class imbalance we’ve been coming across. I will probably provide even more details and simulation results in a post on this site eventually, but here is a preview plot showing (mis)calibration plots at varying degrees of imbalance and different sample sizes.\n\nTwo years at Strong\nHard to believe for me anyway, but I’ve been out of academia for two years now, after previously spending my professional lifetime there. Aside from some of the obvious differences, one of the more satisfying changes for me has been that the skills I’ve acquired are utilized on a daily basis, and something I need to continuously develop for the job. At Strong our clients want good results in a timely fashion, and though the results might be notably complex, they still need to be sufficiently interpretable as well as reproducible/production-ready. I also have come across more desire for causal explanations from clients, which might be surprising to what is typically assumed for academia vs. industry. Clients obviously require buy-in for what we do, but they ultimately defer to us for the expertise we provide.\nStrong Analytics was a great move for me, because they clearly value the strong academic background of its employees, but are practically minded, and focus on skills that allow one to be nimble enough to get the clients what they need. Just like I was in academia, I am surrounded by a diverse group of smart folks I respect a lot, and am happy to solve some tough problems with. I feel I’ve learned how to get things done in a more efficient manner, and do a better job of explaining what I’ve done to wider audience.\nAmong some things I miss with academia, one was working with faculty and grad students who were just starting with an idea, and continuing a relationship with them until ultimately getting to publication or a successful dissertation defense after a very long journey. Another was giving workshops regularly where you could help people with their initial baby steps into the large world of data science. In general, it was easy to feel personally invested in the individuals you were working with, and their successes felt like your own.\nHowever, in academia it was often a struggle to get buy-in for more complicated methods or new techniques, because the stakes were typically lower and people knew the minimum required to get them published, defended or whatever, and mostly just wanted help getting to that point. There’s nothing wrong with that necessarily, that’s just the practical reality, and a reflection of what’s valued in academia. Despite that, I can say I definitely had some good partnerships with people involved in challenging research that was very rewarding, and those projects made it generally very satisfying to work in academia.\nUltimately though, I’m happy to have made the jump. It’s a bit weird to me how much drama there is on this topic on twitter and elsewhere. It’s really not that big of a deal which route you go, and in the grand scheme of things, almost no one will care if you work in academia or industry but you. There are pros and cons to both, and people should just pick what will make them happier.\nComing up\nWhenever I can get around to it, I’ll try and post on those class imbalance simulations mentioned above, conformal prediction, and some other fun stuff. Stay tuned!\n\n\n\n",
    "preview": "posts/2023-03-misc/../../img/198R.png",
    "last_modified": "2023-03-11T08:32:15-05:00",
    "input_file": {},
    "preview_width": 1025,
    "preview_height": 999
  },
  {
    "path": "posts/2022-09-deep-linear-models/",
    "title": "Deep Linear Models",
    "description": "A demonstration using pytorch",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2022-10-10",
    "categories": [
      "deep learning",
      "boosting",
      "GLM",
      "regression",
      "machine learning"
    ],
    "contents": "\n\nContents\nIntroduction\nGetting Started\nInitial Data Processing\nGetting Started with pytorch\nSetup\n\nSetting up a linear model\nDoing a Gradient Descent Step\nTraining the Linear Model\nMeasuring Accuracy\nUsing sigmoid\nCompare to Linear/Logistic Regression\n\nA Neural Network\nDeep Learning\nThe Elephant in the Room\nSummary\n\nIntroduction\nThis post gives a by-hand example of a linear model using pytorch. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some pytorch basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!\nFor this demo we’ll use an example by fastai, which is a great resource for getting started with deep learning. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples1. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions2.\nGetting Started\nLet’s get the primary packages loaded first.\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nNext, we’ll use the well-known titanic dataset, and to start things off, we’ll need to get a sense of what we’re dealing with. The basic idea is that we’d like to predict survival based on key features like sex, age, ticket class and more.\n\n# non-kaggle-requiring url here: https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/data/dl-linear-regression/titanic/train.csv\ndf_titanic_train = pd.read_csv('data/dl-linear-regression/titanic/train.csv')\n# df_titanic_train\n\n\ndf_titanic_train.describe()\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]\n\nInitial Data Processing\nThe data is not ready for modeling as is, so we’ll do some additional processing to get it ready. We’ll check out the missing values and replace them with modes3.\n\ndf_titanic_train.isna().sum()\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\nmodes = df_titanic_train.mode().iloc[0]\n\ndf_titanic_train.fillna(modes, inplace = True)\n\ndf_titanic_train.describe(include = (np.number))\n       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n\n[8 rows x 7 columns]\n\nWith features, sometimes it is worthwhile to log transform data for potentially more efficient optimization search. Since we have zeros, we add 1 before taking the log.\n\ndf_titanic_train['Fare'].hist()\n\n\nNow the transformed data looks a little more manageable. More to the point, we won’t potentially have huge coefficients relative to other covariates because of the range of the data.\n\ndf_titanic_train['LogFare'] = np.log1p(df_titanic_train['Fare'])\n\ndf_titanic_train['LogFare'].hist()\n\n\nThe Pclass (passenger class) feature is actually categorical.\n\npclasses = sorted(df_titanic_train.Pclass.unique())\npclasses\n[1, 2, 3]\n\nHere are the other categorical features.\n\ndf_titanic_train.describe(include = [object])\n                           Name   Sex  Ticket    Cabin Embarked\ncount                       891   891     891      891      891\nunique                      891     2     681      147        3\ntop     Braund, Mr. Owen Harris  male  347082  B96 B98        S\nfreq                          1   577       7      691      646\n\nIn order to use categorical variables, they need to be changed to numbers4, so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use embeddings5, particularly for things that have lots of unique categories.\n\ndf_titanic_train = pd.get_dummies(df_titanic_train, columns = [\"Sex\", \"Pclass\", \"Embarked\"])\n\nLet’s take a look at our data now.\n\ndf_titanic_train.columns\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\ndf_titanic_train.head()\n   PassengerId  Survived  ... Embarked_Q  Embarked_S\n0            1         0  ...          0           1\n1            2         1  ...          0           0\n2            3         1  ...          0           1\n3            4         1  ...          0           1\n4            5         0  ...          0           1\n\n[5 rows x 18 columns]\n\nGetting Started with pytorch\nSetup\nNow we are ready to prep things for specific use with pytorch. I will not use the same terminology as in Jeremy’s original post, so for us, target = ‘dependent variable’ and X is our feature matrix6. Both of these will be pytorch tensors, which for our purposes is just another word for an array of arbitrary size.\n\nfrom torch import tensor\n\ntarget = tensor(df_titanic_train.Survived)\n\n\ndummies = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nall_features = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummies \n\nX = tensor(df_titanic_train[all_features].values, dtype = torch.float)\nX.shape\ntorch.Size([891, 12])\n\nSetting up a linear model\nWe have our feature matrix and target variable prepped. The next step is to map the features to the target by means of predicted values. In linear regression, we typically call the weights that produce the predictions coefficients, but in standard deep/machine learning terminology, they are usually called weights, or more generally, parameters. Here, we generate some random values between -.5 and .5 to get started7:.\n\ntorch.manual_seed(442)\n<torch._C.Generator object at 0x157eb6af0>\nn_coeff = X.shape[1]\ncoeffs = torch.rand(n_coeff) - 0.5  # default would produce values from 0 to 1\ncoeffs\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625])\n\nThe original post did a form of min-max scaling to the features, basically putting everything on a potentially [0, 1] scale. Here we’ll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.\n\n# vals,indices = X.max(dim=0)\n# X = X / vals\nX_means = X.mean(dim = 0, keepdim = True)\nX_sds   = X.std(dim = 0)\n\nX_sc = (X - X_means) / X_sds\n\n# X_sc.mean(dim = 0)  # all means = 0 \n# X_sc.std(dim = 0)   # all sd = 1\n\nAs noted in the original post and worth iterating here for our statistical modeling crowd, we don’t estimate an intercept for this model and keep all the dummy coded features. The following takes our coefficients, multiplies them by their respective feature, and sums them.\n\npreds = (X_sc * coeffs).sum(axis = 1)\npreds[:10]\ntensor([ 0.6000, -1.9341,  0.2080,  0.1723, -0.0032,  0.3088, -0.5066,  1.6219,\n         0.6990, -1.2584])\n\nWe can calculate our loss, the difference in our predictions versus the target values, in many ways. Here we get the mean squared error.\n\nloss = torch.square(preds - target).mean()\nloss\ntensor(1.3960)\n\nNow we’ll create functions that do the previous steps, and finally, give it a test run! In the original fastai formulation, they use mean absolute error for the loss, which actually is just the L1loss that is available in torch. For a change of pace, we’ll keep our mean squared error, which is sometimes called L2 loss (this will create different results from the original notebook). I create the option within the function for you to do either. Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.\n\ndef calc_preds(X, weights):\n    return((X * weights).sum(axis = 1))\n\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n    \n    # torch.abs(calc_preds(X, coeffs)-target).mean()  # original\n\n    if which == 'l2':\n      loss = torch.nn.MSELoss()\n    else: \n      loss = torch.nn.L1Loss()\n      \n    L = loss(preds, target.float())\n      \n    return(L)\n\ncalc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which = 'l1')\n(tensor(1.3960), tensor(0.8891))\n\nDoing a Gradient Descent Step\nWe can continue our journey onward to actually estimating the weights rather than specifying them directly, since we definitely don’t want to just keep guessing! This is an iterative process where we still start with an initial (random) guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps epochs, and getting our next guess requires calculating what’s called a gradient. Here are some resources for more detail:\nHow Does a Neural Net Really Work?: great intro by Jeremy Howard\nSome by-hand code using gradient descent for linear regression, R, Python: By yours truly\nIn any case, this is basic functionality within pytorch, and it will keep track of each step taken.\n\ncoeffs.requires_grad_()\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,\n         0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\nloss = calc_loss(X_sc, coeffs, target)\nloss\ntensor(1.3960, grad_fn=<MseLossBackward0>)\n\nWe use backward to calculate the gradients and inspect them.\n\nloss.backward()\n\ncoeffs.grad\ntensor([-0.9311,  0.6245,  0.4957, -0.7423,  0.6008, -0.6008, -0.9158,  0.0938,\n         0.7127, -1.7183,  0.1715,  1.3974])\n\nEach time backward is called, the gradients are added to the previous values. We can see here that they’ve now doubled.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\ncoeffs.grad\ntensor([-1.8621,  1.2491,  0.9914, -1.4847,  1.2015, -1.2015, -1.8317,  0.1877,\n         1.4254, -3.4366,  0.3431,  2.7947])\n\nWhat we want instead is to set them back to zero after they are used for our estimation step. The following does this.\n\nloss = calc_loss(X_sc, coeffs, target)\n\nloss.backward()\n\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)     # sub subtracts in place\n    coeffs.grad.zero_()                # zeros out in place\n    print(calc_loss(X, coeffs, target))\ntensor([-0.1836, -0.0488,  0.0922, -0.0035, -0.4435, -0.1345,  0.7624,  0.2854,\n         0.0661,  0.0763,  0.1588, -0.0567], requires_grad=True)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\ntensor(37.9424)\n\nTraining the Linear Model\nWe typically would typically split our data into training and test. We can do so here, or keep this data as training and import test.csv for the test set. The latter is actually used for the Kaggle submission, but that’s not a goal here. We’ll use scikit-learn for the splitting.\n\nfrom sklearn.model_selection import train_test_split\n\n# test size .2 in keeping with fastai RandomSplitter default\ntrain_x, valid_x, train_y, valid_y = train_test_split(\n  X_sc, \n  target.float(), \n  test_size = 0.2, \n  random_state = 808\n)\n  \n\nlen(train_x), len(valid_x) # might be one off of the original notebook\n(712, 179)\n\nAs before, we’ll create functions to help automate our steps:\none to initialize the weights\na function to update weights\none to do a full epoch (using weights to calculate loss, updating weights)\none to train the entire model (run multiple times/epochs)\nAs mentioned, the approach here is to create functions that are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each verbose value epoch (e.g. verbose = 10 means you’ll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (possibly not too big a deal depending on your IDE).\n\ndef init_weights(n_wts): \n    return (torch.rand(n_wts) - 0.5).requires_grad_()\n\ndef update_weights(weights, lr):\n    weights.sub_(weights.grad * lr)\n    weights.grad.zero_()\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\ndef train_model(X, target, epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1])\n    \n    for i in range(epochs): \n        one_epoch(X, coeffs, target, lr = lr, i = i, verbose = verbose)\n    return coeffs\n\nTry out the functions if you like (not shown).\n\ncalc_loss(X_sc, init_weights(X_sc.shape[1]), target).backward()\n\n\none_epoch(train_x, init_weights(train_x.shape[1]), train_y, .01)\n\nNow train the model for multiple epochs. The loss drops very quickly before becoming more steady.\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, verbose = 5, lr = .2)\n 1.375618\n  0.296216\n  0.284019\n  0.281221\n  0.280271\n  0.279923\n  0.279794\n  0.279746\n  0.279728\n  0.279721\n \n\nLet’s create a function to show our estimated parameters/weights/coefficients in a pretty fashion.\n\ndef show_coeffs(estimates): \n  coef_dict = dict(zip(all_features, estimates.requires_grad_(False).numpy()))\n  return pd.DataFrame(coef_dict, index = ['value']).T\n\nshow_coeffs(coeffs_est)\n               value\nAge        -0.090825\nSibSp      -0.054449\nParch      -0.016111\nLogFare     0.046320\nSex_male   -0.406538\nSex_female -0.171426\nPclass_1    0.408707\nPclass_2    0.335766\nPclass_3    0.329800\nEmbarked_C  0.057091\nEmbarked_Q  0.032813\nEmbarked_S  0.039464\n\nMeasuring Accuracy\nIt’s one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data. The following function will convert our estimates to a binary value like our target, and compares them to the target. Depending on how you did your training setup, it might be pretty bad or at least better than guessing.\n\ndef acc(X, weights, target): \n    return (target.bool() == (calc_preds(X, weights) > 0.5)).float().mean()\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n(tensor(0.7051), tensor(0.6425))\n\nUsing sigmoid\nNothing about the previous setup actually produces a result on the probability scale, so basing a cutoff of .5 is meaningless. you can inspect them and might see values are above 1 or below zero, which we generally don’t want8. However we do have a solution. The sigmoid function9 allows us to transform our predictions to values between 0 and 1, i.e. probabilities in this context, and in particular, the probability of survival. Then our acc function will be more appropriate, where any probability > .5 will be given a value of 1 (or True technically), while others will be 0/False.\n\ndef calc_preds(X, weights):\n    return torch.sigmoid((X*weights).sum(axis = 1))\n\nWe also will do more iterations, and fiddle with the learning rate (a.k.a. step size)\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  epochs = 500,\n  lr = 1,\n  verbose = 100\n)\n 0.314158\n  0.154329\n  0.154237\n  0.154232\n  0.154232\n \n\nNot too shabby!\n\nacc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)\n(tensor(0.7823), tensor(0.7989))\n\n\nshow_coeffs(coeffs_est)\n               value\nAge        -0.516476\nSibSp      -0.423656\nParch      -0.179623\nLogFare     0.396468\nSex_male   -0.927410\nSex_female  0.349448\nPclass_1    0.713895\nPclass_2    0.320935\nPclass_3    0.078920\nEmbarked_C  0.107378\nEmbarked_Q  0.082943\nEmbarked_S -0.036137\n\nIn implementing the sigmoid, let’s go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster)10. To do this, the coefficients will need to be a column vector, so we change our init_coeffs function slightly11.\n\ndef calc_preds(X, weights): \n    return torch.sigmoid(X@weights)\n\ndef init_coeffs(n_wts): \n    return (torch.rand(n_wts, 1) * 0.1).requires_grad_()\n\nNow our functions are more like the mathematical notation we’d usually see for linear regression.\n\\[\\hat{y} = X\\beta\\]\nCompare to Linear/Logistic Regression\nBefore getting too excited, let’s compare our results to basic linear and logistic regression. The linear regression is more like our model before using the sigmoid transformation, while the logistic is more like when we used it. Depending on your settings, the logistic regression is probably doing better at this point.\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\n\n\nreg = linear_model.LinearRegression()\nreg.fit(train_x, train_y)\nLinearRegression()\nacc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)\n(tensor(0.7989), tensor(0.7821))\n\n\nreg = linear_model.LogisticRegression()\nreg.fit(train_x, train_y)\nLogisticRegression()\naccuracy_score(valid_y, reg.predict(valid_x)).round(4)\n0.7821\n\nIt looks like our coefficient estimates are similar to the logistic regression ones.\n\nshow_coeffs(coeffs_est).assign(logreg = np.round(reg.coef_.T, 4))\n               value  logreg\nAge        -0.516476 -0.4799\nSibSp      -0.423656 -0.4189\nParch      -0.179623 -0.1264\nLogFare     0.396468  0.3439\nSex_male   -0.927410 -0.6262\nSex_female  0.349448  0.6262\nPclass_1    0.713895  0.3943\nPclass_2    0.320935  0.0675\nPclass_3    0.078920 -0.3946\nEmbarked_C  0.107378  0.0545\nEmbarked_Q  0.082943  0.0654\nEmbarked_S -0.036137 -0.0889\n\nA Neural Network\n\nAt this point we’ve basically reproduced a general linear model. A neural network, on the other hand, has from one to many hidden layers of varying types in between input and output. Let’s say we have a single layer with two nodes. For a fully connected or dense network, we’d need weights to map our features to each node of the hidden layer (n_wts * n_hidden parameters total), and then another set of weights to map the hidden nodes to our next layer, which is our output, basically creating the predicted values. You can think of it as a second hidden layer with a single output node. With additional hidden nodes we add more complexity, but also flexibility, to the model. But this may come at a price, e.g. more difficulty with training due to the additional parameters that have to be estimated.\nSo basically we need matrices of weights, and the following function allows us to create those. We also add a bias/intercept/constant for the hidden-to-output processing. In the first layer, we divide the weights by n_hidden to create sums at the next layer that are of similar magnitude as the inputs. In general though, there are many ways to initialize weights.\n\ndef init_weights(n_wts, n_hidden = 20):\n    layer1 = (torch.rand(n_wts, n_hidden) - 0.5) / n_hidden # n_wts x n_hidden matrix of weights\n    layer2 = torch.rand(n_hidden, 1) - 0.3                  # n_hidden weights\n    const  = torch.rand(1)[0]                               # constant\n    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\nNow we revise our calc_preds function to incorporate all the weights. Initially, we extract the different sets of weights that are estimated by the model. For the original inputs, we multiply them by the layer 1 weights and sum. Then we apply a transformation to them to induce nonlinearity. Typical approaches are the sigmoid function we used before, hyperbolic tangent, and, probably the most common, the relu. The original notebook used relu, while I use a more recent one called Mish, which is a variant of relu. The hidden layer nodes then get multiplied by their respective weights and summed with the constant added. We then use our sigmoid function to get the probability scale as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(X, weights):\n    l1, l2, const = weights\n    res = F.mish(X@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res).flatten()\n\nWith additional sets of weights, we use an update loop.\n\ndef update_weights(weights, lr):\n    for layer in weights:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ncoeffs_est = train_model(train_x, train_y, epochs = 50, lr = 1, verbose = 10)\n 0.325837\n  0.155810\n  0.141485\n  0.137652\n  0.136034\n \n\nAt this point we’re doing a little bit better in general, and even better than standard logistic regression on the test set!\n\nacc(train_x, coeffs_est, train_y), \\\nacc(valid_x, coeffs_est, valid_y), \\\naccuracy_score(valid_y, reg.predict(valid_x)).round(4)\n(tensor(0.8160), tensor(0.8045), 0.7821)\n\nDeep Learning\nWe previously used a single hidden layer, but we want to go deeper! That’s the whole point of deep learning right? The following modifies our previous functions to allow for an arbitrary number of layers. You’ll note there are some hacks to get the weights in a good way for each layer12, but you normally wouldn’t have to do that on your own, since most tools will provide sensible modifications.\n\ndef one_epoch(X, weights, target, lr, verbose = 1, i = 1):\n    loss = calc_loss(X, weights, target)\n    loss.backward()\n    \n    with torch.no_grad(): update_weights(weights, lr)\n    \n    if verbose != 0:\n        if i % verbose == 0:\n            print(f'{loss: 3f}', end = '\\n ')\n\n# change loss to binary\ndef calc_loss(X, weights, target, which = 'l2'):\n    preds = calc_preds(X, weights)\n\n    loss = torch.nn.BCELoss()\n\n    L = loss(preds, target.float())\n\n    return(L)\n\n\ndef init_weights(n_wts, hiddens):  \n    sizes = [n_wts] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]\n    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers, consts\n\n\ndef calc_preds(X, weights):\n    layers, consts = weights\n    n = len(layers)\n    res = X\n    \n    for i, l in enumerate(layers):\n        res = res@l + consts[i]\n    \n    if i != n-1: \n      res = F.mish(res)\n      \n    \n    return torch.sigmoid(res).flatten()\n\ndef update_weights(weights, lr):\n    layers, consts = weights\n    for layer in layers + consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\ndef train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):\n    torch.manual_seed(442)\n    coeffs = init_weights(X.shape[1], hiddens)\n    \n    for i in range(epochs): \n        if verbose != 0:\n            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)\n    \n    return coeffs\n\nWith everything set up, let’s do some deep linear modeling! You can play around with the number of hidden layers, number of nodes and other settings. Feel free to explore!\n\ncoeffs_est = train_model(\n  train_x,\n  train_y,\n  hiddens = [500, 250, 100],\n  epochs  = 500,\n  lr      = 1e-4,\n  verbose = 10\n)\n 5.123790\n  0.666971\n  0.653124\n  0.640325\n  0.628476\n  0.617496\n  0.607313\n  0.597861\n  0.589081\n  0.580918\n  0.573322\n  0.566249\n  0.559658\n  0.553510\n  0.547772\n  0.542413\n  0.537403\n  0.532715\n  0.528326\n  0.524212\n  0.520354\n  0.516733\n  0.513330\n  0.510130\n  0.507118\n  0.504281\n  0.501605\n  0.499080\n  0.496695\n  0.494439\n  0.492305\n  0.490283\n  0.488366\n  0.486547\n  0.484820\n  0.483178\n  0.481616\n  0.480129\n  0.478712\n  0.477361\n  0.476072\n  0.474840\n  0.473663\n  0.472538\n  0.471461\n  0.470429\n  0.469440\n  0.468493\n  0.467583\n  0.466710\n \n\nHooray! Our best model yet (at least tied).\n\npd.DataFrame({\n    'acc_train': acc(train_x, coeffs_est, train_y).flatten(), \n    'acc_test': acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm': accuracy_score(valid_y, (reg.predict(valid_x) > .5).astype(int)).round(6)\n}, index = ['value'])\n       acc_train  acc_test  acc_test_glm\nvalue    0.77809  0.804469      0.782123\n\nThe Elephant in the Room\nAs noted in my previous posts [1, 2], probably your biggest challenge in implementing a deep learning model for tabular data, one with mixed data types and other complexities, is beating an off the shelf boosting model. Here is a quick demo with lightgbm.\n\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(\n  # n_estimators = 500,  # the sorts of parameters you can play with (many more!)\n  # max_depth    = 4,\n  # reg_alpha    = .1\n)\n\nmodel.fit(train_x, train_y)\nLGBMClassifier()\nmodel.score(valid_x, valid_y)\n\n\n# sklearn example\n# from sklearn.ensemble import HistGradientBoostingClassifier\n# \n# res = HistGradientBoostingClassifier().fit(train_x.numpy(), train_y.numpy())\n# \n# res.score(valid_x.numpy(), valid_y.numpy())\n0.8491620111731844\n\nNo tuning at all, and we’re already doing significantly better. Granted, if you use a packaged DL model for tabular data like the one in fastai, you should be doing better than our little demo. Even then though, you may still find the boosting results tough to beat.\n\ndf_accs = pd.DataFrame({ \n    'acc_test_dl':   acc(valid_x, coeffs_est, valid_y).flatten(), \n    'acc_test_glm':  accuracy_score(valid_y, (reg.predict(valid_x) > .5).astype(int)).round(6),\n    'acc_test_lgbm': model.score(valid_x, valid_y).round(6)\n}, index = ['value'])\n\ndf_accs\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue     0.804469      0.782123       0.849162\n\n\ndf_perc_improvement = 100 * (df_accs / df_accs.iloc[0,1] - 1)  # % improvement\ndf_perc_improvement\n       acc_test_dl  acc_test_glm  acc_test_lgbm\nvalue     2.857125           0.0       8.571414\n\n\n\n\nSummary\nThis was a lot of work to do slightly better than a logistic regression! However, there is a lot going on with a typical DL model that would likely prove even better. But it also serves as a reminder to have a suitable baseline, and as we saw with the lightgbm model, it can take little effort to do very well without deep learning. Hopefully though, the peek behind the scenes to do some ‘deep’ linear modeling can make it more accessible for you.\n\n\n\n\n\n\nClark, Michael. 2022a. “Deep Learning for Tabular Data.” https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/.\n\n\n———. 2022b. “This Is Definitely Not All You Need.” https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/.\n\n\nHoward, Jeremy. 2022a. “How Does a Neural Net Really Work?” Kaggle. https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work.\n\n\n———. 2022b. “Linear Model and Neural Net from Scratch.” Kaggle. https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch.\n\n\n———. 2022c. “What Is Torch.nn Really?” Kaggle. https://pytorch.org/tutorials/beginner/nn_tutorial.html.\n\n\nRaschka, Sebastian. 2022. “A Short Chronology of Deep Learning for Tabular Data.” https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html.\n\n\nI won’t actually use fastai, since they aren’t up to supporting M1/2 Macs very well. I think it was only used for the train/test data split anyway. I would rant a bit about this, but a lot of fastai is geared toward non-local computing, and the fault is really with Apple and NVidia as near as I can tell.↩︎\nI’m also not going to go into broadcasting, submitting to Kaggle, and other things that I don’t think are necessary for our purposes here.↩︎\nJust as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here, and sometimes that’s enough.↩︎\nEven though every modeling tool requires this, strangely very few in the Python world offer options for automatic handling of such things, but it’s getting better.↩︎\nWe actually aren’t too far removed from this in our model coming up, the main difference is that we don’t treat the categorical feature part of the model separately.↩︎\nI’ll not perpetuate calling features/predictor variables that are clearly not independent as independent. That nomenclature really only works for randomized experiments, and that is definitely not the case here.↩︎\nYou could use torch.randn to get standard normal values, and often times we’ll even start with just zeros if we really are just doing a standard linear model.↩︎\nUnless you are an economist, in which case you call it a linear probability model and ignore the ridiculous predictions because you have very fine standard errors.↩︎\nA lot of R folks seem unaware that the base R plogis function accomplishes this.↩︎\nThe @ operator is essentially the dot product, so x@y is np.dot(x, y)↩︎\nThe fastai demo also changes the target to a column vector, but this doesn’t seem necessary.↩︎\nAnd they probably aren’t as good for the changes I’ve made.↩︎\n",
    "preview": "posts/2022-09-deep-linear-models/../../img/nnet.png",
    "last_modified": "2022-10-10T12:51:43-04:00",
    "input_file": {},
    "preview_width": 1289,
    "preview_height": 1076
  },
  {
    "path": "posts/2021-05-time-series/",
    "title": "Exploring Time",
    "description": "Multiple avenues to time-series analysis",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2022-08-10",
    "categories": [
      "mixed models",
      "GAM",
      "boosting",
      "time series",
      "deep learning"
    ],
    "contents": "\n\nContents\nIntro\nOutline\nQuick\nSummary\nData\nDescription\nImport &\nSetup\nTraining and Validation\nOther\n\nClassical Time Series\nIntro\nModel\nExplore\n\nMixed model with AR Structure\nIntro\nData Prep\nModel\nExplore\n\nGeneralized Additive Models\nIntro\nData Prep\nModel\nExplore\n\nProphet\nIntro\nData Prep\nModel\nExplore\n\nFable\nIntro\nData Prep\nModel\nExplore\n\nGBM\nIntro\nData Prep\nModel\nExplore\n\nTorch\nData\nTorch data\nModel\nTraining\nEvaluations\n\nAll\nSummary\n\nIntro\n\nThis post was mostly complete around May 2021, but for various\nreasons not actually posted until August of 2022. I haven’t changed much\naside from adding a section on boosting, and have used the results I\nconjured up previously (for the most part). However, many package\nupdates since then may mean that parts of the code may not work as well,\nespecially for the torch code. I would also recommend modeltime as\nstarting point for implementing a variety of model approaches for time\nseries data with R. It was still pretty new when this was first written,\nbut has many new features and capabilities, and could do some version of\nthe models shown here.\n\nIt is extremely common to have data that exists over a period of\ntime. For example, we might have yearly sports statistics, daily\nmanufacturing records, server logs that might be occurring many times\nper second, and similar. There are many approaches we could use to model\nthe data in these scenarios. When there are few time points and they are\nclustered within other units, like repeated observations of exercise\ndata for many individuals, we often use tools like mixed models\nfor example, and even with many observations in a series, we can still\nuse tools like that. But sometimes there may be no natural clustering,\nor we might want to use other approaches to handle additional\ncomplexity.\nThis post is inspired by a co-worker’s efforts in using PyTorch to\nanalyze Chicago Transit data. Cody Dirks was writing a\npost where he used a Python module developed\nby our group at Strong Analytics to\nanalyze the ridership across all the ‘L’. This post\ncan be seen as a demonstration of some simpler models which might also\nbe viable for a given situation such as this, allowing for quick dives,\nor even as ends in themselves.\nOutline\nThe models we’ll go through are the following:\nError models and random effects\nGAM\nMore elaborate time series with seasonal and other effects\nBoosting and Deep learning\nIn what follows I will show some more detailed code in the beginning,\nbut won’t show it later for conciseness, focusing mostly just on the\nbasic model code. You can always find the code for these posts on my GitHub.\nQuick Summary\nClassical econometrics approaches like ARIMA models may take\nnotable effort to match the flexibility of other approaches one might\ntake with time series. It’s also difficult to believe a specific lag/ma\nnumber will hold up with any data change.\nGAMs extend mixed models, and should probably be preferred if a\nprobabilistic approach is desired. Prophet-style approaches would likely\ntake notable effort and still likely lack performance, without adding\ninterpretability.\nFor black box methods, boosting can do very well without much\nfeature engineering, but possibly take a bit more for parameter tuning.\nDeep learning methods may be your best bet given data size and other\ndata modeling needs.\nData Description\nAs noted in Cody’s\npost, over 750,000 people use the Chicago Transit Authority’s ‘L’\nsystem to get around the city. There are 8 interconnected rail lines\nnamed after colors- the Red, Blue, Green, Brown, Pink, Orange, Purple,\nand Yellow, 145 entry/exit stations, and over 2,300 combined trips by\nits railcars every day1.\nThe city of Chicago provides ridership data that can be accessed\npublicly.\nridership\nstation\ninfo\nIn Cody’s exploration, he added pertinent information regarding\nweather, sporting events, and more. You can access the processed\ndata.\nFor our demonstrations we have daily ridership from 2012-2018, and we\nwill use a variety of methods to model this. We will use a normalized\nride count (mean of 0, standard deviation of 1) as our target\nvariable.\nImport & Setup\nTo get things started we’ll use the tidyverse for some additional\ndata processing, and lubridate for any date processing, for example,\nconverting to weekdays.\n\n\n# Data Processing\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n# Misc\n\nSTART_DT = '2008-06-01'\nEND_DT   = '2018-12-31'\nSPLIT_DT = '2017-06-01'\n\n\n\nMain data\nI start with data having already been processed, but as mentioned the\nsource is publicly available. I use data.table to read it in more quickly, but it’s default\ndate class can cause issues with other packages, so I deal with that. I\nalso extract the year, month, weekday, etc.\n\n\ndf = data.table::fread('data/time-series/processed_df.csv')\n\ndf_start = df %>% \n  as_tibble() %>% \n  select(-contains('_attributes'), -(tsun:wt22)) %>% \n  mutate(\n    date      = as_date(date), # remove IDATE class\n    rides_log = log(rides),\n    year      = year(date),\n    year_fac  = factor(year),\n    month     = month(date, label = TRUE),\n    day       = factor(wday(date, label = TRUE), ordered = FALSE),\n    year_day  = lubridate::yday(date),\n    line      = factor(line),\n    snow_scaled = scale(snow)[, 1],\n    colors = as.character(line),\n    colors = ifelse(colors == 'purple_express', 'purple4', colors),\n    red_line_modernization = \n      ifelse(\n        between(date, as_date('2013-05-19'), as_date('2013-10-20')), \n        1, \n        0\n      )\n  ) %>% \n  arrange(date, line)\n\n\n\nTraining and Validation\nWe split our data into training and validation sets, such that\neverything before 2017-06-01 is used for training, while everything\nafter will be used for testing model performance2.\n\n\ndf_train = df_start %>% \n  filter(date < SPLIT_DT, !is.na(rides))\n\ndf_validate = df_start %>% \n  filter(date >= SPLIT_DT, !is.na(rides))\n\nred_line_train = df_train %>% \n  filter(line == 'red')\n\nred_line_validate = df_validate %>% \n  filter(line == 'red')\n\n\n\nOther\nHolidays are available via the prophet package, which we’ll be demonstrating a model\nwith later. The data we’re using already has a ‘holiday vs. not’\nvariable for simplicity, though it comes from a different source. The\nprophet version has both the actual\ndate and the observed date counted as a holiday, and I prefer to use\nboth.\n\n\nholidays = prophet::generated_holidays %>% \n  filter(country == 'US') %>% \n  mutate(ds = as.numeric(as_date(ds))) %>%\n  droplevels()\n\n\n\nWe’ll take a quick look at the red line similar to Cody’s post, so we\ncan feel we have the data processed as we should.\n\n\n\nWith the data ready to go, we are ready for modeling, so let’s get\nstarted!\nClassical Time Series\nIntro\nClassical times series from an econometrics perspective often\nconsiders a error model that accounts for the correlation a current\nobservation has with past observations. A traditional example is the\nso-called autoregressive, or AR, model, which lets a\ncurrent observation be predicted by past observations up to a certain\npoint. For example, would could start by just using the last observation\nto predict the current one. Next we extend this to predict the current\nbased on the previous two observations, and so on. How many\nlags we use is part of the model exploration.\n\\[y_t = \\alpha_1y_{t-1} + \\dots\n+\\alpha_{p}y_{t-p} + \\varepsilon_t\\]\nWe can extend this to include not just past observations but also\npast residuals, called a moving average. So formally, our\nARMA (p, q) model now looks like this for an observation \\(y\\) at time \\(t\\):\n\\[y_t = \\alpha_1y_{t-1} + \\dots\n+\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} +\n\\cdots +\\theta_q \\varepsilon_{t-q})\\]\nWe can also use differencing, for\nexample subtracting the previous time value from the current observation\nvalue for all values, to come to the final ARIMA\n(p, d, q) model. See Hyndman and Athanasopoulos (2021) for more\ndetails.\nModel\nEven base R comes with basic time series models such as this.\nHowever, as mentioned, we typically don’t know what to set the values of\nan ARIMA(p, d, q) to. A quick way to explore this is via the forecast package, which will search over\nthe various hyperparameters and select one based on AIC. Note that fable, a package we will be using later,\nwill also allow such an approach, and if you’d like to go ahead and\nstart using it I show some commented code below.\n\n\nmodel_arima = forecast::auto.arima(\n  red_line_train$rides_scaled\n)\n\n# model_arima = red_line_train %>%\n#   select(date, rides_scaled) %>%\n#   tsibble::as_tsibble() %>%\n#   fabletools::model(fable::ARIMA(\n#     rides_scaled ~ 0 + PDQ(0,0,0),\n#     stepwise = FALSE,\n#     approximation = FALSE\n#   ))\n# fabletools::report(model_arima)\n\n\n\nExplore\nIn this case we have a selected AR of 3 and MA of 4 for the centered\nvalue. But looking at the predictions, we can see this is an almost\nuseless result for any number of days out, and does little better than\nguessing.\n\n\nbroom::tidy(model_arima)\n\n\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nar1\n\n\n0.238\n\n\n0.048\n\n\nar2\n\n\n-0.285\n\n\n0.035\n\n\nar3\n\n\n0.354\n\n\n0.043\n\n\nma1\n\n\n-0.725\n\n\n0.046\n\n\nma2\n\n\n-0.189\n\n\n0.046\n\n\nma3\n\n\n-0.575\n\n\n0.029\n\n\nma4\n\n\n0.552\n\n\n0.025\n\n\n\n\n# plot(acf(residuals(model_arima))) # weekly autocorrelation still exists\n\nred_line_validate %>% \n  slice(1:30)  %>%\n  mutate(pred = predict(model_arima, n.ahead = 30)$pred) %>%\n  # mutate(pred = forecast(model_arima, h = 30)$.mean) %>%\n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'darkred')\n\n\n\n\nWe’ll use yardstick to help us\nevaluate performance for this and subsequent models. In this case\nhowever, the visualization told us enough- a basic ARIMA isn’t going cut\nit.\n\n\nlibrary(yardstick)\n\n# this function will be used for all subsequent models!\nmetric_score = metric_set(rmse, mae, rsq) \n\n# validation\ndata.frame(\n  pred = predict(model_arima, newdata = red_line_validate, n.ahead = 30)$pred,\n  observed = red_line_validate$rides_scaled[1:30]\n) %>%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.632\n\n\nmae\n\n\nstandard\n\n\n0.572\n\n\nrsq\n\n\nstandard\n\n\n0.132\n\n\nOne nice thing about the forecast\npackage is that it can include additional features via the\nxreg argument, which is exactly what we need- additional\ninformation. Now our model looks something like this, where \\(X\\) is our model matrix of features and\n\\(\\beta\\) their corresponding\nregression weights.\n\\[y_t = X_t\\beta + \\alpha_1y_{t-1} + \\dots\n+\\alpha_{p}y_{t-p} + (\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} +\n\\cdots +\\theta_q \\varepsilon_{t-q})\\]\nAdding these is not exactly straightforward, since it requires a\nmatrix rather than a data frame, but this is not too big a deal once you\nare used to creating model matrices.\n\n\nmm = model.matrix(\n  ~ . - 1, \n  data = red_line_train %>% \n    select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization)\n)\n\nmodel_arima_xreg = forecast::auto.arima(\n  red_line_train$rides_scaled,\n  max.p = 10,\n  max.q = 10,\n  xreg  = mm\n)\n\n\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nar1\n\n\n-0.444\n\n\n0.018\n\n\nar2\n\n\n-0.430\n\n\n0.018\n\n\nar3\n\n\n-0.370\n\n\n0.019\n\n\nar4\n\n\n-0.325\n\n\n0.019\n\n\nar5\n\n\n-0.312\n\n\n0.019\n\n\nar6\n\n\n-0.356\n\n\n0.019\n\n\nar7\n\n\n0.307\n\n\n0.018\n\n\nar8\n\n\n-0.051\n\n\n0.017\n\n\nis_weekend\n\n\n-1.154\n\n\n0.023\n\n\nis_holiday\n\n\n-1.045\n\n\n0.021\n\n\nis_cubs_game\n\n\n0.208\n\n\n0.015\n\n\nis_sox_game\n\n\n0.072\n\n\n0.015\n\n\ntmax_scaled\n\n\n0.085\n\n\n0.011\n\n\nprcp_scaled\n\n\n-0.031\n\n\n0.004\n\n\nred_line_modernization\n\n\n-0.550\n\n\n0.131\n\n\nThis is looking much better! We can also see how notably different\nthe ARMA structure is relative to the previous model. We also see that\nadding weekend and holiday effects result in a huge drop in ridership as\nexpected, while baseball games and good weather will lead to an\nincrease.\nIn the following code, we create a model matrix similar to the\ntraining data that we can feed into the predict function. The forecast package also offers a\nglance method if desired.\n\n\nnd = red_line_validate %>%\n  select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization) %>% \n  model.matrix( ~ . - 1, data = .)\n\npreds = predict(model_arima_xreg, newxreg = nd, n.ahead = nrow(red_line_validate))$pred\n\n\n\n\n\np_arima_red = red_line_validate %>% \n  mutate(pred = preds) %>% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .25) +\n  geom_line(aes(y = pred), alpha = .25, color = 'red') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'ARIMA')\n\np_arima_red\n\n\n\n\nAnd here we can see performance is notably improved (restrict to\nfirst 30 obs for a direct comparison to the previous.\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.371\n\n\nmae\n\n\nstandard\n\n\n0.282\n\n\nrsq\n\n\nstandard\n\n\n0.747\n\n\n\n\n\nMixed model with AR\nStructure\nIntro\nMore generally, we can think of that original AR error as a random\neffect, such that after the linear predictor is constructed, we add a\nrandom effect based on the correlation structure desired, in this case,\nautoregressive. In the mixed model setting, it is actually quite common\nto use an AR residual structure within a cluster or group, and here we\ncan do so as well, as the data is naturally grouped by line.\nTo make this a bit more clear, we can state the AR effect more\nformally as follows for a single line at time \\(t\\):\n\\[z_t  \\sim N(0, \\Sigma_{ar})\\]\n\\[\\Sigma_{ar} = cov(z(s), z(t)) =\n\\sigma^2\\exp(-\\theta|t-s|)\\]\nWhere t,s are different time points, e.g. within a line.\nIf we were to simulate it for 4 time points, with autocovariance\nvalue of .5, we could do so as follows3.\n\n\nn_clusters   = 1\nn_timepoints = 4\nmu  = 0\nvar = 1  # not actually used if the value is 1\nS = .5^as.matrix(dist(1:n_timepoints))\n\nS\n\n\n      1    2    3     4\n1 1.000 0.50 0.25 0.125\n2 0.500 1.00 0.50 0.250\n3 0.250 0.50 1.00 0.500\n4 0.125 0.25 0.50 1.000\n\nz = MASS::mvrnorm(mu = rep(mu, n_timepoints), Sigma = S)\n\nz\n\n\n         1          2          3          4 \n-1.0185671 -0.6169717 -0.7159917 -0.7421824 \n\nAnd here is our typical model with a single random effect, e.g. for\nline:\n\\[ y_{tl} \\sim X\\beta + z^{line}_{l} +\ne_{tl}\\] \\[\\textrm{z}_{l} \\sim N(0,\n\\sigma_l^2)\\] \\[\\epsilon \\sim N(0,\n\\sigma_e^2)\\]\nThe X may be at either line or observation level, and potentially the\n\\(\\beta\\) could vary by line.\nPutting it all together, we’re just adding the AR random effect to\nthe standard mixed model for a single line.\n\\[ y_{tl} \\sim X\\beta + z^{ar}_t\n+z^{line}_{l} + e_{tl}\\]\nData Prep\nSo let’s try this! First some minor data prep to add holidays.\n\n\ndf_train_mixed = df_train %>% \n  mutate(date = as.numeric(date)) %>% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %>% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\ndf_validate_mixed = df_validate %>% \n  mutate(date = as.numeric(date)) %>% \n  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %>% \n  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))\n\n\n\nModel\nFor the model, we can now easily think of it as we do other standard\nmodeling scenarios. Along with standard features, we’ll add random\neffects for line, day, day x line interaction, etc. Finally we also add\nan AR random effect. For each line, we have an autoregressive structure\nfor days, such that days right next to each other are correlated, and\nthis correlation tapers off as days are further apart. This is not our\nonly option, but seems straightforward to me.\nDepending on what you include in the model, you may have convergence\nissues, so feel free to reduce the complexity if needed. For example,\nmost of the day effect is captured by weekend vs. not, and a by line\nyear trend wasn’t really necessary. In addition, the way the AR random\neffect variance is estimated as noted above, this essentially captures\nthe line intercept variance.\n\n\nmodel_mixed = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  tmax_scaled + \n  prcp_scaled + \n  snow_scaled +\n  # year_day +\n  ar1(0 + day|line) +     # the 0 + is a nuance of tmb's approach\n  (1|holiday) +           # as RE with all holidays instead of just holiday vs. not\n  (1|year) +     \n  (1 | red_line_modernization:line) +  # the project shifted ridership from red to other lines\n  # (1|day) #+ \n  # (1|line) +\n  (1|day:line) #+\n  # (1 + year_day|line)\n\nlibrary(glmmTMB)\n\nfit_mixed = glmmTMB(model_mixed, data = df_train_mixed)\n\n\n\nExplore\nThe mixed model approach is nice because it is highly interpretable.\nWe get both standard regression coefficients, and variance components to\nhelp us understand how the rest of the variance breaks down. For\nexample, I would interpret the following that that line and weekend are\nthe biggest contributors to the variability seen, and that we have high\nautocorrelation, as expected.\n\n\nlibrary(mixedup)\n\nsummarise_model(fit_mixed, digits = 4)\n\n\n\nVariance Components:\n                       Group Variance     SD SD_2.5 SD_97.5 Var_prop\n                        line   0.4758 0.6898 0.4696  1.0132   0.1370\n                     holiday   0.0734 0.2709 0.1868  0.3928   0.0211\n                        year   0.0039 0.0624 0.0393  0.0991   0.0011\n red_line_modernization:line   0.0379 0.1947 0.1226  0.3091   0.0109\n                    day:line   0.0000 0.0001 0.0000     Inf   0.0000\n                    Residual   0.0278 0.1668     NA      NA   0.0080\n\nFixed Effects:\n         Term   Value     SE        Z P_value Lower_2.5 Upper_97.5\n    Intercept -0.2710 0.2343  -1.1567  0.2474   -0.7301     0.1882\n   is_weekend -0.5184 0.0581  -8.9218  0.0000   -0.6323    -0.4045\n is_cubs_game  0.0088 0.0029   3.0009  0.0027    0.0030     0.0145\n  is_sox_game -0.0170 0.0029  -5.8336  0.0000   -0.0227    -0.0113\n  tmax_scaled  0.0467 0.0014  34.3268  0.0000    0.0441     0.0494\n  prcp_scaled -0.0115 0.0010 -11.3201  0.0000   -0.0135    -0.0095\n  snow_scaled -0.0039 0.0010  -3.9389  0.0001   -0.0058    -0.0020\n\nextract_cor_structure(fit_mixed, which_cor = 'ar1')\n\n\n# A tibble: 1 × 2\n  parameter  line\n  <chr>     <dbl>\n1 ar1       0.944\n\nWe can visually inspect how well it matches the data. In the\nfollowing the colored lines are the predictions, while the observed is\ngray. It looks like performance tapers for more recent time periods, and\nholiday effects are not as prevalent for some lines (e.g. yellow). The\nlatter could be helped by adding a holiday:line random\neffect.\n\n\np_mixed = df_validate_mixed %>% \n  droplevels() %>% \n  mutate(pred = predict(fit_mixed, newdata = .)) %>%\n  mutate(date = as_date(date)) %>% \n  ggplot(aes(date, rides_scaled)) +\n  geom_line(alpha = .1) +\n  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +\n  facet_grid(rows = vars(line), scales = 'free_y') +\n  labs(x = '', y = 'Rides (scaled)', subtitle = 'Mixed')\n\np_mixed\n\n\n\n\n\n\n\nAs before we can measure performance via yardstick. This model does\nappears to do very well.\n\n\n# validation\ndata.frame(\n  pred = predict(fit_mixed, newdata = df_validate_mixed, allow.new.levels = TRUE),\n  observed = df_validate_mixed$rides_scaled\n) %>%\n  metric_score(truth = observed, estimate = pred)\n\n\n\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.189\n\n\nmae\n\n\nstandard\n\n\n0.110\n\n\nrsq\n\n\nstandard\n\n\n0.965\n\n\n\n\n\nFor more on autocorrelation structure in the mixed model setting, see\nmy mixed model document here4.\nGeneralized Additive Models\nIntro\nWe can generalize mixed models even further to incorporate nonlinear\ncomponents, which may include cyclic or other effects. Such models are\ntypically referred to as generalized\nadditive models (GAMs). AR processes themselves can be seen as a\nspecial case of gaussian\nprocesses, which can potentially be approximated via GAMs. As GAMs\ncan accommodate spatial, temporal, nonlinear, and other effects, they\nare sometimes more generally referred to as structured additive\nregression models, or STARs.\nData Prep\nThe data prep for the GAM is the same as with the mixed model, so\nwe’ll just use that data.\n\n\ndf_train_gam = df_train_mixed\n\ndf_validate_gam = df_validate_mixed\n\n\n\nModel\nWith data in place we are ready to conduct the model. We have\nnumerous options for how we’d like to take this. However, as an example,\nI tried various smooths, but didn’t really see much difference, which is\nactually a good thing. For any further improvements we’d likely have to\ntweak the core model itself. I also use bam for a quicker result, but this isn’t really\nnecessary, as it didn’t even take a minute to run with standard gam. As\nwith the mixed model, we will use holiday as a random effect, but we add\nthe holiday by line interaction since we saw that need. In addition, our\nyear-day by line interaction should help some with the tailing off of\nmore recent predictions.\n\n\nlibrary(mgcv)\n\n# for year, use year (numeric) or use year_fac, but for latter, it will not be\n# able to predict any year not in the training data unless you use\n# drop.unused.levels.\nmodel_gam = \n  rides_scaled ~ \n  is_weekend +\n  is_cubs_game +\n  is_sox_game +\n  s(tmax_scaled) + \n  s(prcp_scaled) + \n  s(snow_scaled) +\n  s(red_line_modernization, line, bs = 're') +\n  s(holiday,  bs = 're') +\n  s(holiday, line,  bs = 're') +\n  s(year_fac, bs = 're') +      \n  s(day,  bs = 're') + \n  s(line, bs = 're') + \n  s(line, day, bs = 're') + \n  s(year_day, by = line, bs = c('ds', 'fs'))\n\n\n# will take a while!\n# fit_gam = gam(\n#   model_gam, \n#   data     = df_train_gam,\n#   drop.unused.levels = FALSE, \n#   method   = \"REML\"\n# )\n\n# fast even without parallel\nfit_gam = bam(\n  model_gam, \n  data     = df_train_gam,\n  drop.unused.levels = FALSE, \n  method   = \"fREML\",\n  discrete = TRUE     # will fit the model in a second rather than a couple seconds\n  # nthreads = 8,     # this option assumes a cluster is available. not necessary for this.\n)\n\n\n\nExplore\nAs with glmmTMB, I use a custom\nfunction to summarize the model, or extract different components from\nit. From the initial glance we can see things that we expect (e.g. line\nand weekend effects are large).\n\n\nmixedup::summarise_model(fit_gam)\n\n\n\nVariance Components:\n                       Group                 Effect Variance   SD SD_2.5      SD_97.5 Var_prop\n                 tmax_scaled              Intercept     0.06 0.25   0.15 4.200000e-01     0.08\n                 prcp_scaled              Intercept     0.00 0.01   0.00 2.000000e-02     0.00\n                 snow_scaled              Intercept     0.01 0.07   0.04 1.300000e-01     0.01\n                        line red_line_modernization     0.09 0.31   0.19 4.900000e-01     0.12\n                     holiday              Intercept     0.05 0.22   0.14 3.300000e-01     0.06\n                     holiday                   line     0.04 0.21   0.18 2.400000e-01     0.05\n                    year_fac              Intercept     0.00 0.06   0.04 1.000000e-01     0.01\n                         day              Intercept     0.00 0.00   0.00 1.325836e+58     0.00\n                        line              Intercept     0.49 0.70   0.42 1.150000e+00     0.61\n                        line                    day     0.05 0.22   0.18 2.700000e-01     0.06\n           year_day:lineblue              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n          year_day:linebrown              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n          year_day:linegreen              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:lineorange              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n           year_day:linepink              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:linepurple              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n year_day:linepurple_express              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n            year_day:linered              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n         year_day:lineyellow              Intercept     0.00 0.00   0.00 0.000000e+00     0.00\n                    Residual                            0.02 0.13   0.13 1.300000e-01     0.02\n\nFixed Effects:\n         Term Value   SE     t P_value Lower_2.5 Upper_97.5\n    Intercept -0.29 0.24 -1.18    0.24     -0.77       0.19\n   is_weekend -0.54 0.06 -8.27    0.00     -0.66      -0.41\n is_cubs_game  0.04 0.00 13.83    0.00      0.03       0.04\n  is_sox_game  0.01 0.00  4.06    0.00      0.01       0.02\n\nNow we can visualize test predictions broken about by line. The\ngreater flexibility of the GAM for fixed and other effects allows it to\nfollow the trends more easily than the standard linear mixed model\napproach.\n\n\n\nWe can also see improvement over our standard mixed model approach,\nand our best performance yet.\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.140\n\n\nmae\n\n\nstandard\n\n\n0.087\n\n\nrsq\n\n\nstandard\n\n\n0.981\n\n\n\n\n\nProphet\nIntro\nProphet is an approach from Facebook that uses Stan to\nestimate a time series model taking various trends, seasonality, and\nother factors under consideration. By default, it only uses Stan for\noptimization (e.g. via ‘BFGS’), but you can switch to fully Bayesian if\ndesired, and take advantage of all that the Bayesian approach has to\noffer.\nData Prep\nThe prophet package in R takes\nsome getting used to. We have to have specific names for our variables,\nand unfortunately have to do extra work to incorporate categorical\nfeatures. We can use recipes (like\nyardstick, part of the tidymodels ’verse) to set up the data\n(e.g. one-hot encoding).\n\n\nlibrary(prophet)\nlibrary(recipes)\n\ndf_train_prophet = df_train %>% \n  arrange(date, line) %>% \n  rename(y  = rides_scaled,\n         ds = date)\n\nrec = recipes::recipe(~., df_train_prophet)\n\ndf_train_prophet = rec %>% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %>% \n  prep(training = df_train_prophet) %>% \n  bake(new_data = df_train_prophet) %>% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\ndf_validate_prophet = df_validate %>% \n  arrange(date, line)%>%\n  rename(ds = date, y = rides_scaled)\n\nrec = recipe(~., df_validate_prophet)\n\ndf_validate_prophet = rec %>% \n  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %>% \n  prep(training = df_train_prophet) %>% \n  bake(new_data = df_validate_prophet) %>% \n  rename_with(.cols = starts_with('line_'), str_remove, 'line_')\n\n\n\nModel\nWith data in place, we are ready to build the model. Note that later\nwe will compare results to fable.prophet, which will mask some of the functions\nhere, or vice versa depending on which you load first. I would suggest\nonly doing the prophet model, or only doing the fable model, rather than\ntrying to do both at the same time, to avoid any mix-up.\nAfter setting up the model, you have to add additional features in\nseparate steps. Prophet has a nice way to incorporate holidays though.\nWhen you run this model, you may have to wait for a minute or so.\n\n\n# use prophet::prophet in case you have fable.prophet loaded also\nmodel_prophet = prophet::prophet(\n  holidays = generated_holidays %>% filter(country == 'US'),\n  yearly.seasonality = FALSE,\n  seasonality.mode = \"multiplicative\",\n  changepoint.prior.scale = .5\n)\n\nline_names = c(\n  'blue',\n  'brown',\n  'green',\n  'orange',\n  'pink',\n  'purple',\n  'purple_express',\n  'red',\n  'yellow'\n)\n\npredictors = c(\n  'is_weekend',\n  'is_cubs_game',\n  'is_sox_game',\n  # 'is_holiday',\n  'tmax_scaled',\n  'prcp_scaled',\n  'snow_scaled',\n  line_names\n)\n\nfor (i in predictors) {\n  model_prophet = add_regressor(model_prophet, i, standardize = FALSE, mode = 'additive')\n}\n\nmodel_prophet = add_country_holidays(model_prophet, country_name = 'US')\n\nfit_prophet = fit.prophet(model_prophet, df = df_train_prophet)\n\nforecast = predict(fit_prophet, df_validate_prophet)\n\n\n\nExplore\nWe now visualize predictions as we did with the GAM. But one of the nice things\nwith prophet is that you can plot the\nvarious parts of the model results via the plot  method or prophet_plot_components (not shown). Unfortunately, our\nbaseline effort seems to undersmooth our more popular lines (blue, red),\nand overreacts to some of the others (purple, yellow). This is because\nit’s not really changing the predictions according to line.\n\n\n\nWe can also assess performance as before, but note that prophet has it’s own cross-validation\ncapabilities which would be better to utilize if this was your primary\ntool. Between the previous visualization and these metrics, our first\nstab doesn’t appear to do as well as the GAM, so you might like to go\nback and tweak things.\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.280\n\n\nmae\n\n\nstandard\n\n\n0.198\n\n\nrsq\n\n\nstandard\n\n\n0.925\n\n\n\n\n\nFable\nIntro\nI came across fable.prophet as a\npossibly easier way to engage prophet. It is an extension of fable and related packages, which are very\nuseful for time series processing and analysis. Note that it is 0.1.0\nversion development, and hasn’t had much done with it in the past year,\nso your mileage may vary with regard to utility by the time you read\nthis5. But with it we can specify the\nmodel in more of an R fashion, and we now don’t have as much data\npre-processing.\nData Prep\nOne key difference using fable.prophet is that it works with tsibble\nobjects, and thus must have unique date observations. We can do this by\nsetting line as the key6.\n\n\nlibrary(fable.prophet)\n\ndf_train_fable = df_train_prophet %>% \n  as_tsibble(index = ds, key = line)\n\ndf_validate_fable = df_validate_prophet %>% \n  as_tsibble(index = ds, key = line)\n\nholidays_fable = holidays %>% \n  filter(country == 'US') %>% \n  mutate(ds = as_date(ds)) %>% \n  as_tsibble()\n\n\n\nModel\nBeyond this we use functions within our formula to set the various\ncomponents. With line as the key, fable is actually running separate prophet models for\neach line, and we can do so in parallel if desired.\n\n\nmodel_prophet = fable.prophet::prophet(\n  y ~ \n    growth('linear', changepoint_prior_scale = 0.5) +\n    season(\"week\", type = \"multiplicative\") +\n    holiday(holidays_fable) +\n    xreg(\n      is_weekend,\n      is_cubs_game,\n      is_sox_game,\n      # is_holiday,\n      tmax_scaled,\n      prcp_scaled,\n      snow_scaled\n    ) \n)\n\n# library(future)\n# plan(multisession)\n\n# furrr is used under the hood, and though it wants a seed, it doesn't\n# automatically use one so will give warnings. I don't think it can be passed\n# via the model function, so expect to see ignorable warnings (suppressed here).\n\nfit_fable = model(df_train_fable, mdl = model_prophet)\n\nforecast_fable = fit_fable %>% \n  forecast(df_validate_fable) \n\n# plan(sequential)\n\n\n\nExplore\nWith fable.prophet visualization,\nwe have the more automatic plots, but again we’ll stick with the basic\nvalidation plot we’ve been doing.\n\n\ncomponents(fit_fable)\ncomponents(fit_fable) %>%\n  autoplot()\nforecast_fable %>%\n  autoplot(level = 95, color = '#ff5500')\n\n\n\nThis model does well, and better than basic prophet, but we can see its limitations, for example,\nwith the yellow line, and more recent ridership in general.\n\n\n\nAnd we check performance as before. The fable model is doing almost as well as our GAM approach did.\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.155\n\n\nmae\n\n\nstandard\n\n\n0.093\n\n\nrsq\n\n\nstandard\n\n\n0.979\n\n\nOne nice thing about the fable\napproach is its internal performance metrics, which are easily obtained.\nIt will give us results for each line7, validation data results\nshown. We see that we have more error for the popular lines as before,\nbut in terms of percentage error, the other lines are showing more\ndifficulty. You can find out more about the additional metrics available\nhere.\n\n\naccuracy(fit_fable)\naccuracy(forecast_fable, df_validate_fable)\n\n\n\n\n\n.model\n\n\nline\n\n\n.type\n\n\nME\n\n\nRMSE\n\n\nMAE\n\n\nMPE\n\n\nMAPE\n\n\nACF1\n\n\nmdl\n\n\nblue\n\n\nTest\n\n\n0.086\n\n\n0.252\n\n\n0.181\n\n\n-26.534\n\n\n61.789\n\n\n0.642\n\n\nmdl\n\n\nbrown\n\n\nTest\n\n\n0.002\n\n\n0.126\n\n\n0.084\n\n\n-30.341\n\n\n88.109\n\n\n0.622\n\n\nmdl\n\n\ngreen\n\n\nTest\n\n\n0.093\n\n\n0.123\n\n\n0.104\n\n\n-110.198\n\n\n116.601\n\n\n0.669\n\n\nmdl\n\n\norange\n\n\nTest\n\n\n0.036\n\n\n0.084\n\n\n0.064\n\n\n-43.625\n\n\n60.708\n\n\n0.638\n\n\nmdl\n\n\npink\n\n\nTest\n\n\n0.060\n\n\n0.090\n\n\n0.073\n\n\n-18.824\n\n\n20.849\n\n\n0.679\n\n\nmdl\n\n\npurple\n\n\nTest\n\n\n0.006\n\n\n0.012\n\n\n0.009\n\n\n-0.726\n\n\n1.050\n\n\n0.512\n\n\nmdl\n\n\npurple_express\n\n\nTest\n\n\n0.046\n\n\n0.106\n\n\n0.081\n\n\n-39.910\n\n\n213.149\n\n\n0.750\n\n\nmdl\n\n\nred\n\n\nTest\n\n\n0.055\n\n\n0.300\n\n\n0.214\n\n\n-2.442\n\n\n14.890\n\n\n0.600\n\n\nmdl\n\n\nyellow\n\n\nTest\n\n\n-0.026\n\n\n0.030\n\n\n0.027\n\n\n2.851\n\n\n2.872\n\n\n0.720\n\n\nThe fable results suggests what we\nalready knew from our GAM and\nmixed model approach, that\ninteractions of the series with line are important. We weren’t easily\nable to do this with the default prophet (it would likely require adding time x line\ninteraction regresssors), so it’s nice that we have the option here.\n\n\n\nGBM\nIntro\nThis part is actually new from when I first wrote up this post over a\nyear ago. I basically wanted to test if a boosting approach would work\ndecently out of the box without doing anything special regarding the\nstructure of the data. I don’t add it to the summary visualizations, but\nprovide the standard results here.\nData Prep\nI’ll use lightgbm which I’ve been\nincreasingly using of late. It requires a matrix object for input, and\nso some additional processing as well.\n\n\n\n\n\nlibrary(lightgbm)\n\n# lightgbm only accepts a matrix input\ndf_train_lgb_init = df_train %>% \n  select(\n    rides_scaled,\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %>% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  )\n\nX_train = as.matrix(df_train_lgb_init %>% select(-rides_scaled))\n\nX_test  = df_validate %>% \n  select(\n    is_weekday,\n    is_holiday,\n    is_cubs_game,\n    is_sox_game,\n    tmax_scaled,\n    prcp_scaled,\n    line, \n    red_line_modernization,\n    year,\n    month,\n    year_day\n  ) %>% \n  mutate(\n    line = as.integer(line),\n    month = as.integer(month)\n  ) %>% \n  as.matrix()\n\n\ndf_train_lgb = lgb.Dataset(\n  X_train,\n  label = df_train_lgb_init$rides_scaled,\n  categorical_feature = c(\n    'is_weekday',\n    'is_holiday',\n    'is_cubs_game',\n    'is_sox_game',\n    'line',\n    'year',\n    'month'\n  )\n)\n\ndf_test_lgb  = lgb.Dataset.create.valid(\n  df_train_lgb, \n  X_test,\n  label = df_validate$rides_scaled\n)\n\n\n\nModel\nTypically we would perform some sort of search over the (many)\nparameters available to tweak with lightgbm,\nlike the number of trees, learning rate, regularizer parameters and\nmore. I ignore that, but I did fiddle with the learning rate and bumped\nup the nrounds (trees), but that’s it. We internally\nperform cross-validation as well (5-fold).\n\n\nparams <- list(\n  objective       = \"regression\"\n  , metric        = \"l2\"\n  , min_data      = 10L\n  , learning_rate = .01\n)\n\nfit_gbm <- lgb.cv(\n  params    = params\n  , data    = df_train_lgb\n  , nrounds = 2000L\n  , nfold   = 5L\n)\n\n\n\n\n\n\nExplore\nSome may be surprised at how well this does, but regular users of\nboosting probably are not. We didn’t have to do much and it’s already\nthe best performing model.\n\n\n\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.136\n\n\nmae\n\n\nstandard\n\n\n0.078\n\n\nrsq\n\n\nstandard\n\n\n0.982\n\n\n\n\n\nTorch\nAt this point we have a collection of models that are still\nrelatively interpretable, and mostly within our standard regression\nmodel framework. It’s good to see them able to perform very well without\ntoo much complexity. However, we still have other methods available that\nwould be more computationally demanding, are more opaque in operations,\nbut which would potentially provide the most accurate forecasts. For\nthis we turn to using PyTorch, which is now available via the torch package in R8.\nIn using torch, we’re going to\nfollow the demo\nseries at the RStudio AI blog 9. It shows in four parts\nhow to use a recurrent neural network. In their example, they\nuse a data set for a single series with (summarized) daily values,\nsimilar to our daily counts here. We will use the final model\ndemonstrated in the series, a soi disant seq2seq model that\nincludes an attention mechanism. More detail can be found here. The\nconceptual gist of the model can be described as taking a set of time\npoints to predict another set of future time points, and doing so for\nall points in the series.\nTo be clear, they only use a single series, no other information\n(e.g. additional regressors). So we will do the same, coming full circle\nto what we started out with, just looking at daily ridership- a single\ntime series for the red line.\nData\nAs usual we’ll need some data prep, both for initial training-test\nsplit creation, but also specifically for usage with Torch.\n\n\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(torch)\n\n\ndf_train_torch = df_train %>%\n  filter(line == 'red', year < 2017) %>%\n  pull(rides_scaled) %>%\n  as.matrix()\n\ndf_validate_torch = df_validate %>%\n  filter(line == 'red', year >= 2017) %>%\n  pull(rides_scaled) %>%\n  as.matrix()\n\ndf_test_torch = df_validate %>%\n  filter(line == 'red', date > '2017-12-24') %>%\n  pull(rides_scaled) %>%\n  as.matrix()\n\ntrain_mean = mean(df_train_torch)\ntrain_sd   = sd(df_train_torch)\n\n\n\nTorch data\nFor our data, we will use a week behind lag to predict the following\nweek. This seems appropriate for this problem, but for any particular\ntime series problem we’d want to probably think hard about this and/or\ntest different settings.\n\n\nn_timesteps = 7    # we use a week instead of 14 days in blog\nn_forecast  = 7    # look ahead one week\n\n\n\n\n\ncta_dataset <- dataset(\n  name = \"cta_dataset\",\n\n  initialize = function(x, n_timesteps, sample_frac = 1) {\n\n    self$n_timesteps <- n_timesteps\n    self$x <- torch_tensor((x - train_mean) / train_sd)\n\n    n <- length(self$x) - self$n_timesteps - 1\n\n    self$starts <- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n\n  },\n\n  .getitem = function(i) {\n\n    start <- self$starts[i]\n    end <- start + self$n_timesteps - 1\n    lag <- 1\n\n    list(\n      x = self$x[start:end],\n      y = self$x[(start+lag):(end+lag)]$squeeze(2)\n    )\n\n  },\n\n  .length = function() {\n    length(self$starts)\n  }\n)\n\nbatch_size = 32\n\ntrain_ds = cta_dataset(df_train_torch, n_timesteps)\ntrain_dl = dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds = cta_dataset(df_validate_torch, n_timesteps)\nvalid_dl = dataloader(valid_ds, batch_size = batch_size)\n\ntest_ds  = cta_dataset(df_test_torch, n_timesteps)\ntest_dl  = dataloader(test_ds, batch_size = 1)\n\n\n\nModel\nI leave it to the blog\nfor details, but briefly, there are four components to the model:\nEncoder: takes input, and produces outputs and\nstates via RNN\nDecoder: takes the last predicted value as input\nand current context to make a new prediction\nSeq2Seq: essentially encodes once, and calls the\ndecoder in a loop\nAttention: allows output from the encoder at a\nspecific time point to provide ‘context’ for the decoder\n\n\nnet =\n  seq2seq_module(\n    \"gru\",\n    input_size     = 1,\n    hidden_size    = 32,\n    attention_type = \"multiplicative\",\n    attention_size = 8,\n    n_forecast     = n_forecast\n  )\n\nb = dataloader_make_iter(train_dl) %>% dataloader_next()\n\nnet(b$x, b$y, teacher_forcing_ratio = 1)\n\n\n\nTraining\nWith data in place, we’re ready to train the model. For the most\npart, not much is going on here that would be different from other deep\nlearning situations, e.g. choosing an optimizer, number of epochs, etc.\nWe’ll use mean squared error as our loss, and I create an object to\nstore the validation loss over the epochs of training. I played around\nwith it a bit, and you’re probably not going to see much improvement\nafter letting it go for 100 epochs.\n\n\noptimizer = optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs = 100\n\ntrain_batch <- function(b, teacher_forcing_ratio) {\n\n  optimizer$zero_grad()\n  output <- net(b$x, b$y, teacher_forcing_ratio)\n  target <- b$y\n\n  loss <- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n  loss$backward()\n  optimizer$step()\n\n  loss$item()\n\n}\n\nvalid_batch <- function(b, teacher_forcing_ratio = 0) {\n\n  output <- net(b$x, b$y, teacher_forcing_ratio)\n  target <- b$y\n\n  loss <- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])\n\n  loss$item()\n\n}\n\n\nall_valid_loss = c()\n\nfor (epoch in 1:num_epochs) {\n\n  net$train()\n  train_loss <- c()\n\n  coro::loop(for (b in train_dl) {\n    loss <- train_batch(b, teacher_forcing_ratio = 0.0)\n    train_loss <- c(train_loss, loss)\n  })\n\n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n\n  net$eval()\n  valid_loss <- c()\n\n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  all_valid_loss = c(all_valid_loss, mean(valid_loss))\n\n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\n\n\n\n\nEvaluations\n\n\n\n\n\nnet$eval()\n\ntest_preds = vector(mode = \"list\", length = length(test_dl))\n\ni = 1\n\ncoro::loop(for (b in test_dl) {\n\n  if (i %% 100 == 0)\n    print(i)\n\n  output <- net(b$x, b$y, teacher_forcing_ratio = 0)\n  preds <- as.numeric(output)\n\n  test_preds[[i]] <- preds\n  i <<- i + 1\n})\n\n\n\n\n\n\nFor this visualization, we do things a little different. In our\ncurrent setup, we have 7 timesteps predicting 7 day windows. We started\nour test set at the beginning of December so that the first prediction\nis January first, and then proceeds accordingly.\n\n\n# same as test\ndf_eval_torch = df_validate %>%\n  filter(line == 'red', date > '2017-12-01') %>%\n  select(rides_scaled, date) %>%\n  as_tsibble()\n\ntest_preds_plot = vector(mode = \"list\", length = length(test_preds))\n\nfor (i in 1:(length(test_preds_plot)-  n_forecast)) {\n  test_preds_plot[[i]] =\n    data.frame(\n      pred = c(\n        rep(NA, n_timesteps + (i - 1)),\n        test_preds[[i]] * train_sd + train_mean,\n        rep(NA, nrow(df_eval_torch) - (i - 1) - n_timesteps - n_forecast)\n      )\n    )\n}\n\ndf_eval_torch_plot0 =\n  bind_cols(df_eval_torch, bind_cols(test_preds_plot))\n\n\n\nA visualization of the predictions makes this more clear. Each 30 day\nsegment is making predictions for the next 14 days.\n\n\n\nSo for our red line plot, we’ll just use the average prediction at\neach date to make it comparable to the other plots. In general it looks\nto be doing okay, even armed with no contextual information. Certainly\nbetter than the base ARIMA plot.\n\n\n\nHowever, we can see that there is much information lost just adhering\nto the series alone.\n\n\n.metric\n\n\n.estimator\n\n\n.estimate\n\n\nrmse\n\n\nstandard\n\n\n0.804\n\n\nmae\n\n\nstandard\n\n\n0.584\n\n\nrsq\n\n\nstandard\n\n\n0.120\n\n\nAll\n\n\n\n\n\n\n\n\n\nSummary\nARIMA: no real reason to still be doing such a simplified model\nMixed Model: may be just what you need, but may lack in other\nsettings\nGAM: great, more viable than some might suspect, easy\nimplementation\nProphet/Fable: Prophet needs notable work out of the box, though\nFable saves you some of that work, and did great in this situation via\nby-group models\nGBM: can it really be this easy?\nTorch: pretty good even with minimal information\nTo get some information on what Torch would do at the next level,\ni.e. adding additional features and other considerations, see Cody’s\npost.\n\n\n\nHyndman, R. J., and G. Athanasopoulos. 2021. Forecasting: Principles\nand Practice. 3rd ed. https://OTexts.com/fpp3.\n\n\nWest, Brady T, Kathleen B Welch, and Andrzej T Galecki. 2022. Linear\nMixed Models: A Practical Guide Using Statistical Software. Crc\nPress.\n\n\nThere is also the Purple express\nline, which is very irregular compared to the others.↩︎\nTechnically we should scale the test\nset using the mean/sd of the training set, and though with very large\ndata this should not matter, for time series it’s a particular concern\nas data can ‘shift’ over time.↩︎\nThis follows Bolker’s\ndemo.↩︎\nI always appreciated the depiction of\nthis topic in West, Welch,\nand Galecki (2022) quite a bit.↩︎\nA year plus later after that\nstatement, it still hasn’t gone beyond 0.1.0, so I don’t think this will\ncontinue to be useful for very long. Unfortunate, but honestly, it’s not\nclear prophet<\/span itself can do much\nbetter than many other tools.↩︎\nfable.prophet may have a bug enabling the holidays\nfunctionality with parallel, so you can just use the original holiday\ncolumn if you do so (single core doesn’t take too long).↩︎\nWe can also do this with our previous\nmethod with a split-by-apply approach. You would obtain the same\nresults, so this serves as a nice supplement to our ‘overall’ metrics.↩︎\nFor the basics of using PyTorch via\nR, including installation, see the\nRStudio post.↩︎\nThe blog code actually has several\nissues, but the github\nrepo should work fine and is what is followed for this demo.↩︎\n",
    "preview": "posts/2021-05-time-series/../../img/time-series/dalle_mini_time_series_seasonal_effect.png",
    "last_modified": "2022-08-18T15:50:57-04:00",
    "input_file": {},
    "preview_width": 1460,
    "preview_height": 1456
  },
  {
    "path": "posts/2022-07-25-programming/",
    "title": "Programming Odds & Ends",
    "description": "Explorations in faster data processing and other problems.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2022-07-25",
    "categories": [
      "programming"
    ],
    "contents": "\n\nContents\nIntroduction\nQuick\nsummary\nSetup and orientation\nFill in missing values\nAntijoins\nLag/lead/differences\nFirst/Last\nCoalesce/ifelse\nConditional\nSlide\nTake the\nfirst TRUE\nGroup by filtering/slicing\nTidy timings\nOverview\nStandard grouped operation\nCount\n\nSummary\n\nIntroduction\nOftentimes I’m looking to gain speed/memory advantages, or maybe just\nexploring how to do the same thing differently in case it becomes useful\nlater on. I thought I’d start posting them, but then I don’t get around\nto it. Here are a few that have come up over the past year (or two 😞),\nand even as I wrote this, more examples kept coming up, so I may come\nback to add more in the future.\nQuick summary\nData processing efficiency really depends on context. Faster doesn’t\nmean memory efficient, and what may be the best in a standard setting\ncan be notably worse in others. Some approaches won’t show their value\nuntil the data is very large, or there are many groups to deal with,\nwhile others will get notably worse. Also, you may not want an\nadditional package dependency beyond what you’re using, and may need a\nbase R approach. The good news is you’ll always have options!\nOne caveat: I’m not saying that the following timed approaches are\nnecessarily the best/fastest, I mostly stuck to ones I’d try first. You\nmay find even better for your situation! A great resource to keep in\nmind is the fastverse, which\nis a collection of packages with speed and efficiency in mind, and\nincludes a couple that are demonstrated here.\nSetup and orientation\nRequired packages.\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(dtplyr)\nlibrary(tidyfast)\nlibrary(data.table)\nlibrary(collapse)\nlibrary(vctrs)\nlibrary(bench)   # for timings\n\n\n\nAlso, in the following bench marks I turn off checking if the results\nare equivalent (i.e. check = FALSE) because even if the\nresulting data is the same, the objects may be different classes, or\nsome objects may even be of the same class, but have different\nattributes. You are welcome to double check that you would get the same\nthing as I did. Also, you might want to look at the\nautoplot of the bench mark summaries, as many results have\nsome variability that isn’t captured by just looking at median/best\ntimes.\n\n\n\nFill in missing values\nWe’ll start with the problem of filling in missing values by group.\nI’ve created a realistic example where the missingness is seen randomly\nacross multiple columns, and differently across groups. I’ve chosen to\ncompare tidyr, tidyfast, the underlying approach of tidyr via vctrs, and data.table.\nNote that only data.table is not last\nobservation carried forward by default (‘down’ in tidyr parlance), so that argument is made explicit.\nAll of these objects will have different attributes or classes. tidyfast for some reason renames the grouping\nvariable to ‘by’. If you wrap all of these in data.frame,\nthat will remove the attributes and give them all the same class, so you\ncan verify they return the same result.\n\nset.seed(1234)\n\nN = 1e5\nNg = 5000\n\ncreate_missing <- function(x) {\n  x[sample(1:length(x), 5)] = NA\n  x\n}\n\n\ndf_missing = tibble(grp = rep(1:Ng, e = N / Ng)) %>%\n  arrange(grp) %>%\n  group_by(grp) %>%\n  mutate(\n    x = 1:n(),\n    y = rpois(n(), 5),\n    z = rnorm(n(), 5),\n    across(x:z, create_missing)\n  ) %>% \n  ungroup()\n\ndf_missing %>% head(5)\n\ndt_missing = as.data.table(df_missing) %>% setkey(grp)\ntf_missing = copy(dt_missing)\n\nbm_fill <-\n  bench::mark(\n    %!in%\n    tidyr    = fill(group_by(df_missing, grp), x:z),  \n    tidyfast = dt_fill(tf_missing, x, y, z, id = grp),\n    vctrs    = df_missing %>% group_by(grp) %>% mutate(across(x:z, vec_fill_missing)),\n    dt = dt_missing[\n      ,\n      .(x = nafill(x, type = 'locf'),\n        y = nafill(y, type = 'locf'),\n        z = nafill(z, type = 'locf')),\n      by = grp\n    ],\n    check = FALSE,\n    iterations = 10\n  )\n\n\n\n\nThis is a great example of where there is a notable speed/memory\ntrade-off. Very surprising how much memory data.table uses1, while not giving much\nspeed advantage relative to the tidyr. Perhaps\nthere is something I’m missing (😏)? Also note that we can get an even\n‘tidier’ advantage by using vctrs directly,\nrather than wrapping it via tidyr, and seeing\nhow easy it is to use, it’s probably the best option.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    tidyfast\n 18.3ms\n 24.5ms\n 39.87MB\n1\n6.06vctrs\n 27ms\n 28.5ms\n  6.58MB\n1.16\n1dt\n111.7ms\n117.8ms\n237.08MB\n4.81\n36.04tidyr\n132.3ms\n147.2ms\n  6.59MB\n6.01\n1\n\nAntijoins\nSometimes you just don’t want that! In the following we have a\nsituation where we want to filter values based on the negation of some\ncondition. Think of a case where certain person IDs are not viable for\nconsideration for analysis. Many times, a natural approach would be to\nuse something like a filter where instead of using\nvals %in% values_desired, we just negate that with a bang\n(!) operator. However, another approach is to create a data\nframe of the undesired values and use an anti_join. When\nusing joins in general, you get a relative advantage by explicitly\nnoting the variables you’re joining on, so I compare that as well for\ndemonstration. Finally, in this particular example we could use data.table’s built-in character match,\nchin.\n\n\nset.seed(123)\n\ndf1 = tibble(\n  id = sample(letters, 10000, replace = TRUE)\n)\n\ndf2 = tibble(\n  id = sample(letters[1:10], 10000, replace = TRUE)\n)\n\ndf1_lazy = lazy_dt(df1)\ndf2_lazy = lazy_dt(df2)\n\ndf1_dt = data.table(df1)\ndf2_dt = data.table(df2)\n\nsuppressMessages({\n  bm_antijoin = bench::mark(\n    in_     = filter(df1, !id %in% letters[1:10]),\n    in_dtp  = collect(filter(df1_lazy, !id %in% letters[1:10])),     # not usable until collected/as_tibbled\n    chin    = filter(df1, !id %chin% letters[1:10]),                 # chin for char vector only, from data.table\n    chin_dt = df1_dt[!df1_dt$id %chin% letters[1:10],],              \n    coll    = fsubset(df1, id %!in% letters[1:10]),                  # can work with dt or tidyverse\n    aj      = anti_join(df1, df2, by = 'id'),\n    aj_noby = anti_join(df1, df2),\n\n    iterations = 100,\n    check      = FALSE\n  )\n})\n\n\n\n\n\n\nIn this case, the fully data.table approach\nis best in speed and memory, but collapse is\nnot close behind2. In addition, if you are in the\ntidyverse, the anti_join function is a very good option.\nHopefully the lesson about explicitly setting the by\nargument is made clear.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    chin_dt\n152µs\n160µs\n206KB\n1\n1coll\n167µs\n185µs\n268KB\n1.16\n1.3aj\n529µs\n559µs\n293KB\n3.5\n1.42chin\n626µs\n662µs\n270KB\n4.15\n1.31in_\n716µs\n804µs\n387KB\n5.04\n1.88in_dtp\n923µs\n988µs\n479KB\n6.2\n2.33aj_noby\n  9.95ms\n 10.962ms\n323KB\n68.72\n1.57\n\nLag/lead/differences\nHere we are interested in getting the difference in the current value\nof some feature from it’s last (or next) value, typically called a lag\n(lead). Note that it doesn’t have to be the last value, but that is most\ncommon. In the tidyverse we have lag/lead\nfunctions, or with data.table, we have the\ngeneric shift function that can do both. In the following I\nlook at using that function in the fully data.table situation or within a tibble.\n\n\nset.seed(1234)\n\nN = 1e5\nNg = 5000\n\ndf  = tibble(\n  x = rpois(N, 10),\n  grp = rep(1:Ng, e = N / Ng)\n)\n\ndt = as.data.table(df)\n\nbm_lag = bench::mark(\n  dplyr_lag  = mutate(df, x_diff = x - lag(x)),\n  dplyr_lead = mutate(df, x_diff = x - lead(x)),\n  dt_lag     = dt[, x_diff := x - shift(x)],\n  dt_lead    = dt[, x_diff := x - shift(x, n = -1)],\n  dt_dp_lag  = mutate(df, x_diff = x - shift(x)),\n  dt_dp_lead = mutate(df, x_diff = x - shift(x, n = -1)),\n  coll_lag   = ftransform(df, x_diff = x - flag(x)),\n  coll_lead  = ftransform(df, x_diff = x - flag(x, n = -1)),\n  iterations = 100,\n  check      = FALSE\n)\n\n\n\n\n\n\nIn this case, collapse is best, with data.table not far behind, but using the\nshift function within the tidy approach is a very solid\ngain. Oddly, lag and lead seem somewhat\ndifferent in terms of speed and memory.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    coll_lag\n226µs\n260µs\n783.84KB\n1\n1coll_lead\n250µs\n309µs\n783.84KB\n1.18\n1dt_lag\n316µs\n354µs\n813.87KB\n1.36\n1.04dt_lead\n319µs\n355µs\n813.87KB\n1.36\n1.04dt_dp_lag\n792µs\n818µs\n782.91KB\n3.14\n1dt_dp_lead\n797µs\n892µs\n782.91KB\n3.42\n1dplyr_lead\n994µs\n  1.115ms\n  1.53MB\n4.28\n2dplyr_lag\n  1.044ms\n  1.2ms\n  1.15MB\n4.61\n1.5\n\nWhat about a grouped scenario? To keep it simple we’ll just look at\nusing lagged values.\n\n\nbm_lag_grp = bench::mark(\n  dt_lag    = dt[, x_diff := x - shift(x), by = grp],\n  dt_dp_lag = mutate(group_by(df, grp), x_diff = x - shift(x)),\n  dplyr_lag = mutate(group_by(df, grp), x_diff = x - lag(x)),\n  coll_lag  = fmutate(fgroup_by(df, grp), x_diff = x - flag(x)),\n  \n  iterations = 10,\n  check      = FALSE\n)\n\n\n\n\n\n\nIn the grouped situation, using a collapse\nisn’t best for memory, but the speed gain is ridiculous!!\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    coll_lag\n484µs\n548µs\n  1.59MB\n1\n3.52dt_lag\n 25.692ms\n 28.067ms\n462.32KB\n51.2\n1dt_dp_lag\n 30.843ms\n 32.969ms\n  2.61MB\n60.15\n5.77dplyr_lag\n 77.156ms\n 78.771ms\n  5.19MB\n143.7\n11.49\n\nFirst/Last\nIn this demo, we want to take the first (last) value of each group.\nSurprisingly, for the same functionality, it turns out that the number\nof groups matter when doing groupwise operations. For the following I’ll\neven use a base R approach (though within dplyr’s mutate) to demonstrate some differences.\n\n\nset.seed(1234)\n\nN = 100000\n\ndf = tibble(\n  x  = rpois(N, 10),\n  id = sample(1:100, N, replace = TRUE)\n)\n\ndt = as.data.table(df)\n\nbm_first = bench::mark(\n  base_first  = summarize(group_by(df, id), x = x[1]),\n  base_last   = summarize(group_by(df, id), x = x[length(x)]),\n  dplyr_first = summarize(group_by(df, id), x = dplyr::first(x)),\n  dplyr_last  = summarize(group_by(df, id), x = dplyr::last(x)),\n  dt_first    = dt[, .(x = data.table::first(x)), by = id],\n  dt_last     = dt[, .(x = data.table::last(x)),  by = id],\n  coll_first  = ffirst(fgroup_by(df, id)),\n  coll_last   = flast(fgroup_by(df, id)),\n  \n  iterations  = 100,\n  check       = FALSE\n)\n\n\n\n\n\n\nThe first result is actually not too surprising, in that the fully dt\napproaches are fast and memory efficient, though collapse is notably faster. Somewhat interesting is\nthat the base last is a bit faster than dplyr’s last (technically\nnth) approach.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    coll_last\n307µs\n341µs\n783.53KB\n1\n1.72coll_first\n299µs\n345µs\n783.53KB\n1.01\n1.72dt_last\n698µs\n876µs\n454.62KB\n2.57\n1dt_first\n689µs\n895µs\n454.62KB\n2.62\n1base_last\n  1.944ms\n  2.036ms\n  2.06MB\n5.97\n4.64dplyr_first\n  2.026ms\n  2.127ms\n  2.06MB\n6.24\n4.64base_first\n  1.984ms\n  2.165ms\n  2.06MB\n6.35\n4.64dplyr_last\n  2.111ms\n  2.553ms\n  2.06MB\n7.49\n4.64\n\nIn the following, the only thing that changes is the number of\ngroups.\n\n\nset.seed(1234)\n\nN = 100000\n\ndf = tibble(\n  x = rpois(N, 10),\n  id = sample(1:(N/10), N, replace = TRUE) # <--- change is here\n)\n\ndt = as.data.table(df)\n\nbm_first_more_groups = bench::mark(\n  base_first  = summarize(group_by(df, id), x = x[1]),\n  base_last   = summarize(group_by(df, id), x = x[length(x)]),\n  dplyr_first = summarize(group_by(df, id), x = dplyr::first(x)),\n  dplyr_last  = summarize(group_by(df, id), x = dplyr::last(x)),\n  dt_first    = dt[, .(x = data.table::first(x)), by = id],\n  dt_last     = dt[, .(x = data.table::last(x)),  by = id],\n  coll_first  = ffirst(group_by(df, id)),\n  coll_last   = flast(group_by(df, id)),\n  iterations  = 100,\n  check       = FALSE\n)\n\n\n\n\n\n\nNow what the heck is going on here? The base R approach is way faster\nthan even data.table, while not using any more\nmemory than what dplyr is doing (because of\nthe group-by-summarize). More to the point is that collapse is notably faster than the other options,\nbut still a bit heavy memory-wise relative to data.table.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    coll_last\n 2.813ms\n 3.006ms\n  2.79MB\n1\n3.8coll_first\n 2.838ms\n 3.128ms\n  2.79MB\n1.04\n3.8base_first\n10.596ms\n11.4ms\n  3.06MB\n3.79\n4.17base_last\n12.551ms\n13.151ms\n  3.06MB\n4.37\n4.17dt_last\n17.588ms\n18.318ms\n752.15KB\n6.09\n1dt_first\n17.671ms\n18.379ms\n752.15KB\n6.11\n1dplyr_first\n20.066ms\n20.943ms\n  3.06MB\n6.97\n4.17dplyr_last\n20.916ms\n21.357ms\n  3.16MB\n7.1\n4.31\n\nCoalesce/ifelse\nIt’s very often we want to change a single value based on some\ncondition, often starting with ifelse. This is similar to\nour previous fill situation for missing values, but applies a constant\nas opposed to last/next value. Coalesce is similar to tidyr’s fill, and is often used in\ncases where we might otherwise use an ifelse style approach\n. In the following, we want to change NA values to zero, and there are\nmany ways we might go about it.\n\n\nset.seed(1234)\nx = rnorm(1000)\nx[x > 2] = NA\n\n\nbm_coalesce = bench::mark(\n  base      = {x[is.na(x)] <- 0; x},\n  ifelse    = ifelse(is.na(x), 0, x),\n  if_else   = if_else(is.na(x), 0, x),\n  vctrs     = vec_assign(x, is.na(x), 0),\n  tidyr     = replace_na(x, 0),\n  fifelse   = fifelse(is.na(x), 0, x),\n  coalesce  = coalesce(x, 0),\n  fcoalesce = fcoalesce(x, 0),\n  nafill    = nafill(x, fill = 0),\n  coll      = replace_NA(x)   # default is 0\n)\n\n\n\n\n\n\nThe key result here to me is just how much memory the dplyr if_else approach is using, as\nwell as how fast and memory efficient the base R approach is even with a\nsecond step. While providing type safety, if_else is both\nslow and a memory hog, so probably anything else is better. tidyr itself would be a good option here, and while\nit makes up for the memory issue, it’s relatively slow compared to other\napproaches, including the function it’s a wrapper for\n(vec_assign), which is also demonstrated. Interestingly,\nfcoalesce and fifelse would both be better\noptions than data.table’s other approach that\nis explicitly for this task.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    fcoalesce\n 1µs\n 1µs\n 7.86KB\n1\n1coll\n 1µs\n 1µs\n 7.86KB\n1.03\n1base\n 2µs\n 3µs\n 7.91KB\n2.17\n1.01fifelse\n 3µs\n 4µs\n11.81KB\n2.6\n1.5vctrs\n 4µs\n 4µs\n11.81KB\n3.09\n1.5nafill\n 5µs\n 6µs\n23.95KB\n4.14\n3.05tidyr\n 9µs\n10µs\n11.81KB\n7.09\n1.5coalesce\n15µs\n17µs\n20.19KB\n11.71\n2.57ifelse\n15µs\n17µs\n47.31KB\n12.11\n6.02if_else\n21µs\n25µs\n71.06KB\n17.54\n9.04\n\nConditional Slide\nI recently had a problem where I wanted to do a apply a certain\nfunction that required taking the difference between the current and\nlast value as we did in the lag demo. The problem was that ‘last’\ndepended on a specific condition being met. The basic idea is that we\nwant to take x - lag(x) but where the condition is FALSE,\nwe need to basically ignore that value for consideration as the last\nvalue, and only use the previous value for which the condition is\nTRUE. In the following, for the first two values where the\ncondition is met, this is straightforward (6 minus 10). But for the\nfourth row, 4 should subtract 6, rather than 5, because the condition is\nFALSE.\n\n\nset.seed(1234)\n\ndf = tibble(\n  x = sample(1:10),\n  cond = c(TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE),\n  group = rep(c('a', 'b'), e = 5)\n)\n\ndf \n\n\n# A tibble: 10 × 3\n       x cond  group\n   <int> <lgl> <chr>\n 1    10 TRUE  a    \n 2     6 TRUE  a    \n 3     5 FALSE a    \n 4     4 TRUE  a    \n 5     1 FALSE a    \n 6     8 TRUE  b    \n 7     2 FALSE b    \n 8     7 FALSE b    \n 9     9 TRUE  b    \n10     3 FALSE b    \n\nWhile somewhat simple in concept, it doesn’t really work with simple\nlags, as the answer would be wrong, or sliding functions, because the\nwindow is adaptive. I wrote the following function to deal with this. By\ndefault, it basically takes our vector under consideration,\nx, makes it NA where the condition doesn’t\nhold, then fills in the NA values with the last value using the\nvec_fill_missing (or a supplied constant/single value).\nHowever there is flexibility beyond that type of fill. In addition, the\nfunction applied is generic, and could be applied to the newly created\nvariable (.x), or use both the original (x)\nand the newly created variable.\n\n\nconditional_slide <-\n  function(x,\n           condition,\n           fun,\n           direction  = c(\"down\"),\n           fill_value = NA,\n           na_value   = NA,\n           ...) {\n    \n    if (!direction %in% c(\"constant\", \"down\", \"up\", \"downup\", \"updown\"))\n      rlang::abort('direction must be one of \"constant\", \"down\", \"up\", \"downup\", \"updown\"')\n    \n    if (length(x) != length(condition))\n      rlang::abort('condition and x must be the same length')\n    \n    # can't use dplyr/dt ifelse since we won't know class type of fill_value\n    conditional_val <- ifelse(direction == 'constant', fill_value, NA)    \n    .x <- ifelse(condition, x, conditional_val)\n    \n    if (direction != 'constant')\n      .x <- vctrs::vec_fill_missing(.x, direction = direction)\n    \n    class(.x) <- class(x)\n    \n    result <- fun(x, .x, ...)\n    \n    if (!is.na(na_value))\n      result[is.na(result)] <- na_value\n    \n    result\n  }\n\n\n\nThe first example applies the function, x - lag(x), to\nour dataset, and which in my case, I also wanted to apply within groups,\nwhich caused further problems for some of the available functions I\nthought would otherwise be applicable. I also show it for another type\nof problem, taking the cumulative sum, as well as just conditionally\nchanging the values to zero.\n\n\ndf %>%\n group_by(group) %>%\n mutate(\n   # demo first difference\n   simple_diff = x - dplyr::lag(x),\n   cond_diff = conditional_slide(x, cond, fun = \\(x, .x) x - lag(.x), na_value = 0),\n   \n   # demo cumulative sum\n   simple_cumsum = cumsum(x),\n   cond_cumsum   = conditional_slide(\n     x,\n     cond,\n     fun = \\(x, .x) cumsum(.x),\n     direction = 'constant',\n     fill = 0\n   ),\n   \n   # demo fill last\n   simple_fill_last = vec_fill_missing(x),\n   cond_fill_last = conditional_slide(\n     x,\n     cond,\n     fun = \\(x, .x) .x,\n     direction = 'down'\n   )\n )\n\n\n# A tibble: 10 × 9\n# Groups:   group [2]\n       x cond  group simple_diff cond_diff simple_cumsum cond_cumsum simple_fill_last cond_fill_last\n   <int> <lgl> <chr>       <int>     <dbl>         <int>       <int>            <int>          <int>\n 1    10 TRUE  a              NA         0            10          10               10             10\n 2     6 TRUE  a              -4        -4            16          16                6              6\n 3     5 FALSE a              -1        -1            21          16                5              6\n 4     4 TRUE  a              -1        -2            25          20                4              4\n 5     1 FALSE a              -3        -3            26          20                1              4\n 6     8 TRUE  b              NA         0             8           8                8              8\n 7     2 FALSE b              -6        -6            10           8                2              8\n 8     7 FALSE b               5        -1            17           8                7              8\n 9     9 TRUE  b               2         1            26          17                9              9\n10     3 FALSE b              -6        -6            29          17                3              9\n\nThis is one of those things that comes up from time to time where\ntrying to apply a standard tool likely won’t cut it. You may find\nsimilar situations where you need to modify what’s available and create\nsome functionality tailored to your needs.\nTake the first TRUE\nSometimes we want the first instance of a condition. For example, we\nmight want the position or value of the first number > than some\nvalue. We’ve already investigated using dplyr\nor data.table’s first, and I\nwon’t do so again here except to say they are both notably slower and\nworse on memory here. We have a few approaches we might take in base R.\nUsing which would be common, but there is also\nwhich.max, that, when applied to logical vectors, gives the\nposition of the first TRUE (which.min gives\nthe position of the first FALSE). In addition, there is the\nPosition function, which I didn’t even know about until\nmessing with this problem.\n\n\nset.seed(123)\n\nx = sort(rnorm(10000))\n\nmarker = 2\n\nbm_first_true_1 = bench::mark(\n  which     = which(x > marker)[1],\n  which_max = which.max(x > marker),\n  pos       = Position(\\(x) x > marker, x)\n)\n\n\n# make it slightly more challenging\nx = sort(rnorm(1e6))\n\nmarker = 4 \n\nbm_first_true_2 = bench::mark(\n  which     = which(x > marker)[1],\n  which_max = which.max(x > marker),\n  pos       = Position(\\(x) x > marker, x)\n)\n\n\n\n\n\n\nInterestingly Position provides the best memory\nperformance, but is prohibitively slower. which.max is\nprobably your best bet.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    which\n15µs\n20µs\n79.14KB\n1\n14.19which_max\n18µs\n20µs\n39.11KB\n1.01\n7.01pos\n 1.986ms\n 2.158ms\n 5.58KB\n109.67\n1\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    which\n  1.44ms\n  1.69ms\n7.63MB\n1\n1400.58which_max\n  1.79ms\n  2ms\n3.81MB\n1.18\n700.29pos\n213.55ms\n219.13ms\n5.58KB\n129.4\n1\n\nBut not so fast? The following makes the first case come very\nquickly, where Position blows the other options out of the\nwater! I guess if you knew this was going to be the case you could take\nserious advantage.\n\n\nset.seed(123)\n\nx = sort(rnorm(100000), decreasing = TRUE)\n\nx[1:30] = 4\n\n\nbm_first_true_3 = bench::mark(\n  which     = which(x < marker)[1],\n  which_max = which.max(x < marker),\n  pos       = Position(\\(x) x < marker, x) \n)\n\n\n\n\n\n\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    pos_rev\n 10µs\n 10µs\n  5.58KB\n1\n1which_max_rev\n110µs\n130µs\n390.67KB\n13.04\n70.04which_rev\n160µs\n220µs\n  1.14MB\n22.55\n210.09\n\nGroup by filtering/slicing\nThe previous situation was the basis for this next demo where we\nutilize which.max. Here we want to filter in one scenario,\nsuch that if all values are zero, we drop them, and in the second, we\nwant to only retain certain values based on a condition. In this latter\ncase, the condition is that at least one non-zero has occurred, in which\ncase we want to keep all of those values from that point on (even if\nthey are zero).\nTo make things more clear, for the example data that follows, we want\nto drop group 1 entirely, the initial part of group 2, and retain all of\ngroup 3.\n\n\nlibrary(tidyverse)\n\nset.seed(12345)\n\ndf = tibble(\n  group = rep(1:3, e = 10),\n  value = c(\n    rep(0, 10),\n    c(rep(0, 3), sample(0:5, 7, replace = TRUE)), \n    sample(0:10, 10, replace = TRUE)\n  )\n)\n\n\ndf %>% \n  group_by(group) %>% \n  filter(!all(value == 0)) %>% \n  slice(which.max(value > 0):n())\n\n\n# A tibble: 17 × 2\n# Groups:   group [2]\n   group value\n   <int> <dbl>\n 1     2     5\n 2     2     2\n 3     2     1\n 4     2     3\n 5     2     1\n 6     2     4\n 7     2     2\n 8     3     7\n 9     3     1\n10     3     5\n11     3    10\n12     3     5\n13     3     6\n14     3     9\n15     3     0\n16     3     7\n17     3     6\n\nIn the above scenario, we take two steps to illustrate our desired\noutcome conceptually. Ideally though, we’d like one step, because it is\njust a general filtering. You might think maybe to change\nwhich.max to which and just slice, but this\nwould remove all zeros, when we want to retain zeros after the point\nwhere at least some values are greater than zero. Using\nrow_number was a way I thought to get around things.\n\n\ndf %>% \n  group_by(group) %>% \n  filter(!all(value == 0) & row_number() >= which.max(value > 0))\n\n\nbm_filter_slice = bench::mark(\n  orig = df %>% \n    group_by(group) %>% \n    filter(!all(value == 0)) %>% \n    slice(which.max(value > 0):n()),\n  \n  new = df %>% \n    group_by(group) %>% \n    filter(!all(value == 0) & row_number() >= which.max(value > 0))\n)\n\n\n\n\n\n\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    orig\n2ms\n2.2ms\n10.25KB\n1\n1.11new\n6.1ms\n7.1ms\n 9.21KB\n3.28\n1\n\nWell we got it to one operation, but now it takes longer and has no\nmemory advantage. Are we on the wrong track? Let’s try with a\nrealistically sized data set with a lot of groups.\n\n\nset.seed(1234)\n\nN = 100000\ng = 1:(N/4)\n\ndf = tibble(\n  group = rep(g, e = 4),\n  value = sample(0:5, size = N, replace = TRUE)\n)\n\nbm_filter_slice2 = bench::mark(\n  orig = df %>% \n    group_by(group) %>% \n    filter(!all(value == 0)) %>% \n    slice(which.max(value > 0):n()),\n  \n  new = df %>% \n    group_by(group) %>% \n    filter(!all(value == 0) & row_number() >= which.max(value > 0))\n)\n\n\n\n\n\n\nNow we have the reverse scenario. The single filter is notably faster\nand more memory efficient.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    new\n208.6ms\n209.9ms\n12.9MB\n1\n1orig\n949.8ms\n949.8ms\n28.3MB\n4.53\n2.19\n\nTidy timings\nOverview\n\nThis tidy timings section comes from a notably old exploration I\nrediscovered (I think it was originally Jan 2020), but it looks like\ntidyfast still has some functionality beyond dtplyr, and it doesn’t hurt\nto revisit. I added a result for collapse. My original timings were on a\nnicely suped up pc, but the following are on a year and a half old\nmacbook with an M1 processer, and were almost 2x faster.\n\nHere I take a look at some timings for data processing tasks. My\nreason for doing so is that dtplyr has\nrecently arisen from the dead, and tidyfast\nhas come on the scene, so I wanted a quick reference for myself and\nothers to see how things stack up against data.table.\nSo we have the following:\nBase R: Just kidding. If you’re using base R approaches for\nthis aggregate you will always be slower. Functions like\naggregate, tapply and similar could be used in\nthese demos, but I leave that as an exercise to the reader. I’ve done\nthem, and it isn’t pretty.\ndplyr: standard data wrangling workhorse package\ntidyr: has some specific functionality not included in\ndplyr\ndata.table: another commonly used data processing package\nthat purports to be faster and more memory efficient (usually but not\nalways)\ntidyfast: can only do a few things, but does them\nquickly.\ncollapse: many replacements for base R functions.\nStandard grouped operation\nThe following demonstrates some timings from this\npost on stackoverflow. I reproduced it on my own machine based on 50\nmillion observations. The grouped operations that are applied are just a\nsum and length on a vector. As this takes several seconds to do even\nonce, I only do it one time.\n\n\nset.seed(123)\nn = 5e7\nk = 5e5\nx = runif(n)\ngrp = sample(k, n, TRUE)\n\ntiming_group_by_big = list()\n\n\n# dplyr\ntiming_group_by_big[[\"dplyr\"]] = system.time({\n    df = tibble(x, grp)\n    r.dplyr = summarise(group_by(df, grp), sum(x), n())\n})\n\n# dtplyr\ntiming_group_by_big[[\"dtplyr\"]] = system.time({\n    df = lazy_dt(tibble(x, grp))\n    r.dtplyr = df %>% group_by(grp) %>% summarise(sum(x), n()) %>% collect()\n})\n\n# tidyfast\ntiming_group_by_big[[\"tidyfast\"]] = system.time({\n    dt = setnames(setDT(list(x, grp)), c(\"x\",\"grp\"))\n    r.tidyfast = dt_count(dt, grp)\n})\n\n# data.table\ntiming_group_by_big[[\"data.table\"]] = system.time({\n    dt = setnames(setDT(list(x, grp)), c(\"x\",\"grp\"))\n    r.data.table = dt[, .(sum(x), .N), grp]\n})\n\n# collapse\ntiming_group_by_big[[\"collapse\"]] = system.time({\n     df = tibble(x, grp)\n    r.data.table = fsummarise(fgroup_by(df, grp), x = fsum(x),  n = fnobs(x))\n})\n\ntiming_group_by_big = timing_group_by_big %>% \n  do.call(rbind, .) %>% \n  data.frame() %>% \n  rownames_to_column('package')\n\n\n\n\n\n\n\n\npackage\n      elapsed\n    dplyr\n7.03dtplyr\n1.30collapse\n1.02data.table\n0.67tidyfast\n0.47\n\nWe can see that all options are notable improvements on dplyr. tidyfast is a\nlittle optimistic, as it can count but does not appear to do a summary\noperation like means or sums.\nCount\nTo make things more evenly matched, we’ll just do a simple grouped\ncount. In the following, I add a different option for dplyr if all we want are group sizes. In addition,\nyou have to ‘collect’ the data for a dtplyr\nobject, otherwise the resulting object is not actually a usable tibble,\nand we don’t want to count the timing until it actually performs the\noperation. You can do this with the collect function or\nas_tibble.\n\n\ndata(flights, package = 'nycflights13')\nhead(flights)\n\nflights_dtp = lazy_dt(flights)\n\nflights_dt = data.table(flights)\n\nbm_count_flights = bench::mark(\n  dplyr_base = count(flights, arr_time),\n  dtplyr     = collect(count(flights_dt, arr_time)),\n  tidyfast   = dt_count(flights_dt, arr_time),\n  data.table = flights_dt[, .(n = .N), by = arr_time],\n  iterations = 100,\n  check = FALSE\n)\n\n\n\n\n\n\nHere are the results. It’s important to note the memory as well as\nthe time. The faster functions here are taking a bit more memory to do\nit. If dealing with very large data this could be more important if\noperations timings aren’t too different.\n\n\nexpression\n      min\n      median\n      mem_alloc\n      median_relative\n      mem_relative\n    data.table\n2ms\n 2.4ms\n9.07MB\n1\n1.5tidyfast\n2ms\n 3.3ms\n9.05MB\n1.38\n1.49dplyr_gs\n3.7ms\n 4ms\n6.06MB\n1.65\n1dtplyr\n3.5ms\n 4.2ms\n9.06MB\n1.73\n1.5dplyr_base\n9.3ms\n10.5ms\n6.12MB\n4.31\n1.01\n\nJust for giggles I did the same in Python with a pandas DataFrame, and depending\non how you go about it you could be notably slower than all these\nmethods, or less than half the standard dplyr\napproach. Unfortunately I can’t reproduce it here3,\nbut I did run it on the same machine using a\ndf.groupby().size() approach to create the same type of\ndata frame. Things get worse as you move to something not as simple,\nlike summarizing with a custom function, even if that custom function is\nstill simple arithmetic.\nA lot of folks that use Python primarily still think R is slow, but\nthat is mostly just a sign that they don’t know how to effectively\nprogram with R for data science. I know folks who use Python more, but\nalso use tidyverse, and I use R more but also use pandas quite a bit.\nIt’s not really a debate - tidyverse is easier, less verbose, and\ngenerally faster relative to pandas, especially for more complicated\noperations. If you start using tools like data.table, then there is really no comparison for\nspeed and efficiency.\n\n\n\n\nimport pandas as pd\n\n\n# flights = r.flights\nflights = pd.read_parquet('~/Repos/m-clark.github.io/data/flights.parquet')\n\nflights.groupby(\"arr_time\", as_index=False).size()\n\ndef test(): \n  flights.groupby(\"arr_time\", as_index=False).arr_time.count()\n \ntest()\n\n\nimport timeit\n\ntimeit.timeit() # see documentation\n\ntest_result = timeit.timeit(stmt=\"test()\", setup=\"from __main__ import test\", number = 100)\n\n# default result is in seconds for the total number of 100 runs\ntest_result/100*1000  # ms per run \n\nSummary\nProgramming is a challenge, and programming in a computationally\nefficient manner is even harder. Depending on your situation, you may\nneed to switch tools or just write your own to come up with the best\nsolution.\n\ndata.table\nmodifies in place, so it technically it doesn’t have anything to fill\nafter the first run. As a comparison, I created new columns as the\nfilled in values, and this made almost no speed/memory difference. I\nalso tried copy(dt_missing)[...], which had a minor speed\nhit. I also tried using setkey first but that made no\ndifference. Note also that data.table has\nsetnafill, but this apparently has no grouping argument, so\nis not demonstrated.↩︎\nAs of this writing, I’m new to the\ncollapse package, and so might be missing\nother uses that might be more efficient.↩︎\nThis is because reticulate\nstill has issues with M1 out of the box, and even then getting it to\nwork can be a pain.↩︎\n",
    "preview": "posts/2022-07-25-programming/../../img/198R_small.png",
    "last_modified": "2022-07-27T06:29:21-04:00",
    "input_file": {},
    "preview_width": 256,
    "preview_height": 250
  },
  {
    "path": "posts/2022-04-01-more-dl-for-tabular/",
    "title": "Deep Learning for Tabular Data",
    "description": "A continuing exploration",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2022-05-01",
    "categories": [
      "deep learning",
      "machine learning"
    ],
    "contents": "\n\nContents\nIntroduction\nTLDR: the meta-analysis\nOn\nEmbeddings for Numerical Features in Tabular Deep Learning\nOverview\nData\nModels Explored\nQuick Summary\n\nSAINT:\nImproved neural networks for tabular data via row attention and\ncontrastive pre-training\nOverview\nData\nModels Explored\nQuick Summary\n\nSelf-Attention\nBetween Datapoints: Going Beyond Individual Input-Output Pairs in Deep\nLearning\nOverview\nData\nModels Explored\nQuick Summary\n\nConclusion\nGuidelines for future\nresearch\n\nIntroduction\nIn a previous post, I offered a summary of several articles\nthat came out over the summer of 2021 regarding the application of deep\nlearning (DL) methods to tabular data. DL has shown astounding success\nin the natural language processing, computer vision, and other fields,\nbut when it comes to the sorts of data common in other situations,\nespecially where data is usually smaller and of mixed source and type\n(e.g. demographic, social science, biological data), results were mostly\nunimpressive for complex DL architectures. In particular, it did not\nappear that DL methods could consistently compete with, much less\nconsistently beat, common machine learning (ML) approaches such as\ngradient boosting (e.g. XGBoost). Here I provide a bit of an update, as\nanother few articles have come along continuing the fight.\nTLDR: the meta-analysis\n\n\n\nI collected most of the results from the summarized articles here and\nthose covered in the previous post to see if we come to any general\nconclusions about which methods are best or work best in certain\nsettings. In the following tables, I excluded those I knew to be image\ndata, as well as datasets where I thought results were indistinguishable\nacross all models tested (e.g. less than 1% difference in accuracy).\nThis left comparisons for 92 datasets across six articles. However, it’s\nimportant to note that these were not independent datasets or studies.\nFor example, Gorishniy et al. are the source of two papers and\nessentially the same testing situations, and other datasets were common\nacross papers (e.g. Higgs Boson). In the rare situations there was a\ntie, I gave the nod to boosting methods as a. the whole point is to do\nbetter than those, b. they are the easier model to implement, and\nc. they are not always given the same advantages in these studies\n(e.g. pre-processing).\nFeature Type\nThe following shows results by feature type.\nHeterogeneous: at least 10% of categorical or numeric data\nwith the rest of the other\nMinimal combo: means any feature inclusion of a different\ntype. In the second table I collapse to ‘any heterogeneous’.\nBoost: Any boosting method (most of the time it’s XGBoost\nbut could include lightGBM or other variant)\nMLP: multilayer perceptron or some variant\nDL_complex: A DL method more complex than MLP and which is\ntypically the focus of the paper\nThe results suggest that current DL approaches’ strength is mostly\nwith purely numeric data, and for heterogeneous data, simpler MLP or\nBoosting will generally prevail. I initially thought that boosting would\ndo even better with heterogeneous data, and I still suspect that with\nmore heterogeneous data and on more equal footing, results would tilt\neven more.\n\n\nwinner_model_type\n      All Cat\n      All Num\n      Heterogeneous\n      Min. Combo\n    Boost\n2\n10\n14\n6MLP\n2\n4\n9\n11DL_complex\n0\n22\n7\n5\n\nwinner_model_type\n      All Cat\n      All Num\n      Any Combo\n    Boost\n2\n10\n20MLP\n2\n4\n20DL_complex\n0\n22\n12\n\nSample/Feature Set Size\nThe following suggests that complex DL methods are going to require a\nlot of data to perform better. This isn’t that surprising but the\ndifference here is quite dramatic. Interestingly, MLP methods worked\nwell for fewer features. N total in this case means total size reported\n(not just training).\n\n\nwinner_model_type\n      N features\n      N total\n    Boost\n209\n133,309DL_complex\n207\n530,976MLP\n114\n114,164\n\nTarget Type\nIn the following we compare binary (bin), multiclass (mc), and\nnumeric (num) target results1, but there’s no strong\nconclusion for this. The main thing to glean from this is that these\npapers do not test numeric targets nearly enough. Across dozens of\ndisciplines and countless datasets that I’ve come across in various\nsettings, if anything, this ratio should be reversed.\n\n\nwinner_model_type\n      bin\n      mc\n      num\n    Boost\n17\n10\n5DL_complex\n17\n11\n6MLP\n10\n14\n2\n\nCombinations\nIn the following I look at any heterogeneous, smaller data (N <\n200,000). A complex DL model will likely not do great in this\nsetting.\n\n\nwinner_model_type\n      n\n    Boost\n19DL_complex\n8MLP\n19\n\nNow, on to the details of some of the recent results that were\nincluded.\nOn\nEmbeddings for Numerical Features in Tabular Deep Learning\nAuthors: Gorishniy, Rubachev, & Babenko\nYear: 2022\nArxiv Link\nOverview\nYura\nGorishniy, Rubachev, and Babenko (2022) pit several architectures against\none another, such as standard multilayer perceptron (MLP), ResNet, and\ntheir own transformer approach (see Yuri Gorishniy et al. (2021)).\nTheir previous work, which was summarized in my earlier post, was\nfocused on the architecture, while here they focus on embedding\napproaches. The primary idea is to take the value of some feature and\nexpand it to some embedding space, then use the embedding in lieu of the\nraw feature. It can essentially be seen as a pre-processing task.\nOne approach they use is piecewise linear encoding (PLE),\nwhich they at one point describe as ‘a continuous alternative to the\none-hot encoding’2. Another embedding they use is\nbasically a fourier transform.\nData\n12 public datasets mostly from previous works on tabular DL and\nKaggle competitions.\nSizes were from ~10K to >1M.\nTarget variables were binary, multiclass, or numeric.\nThe number of features ranged from 8 to 200.\n9 of 12 data sets had only numeric features, two had a single\ncategorical feature, and unfortunately, only one of these might be\ncalled truly heterogeneous, i.e., with a notable mix of\ncategorical and numeric features3.\nModels Explored\nCatBoost\nXGBoost\nMLP, MLP*\nResNet, ResNet*\nTransformer*\n* Using proposed embeddings\nQuick Summary\nA mix of results with no clear/obvious winners (results are less\ndistinguishable if one keeps to the actual precision of the performance\nmetrics, and even less so if talking about statistical differences in\nperformance).\nSeveral datasets showed no practical difference across any model\n(e.g. all accuracy results within ~.01 of each other).\n\nEmbedding-based approaches generally tend to improve over their\nnon-embedding counter parts (e.g. MLP + embedding > MLP), this was\npossibly the clearest result of the paper.\nI’m not sure we could say the same for ResNet, where results were\nsimilar with or without embedding\nXGBoost was best on the one truly heterogeneous dataset.\nIn general this was an interesting paper, and I liked the simple\nembedding approaches used. It was nice to see that they may be useful in\nsome contexts. The fourier transform is something that analysts\n(including our team at Strong) have used\nin boosting, so I’m a bit curious why they don’t do Boosting +\nembeddings for comparison for that or both embedding types. These\nembeddings can be seen as a pre-processing step, so nothing would keep\nsomeone from using them for any model.\nAnother interesting aspect was how little difference there was in\nmodel performance. It seemed half the datasets showed extremely small\ndifferences between any model type.\nSAINT:\nImproved neural networks for tabular data via row attention and\ncontrastive pre-training\nAuthors: Somepalli, Goldblum, Schwarzschild, Bayan-Bruss,\n& Goldstein\nYear: 2021\nArxiv Link\nOverview\nThis paper applies BERT-style attention over rows and columns, along\nwith embedding/data augmentation. They distinguish the standard\nattention over features, with intersample attention of rows. In\naddition, they use CutMix for data augmentation (originally\ndevised for images), which basically combines pairs of observations to\ncreate a new observation4. Their model is called\nSAINT, the Self-Attention and Intersample Attention\nTransformer.\nData\n16 data sets\nAll classification, 2 multiclass\n6 are heterogeneous, 2 notably so\nSizes 200 to almost 500K\nModels Explored\nLogistic Regression (!)\nRandom Forest\nBoosting\nCatBoost\nXGBoost\nLightGBM\n\nMLP\nTabNet\nVIME\nTabTransformer\nSAINT\nQuick Summary\nIt seems the SAINT does quite well on some of the data, and\naverage AUROC across all datasets is higher than XGB.\nMain table shows only 9 datasets though, which they call\n‘representative’ but it’s not clear what that means when you only have\n16 to start. One dataset showed near perfect classification for all\nmodels so will not be considered. Of the 15 total remaining:\nSAINT wins 10 (including 3 heterogeneous)\nBoosting wins 5 (including 2 heterogeneous)\n\nSAINT benefits from data augmentation. This could have\nbeen applied to any of the other models, but doesn’t appear to have been\ndone.\nAt least they also used some form of logistic regression as a\nbaseline, though I couldn’t find details on its implementation\n(e.g. regularization, including interactions). I don’t think this sort\nof simple baseline is utilized enough.\nThis is an interesting result, but somewhat dampened by lack of\nincluding numeric targets and more heterogeneous data. The authors\ninclude small data settings which is great, and are careful to not\ngeneralize despite some good results, which I can appreciate.\nI really like the fact they also compare a simple logistic regression\nto these models, because if you’re not able to perform notably better\nrelative to the simplest model one could do, then why would we care? The\nfact that logistic regression is at times competitive and even beats\nboosting/SAINT methods occasionally gives me pause though. Perhaps some\nof these data are not sufficiently complex to be useful in\ndistinguishing these methods? It is realistic though. While it’s best\nnot to assume as such, sometimes a linear model is appropriate given the\nfeatures and target at hand.\nSelf-Attention\nBetween Datapoints: Going Beyond Individual Input-Output Pairs in Deep\nLearning\nAuthors: Kossen, Band, Lyle, Gomez, Rainforth, &\nGal\nYear: 2021\nArxiv Link\nOverview\nThis paper introduces Non-Parametric Transformers, which\nfocus on holistic processing of multiple inputs, and attempts to\nconsider an entire dataset as input as opposed to a single row. Their\nmodel attempts to learn relations between data points to aid prediction.\nThey use a mask to identify prediction points from the non-masked data,\ni.e. the entire \\(X_{\\textrm{not\nmasked}}\\text{ }\\) data used to predict \\(X_{\\textrm{masked}}\\text{ }\\). The X matrix\nactually includes the target (also masked vs. not). At prediction, the\nmodel is able to make use of the correlations of inputs of training to\nultimately make a prediction.\nData\n10 datasets from UCI, 2 are image (CIFAR MNIST)\n4 binary, 2 multiclass, 4 numeric targets\nModels Explored\nNPT\nBoosting\nGB\nXGB\nCatBoost\nLightGBM\n\nRandom Forest\nTabNet\nKnn\nQuick Summary\nGood performance of these models, but not too different from best\nboosting model for any type of data.\nNPT best on binary classification, but similar to CatBoost\nSame as XGB and similar to MLP on multiclass\nBoosting slightly better on numeric targets, but NPT similar\n\nAs seen several times now, TabNet continues to underperform\nk-nn regression worst (not surprising)\nWhen I first read the abstract where they say “We challenge a common\nassumption underlying most supervised deep learning: that a model makes\na prediction depending only on its parameters and the features of a\nsingle input.”, I immediately was like ‘What about this, that, and\nthose?’. The key phrase was ‘deep learning’, because the authors note\nlater that this has a very long history in the statistical modeling\nrealm. I was glad to see in their background of the research that they\nexplicitly noted the models that came to my mind, like gaussian\nprocesses, kernel regression, etc. Beyond that, many are familiar with\ntechniques like knn-regression and predictive mean matching, so it’s\ndefinitely not new to consider more than a single data point for\nprediction. I thought it was good of them to add k-nn regression to the\nmodel mix, even though it was not going to do well compared to the other\napproaches.\nThough the author’s acknowledge a clear thread/history here, I’m not\nsure this result is the fundamental shift they claim, versus a further\nextension/expansion into the DL domain. Even techniques that may work on\na single input at a time may ultimately be taking advantage of\ncorrelations among the inputs (e.g. spatial correlations in images).\nAlso, automatic learning of feature interactions is standard even in\nbasic regularized regression settings, but here their focus is on\nobservation interactions (but see k-nn regression).\nConclusion\nIn the two reviews on DL for tabular data that I’ve done, it appears\nthere is more work in store for DL methods applied to tabular data.\nWhile it’d be nice to have any technique that would substantially\nimprove prediction for such settings, I do have a suspicion results are\nlikely rosier than they are, since that is just about the case for any\nnewly touted technique, and at least in some cases, I don’t think we’re\neven making apple to apple comparisons.\nThat said, I do feel like some ground has been made for DL\napplications for tabular data, in that architectures can now more\nconsistently performing as well as boosting methods in certain settings,\nespecially if we include MLP. In the end though, results don’t appear\nstrong enough to warrant a switch from boosting for truly heterogeneous\ndata, or even tabular data in general. I feel like someday we’ll maybe\nhave a breakthrough, but in the meantime, we can just agree that messy\ndata is hard stuff to model, and the best tool is whichever one works\nfor your specific situation.\nGuidelines for future\nresearch\nI was thinking about what would be a convincing result, the type of\nsetting and setup where if a DL technique was consistently performing\nstatistically better than boosting methods, I’d be impressed. So I’ve\nmade a list of things I’d like to see more of, and which would make for\na better story if the DL method were to beat out other techniques.\nAlways use heterogeneous data. For giggles let’s say 20%+ of the\nminority feature type.\nFeatures should at least be minimally correlated, if not notably\nso.\nImage data results are not interesting (why would we use boosting\non this in practice?).\nNumeric targets should at least be as much of focus as\ncategorical targets.\nInclude ‘small’ datasets.\nInclude very structured data (e.g. clustered with repeated\nobservations, geographical points, time series).\nUse a flexible generalized additive or similar penalized\nregression with interactions as a baseline statistical model.\nMaybe add survival targets to the mix.\nIf using a pre-processing step that is done outside of modeling,\nthis likely should be applied to non-DL methods for better comparison,\nespecially, if we’re only considering predictive accuracy and don’t care\ntoo much about interpretation.\nNote your model variants before analyzing any\ndata. Tweaking/torturing model architecture after results don’t pan out\nis akin to p-hacking in the statistical realm, and likewise wastes both\nresearcher and reader’s time.\nRegarding results…\nDon’t claim differences that you don’t have precision to do so, or\nat least back them up with an actual statistical test.\nIf margin of error in the metrics is overlapping, while\nstatistically they could be different, practically they probably aren’t\nto most readers. Don’t make a big deal about it.\nIt is unlikely anyone will be interested in three decimal place\ndifferences for rmse/acc type metrics, and statistically, results often\ndon’t even support two decimal precision.\nReport how you are obtaining uncertainty in any error\nestimates.\nIf straightforward, try to give an estimate of total tuning/run\ntimes.\n\nWith the datasets\nName datasets exactly how they are named at the source you obtained\nthem from, provide direct links\nProvide a breakdown for both feature and target types\nProvide clear delineation of total/training/validation/test\nsizes\n\n\n\n\nGorishniy, Yura, Ivan Rubachev, and Artem Babenko. 2022. “On\nEmbeddings for Numerical Features in Tabular Deep Learning.”\narXiv Preprint arXiv:2203.05556.\n\n\nGorishniy, Yuri, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n2021. “Revisiting Deep Learning Models for Tabular Data.”\narXiv Preprint arXiv:2106.11959.\n\n\nKadra, Arlind, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021.\n“Regularization Is All You Need: Simple Neural Nets Can Excel on\nTabular Data.” arXiv Preprint arXiv:2106.11189.\n\n\nShwartz-Ziv, Ravid, and Amitai Armon. 2021. “Tabular Data: Deep\nLearning Is Not All You Need.” arXiv Preprint\narXiv:2106.03253.\n\n\nI don’t refer to numeric targets as\n‘regression’ because that’s silly for so many reasons. 😄↩︎\nA quick look suggests it’s not too\ndissimilar from a b-spline.↩︎\nI’ll let you go ahead and make your\nown prediction about which method was best on that data set.↩︎\nIt’s not clear to me how well this\nCutUp approach would actually preserve feature correlations. My gut\ntells me the feature correlations of this approach would be reduced\nrelative to the observed, since the variability of the new observations\nis likely reduced. This ultimately may not matter for predictive\npurposes or their ultimate use in embeddings. However, I wonder if\nsomething like SMOTE, random (bootstrap) sampling, other DL methods like\nautoencoders, or similar approaches might do the same or better.↩︎\n",
    "preview": "posts/2022-04-01-more-dl-for-tabular/../../img/nnet.png",
    "last_modified": "2022-05-02T19:08:01-04:00",
    "input_file": {},
    "preview_width": 1289,
    "preview_height": 1076
  },
  {
    "path": "posts/2021-10-30-double-descent/",
    "title": "Double Descent",
    "description": "Rethinking what we thought we knew.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2021-11-13",
    "categories": [
      "deep learning",
      "machine learning"
    ],
    "contents": "\n\nContents\nWhat is double descent?\nBias-variance trade-off\nDouble Descent\n\nAn example\np ≤ N\np > N\n\nWhy does this happen?\nConclusion\n\nA co-worker passed along a recent article (Dar, Muthukumar, and Baraniuk 2021) on the topic of double descent in machine learning. I figured I’d summarize some key points I came across while perusing it and some referenced articles. In addition, I’ll provide an accessible example demonstrating the phenomenon.\nWhat is double descent?\nBias-variance trade-off\nTo understand double descent you have to revisit the concept of the bias-variance trade-off. Without going into too much detail, the main idea with it is that having an overly complex model leads to overfitting the training data, which results in worse prediction on new data, at least relative to what simpler models would have done. The classic figure looks like the following, where blue is the training error and the red is the test error. Thin lines represent one path of complexity (e.g. across a random sample of the data), while the thicker lines are the average at a particular point of model complexity.\n\nIf we don’t have a sufficiently complex model, both training and test error will be poor, the case of underfitting. Our model is a poor approximation of the true underlying function, and predicts poorly on data both seen and unseen. When we have too much model complexity relative to the size of our data (e.g. more covariates, nonlinear effects, interactions, etc.), we pass into the overfit situation. Essentially, while our model function would result in a decrease in error with the data it’s trained on (lower bias as it better approximates the true underlying function), with too much complexity, you’d also eventually have notable changes in prediction (high variance) with any slight deviation in the underlying training data. We can even get to the point where we fit the training data perfectly, but it will be overly susceptible to the noise in the data, and not do well with unseen observations.\nTo combat this, we usually attempt to find a balance between overly simple and overly complex models. This would be the point where test error is among its lowest point for a desirable level of complexity (e.g. around 20-25 df in the figure above), before it begins to rise again. This may be accomplished more explicitly, for example, picking a model through cross-validation, or more implicitly, for example, through regularization (Belkin et al. (2019)). For more detail on the bias-variance trade-off, you can look at the exposition in the main article noted above, my document here, or any number of places, as it is an extremely well-known idea in machine learning.\nDouble Descent\nThe funny thing is, it turns out that the above actually only applies to a specific scenario, one which we will call underparameterized models. We can simplify this notion by just thinking of the case where the number of our parameters to estimate is less than or equal to the number of observations we have to work with. Nowadays though, it’s not uncommon to have what we’d call overparameterized models, such as random forests and neural networks, sometimes with even billions of parameters, far exceeding the data size. In this scenario, when we revisit the trade-off, something unusual happens!\nFigure from Dar, Muthukumar, and Baraniuk (2021)Such models may have near zero training error, yet do well on unseen data. As we increase complexity, we see something like a second bias-variance trade-off beyond the point where the data is perfectly fit (interpolated). This point is where model complexity (e.g. in terms of number of parameters) p equals the number of observations N, and this is where the realm of the overparameterized models begins. Now test error begins to drop again with increasing complexity.\nAn example\nI thought it would be amusing to try this with the classic mtcars data set available in base R. With this data, our goal will be to predict fuel consumption in miles per gallon (mpg). First we will split the data into training and test components. We create a model where our number of parameters (p), in this case standard regression coefficients, will equal the number of observations (N). Some of the more technically savvy will know that if the number of features and/or parameters to estimate p equals the number of observations N, a standard linear regression model will fit the data perfectly1, demonstrated below.\n\nIf not familiar, the mtcars object is a data frame that comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n\n\nnc = ncol(mtcars) \nnr = nc\nfit_perfect = lm(mpg ~ ., mtcars[1:nr, ])\n# summary(fit_perfect) # not shown, all inferential estimates are NaN\n\n\n\n\n\n\nNow let’s look at the test error, our prediction on the unseen data we didn’t use in fitting the model. When we do, we see the usual bias-variance trade-off. Our generalizability capabilities have plummeted, as we have overfit the training data and were unable to accommodate unseen observations. We are even predicting negative mpg in some cases!\n\n\n\np ≤ N\nLet’s extend the demonstration more fully. We now create models of increasing complexity, starting with an intercept only model (i.e. just using the mean for prediction), to one where all other columns (10) in the data are predictors. Here I repeatedly sampled mtcars of size \\(N = 10\\) for training, the remainder for test, and also shuffled the columns each time, doing so for a total of 250 times2. Here is the result- the classic bias variance trade-off curve. The larger dot shows the test error minimum, at about 3 covariates (plus intercept). The vertical line denotes our point of interpolation.\nDouble Descent in the underparameterized setting.p > N\nSo with one of the simpler data sets around we were able to demonstrate the bias-variance trade-off clearly. But now let’s try overparameterized models! We don’t need anything fancy or complicated to do this, so for our purposes, I’m just going to add cubic spline basis expansions for the wt, disp, and hp features3. This will definitely be enough to put us in a situation where we have more parameters than data, i.e. p > N, but doesn’t make things too abstract4.\nThe basic linear model approach we might typically use fails to estimate the additional parameters in this situation, so we need a different estimator. Some are familiar with penalized regression techniques such as lasso and ridge regression, and we could use those here. However, I’ll use ridgeless regression, as depicted in Hastie et al. (2019), and which, like ridge regression, is a straightforward variant of the usual least squares regression5. I estimate the coefficients/weights on the training data, and make predictions for the training and test set, calculating their respective errors. Here is an example of the primary function used.\n\n\nShow code\n\nfit_ridgeless = function(X_train, y, X_test, y_test){\n  # get the coefficient estimates\n  b = pseudo_inv(crossprod(X_train)) %*% crossprod(X_train, y)\n  \n  # get training/test predictions\n  predictions_train = X_train %*% b\n  predictions_test  = X_test %*% b\n  \n  # get training/test error\n  rmse_train = sqrt(mean((y - predictions_train[,1])^2))\n  rmse_test  = sqrt(mean((y_test - predictions_test[,1])^2))\n  \n  # return result\n  list(\n    b = b,\n    predictions_train = predictions_train,\n    predictions_test  = predictions_test,\n    rmse_train = rmse_train,\n    rmse_test  = rmse_test\n  )\n}\n\n\n\nWe can test the function as follows with as little as 10 observations, where p (all predictor coefficients plus intercept = 11 parameters) is greater than N (10). This demonstrates that the ridgeless approach can provide an estimate for all the parameters (unlike the standard lm function), and we also see very low training error, but relatively high test error (in terms of the root mean square error.)\n\n\nShow code\n\nn = 10\n\nX = as.matrix(cbind(1, mtcars[, -1]))\ny = mtcars$mpg # mpg is the first column\n\nX_train = X[1:n, ]\ny_train = mtcars$mpg[1:n]\nX_test  = X[-(1:n),]\ny_test  = y[-(1:n)]\n\nresult = fit_ridgeless(X_train, y_train, X_test, y_test)\n\n\n\n\n\nb\n    0.84−1.690.08−0.082.76−1.290.242.323.262.260.66\n\nrmse_train\n      rmse_test\n    0.05\n5.79\n\nIf we do this for more complex models (max linear features, plus each additional set of features associated with a cubic spline basis expansions), we obtain the following. Now we see the second descent in test error takes form!\nDouble Descent in the overparameterized setting.Putting our results together gives us the double descent curve.\nDouble Descent in the overparameterized setting.\nNote that this all holds for the most part with classification problems, including multiclass (or multivariate/class targets).\nWe not only see the double descent pattern, but we can also note that the global test error minimum occurs with the model with the most parameters. The gray dot is the lowest test error with the underparameterized settings, while the dark red is the global test error minimum.\nWhy does this happen?\nUnderstanding the double descent phenomenon is an area of active research, and there are some technical issues we won’t cover here. However, we can note a couple things more broadly. When we’re in the underparameterized situation, we ultimately begin to force features that have no association with the target to fit the data anyway. Once you move beyond the point of where these features are useful, test error begins to rise again, until the point of interpolation where test error is even worse than guessing (or just guessing in the classification case).\nBeyond the interpolation point, all models we potentially employ using this estimation technique will have the capacity to fit the training data perfectly, i.e. zero bias. This allows us to fit the remaining noise in the data with the additional features employed by the more complex models. There is no guarantee that among the models you fit that the lowest test error will be found relative to the underparameterized setting. However, the lowest test error to be found is ‘out there’ somewhere6. So adding complexity will potentially allow you to find improved test error.\nAnother way to put it is that we have a single class of models to consider, and under and overparameterized are special cases of that more general class. Any one of these might result in the lowest test error. The overparameterized models, which may contain complex nonlinearities and interactions, are likely to be more compatible with the data than the simpler models7. So odds are good that at least one of them will have a smaller test error as well. In any case, restricting ourselves to the underparameterized setting is definitely no guarantee that we will find the most performant model.\nOne caveat is that the model we used is an example of ‘implicit’ regularization, one in which there is no hyper-parameter to set (or discover through cross-validation), like with ridge and lasso. With other techniques (e.g. optimally chosen ridge regression estimator) we may still be able to achieve optimal test error without complete interpolation, and show a reduced peak.\nDar, Muthukumar, and Baraniuk (2021) note that in the overparameterized setting, we can distinguish the signal part of the error term that reduces as a function of N/p, where the noise part of the error term is a function of p/N. In addition, there is a portion of test error related to model misspecification, which will always decrease with overparameterization. In addition, one must consider both feature correlations as well as correlations among observations. Having more complex covariance structure doesn’t negate the double descent phenomenon, but they suggest that, for example, cases where there is low effective dimension within these additional features will more readily display the double descent.\nAnother issue is that in any given situation it is difficult to know where in the realm of available models we exist presently. So additional complexity, or even additional data, may in fact hurt performance (Nakkiran et al. 2019).\nConclusion\nThe double descent phenomenon is a quite surprising scenario, especially for those who have only heard of the classical bias-variance trade off. There is still much to learn regarding it, but such research is off and running. For practical purposes, it is worth keeping it in mind to aid us in model selection and thinking about our modeling strategies in general.\n\n\n\n\n\n\n\nBelkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019. “Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off.” Proceedings of the National Academy of Sciences 116 (32): 15849–54.\n\n\nDar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. “A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.” arXiv Preprint arXiv:2109.02355.\n\n\nHastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. 2019. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation.” arXiv Preprint arXiv:1903.08560.\n\n\nNakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2019. “Deep Double Descent: Where Bigger Models and More Data Hurt.” arXiv Preprint arXiv:1912.02292.\n\n\nR2 = 1 in the standard linear model setting.↩︎\nNote that the intercept term is added after data shuffling so when p = 1 it is the intercept only model, i.e. guessing the mean.↩︎\nI used mgcv to so this, then added them in whole for each term to the previously shuffled model matrix. These columns are not shuffled. By default these will add 10 columns each to the model matrix.↩︎\nFor more on generalized additive models, see my document.↩︎\nRidgeless regression has the same form as the ‘normal’ equations for least squares, but instead of \\(\\beta \\sim (X^TX)^{-1} \\cdot X^Ty\\), we have \\(\\beta \\sim (X^TX)^{+} \\cdot X^Ty\\) where the first part is the pseudo-inverse of \\(X\\). It is similar to equations for ridge regression (see my demo here) and can be seen as an approximation to it as the ridge penalty tends toward zero.↩︎\nFox Mulder told me so.↩︎\nBecause nature is just funny that way.↩︎\n",
    "preview": "posts/2021-10-30-double-descent/../../img/double-descent/dd_mtcars.png",
    "last_modified": "2021-12-19T12:16:34-05:00",
    "input_file": {},
    "preview_width": 3229,
    "preview_height": 1929
  },
  {
    "path": "posts/2021-07-15-dl-for-tabular/",
    "title": "This is definitely not all you need",
    "description": "A summary of findings regarding deep learning for tabular data.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2021-07-19",
    "categories": [
      "deep learning",
      "machine learning"
    ],
    "contents": "\n\nContents\nMotivation\nGoal\nCaveats\nQuick Take\nTabular\nData: Deep Learning is Not All You Need\nPaper Info\nFrom the Abstract\nOverview\nData\nModels Explored\nQuick Summary\nOther stuff\n\nRegularization\nis all you Need: Simple Neural Nets can Excel on Tabular Data\nPaper Info\nFrom the Abstract\nOverview\nData\nModels Explored\nQuick Summary\nOther Stuff\n\nRevisiting Deep\nLearning Models for Tabular Data\nPaper Info\nFrom the Abstract\nOverview\nData\nModels Explored\nQuick Summary\nOther Stuff\n\nOverall Assessment\n\nMotivation\nI’ve been a little perplexed at the lack of attention of deep\nlearning (DL) toward what I consider to be ‘default’ data in my world,\noften referred to as tabular data, where typically we have a\ntwo dimensional input of observations (rows) and features (columns) and\ninputs are of varying type, scale and source. Despite the ubiquity of\nsuch data in data science generally, and despite momentous advances in\nareas like computer vision and natural language processing, at this\ntime, it’s not very clear what the status of DL for tabular data is.\nThere have been developments in the area recently though, with some\nmodeling approaches, such as TabNet, gaining traction. In June of 2021,\nI actually came across three papers on Arxiv\nthat were making related claims about the efficacy of DL for tabular\ndata. As in many academic and practical (and life) pursuits, results of\nthese studies are nuanced, so I thought I’d help myself and others by\nsummarizing here.\nGoal\nI want to know if, e.g. time and/or resources are limited, whether it\nwill be worth diving into a DL model if I have a satisfactory\nsimpler/easier one ready to implement that does pretty well. Perhaps\nthis answer is already, ‘if it ain’t broke, don’t fix it’, but given the\nadvancements in other data domains, it would be good to assess what the\ncurrent state of DL with tabular data is.\nCaveats\nI’m not going to do more than give a cursory summary of the\narticles, and provide no in-depth explanation of the models. For more\ndetail, see the corresponding articles and references for the models\ntherein. You are not going to learn how to use TabNet, NODE,\ntransformers, etc., for tabular data.\nThere are other decent articles on the topic not covered here. Some\nare referenced in these more recent offerings, so feel free to\nperuse.\nQuick Take\nIn case you don’t want any detail, here’s a quick summary based on my\nimpressions from these articles. Right now, if you want to use DL on\ntabular data, don’t make a fuss of it. A simple architecture, even a\nstandard multi-layer perceptron, will likely do as well as more\ncomplicated ones. In general though, the amount of effort put into\nprep/tuning may not be worth it for many typical tabular data settings,\nfor example, relative to a suitably flexible statistical model\n(e.g. GAMM) or a default fast boosting implementation like XGBoost.\nHowever, DL models are already thinking ‘big data’, so for very large\ndata situations, a DL model might make a great choice, as others may not\nbe computationally very viable. It also will not be surprising at all\nthat in the near future some big hurdle may be overcome as we saw with\nDL applications in other fields, in which case some form of DL may be\n‘all you need’.\nNow, on to the rest!\nTabular Data:\nDeep Learning is Not All You Need\nPaper Info\nWho: Shwartz-Ziv & Armon\nWhere: Intel\nWhen: 2021-06-21 V1\nArxiv Link\nFrom the Abstract\n\nWe analyze the deep models proposed in four recent papers across\neleven datasets, nine of which were used in these papers, to answer\nthese questions. We show that in most cases, each model performs best on\nthe datasets used in its respective paper but significantly worse on\nother datasets. Moreover, our study shows that XGBoost (Chen and\nGuestrin, 2016) usually outperforms the deep models on these datasets.\nFurthermore, we demonstrate that the hyperparameter search process was\nmuch shorter for XGBoost.\n\nOverview\nFor each model they used the data that was implemented in the\noriginal model papers by the authors (e.g. the dataset used in the\nTabNet article), and also used their suggested parameter settings. They\ntested all the models against their own data, plus the other papers’\ndata, plus two additional data sets that were not used in any of the\noriginal papers.\nData\nThey use eleven total datasets. Nine datasets are those used in the\noriginal papers on TabNet, DNF-Net, and NODE, drawing three datasets\nfrom each paper. Additionally, Shwartz-Ziv & Armon use two Kaggle\ndatasets not used in any of those papers. Sample sizes ranged from 7k to\n1M, 10-2000 features, with two being numeric targets, while the other\ntarget variables ranged from 2-7 classes. Datasets are described in\ndetail in the paper along with links to the source (all publicly\navailable).\nModels Explored\nBrief summaries of the DL models are found in the paper.\nXGBoost\nTabNet\nNeural Oblivious Decision Ensembles (NODE)\nDNF-Net\n1D-CNN\nQuick Summary\nNot counting the ensemble\nmethods…\nTabNet did best on all of its own data sets, but was not the best\nmodel on any other.\nNODE each did best on 2 of its own 3 data sets, but not on any\nother.\nDNF-Net best on one of its own 3 data sets, but not on any\nother.\nXGBoost was best on the remaining 5 datasets.\nCounting the ensemble\nmethods…\nTabNet did best on 2 of its own 3 data sets, but was not the best\nmodel on any other.\nDNF-Net and NODE each did best on one of its own 3 data sets, but\nnot on any other.\nXGBoost was best on one dataset.\nOf those, XGB was notably better on ‘unseen’ data, and comparable to\nthe best performing ensemble. A simple ensemble was also very\nperformant. From the paper:\n\nThe ensemble of all the models was the best model with 2.32% average\nrelative increase, XGBoost was the second best with 3.4%, 1D-CNN had\n7.5%, TabNet had 10.5%, DNF-Net had 11.8% and NODE had 14.2% (see Tables\n2 and 3 in the appendix for full results).\n\nAs a side note, XGBoost + DL was best, but that defeats the purpose\nin my opinion. Presumably any notably more complicated setting will be\npotentially better with enough complexity, but unless there is an\nobvious way on how to add such complexity, it’s mostly an academic\nexercise. However, as the authors note, if search is automated, maybe\nthe complexity of combining the models is less of an issue.\nOther stuff\nKudos\nThe authors cite the No Free Lunch\ntheorem in the second paragraph, something that appears to be lost\non many (most?) of these types of papers touting small increases in\nperformance for some given modeling approach.\nIssues\nThere are always things like training process/settings that are\ndifficult to fully replicate. By the time authors publish any paper,\nunless exact records are kept, the iterations (including discussions\nthat rule out various paths) are largely lost to time. This isn’t a\nknock on this paper, just something to keep in mind.\nOpinion\nI liked this one in general. They start by giving the competing\nmodels their best chance with their own settings and data, which was\nprocessed and trained in the same way. Even then, those models still\neither didn’t perform best, and/or performed relatively poorly on any\nother dataset.\nRegularization\nis all you Need: Simple Neural Nets can Excel on Tabular Data\nPaper Info\nWho: Kadra et al.\nWhere: U of Freiburg, Leibniz U (Germany)\nWhen: 2021-06-06 V1\nArxiv Link\nFrom the Abstract\n\nTabular datasets are the last “unconquered castle” for deep learning…\nIn this paper, we hypothesize that the key to boosting the performance\nof neural networks lies in rethinking the joint and simultaneous\napplication of a large set of modern regularization techniques. As a\nresult, we propose regularizing plain Multilayer Perceptron (MLP)\nnetworks by searching for the optimal combination/cocktail of 13\nregularization techniques for each dataset using a joint optimization\nover the decision on which regularizers to apply and their subsidiary\nhyperparameters.\n\n\nWe empirically assess the impact of these regularization cocktails\nfor MLPs on a large-scale empirical study comprising 40 tabular datasets\nand demonstrate that (i) well-regularized plain MLPs significantly\noutperform recent state-of-the-art specialized neural network\narchitectures, and (ii) they even outperform strong traditional ML\nmethods, such as XGBoost.\n\n\nWe emphasize that some of these publications claim to outperform\nGradient Boosted Decision Trees (GDBT) [1, 37], and other papers\nexplicitly stress that their neural networks do not outperform GBDT on\ntabular datasets [38, 22]. In contrast, we do not propose a new kind of\nneural architecture, but a novel paradigm for learning a combination of\nregularization methods.**\n\nOverview\nThis data is more about exploring regularization techniques\n(e.g. data augmentation, model averaging via dropout) rather than\nsuggesting any particular model is superior. Even in the second\nparagraph they state their results do not suggest a performance gain\nover boosting methods. Their focus is on potentially improving DL for\ntabular data through regularization with two hypotheses:\nRegularization cocktails outperform state-of-the-art deep learning\narchitectures on tabular datasets.\nRegularization cocktails outperform Gradient-Boosted Decision Trees,\nas the most commonly used traditional ML method for tabular data.\nData\nForty total datasets ranging from as little as ~400 observations to\nover 400k, and between 4 and 2000 features. All were categorical\ntargets, with about half binary. All available at openml.org with target\nID provided.\nModels Explored\nComparison models:\nTabNet: (with author’s proposed defaults)\nNODE: (with author’s proposed defaults)\nAutogluon: Tabular: can use other techniques but restricted\nto ensembles of neural nets for this demo\nASK-GBDT: GB via Auto-sklearn (Note this tool comes from\none of the authors )\nXGBoost: Original implementation\nMLP: Multilayer Perceptron - 9 layers with 512 hidden units\neach.\nMLP+D: MLP with Dropout\nMLP+C: MLP with regularization cocktail\nQuick Summary\nTo begin, their regularization cocktail approach is the clear winner\non these datasets, having one outright on over 40% of them (based on\ntable 2).\nStandard XGB performed best (or tied for best) 8 of the 40 data\nsets, while it or ASK-GBDT were best for 12 datasets combined.\nSimple MLP was best once, while MLP with dropout was best 5 times,\nwhile the cocktail method was best in general, across 19 datasets.\nThe ‘fancy’ DL models were the worst performers across the board.\nTabNet never performed best, and NODE only did once, but the latter also\nrepeatedly failed due to memory issues or run-time limitations (this\nmemory issue was mentioned in the previous paper also).\nHead-to-head, the cocktail beat the standard XGB 26 out of 38 times\nwith three ties. So it wins 65% of the time against XGB, 70% against\nASK-GBDT, but 60% against either (i.e. some XGB approach).\n\n\n\n\n\n\nOther Stuff\nKudos\nRecognize that tabular data is understudied in mainstream DL\nliterature\nThey used a lot of datasets\nThey look at the simplest DL models for comparison\nIssues\nI wonder why there was not a single numeric outcome among so many\ndatasets. Furthermore, some of the data are image classification\n(e.g. Fashion-MNIST), so I’m not sure why they’re included. I wouldn’t\nuse a ‘tabular’ technique when standard computer vision approaches\nalready work so well.\nI’m not familiar with the augmentation techniques they mention,\nwhich were devised for image classification, but there have been some\nused for tabular data for a couple decades at this point that were not\nmentioned, including simple upsampling, or imputation methods\n(e.g. SMOTE). That’s not a beef with the article at all, I’ve long\nwondered why people haven’t been using data augmentation for tabular\ndata given it’s success elsewhere (including for tabular\ndata!).\nThey use a standard t-tests of ranks, but if we’re going to use\nthis sort of approach, we’d maybe want to adjust for all the tests done,\nand probably for all pairwise comparisons (they show such a table for\nthe regularization methods). Depending on the approach and cutoff, the\nXGB vs. Cocktail difference may not be significant.\nAlso, I couldn’t duplicate these p-values with R’s default\nsettings for Wilcoxon signed rank tests, and there does in fact seem to\nbe inconsistency between the detailed results and Wilcoxon summaries.\nFor example, in the regularization tests of Table 9,\nCocktail vs. WD and DO shows two\nties in the first four data sets, yet only 1 tie is reported in the\ncomparison chart for both (Figure 4). For the models, Table 2 show 3\nties of XGB & the Cocktail, with 1 for\nASK-G and Cocktail, but 2 and 0 are reported\nfor their Wilcoxon tests. It’s not clear what they did for NODE with all\nthe NAs. I do not believe these discrepancies, nor adjusting for\nmultiple comparisons, will change the results (I re-did those\nmyself).\nOpinion\nIf we ignore the regularization focus and just look at the model\ncomparisons, I’m not overly convinced we have a straightforward victory\nfor cocktail vs. GB as implied in the conclusion. Results appear to be\nin favor of their proposed method, but not enough to be a near-guarantee\nin a particular setting, so we’re back to square one of just using the\neasier/faster/better tool. I’m also not sure who was questioning the use\nof regularization for neural networks or modeling in general, so the\ncomparison to any model without some form of regularization isn’t as\ninteresting to me. What is interesting to me is that we have another\nround of evidence that the fancier DL models like TabNet do not perform\nthat well relative to GB or simpler DL architectures.\nRevisiting\nDeep Learning Models for Tabular Data\nPaper Info\nWho: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,\nArtem Babenko\nWhere: Yandex (Russia)\nWhen: 2021-06-22\nArxiv Link\nSource\ncode\nFrom the Abstract\n\nThe necessity of deep learning for tabular data is still an\nunanswered question addressed by a large number of research efforts. The\nrecent literature on tabular DL proposes several deep architectures\nreported to be superior to traditional “shallow” models like Gradient\nBoosted Decision Trees. However, since existing works often use\ndifferent benchmarks and tuning protocols, it is unclear if the proposed\nmodels universally outperform GBDT. Moreover, the models are often not\ncompared to each other, therefore, it is challenging to identify the\nbest deep model for practitioners.\n\n\nIn this work, we start from a thorough review of the main families of\nDL models recently developed for tabular data. We carefully tune and\nevaluate them on a wide range of datasets and reveal two significant\nfindings. First, we show that the choice between GBDT and DL models\nhighly depends on data and there is still no universally superior\nsolution. Second, we demonstrate that a simple ResNet-like architecture\nis a surprisingly effective baseline, which outperforms most of the\nsophisticated models from the DL literature. Finally, we design a simple\nadaptation of the Transformer architecture for tabular data that becomes\na new strong DL baseline and reduces the gap between GBDT and DL models\non datasets where GBDT dominates.\n\nOverview\nThis paper compares different models on a variety of datasets. They\nare interested in the GB vs. DL debate, but like the previous paper,\nalso interested in how well a simpler DL architecture might perform, and\nwhat steps might help the more complicated ones do better.\nData\nThey have 11 datasets with a mix of binary, multiclass and numeric\ntargets. Sizes range from 20K to 1M+. There appears to be some overlap\nwith the first paper (e.g. Higgs, Cover type).\nModels Explored\n‘Baselines’\nXGBoost\nCatBoost\nMLP\nResNet\nDL Comparisons\nSNN\nNODE\nTabNet\nGrowNet\nDCN V2\nAutoInt\nIn addition, they look at ensembles of these models, but this is not\nof interest to me for this post.\nQuick Summary\nNote that these refer to the ‘single model’ results, not the results\nfor ensembles.\nSome form of boosting performed best on 4 of the 11\ndatasets.\nResNet was best on four classification tasks, but not once for\nnumeric targets.\nAt this point you won’t be surprised at what doesn’t perform as\nwell- TabNet, NODE, and similar. TabNet, DCN, and GrowNet were never the\nbest performer, and the other three were best one time a piece.\nMLP did not perform best on any data, however the authors note\nthat it ‘is often on par or even better than some of the recently\nproposed DL models’.\nThey also looked at models with a ‘simple’ transformer\narchitecture. Their results suggest better performance than the other DL\nmodels, and similar performance to ResNet.\nOther Stuff\nKudos\nSharing the source code!\nRecognizing that results at this point are complex at best given\nthe lack of standard datasets\nIssues\nThey note a distinction between heterogeneous vs. other\ntypes of data. They call data heterogeneous if the predictors are of\nmixed data types (e.g. categorical, numeric, count), while something\nlike pixel data would be homogeneous because all the columns\nare essentially the same type. The latter isn’t as interesting to me for\nthis sort enterprise, and I think the former is what most are thinking\nabout for ‘tabular’ data, otherwise we’d just call it what it is\n(e.g. image or text data), and modeling/estimation is generally quite a\nbit easier when all the data is the same type. I do think it’s important\nthat they point out that GB is better with heterogeneous data, and I\nthink if you only look at such data, you’d likely see GB methods still\noutperforming or at worst on par with the best DL methods.\nOpinion\nThese results seem consistent with others at this point. Complex DL\nisn’t helping, and simpler architectures, even standard MLP show good\nperformance. In the end, we still don’t have any clear winner over GB\nmethods.\nOverall Assessment\nThese papers put together are helpful in painting a picture of where\nwe are at present with deep learning for tabular data, especially with\nmixed data types. In this setting, it seems that more complicated DL\nmodels do not seem to have any obvious gain over simpler architectures,\nwhich themselves do not consistently beat boosting methods. It may also\nbe the case that for data of mixed data types/sources, boosting is still\nthe standard to beat.\nEven though these articles are geared toward comparisons to\nGB/XGBoost, in several settings I’ve applied them, I typically do not\nnecessarily have appreciably greater success compared to a default\nsetting random forest (e.g. from the ranger package in R), or sufficiently flexible\nstatistical model. Unfortunately this comparison is lacking from the\npapers, and would have been nice to have, especially for smaller data\nsettings where such models are still very viable. I think a viable fast\nmodel, preferably one without any tuning required (or which simply is\ntaken off the shelf) should be the baseline.\nIn that light, for tabular data I think one should maybe start with a\nbaseline of a penalized regression with appropriate interactions\n(e.g. ridge/lasso), or a more flexible penalized approach (GAMM) as a\nbaseline, the latter especially, as it can at least automatically\nincorporate nonlinear relationships, and tools like mgcv or gpboost in R\ncan do so with very large data (1 million +) in a matter of seconds. In\nsettings of relatively higher dimensions, interactions and\nnonlinearities should be prevalent enough such that basis function,\ntree, and DL models should be superior. Whether they are practically so\nis the key concern even in those settings. With smaller, noisier data of\nless dimension, I suspect the tuning/time effort with present day DL\nmodels for tabular data will likely not be worth it. This may change\nvery soon however, so such an assumption should be regularly\nchecked.\n\nlast updated: 2022-05-02\nNeural Net image source from UC\nBusiness Analytics R Programming Guide\n\n\n\nGorishniy, Yuri, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n2021. “Revisiting Deep Learning Models for Tabular Data.”\narXiv Preprint arXiv:2106.11959.\n\n\nKadra, Arlind, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021.\n“Regularization Is All You Need: Simple Neural Nets Can Excel on\nTabular Data.” arXiv Preprint arXiv:2106.11189.\n\n\nShwartz-Ziv, Ravid, and Amitai Armon. 2021. “Tabular Data: Deep\nLearning Is Not All You Need.” arXiv Preprint\narXiv:2106.03253.\n\n\n\n\n",
    "preview": "posts/2021-07-15-dl-for-tabular/../../img/dl-for-tab/deep_nn.png",
    "last_modified": "2022-05-02T18:50:46-04:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 313
  },
  {
    "path": "posts/2021-02-28-practical-bayes-part-i/",
    "title": "Practical Bayes Part I",
    "description": "Dealing with common model problems.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "bayesian"
    ],
    "contents": "\n\nContents\nOverview\nThe Stated Problem\nAudience\nOutline\nInstallation issues\nMac\nWindows\nLinux\nOther programming languages besides R\n\nExample data\nEssential Steps for Practical Modeling\nWarnings, What They Mean, and What to do About Them\nRhat & Effective Sample Size\nSome details\nRhat\nESS\nTrace plot\nDensity plot\nRank plot\nACF plot\nEfficiency plots\nSolution for Rhat/ESS warnings\n\nBFMI low\nTree Depth\nDivergent Transitions\nOther messages\nParser warnings\nCompilation warnings\nPackage warnings\nSolutions for other messages\n\nModel Comparison Problems\n\nOther Issues\nBut it’s slow!\nSolutions for a slow model\n\nshinystan\n\nSummary: The Practical Approach to Dealing with Model Problems\nResources\nGeneral\nPriors\nRhat/ESS\nDivergence\nOther warnings\nVisual diagnostics\nMisc\n\n\nOverview\n\n\n\n\n\nBayesian analysis takes some getting used to, but offers great advantages once you get into it. While it can be difficult to get started, it typically should not take much to repeat an analysis one is already familiar with, say a standard regression with some (common) additional complexity like a binary outcome, interactions, random effects, etc. What’s great is that Stan, a programming language for Bayesian analysis, has come a very long way, and provides a means for (relatively) fast computation. What’s more, applied analysts do not need to know the Stan programming language to do common and even notably complicated models. Packages like rstanarm and brms, coupled with additional tools like bayesplot, tidybayes, and more, make getting and exploring results even easier than the R packages one already uses.\nOne of the advantages of doing Bayesian analysis with these tools is that there are many ways to diagnose model issues, problems, and failures. This is great! Traditional analysis can often be notably more difficult in this regard, or at least typically has fewer ready tools to work with, and often there can be serious model deficiencies without much notification or, if there is, there might be little one can do about it. On the other hand, current Bayesian packages are providing diagnostic information as a default part of results, have ready visualizations to explore issues, and more.\n\nThis is part one of two posts on doing practical modeling with R and Stan in an applied fashion. Due to unforeseen circumstances (and plenty of procrastination), the bulk of the content of these posts was created many months before actual posting. It’s hoped that most of this will still be applicable and have the conceptual continuity as originally intended, but apologies in advance if some parts seem a little disjointed, off topic, etc.\nThe Stated Problem\n\n\n\nThis is all great in general. However, in consulting I saw a consistent issue among clients starting out with Bayesian analysis and Stan packages. Someone has a standard model, e.g. a basic regression or mixed model, but is getting problematic warnings, messages, or errors. When they look up how to solve this problem, the documentation, forums, and other outlets, while great resources for the initiated, often overwhelm newer and/or more applied users, as they are too technical, assume a great deal of background knowledge, are even under-explained (especially in the case of interpreting visualizations), and even suggest things that aren’t typically possible (e.g. getting better data/model). What seems a straightforward suggestion to a more advanced user or developer, may not be even close to that for many applied analysts, and they can be left a bit deflated by their experience.\nIn the beginning of Stan’s ascension, the majority of people using Stan/rstan were more technically inclined, coded in Stan directly, and, when problems arose, they were willing to do a lot of extra work to solve the problems (reading technical articles, developing pseudo-expertise with various techniques, and more). But tools like rstanarm and brms make running Bayesian models as easy to do as using base R functions and other non-Bayesian packages, and as Bayesian analysis has become more widely used, accepted, and presented, this has opened the Bayesian approach to practically anyone that wants to do it.\nI personally don’t think one needs to be an expert in Bayesian analysis to enjoy its benefits for common models, even if difficulties come up. Nor do I think an exhaustive search through forums, technical articles, function documentation, and more is necessary to deal issues that arise often. So this is an attempt at a practical guide and one-stop shop for dealing with the most common issues that arise, interpreting results of diagnostics, and summarizing the options one can take. The goal here is to provide some practical, not perfect(!), suggestions on how to go about your applied Bayesian modeling.\nAudience\n\n\n\n\nHere is the presumed audience for this post:\nWants to conduct Bayesian analysis\nIs not going to code anything in Stan directly (i.e. will use a specific package like rstanarm or brms)\nIs not a statistician, nor desires deep technical insights about Bayesian analysis (at least not yet!)\nIs already having to learn a new tool/functions/possibly a whole system of inference\nHas probably never used the control argument for any model\nWhile you can do a Bayesian analysis just for the heck of it, you really need to understand a few key ideas to take advantage of what it offers. Some things you do need to know in order to use it on a basic level:\nThe distributions: priors, likelihood, posterior, posterior predictive\nIterative sampling to estimate parameters. Even a cursory understanding of maximum likelihood would probably be enough.\nYou can obtain this basic info from my document Bayesian Basics.\nOutline\nHere is what we’re going to do:\nDiscuss some of the more common problems\nExplain conceptually what they indicate\nProvide quick solutions that should work most of the time\nOutline a practical approach for future endeavors (Part II)\nEach section discusses a particular problem and ends with a general recommendation, and potentially, specific links to look further into the issue.\nInstallation issues\nYour first hurdle is installation, so let’s start there. Applied users will use rstanarm, brms, or other higher level interfaces to the Stan programming language. These tools use Stan, but Stan itself requires compilation to C++. This means that to run a basic regression with brms, you’ll likely be depending on multiple languages and packages for it to work. When using these packages, here is a high-level view of how things work:\n\\(\\rightarrow\\quad\\) Package interprets your R code (brms, rstanarm)\n\\(\\quad\\rightarrow\\quad\\) Model is translated to Stan (or use rstan with Stan code)\n\\(\\quad\\quad\\rightarrow\\quad\\) Compiled to C++ (requires compiler)\nIn general, installing the package you want to use will install the appropriate dependencies, and that should be enough as far as your interactive R part goes. In some cases it may not be. If you have issues, it’s maybe best to install rstan first, then your package of choice. Issues beyond that may indicate an issue with the compiler, at which point you’ll want to consult the forums.\nMac\nTo be nice about it, Mac’s Catalina OS was a problematic release to say the least. For our purposes, this meant that there was a period of time where Stan wasn’t viable for many users without extensive workarounds, and issues seemingly arose with every update. This was not specific to Stan, or R, by the way1. Then R 4.0 came along and helped some things, but also resulted in new issues. That said, the Stan community were excellent at helping people here, and many of the issues were ultimately resolved by the end of 2020.\nNow that Big Sur has been out for a while I get the sense that things have been better there, or at least there aren’t four separate dedicated threads trying to deal with the various problems arising. My own recent luck has been without installation issues on Macs with either OS.\nSearch Mac Issues on Stan Forums\nWindows\nI rarely had installation issues on Windows’ releases, though haven’t had to use it lately. It does require rtools to be installed, which is good to install for R with Windows anyway. If you are starting out, I would install it, then rstan, then your package of choice (e.g. brms). Again though, if you have issues, the Stan community will be very helpful.\nSearch Windows Issues on Stan Forums\nLinux\nI’ve only used Stan with Linux in a cluster computing environment. I generally have not had issues, but it’s not something I’ve done recently, and I don’t have any basic Linux desktop experience. Applied users also need cluster computing on occasion, it’s just that the problems will likely require IT support in that case. Again though, plenty of Stan folks are willing to help.\nSearch Linux Issues on Stan Forums\nOther programming languages besides R\nIf you’re using Stan with Python, Stata, Julia, etc., then you’re using the Stan language directly, and you’re likely already very familiar with the forums and dealing with a variety of issues. That’s not to say that you won’t find something useful here, it’s just that I have nothing to offer you regarding those platforms specifically.\nExample data\n\n\n\n\nTo start out, I’m going to create some data for us to run some basic models with. To make things interesting, the true underlying model has categorical and continuous covariates, interactions, nonlinear relationships, random effects (observations are clustered in groups), and some variables are collinear. You can skip these details if uninterested, but note that we will be purposely using under- and over-fitted models relative to this one to see what happens.\n\n\nlibrary(tidyverse)\n\ncreate_data <- function(N = 1000, ng = 100, seed = 1234) {\n  \n  set.seed(seed)\n  \n  # the model matrix\n  X_mm = cbind(\n    # a standard binary\n    binary_1 = sample(0:1, N, replace = TRUE),                      \n    # a relatively rare categorical\n    binary_2 = sample(0:1, N, replace = TRUE, prob = c(.05, .95)),  \n    # two partly collinear numeric\n    mvtnorm::rmvnorm(N,\n                     mean = rep(0, 3),\n                     sigma = lazerhawk::create_corr(runif(3, max = .6)))\n  )\n  \n  X_mm = cbind(\n    # intercept\n    1,\n    X_mm,\n    # a cubic effect\n    scale(poly(X_mm[,5], 3))[,2:3],\n    # interaction of binary variables\n    X_mm[,1]*X_mm[,2], \n    # interaction of binary 2 with numeric 1\n    X_mm[,2]*X_mm[,3]\n  )\n  \n  # add names\n  colnames(X_mm) = c(\n    'Intercept',\n    'b1',\n    'b2',\n    'x1',\n    'x2',\n    'x3',\n    'x3_sq',\n    'x3_cub',\n    'b1_b2',\n    'b2_x1'\n  )\n  \n  # coefficients\n  beta = c(\n    3.0,   # intercept\n     .3,   # b1\n    -.3,   # b2\n     .5,   # x1\n     .0,   # x2\n     .3 ,  # x3 \n     .3,   # x3_sq\n    -.2,   # x3_cub\n     .5,   # b1_b2 \n    -.5    # b2_x1\n  )\n  \n  # create target variable/linear predictor\n  y = X_mm %*% beta\n  \n  # add random effect\n  groups = sort(sample(1:ng, N, replace = T))\n  \n  # random effect sd = .5\n  re = rnorm(ng, sd = .5)[groups]  \n  \n  # add re and residual noise with sd = 1\n  y = y + re + rnorm(N)\n  y = cbind(y, groups)\n  colnames(y) = c('y', 'group')\n  \n  as_tibble(cbind(X_mm, y))\n}\n\n\n\n\n\n\nIf you want to check that the parameters are recovered, you can run something like the following.\n\n\ndat = create_data(N = 10000)\n\nmod = lme4::lmer(y ~ . -group + (1|group), data.frame(dat[,-1]))\n\nmixedup::summarise_model(mod, ci = FALSE)  # or just summary(mod)\n\n\n\nWe do not need much data for our purposes so we’ll set the total sample size to 1000. We’ll drop unnecessary columns (e.g. we’d normally specify interactions via the formula rather than create the columns explicitly), and also make our binary covariates explicit factors, which will make things easier when we want to visualize grouped effects later.\n\n\n# create the primary data frame\n\nmain_df = \n  create_data(N = 1000) %>% \n  as_tibble() %>% \n  select(group, b1:x3, y) %>% \n  mutate(\n    b1 = factor(b1),   # will help with visuals\n    b2 = factor(b2)\n  )\n\nmain_df\n\n\n# A tibble: 1,000 x 7\n   group b1    b2         x1      x2     x3     y\n   <dbl> <fct> <fct>   <dbl>   <dbl>  <dbl> <dbl>\n 1     1 1     1      0.155   0.145  -1.84   4.01\n 2     1 1     1     -0.356   1.33   -0.604  3.84\n 3     1 1     1     -0.329   0.488  -0.441  3.37\n 4     1 1     1      1.08   -1.50    1.00   3.80\n 5     1 0     1     -0.563  -1.07   -0.150  1.81\n 6     1 1     1     -0.0694 -0.229  -0.387  4.54\n 7     1 0     1      0.374   0.606   2.04   4.17\n 8     1 0     1      2.06   -0.266   0.168  1.83\n 9     2 0     0      0.278   0.0559  1.04   1.58\n10     2 1     0      0.184   0.230   0.174  3.56\n# … with 990 more rows\n\nEssential Steps for Practical Modeling\n\n\n\n\nOnce data is in hand there are basic steps to take for a practical modeling approach with Stan tools.\nUse standard/default priors for the model\nFor common regression models, normal or student t for the (fixed effect) coefficients. You will have to explicitly set this if using brms, but rstanarm will have viable defaults already.\nUse (half) normal/student-t for variances (esp. hierarchical models). Defaults are usually fine.\nOtherwise, look at the recommendations.\nSee Part II for more on choosing priors.\nBuild models in increasing complexity\nIt’s a good idea not to start with the ‘final’ model, especially in complex settings. If the fitting has issues with simpler models, things will only get worse with more complex models.\nGetting model settings squared away earlier will save time later (e.g. setting number of iterations, other options).\nExamine convergence via Rhat, ESS, visualization\nIf you run the model and get no warnings or messages, you are okay to proceed to summarize and visualize it in my opinion. Any remaining issues, e.g. poor prediction, are likely to not have obvious solutions, other than things like getting better data, adding additional covariates, revising theory, etc.\nDiagnostic plots are the main tool to explore issues that come from warnings.\nExamine model effectiveness visually\nThere are entire packages at your disposal for visualizing model results such as bayesplot, tidybayes, etc. However, standard plots can be called from rstanarm or brms functions, which are wrappers for bayesplot functions.\nPosterior predictive checks are a fundamental part of Bayesian analysis.\nAvoid using approaches that are merely substitutes for the null hypothesis test you would have done in the non-Bayesian setting (e.g. using bayes factors).\nCompare models using loo, posterior probabilities of models\nCompare models via predictive capabilities (leave-one-out approaches, see Part II).\nThere is no ‘best’ model in Bayesian approach or otherwise. Consider model averaging for final predictions.\nIt is not necessary to automatically prefer simpler models, though it may make things easier to do so in some contexts.\nAssuming you have no problems in the above process, you have more or less fulfilled the basic requirements to do standard analyses in Bayesian form. Great!\nWarnings, What They Mean, and What to do About Them\n\n\n\nOf course, if it was always that easy, we wouldn’t be posting this. There are a few warnings that you’re bound to come across at some point in modeling with the Stan ecosystem. We’ll cover these, as well as the most common solutions.\nPrimary reference: Brief Guide to Stan’s Warnings\nRhat & Effective Sample Size\nWe will start with a simple standard regression model that we know is not adequate. We will use default priors2 and run very few iterations.\n\n\n# no priors, no complexity, all default settings, few iterations\nlibrary(brms)\n\nmodel_start_100 = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 100,\n  verbose = F,\n  seed = 123\n)\n\n\nWarning: The largest R-hat is 1.06, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#r-hat\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#bulk-ess\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\nsummary(model_start_100)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ b1 + b2 + x1 + x2 + x3 \n   Data: main_df (Number of observations: 1000) \nSamples: 4 chains, each with iter = 100; warmup = 50; thin = 1;\n         total post-warmup samples = 200\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.68      0.19     2.33     3.02 1.06       59      114\nb11           0.83      0.07     0.68     0.94 1.01      144      136\nb21          -0.08      0.18    -0.41     0.27 1.06       56      112\nx1            0.03      0.03    -0.04     0.09 0.99      342      156\nx2           -0.03      0.03    -0.08     0.03 1.01      303      195\nx3            0.28      0.03     0.22     0.33 1.01      271      173\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.10      0.03     1.06     1.16 1.02      207      136\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs is common in other R models, for factors, the effect is the stated category vs. the reference. Since we only have binary variables, b11 is show the level 1 vs. level 0 (reference) for the first binary covariate.\nThe warnings about Rhat and effective sample size (ESS) are likely to pop up if you use default iterations for more complex models. They mostly regard the efficiency of the sampling process, and whether you have enough samples to have stable parameter estimates. Ideally Rhat is close to 1.0, and ESS is at least a notable percentage of the total posterior samples (e.g. 50%).\nThe fix for these warnings is usually simple, just let the model run for more iterations beyond your warmup. The default is 2000 iterations, with warmup half of that. Warmup iterations are not used in calculation of parameter estimates, so you can just increase the number of iterations relative to it, or increase both by only increasing the iter argument.\nIn the following, we plot the estimated values across each iteration for each chain, called a trace plot, as well as the density plot of the values from the entire chain. From this we could see that things might be problematic (e.g. we’d want more symmetric density plots for the regression coefficients), but only if you are used to looking at these things, so it will take some practice.\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'combo')\n\n\n\n\nTo get some intuition for what you would expect, just plot a series from a normal distribution.\n\n\nggplot2::qplot(x = 1:1000, y = rnorm(1000), geom = 'line')\n\n\n\nggplot2::qplot(x = rnorm(1000), geom = 'density')\n\n\n\n\nOther plots allow us to look at the same sorts of things from a different perspective, or break out results by each chain.\n\n\nmcmc_plot(model_start_100, type = 'areas')\n\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'violin')\n\n\n\n\nWhile things seem okay, if we single out a particular chain, we might think otherwise. In this case, the Intercept and b2 coefficients may be problematic, given they do not seem to vary as much as the others (chain 2 for example, but also for other chains).\n\n\nmcmc_plot(model_start_100, highlight = 2, type = 'trace_highlight')\n\n\n\n\nSome suggest to look at rank plots instead of traditional trace plots. Really, all we’ve changed is looking for something ‘fuzzy’, to looking for something ‘approximately uniform’, so my opinion is that it’s not much of an improvement visually or intuitively. In general, histograms, which are variants of bar charts, are rarely an improvement for any visualization. If you do use it, you can use an overlay approach to see if the ranks are mixing, but this looks a lot like what I’d be looking for from a trace plot.\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'rank_hist')\n\n\n\nmcmc_plot(model_start_100, pars = c('b1', 'b2', 'x1'), type = 'rank_overlay')\n\n\n\n\nSome details\nA little more detail will possibly provide additional understanding, and also some guidance, when looking at your own results.\nRhat\nThe \\(\\hat{R}\\) (or Rhat) statistic measures the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains. If all chains are at equilibrium, these will be the same and \\(\\hat{R}\\) will be 1.0. If the chains have not converged to a common distribution, the \\(\\hat{R}\\) statistic will be greater than one.\nWhat we want: values near 1.0 (< 1 okay) and less than 1.05\n\n\nmcmc_plot(model_start_100, type = 'rhat')\n\n\n\n\nESS\nEffective sample size is an estimate of the effective number of independent draws from the posterior distribution of the parameter of interest. Because the draws within a chain are not independent if there is autocorrelation, the effective sample size will typically be smaller than the total number of iterations, but the calculation of this statistic is bit more art than science, and can even be greater than the number of posterior draws. Note also that the plot is based on slightly different values than reported by the brms summary function.\nBulk ESS: ESS for the mean/median (n_eff in rstanarm). Tells us whether the parameter estimates are stable.\nTail ESS: ESS for the 5% and 95% quantiles. Tells us whether the interval estimates for the parameters are stable. Tail-ESS can help diagnose problems due to different scales of the chains and slow mixing in the tails.\nWhat we want: ESS > 10% percent of total posterior samples for sure, but > 50% is best. At least 100 is desired for decent estimates of autocorrelation. For Bulk-ESS we want > 100 times the number of chains.\n\n\nmcmc_plot(model_start_100, type = 'neff')  # < .1 problem\n\n\n\n\nTrace plot\nShows the estimated parameter values at each iteration. In general you would like a random bouncing around an average value.\n\n\nmcmc_plot(model_start_100, type = 'trace')\n\n\n\n\nWhat we want: Something like random normal draws over a series. Trace plots in general should look ‘grassy’, or like a ‘fuzzy caterpillar’, which might not be very descriptive, but deviations are usually striking and obvious in my experience. If you see chains that look like they are getting stuck around certain estimates, or separating from one another, this would indicate a serious problem. If the chains are not converging with one another, you were probably already getting warnings and messages.\nDensity plot\nShows the density of the posterior draws for the parameters.\n\n\nmcmc_plot(model_start_100, type = 'dens')\n\n\n\n\nWhat we want: For the density plots of regression coefficients, these should be roughly normal looking. For variance parameters you may see skewness, especially if the estimate is relatively near zero with smaller data sets. In general, we would not want to see long tails or bimodality for the typical parameters of interest with models you’d be doing with rstanarm and brms.\nRank plot\nFrom bayesplot help file:\n\nWhereas traditional trace plots visualize how the chains mix over the course of sampling, rank histograms visualize how the values from the chains mix together in terms of ranking. An ideal plot would show the rankings mixing or overlapping in a uniform distribution.\n\n\n\nmcmc_plot(model_start_100, type = 'rank_hist')\n\n\n\n\nWhat we want: Uniform distribution, a good mixing of lines for the overlay version.\nACF plot\nThe acf, or autocorrelation function plot, is exactly the same thing you’d visualize for any time series. It is a plot of a series of correlations of a parameter with specific lags of itself. Autocorrelation does not bias estimates, but increased autocorrelation may suggest a more inefficient/slower exploration of the parameter space. At lag zero, the series estimates are perfectly correlated with themselves, so that’s where the plot usually starts.\nWhat we want: Quick drop off, but not really that important. By the time you find it’s an issue, your model has already run.\n\n\nmcmc_plot(model_start_100, type = 'acf')\n\n\n\n\nEfficiency plots\nI have seen these often recommended, but I’m not aware of a package does them, though it seems they may make their way to shinystan at some point. Aki Vehtari has supplied a walk-through and some code (see the resources section below), but there isn’t really documentation for the functions, and they likely won’t work outside of rstan objects, or at least I had limited success with applying them to brms objects. Furthermore, these are getting into waters that are beyond what I’d expect applied users to be wading through.\nSolution for Rhat/ESS warnings\nTo summarize, the general solution to Rhat and ESS warnings is simply to do more iterations (relative to the warmup). To keep posterior samples and model objects from becoming unwieldy in size3, consider thinning also. Thinning saves only a select amount of the available posterior samples. For example, setting thin = 10 means only every tenth sample will be saved. This will also reduce autocorrelation, as the draws retained after thinning are not as correlated with one another as successive draws would be. However, if you thin too much, you may not have enough for effective sample size.\n\n\n# default with 4 chains, 1000 warmup 2000 total = 4*(2000 - 1000) = 4000 post-warmup draws\nbrm(model)\n\n# 4*(4000 - 2000) = 8000 posterior draws\nbrm(model, warmup = 2000, iter = 4000)\n\n# 4*(4000 - 2000)/8 = 1000 posterior draws\nbrm(model, warmup = 2000, iter = 4000, thin = 8)\n\n\n\nBFMI low\nYou may see a warning that says some number of chains had an ‘estimated Bayesian Fraction of Missing Information (BFMI) that was too low’. This implies that the adaptation phase of the Markov Chains did not turn out well, and those chains likely did not explore the posterior distribution efficiently. For more details on this diagnostic, you can see Betancourt’s article, but this will almost surely be too technical for many applied and even more advanced users.\nIn this case, the problem here is often remedied by just adding more iterations. I typically keep to 1000 posterior samples, which makes for nicer visualizations of distributions without creating relatively large model objects. However, you may need more to get a satisfactory result.\n\n\nmodel_start = update(\n  model_start_100,\n  warmup = 2000,\n  iter = 2250,      # 1000 posterior draws\n  cores = 4,\n  seed = 123\n) \n\n\n\nIn this case, we no longer have any warnings, and even one of our more problematic coefficients looks fine now.\n\n\nsummary(model_start)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ b1 + b2 + x1 + x2 + x3 \n   Data: main_df (Number of observations: 1000) \nSamples: 4 chains, each with iter = 2250; warmup = 2000; thin = 1;\n         total post-warmup samples = 1000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.73      0.20     2.32     3.12 1.01     1390      843\nb11           0.83      0.07     0.69     0.97 1.01     1829      777\nb21          -0.13      0.20    -0.53     0.26 1.00     1401      729\nx1            0.03      0.03    -0.04     0.09 1.00     1388      865\nx2           -0.03      0.04    -0.11     0.05 1.01     1377      813\nx3            0.28      0.04     0.21     0.35 1.01     1105      846\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.11      0.03     1.06     1.16 1.00     1385      674\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(model_start, par = 'b2')\n\n\n\n\nSolution for low BFMI\nIf there aren’t other serious problems, add more iterations to deal with low BFMI. In some cases, switching from a heavy-tailed prior (e.g. student t) to something else (e.g. normal) would be helpful, but some other approaches typically would involve having to write Stan code directly to reparameterize the model. Otherwise, you may need to approach it similarly to the problem of divergent transitions.\nTree Depth\nTree depth is a more technical warning that has to do with the details of Hamiltonian Monte Carlo. Practically speaking:\n\nLack of convergence and hitting the maximum number of leapfrog steps (equivalently maximum tree depth) are indicative of improper posteriors ~ Stan User Guide\n\nSometimes you’ll get a warning about hitting maximum tree depth, and without getting overly technical, the fix is easy enough. Just set the maximum higher.\nSolution for max tree depth\nUse the control argument to increase the value beyond the default of 10.\n\n\nmodel_update_treedepth = update(\n  model,\n  control = list(max_tree_depth = 15)\n)\n\n\n\nDivergent Transitions\nDivergent transitions are a technical issue that indicates something may be notably wrong with the data or model (technical details). They indicate that the sampling process has ‘gone off the rails’, and that the divergent iteration’s results, and anything based on them (i.e. subsequent draws, parameter estimates), can’t be trusted. Unlike the other problems we’ve discussed, this is more difficult to navigate.\nWhy might this happen?\ninsufficient data for the model’s complexity\npoor model\nhigh collinearity\nimproper or otherwise problematic priors\nseparability (logistic regression)\nany number of other things\nAs an example, I’ll make an overly complex model with only a small random sample of the data, improper priors, and use very few warmups/iterations.\n\n\nmodel_problem = brm(\n  bf(\n    y ~ b1*b2*x1 + x2*b2 + x3*b2 + (1 + x1 + b2|group),\n    sigma ~ x1 + b1*b2\n  ),\n  data   = main_df %>% slice_sample(prop = .1),\n  family = student,\n  cores  = 4,\n  warmup = 5,\n  iter   = 1005,\n  thin   = 4,\n  seed   = 123\n)\n\n\nWarning: There were 80 divergent transitions after warmup. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\nWarning: Examine the pairs() plot to diagnose sampling problems\nWarning: The largest R-hat is 2.96, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#r-hat\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#bulk-ess\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttp://mc-stan.org/misc/warnings.html#tail-ess\n\n\nSo what do we do in this case? Well let’s start with visual inspection.\nVisualization: Pairs plot\nA diagnostic tool that is typically suggested to look at with divergent transitions is the pairs plot. It is just a scatterplot matrix of the parameters estimates (and log posterior value), but it suffers from a few issues. The plot is slow to render even with few parameters, and simply too unwieldy to use for many typical modeling situations. If you somehow knew in advance which parameters were causing issues, you could narrow it down by only looking at those parameters. But if you knew which parameters were the problem, you wouldn’t need the pairs plot.\nAnother issue is that it isn’t what you think it is at first glance. The upper diagonal is not just the flipped coordinates of the lower diagonal like every other scatterplot matrix you’ve seen. The chains are split such that half are used for the above diagonal plots, and the other for the lower, with the split being based on the amount of numerical error (above or below the median). I suspect this may not help applied users interpret things, but the gist is, that if your red points show up only on the upper diagonal, changing the adapt_delta part of the control argument may help (see below), otherwise it likely won’t4.\nLet’s take a look at the pairs plot anyway. I’ll use hex bins instead of standard points because the point plots have no transparency by default. In addition, we’ll use a density plot on the diagonal, instead of the histogram.\n\n\nmcmc_plot(\n  model_problem,\n  pars = c('b_b11', 'b_b21', 'b_x1'),\n  type = 'pairs',\n  diag_fun = 'dens',\n  off_diag_fun = 'hex',\n  fixed = TRUE\n)\n\n\n\n\n\n\n\nWith problematic cases, what you might see on the off-diagonal plots is some sort of ‘funneling’, which would indicate where the sampler is getting stuck in the parameter space. However, this visual notion isn’t defined well, as it may be happening without being obvious, displaying just a bump, or just some weird patterns as above. But you’ll also regularly see correlated parameters, but it’s unclear whether these might necessarily be a problem in a given situation.\nFor the initial model we ran, the pairs plot for all parameters takes several seconds to produce, and even with the hex option, it is still difficult to parse without closer inspection. It shows the intercept and b2 parameters to be notably correlated, possibly indirectly due to the poor priors.\n\n\nmcmc_plot(\n  model_start_100,\n  type = 'pairs',\n  off_diag_fun = 'hex',\n  diag_fun = 'dens'\n)\n\n\n\n\nWhat we want: Roughly little correlation among parameters, mostly symmetric densities for typical regression parameters.\nVisualization: Parallel Coordinates Plot\nIt is also suggested to look at parallel coordinates plots, but unfortunately there are issues with these plots as well. The order of the variable/parameter axis is arbitrary, and yet the order can definitely influence your perception of any patterns. Also, unless everything is on similar scales, they simply aren’t going to be very useful, but even if you scale your data in some fashion, the estimates given divergent transitions may be notably beyond a reasonable scale.\nAs in our pairs plot, we’d be looking for a pattern among the divergences, specifically a concentration for a parameter where the lines seemingly converge to a point. If this isn’t the case, the divergences are probably false positives5. I had to add some ggplot options to help this to be more legible, and you will likely have to as well. In the following, you might think the b_sigma_x1 coefficient for dispersion is a problem, which might suggest we need to rethink the prior for it. In reality it’s likely just that it’s being estimated to be near zero, as it should be, especially since non-divergent transitions are also bouncing around that value. For the most part we don’t see much pattern here.\n\n\nmcmc_parcoord(\n  model_problem,\n  pars = vars(matches('^b')),\n  size = .25, \n  alpha = .01,\n  np = nuts_params(model_problem),  # without this div trans won't be highlighted\n  np_style = parcoord_style_np(\n    div_color = \"#ff5500\",\n    div_size = 1,\n    div_alpha = .1\n  )\n) +\n  guides(x = guide_axis(n.dodge = 2)) +\n  theme(\n    axis.text.x = element_text(size = 6),\n    panel.grid.major.x = element_line(color = '#00AAFF80', size = .1)\n  )\n\n\n\n\nWhat we want: Roughly no obvious pattern. If the divergence lines are not showing any particular concentration, it could be that these are false positives.\nSolution for divergent transitions\nUnfortunately the solution to divergent transitions is usually not straightforward. The typical starting point for solving the problem of divergent transitions is to use the control argument to increase adapt_delta, for example, from .80 to .996, and let your model have more warmup/total iterations, which is the primary issue here. In the cases I see for myself and clients, increasing adapt_delta rarely helps, but it doesn’t hurt to try. I often will just start with it increased for more complex models, just to save messing with it later.\n\n\nmodel = brm(..., control = list(adapt_delta = .99))\n\n\n\nAside from that you will have to look more deeply, including issues with priors, model specification, and more. I find this problem often comes from poor data (e.g. not scaled, possible separation in logistic models, etc.), combined with a complex model (e.g. complicated random effects structure), and beyond that, the priors may need to be amended. You should at least not have uniform priors for any parameter, and as we’ll see in Part II, you can use a simulation approach to help choose better priors. Often these data and prior checks are a better approach to solving the problem than doing something after the fact. You may also need to simplify the model to better deal with your data nuances. As an example, for mixed models, it may be that some variance components are not necessary.\nSome solutions offered on the forums assume you are coding in Stan directly, such as reparameterizing your model, using specific types of priors, etc. If you are writing Stan code rather than using a modeling package, you definitely need to double check it, as typos or other mistakes can certainly result in a problematic model. However, this post is for those using modeling packages, so I will not offer such remedies, and they are usually not obvious anyway.\nOther messages\nCertain builds of rstan for some types of settings (e.g. specific operating systems) will often have warnings or other messages. Sometimes it looks like a bunch of gobbledygook, which is typically something happening at the C++ level. If your model runs and produces output despite these messages, you can typically ignore them in most cases. Even then, you should look it up on the forums just to be sure.\nParser warnings\nParser warnings are either a deprecation warning, or another more serious kind (Jacobian). The latter will not happen if you’re using higher level interfaces (e.g. brms), rather than programming in Stan directly. The other kind, deprecation warnings, are not something you can do anything about, but the developer of the package will likely need to make minor changes to the code to avoid them in the future. I’ve never seen parser warnings from using rstanarm or brms.\nCompilation warnings\nCompiler warnings happen regularly and indicate something going on at the compiler level, typically that something in Stan is being compiled but not used. You can ignore these.\nPackage warnings\nLike any good package, when things go unexpectedly, or just to be informative, modeling packages like rstanarm and brms will provide you messages or warnings. These do not have to do with the Stan part of things. For example, brms will warn you that it will drop cases with missing values.\n\nRows containing NAs were excluded from the model.\n\nSome issues can be more subtle. For example, you may get a message that the model is compiling but then nothing happens. This might be because of a typo in a distribution name for your priors, or some similar goof7.\nSolutions for other messages\nIf you are using a package to interface with Stan and not having an issue with the model (i.e. it runs, converges), these messages can largely be ignored, unless it is a warning from the package itself, which typically should be investigated.\nModel Comparison Problems\nFor a discussion of loo and related issues, see Part II.\nOther Issues\nBut it’s slow!\nAnother problem people seem to be concerned with is the speed of the analysis. If you have a lot of data, or more to the point, a lot of parameters, your model can be very slow. For standard models with rstanarm and brms, there may be no real benefit doing it Bayesian style if you have millions of data points and simpler models. If your model is only taking a couple minutes, then you really have nothing to complain about- watch some YouTube or something while it runs. If your model takes on the order of hours, work with less data or simpler models until you have your modeling code, plots, etc. squared away. At that point you can run your primary analysis and wait it out. A slow model may also be indicative of a poorly specified/understood model, so you may have to think hard about how you are approaching the problem.\nSolutions for a slow model\nFor any model, Bayesian or otherwise, doing things like standardizing, logging or other variable transformations will put parameters in more reasonable domains, resulting in a more manageable estimation process. For example, if you standardize predictors, then the coefficients are on similar scales and a Normal(0, 1) prior can typically be applied.\nUsing more informative priors can avoid exploring areas of the posterior that aren’t going to lead to plausible results. We will show how to do this in Part II.\nUse less iterations if there are no other issues.\nIf possible, work with less data or simpler models until ready for the full model.\nIf possible, work with a different version of the model that can still answer the question of interest. For example, if an ordinal model is causing a lot of issues, and you’re not interested in category specific probabilities, just treat the target variable as numeric8.\nStop fishing/p-hacking/torturing your data so that you don’t have to run dozens of models. You’re likely not to care if the models takes hours if you only run them once or twice.\n\nshinystan\nThe shinystan package makes exploring model diagnostics actually fun! Using launch_shinystan on a model opens a browser window, providing interactive visualizations to help you see what’s going on with it. You can look at many of the previous plots, plus a few others we haven’t shown.\nUnfortunately the documentation in the browser doesn’t really tell you what to look for in these plots. The glossary contains information that is likely overly technical for applied users, and if there is a problem, there’s not really a whole lot to go on. In addition, some plots are difficult to use (e.g. trying to assess whether overlapping histograms are similar), or are probably only useful if you have very obvious problem (e.g. lots of divergent transitions). As an example, consider the tree depth plots. What would be good here?\n\nThe documentation tells you the value should be somewhere between 0 and whatever max_treedepth is set at. If they are ‘large’, the documentation states the problem could be due to different things, but the solutions are to either reparameterize of the model (probably not possible unless using Stan code directly), or just increase the value. It doesn’t seem to tell you what those plots are supposed to look like, and unfortunately that severely limits their utility. The divergence and energy plots are similarly under-explained. Many refer users to Betancourt’s wonderful articles on the details, but these are far too technical for those not already steeped in the approach9.\nAll that said, luckily there is a nice walkthrough if you do have a hankering to go down that path, and provides more details on the statistics and what you should be looking for in the visualizations.\nSummary: The Practical Approach to Dealing with Model Problems\nFor applied analysts, you can do a couple things when faced with warnings or just otherwise assessing the model integrity. I would suggest first focusing on the density and trace plots for parameters. You can then examine other visualizations that might be appropriate to the problem, and take the appropriate steps outlined above to try and solve those issues. Pay extra attention to parameters with relatively low effective sample sizes, as these are the ones the model is struggling to estimate.\nMany of the problems can be solved by increasing the warmup and total number of iterations. After that, setting control parameters may be enough. For more serious issues, you may need to try different priors or even a different model. If none of the above solves your problems, you may be trying too complex of a model for your data, have a data-specific problem, or some other issue.\nIn general, doing Bayesian analysis can be easy, and hopefully it will be for you. Many common problems have simple solutions, and the more serious ones will only make you a better Bayesian though the efforts you take to solve them. Rather than see them as a nuisance, see them as a learning opportunity, and you’ll enjoy your results even more when you finally resolve them. Remember that you may not be in your comfort zone, and that’s okay! Things will come around eventually, and you’ll solve these problems more easily in the future!\nOn to PART II: Do Bayes Better.\nResources\nGeneral\nAki Vehtari’s website has demos and explanation for things like model comparison, Rhat/Neff, and more.\nbayesplot Vignette for Diagnostic Plots\nJeffrey Arnold’s Bayesian Notes has nice examples of many models and good summaries otherwise.\nBayesian Basics, an applied overview of Bayesian analysis with Stan as the backdrop.\nPriors\nPrior Choice Recommendations by the Stan group.\nrstanarm vignette\nRhat/ESS\nStan Reference on Effective Sample Size\nVehtari et al. on Rank plots for details.\nVehtari’s appendix for the above (probably more accessible)\nVehtari’s code for efficiency plots, Code for functions\nDivergence\nStan Reference on Divergent Transitions\nDivergent Transitions A Primer\nCase study: Divergences and Bias\nOther warnings\nBrief Guide to Stan’s Warnings\nVisual diagnostics\nVisual MCMC diagnostics\nMisc\nGelman’s advice for slow models\nbrms vignettes\nUse CmdStan to save memory\nStan case studies\n\nI will spare the details of my opinions, which are almost entirely negative.↩︎\nFor this model, they are uniform/improper priors for the regression coefficients and half-t for the residual variance. You can always use prior_summary on a brms model to see this.↩︎\nWith more posterior samples comes slower visualizations and possibly other computations.↩︎\nAnother issue is that you could change how the chains are split and it could potentially dramatically change the how the pattern of divergent transitions looks.↩︎\nThis seems counter to the common suggestion on forums and GitHub issues that even 1 divergent transition renders results suspect. I’ve never seen results meaningfully change for something with just a couple divergent transitions to one that has none, and often when there are that few, even rerunning the model will result in no divergences.↩︎\nIn my experience, there isn’t a need to guess some value between .80 and .99 as the time differences are typically not great, say between .9, .95, and .99, unless your model already takes a very long time. Also, if it doesn’t work at .99, it won’t at .9999 either.↩︎\nI’m definitely not speaking from experience here or anything! Nomral distributions do exist, I’m sure of it! 😬↩︎\nIt is often suggested in the Stan world to reparameterize models. However, this advice doesn’t really apply in the case of using rstanarm or brms (i.e. where you aren’t writing Stan code directly), and it assumes a level of statistical expertise many would not have, or even if they do, the route to respecifying the model may not be obvious.↩︎\nBetancourt, whose work I admire greatly, typically makes my head spin.↩︎\n",
    "preview": "posts/2021-02-28-practical-bayes-part-i/../../img/r_and_stan.png",
    "last_modified": "2021-03-04T12:18:09-05:00",
    "input_file": {},
    "preview_width": 816,
    "preview_height": 303
  },
  {
    "path": "posts/2021-02-28-practical-bayes-part-ii/",
    "title": "Practical Bayes Part II",
    "description": "Taking a better approach and avoiding issues.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "bayesian"
    ],
    "contents": "\n\nContents\nOverview\nOutline for Better Bayesian Analysis\n\nExample data\nSimulate from priors\nSummarizing a Model\nCheck Priors\nExplore and Visualize Results\nModel Effectiveness\nPosterior predictive checks\nBayes R-squared\n\n\nPrediction & Model Comparison\nBasic prediction\nModel Comparison\nChoosing a model\nProblems at the loo\n\nModel Averaging\nCross-Validation\nVariable Selection\n\nThe rabbit hole of model comparsion\nSolutions for Model Comparison\n\nSummary: The Practical Approach to Bayesian Models\nResources\nPrior Checks\nR2\nModel Comparison\nPareto values\nModel Averaging\nCross-Validation\nMisc\n\n\nOverview\nIn Part I, we talked about the basics one can do to run a Bayesian model with a high-level Stan package like brms, and what to do if there is a problem. But it might be nice if we could take steps to avoid the problems in the first place, and what’s more, our model might still be inadequate without any warnings, and we’ll still need to inspect diagnostics regardless. So let’s engage in some better practices you can use every time to help things run more smoothly, and get more from your models after you’ve run them.\n\nThis is part two of two posts on doing practical modeling with R and Stan in an applied fashion. Due to unforeseen circumstances (and plenty of procrastination), the bulk of the content of these posts was created many months before actual posting. It’s hoped that most of this will still be applicable and have the conceptual continuity as originally intended, but apologies in advance if some parts seem a little disjointed, off topic, etc.\nOutline for Better Bayesian Analysis\nWe’ll cover the following steps in more detail, but here is a general outline.\nFirst generate ‘fake data’ to assess viability of our priors\nWith adequate priors, start with a simple, but plausible model\nFor simple models you likely do not need many iterations, and for debugging/troubleshooting, starting with few iterations can possibly give you a sense of whether there will be problems1. If you are doing standard GLM or simpler versions of common extensions, even the defaults are likely overkill. For example, a basic linear regression should converge almost immediately.\nIf you have problems at this point, see Part I\nExplore and Visualize the Model Results\nVisualize covariate relationships\nAssess model effectiveness\nUse posterior predictive checks\nOther avenues\n\n\nPrediction and Model Comparisons\nGet basic predictions for observations of interest\nExplore a more viable model\nAdd interactions\nAdd nonlinear relations\nAccount for other structure (e.g. random effects)\n\nCompare and/or average models\nUse cross-validation to better assess performance\n\nWe’ll now demonstrate these steps.\nExample data\nAs in Part I, I’m going to create some data for us to run some basic models with, the same as Part I. As a reminder, the true underlying model has categorical and continuous covariates, interactions, nonlinear relationships, random effects (observations are clustered in groups), and some variables are collinear.\n\n\n\n\n\n\nFor our purposes so we’ll create a data frame with the total sample size of 1000.\n\n\n# create the primary data frame\n\nmain_df = \n  create_data(N = 1000) %>% \n  as_tibble() %>% \n  select(group, b1:x3, y) %>% \n  mutate(\n    b1 = factor(b1),   # will help with visuals\n    b2 = factor(b2)\n  )\n\n\n\nSimulate from priors\nA good initial step in Bayesian analysis is to think about and produce some viable priors for the parameters. But the obvious question is, what priors should we choose? Thankfully, for standard models there is not much guesswork involved. Bayesian analysis has been around a long time, so the bulk of the work in determining suitable priors for standard models has been done for you. Even default settings should not affect things much, especially for rstanarm, which has some basic defaults that are informed by the data. However, due to the flexibility of the brms modeling functions, some priors are unspecified and left ‘flat’ (i.e. uniform), which is something we definitely don’t want. And even defaults could still cause problems for more complex situations. So how might we choose better ones?\nThe basic idea here is to generate parameters (e.g. regression coefficients) based on their corresponding prior distributions, predict data based on those prior draws, and then compare the predictions to our observed target variable that we are attempting to understand.\nThankfully, the brms package makes this very easy to do. We will check the following types of priors, that range from default settings to increasing specification for all parameters of interest.\n\n\n\nWe can use pp_check to examine the prior-generated data versus the observed target y, but I wait to show them all together at the end. Note the argument sample_prior, which we set to 'only'.\n\n\nlibrary(brms)\n\n# essentially the same as the defaults\npr_uniform = prior(uniform(-100, 100), lb = -100, ub = 100, 'b')\n\nmodel_default_prior = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 1000,\n  sample_prior = 'only',\n  prior = pr_uniform\n)\n\n# pp_check(model_default_prior, nsamples = 50)\n\n# diffuse normal for reg coefficients 'b'\npr_norm_b_0_10 = prior(normal(0, 10), 'b')\n\nmodel_0_norm_b_0_10 = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 1000,\n  sample_prior = 'only',\n  prior = pr_norm_b_0_10\n)\n\n# pp_check(model_0_norm_b_0_10, nsamples = 50)\n\n# rstanarm-like prior\npr_auto = sjstats::auto_prior(\n  y ~ b1 + b2 + x1 + x2 + x3,\n  data = main_df,\n  gaussian = TRUE\n)\n\nmodel_auto_prior = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 1000,\n  sample_prior = 'only',\n  prior = pr_auto\n)\n\n# pp_check(model_auto_prior, nsamples = 50)\n\n# Since we have standardized data, Normal(0, 1) is reasonable for reg coefs\npr_norm_b_0_1 = prior(normal(0, 1), 'b')\n\nmodel_0_norm_b_0_1 = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 1000,\n  sample_prior = 'only',\n  prior = pr_norm_b_0_1\n)\n\n# pp_check(model_0_norm_b_0_1, nsamples = 50)\n\n# Now we add one for the intercept based on the mean of y\npr_norm_b_norm_int = c(\n  prior(normal(0, 1), class = 'b'),\n  prior(normal(3, 1), class = 'Intercept')\n)\n\nmodel_0_norm_b_0_1_norm_Int = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 1000,\n  sample_prior = 'only',\n  prior = pr_norm_b_norm_int\n)\n\n# pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)\n\n# Now add a prior for sigma based on the sd of y\npr_norm_b_norm_int_t_sigma = c(\n  prior(normal(0, 1), class = 'b'),\n  prior(normal(3, 1), class = 'Intercept'),\n  prior(student_t(10, 1, 1), class = 'sigma') # first value is deg of freedom\n)\n\nmodel_0_norm_b_0_1_norm_Int_sigma = brm(\n  y ~ b1 + b2 + x1 + x2 + x3, \n  data = main_df,\n  iter = 1000,\n  sample_prior = 'only',\n  prior = pr_norm_b_norm_int_t_sigma\n)\n\n# pp_check(model_0_norm_b_0_1_norm_Int_sigma, nsamples = 50)\n\n\n\nThe following plot shows the model predictions based on priors only. We restrict the range of values for display purposes, so note that some of these settings would actually generate more extreme results. For example, the default prior setting could generate values into the \\(\\pm\\) 500 and beyond. I also mark the boundaries of the observed target variable with the vertical lines.\n\n\n\nWe can see in the visualization that the left side using defaults or notably diffuse priors results in nonsensical ranges for our target variable. When we actually run the model, this means we’d explore possible (the space of) parameter values that aren’t going to be useful for prediction. We would likely still should come to the same conclusions, it’s just that we might need many more iterations, and as we know from Part I, not having enough iterations can lead to many warnings.\nSo given that our target variable is between -2 and 8, it seems that just adding some basic, data-informed information to our priors resulted in more plausible results. This will generally help our models be more efficient and better behaved. Note that if all else fails, you can use a convenience function like auto_prior demonstrated above.\nSummarizing a Model\nNow let’s run a baseline model, one that’s simple but plausible. Given that there will eventually be other complexities added to the model, I’ll go ahead and add some iterations, and increase adapt_delta and max_treedepth now to make the code reusable.\n\n\nlibrary(brms)\n\npr = c(\n  prior(normal(0, 1), class = 'b'),\n  prior(student_t(10, 1, 1), class = 'sigma'),\n  prior(student_t(10, 1, 1), class = 'sd')  # prior for random intercept std dev\n)\n\nmodel_baseline = brm(\n  y ~ b1 + b2 + x1 + x2 + x3 + (1 | group), \n  data    = main_df,\n  warmup  = 5000,\n  iter    = 6000,\n  thin    = 4,\n  prior   = pr, \n  cores   = 4,\n  seed    = 1234, \n  control = list(\n    adapt_delta   = .95,\n    max_treedepth = 15\n  ),\n  save_pars = save_pars(all = TRUE)  # potentially allows for more  more post-processing functionality\n)\n\n\n\n\n\nsummary(model_baseline)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ b1 + b2 + x1 + x2 + x3 + (1 | group) \n   Data: main_df (Number of observations: 1000) \nSamples: 4 chains, each with iter = 6000; warmup = 5000; thin = 4;\n         total post-warmup samples = 1000\n\nGroup-Level Effects: \n~group (Number of levels: 100) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.87      0.07     0.74     1.03 1.00      771      762\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.69      0.21     2.28     3.12 1.01      788      687\nb11           0.80      0.07     0.66     0.93 1.00      935      955\nb21          -0.15      0.19    -0.51     0.23 1.01      900      931\nx1            0.04      0.04    -0.03     0.11 1.00      861      891\nx2           -0.03      0.04    -0.11     0.04 1.00      937      903\nx3            0.27      0.04     0.20     0.34 1.00      823      975\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.03      0.02     0.99     1.08 1.00      998      988\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nFor reporting purposes, generally all you need are the Estimate and lower and upper bounds. You might also mention that the basic diagnostics suggested no problems, but you’ll still want to explore this a bit for yourself (as in Part I). If you want a visual approach to these basic results, you can use something like the following types of plots.\n\n\nmcmc_plot(model_baseline, type = 'areas')\n\n\n\nmcmc_plot(model_baseline, type = 'intervals')\n\n\n\n\nThe tidybayes package offers some nice options as well. It takes a bit of getting used to, but can be very handy. As an example, the following gives a visual sense of the probability of regression coefficient values beyond a chosen point of interest, in this case, arbitrarily chosen as abs(.25).\n\n\nlibrary(tidybayes)\n\n# get_variables(model_baseline) %>% as_tibble() # to see variable names as required for plotting.\n\n# grab fixed effects - intercept\ntidy_plot_data_fe = model_baseline %>%\n  spread_draws(`^b_(b|x).*`, regex = TRUE) %>% \n  pivot_longer(b_b11:b_x3, names_to = 'coefficient')\n\ntidy_plot_data_fe %>%\n  ggplot(aes(y = rev(coefficient), x = value)) +\n  geom_vline(xintercept = c(-.25, .25), color = 'gray92', size = .5) +\n  stat_dotsinterval(\n    aes(fill = stat(abs(x) < .25)),\n    quantiles = 40,\n    point_color = '#b2001d',\n    interval_color = '#b2001d',\n    interval_alpha = .6\n  ) +\n  scico::scale_fill_scico_d(begin = .2, end = .6) +\n  labs(y = '') +\n  guides(fill = 'none') +\n  theme_clean()\n\n\n\n\n\n\n\nCheck Priors\nThough we did some work to select our prior distributions beforehand, we might still be concerned about how influential our priors were. So how can we check whether our priors were informative? The following uses the bayestestR package to do a simple check of whether the posterior standard deviation is greater than 10% of the prior standard deviation2. Having an informative prior isn’t really a problem in my opinion, unless it’s more informative than you wanted. For example, shrinkage of a coefficient towards zero will generally help avoid overfitting.\n\n\nprior_summary(model_baseline)\n\n\n                  prior     class      coef group resp dpar nlpar bound       source\n           normal(0, 1)         b                                               user\n           normal(0, 1)         b       b11                             (vectorized)\n           normal(0, 1)         b       b21                             (vectorized)\n           normal(0, 1)         b        x1                             (vectorized)\n           normal(0, 1)         b        x2                             (vectorized)\n           normal(0, 1)         b        x3                             (vectorized)\n student_t(3, 2.9, 2.5) Intercept                                            default\n    student_t(10, 1, 1)        sd                                               user\n    student_t(10, 1, 1)        sd           group                       (vectorized)\n    student_t(10, 1, 1)        sd Intercept group                       (vectorized)\n    student_t(10, 1, 1)     sigma                                               user\n\nbayestestR::check_prior(model_baseline)\n\n\n    Parameter Prior_Quality\n1 b_Intercept   informative\n2       b_b11 uninformative\n3       b_b21   informative\n4        b_x1 uninformative\n5        b_x2 uninformative\n6        b_x3 uninformative\n\nThese results suggest that we might be more informative, but for the intercept, which we largely aren’t too worried about, and for the factor that is highly unbalanced (b2), but which has no obvious solution. I personally would be fine with this result, especially since we took initial care in choosing these priors. If you really wanted to, you could change the priors that were informative.\nExplore and Visualize Results\nNow that we are feeling pretty good about the results we have, we can explore the model further. We can plot covariate effects easily with brms. The conditional_effects function is what we want here. I show results for one effect below. Without interactions or other things going, on they aren’t very interesting, but it’s a useful tool nonetheless. We’ll come back to this later.\n\n\nconditional_effects(model_baseline, 'b2')\n\n\n\n\nWe can also use the hypothesis function to test for specific types of effects. By default they provide a one-sided probability and uncertainty interval. For starters, we can just duplicate what we saw in the previous summary for the b2 effect. The only benefit is to easily obtain the one-sided p-value (e.g. that b2 is less than zero) and the corresponding evidence ratio, which is just p/(1-p).\n\n\nhypothesis(model_baseline, 'b21 < 0')\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1  (b21) < 0    -0.15      0.19    -0.46     0.16       3.67      0.79     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nBut we can really try anything, which is the power of this function. As an example, the following tests whether the combined effect of our categorical covariates is greater than twice the value of the x1 effect3.\n\n\nhypothesis(model_baseline, 'abs(b11) + abs(b21) > 2*x1')\n\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (abs(b11)+abs(b21... > 0     0.91      0.17     0.65     1.21        Inf         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\nOne should get used to whatever tools are available for further understanding covariate effects or other parameters. This will likely lead to some of the more interesting discussion of your findings, or at least, notably more interesting than a standard regression table.\nModel Effectiveness\nIt is one thing to look at specific effects. but a natural question to ask is how useful our model actually is as a whole. This then suggests we need to know how to define such utility. Such an assessment definitely cannot be made with something like ‘statistical significance’. Science of any kind is nothing without prediction, so we we can start there.\nPosterior predictive checks\nPosterior predictive checks are a key component of Bayesian analysis. The prior checks we did before are just a special case of this. Here we instead use the posterior distributions of parameters to generate the data, and compare this model-implied/synthetic data to what we actually observe. Doing so can give insight to where the model succeeds and fails.\n\n\npp_check(model_baseline, nsamples = 100)\n\n\n\npp_check(model_baseline, nsamples = 10, type ='error_scatter_avg', alpha = .1)\n\n\n\n\nIn this case, we see good alignment between model and data, and no obvious pattern to the types of errors we are getting. It is often the case that we see that the model does not capture the most extreme values well, but that’s not terribly surprising. With simulated data, our situation is more pristine to begin with, but you generally can’t expect such a clean result in practice.\nAs an example, consider predictions with and without random effects. Including the cluster-specific effects for prediction appear to do better with the capturing the tails.\n\n\n\nWe can use the same approach to look at specific statistical measures of interest. For example, the following suggests our model is pretty good at capturing the minimum value, but typically underestimates the maximum value, which we noted earlier, is not especially unexpected in practice, particularly with smaller sample data.\n\n\npp_check(model_baseline, nsamples = 100, type ='stat', stat='median')\n\n\n\npp_check(model_baseline, nsamples = 100, type ='stat', stat = 'max')\n\n\n\n\nWe can define any function to use for our posterior predictive check. The following shows how to examine the 10th and 90th quantiles. Minimum and maximum values are unlikely to be captured very well due to their inherent variability, so looking at less extreme quantiles (e.g. 10th or 90th percentile) might be a better way to assess whether the model captures the tails of a distribution.\n\n\nq10 = function(y) quantile(y, 0.1)\nq90 = function(y) quantile(y, 0.9)\n\npp_check(model_baseline, nsamples = 100, type ='stat', stat = 'q90')\n\npp_check(model_baseline, nsamples = 100, type ='stat_2d', stat = c('q10', 'q90'))\n\n\n\nBayes R-squared\nIn this modeling scenario, we can examine the amount of variance accounted for in the target variable by the covariates. I don’t really recommend this beyond linear models that assume a normal distribution for the target, but people like to report it. Conceptually, it is simply a (squared) correlation of fitted values with the observed target values, so can be seen as descriptive statistic. Since we are Bayesians, we also get a ready-made interval for it, as it is based on the posterior predictive distribution. But to stress the complexity in trying to assess this, in this mixed model we can obtain the result with the random effect included (conditional) or without (unconditional). Both are reasonable ways to express the statistic, but the one including the group effect naturally will be superior, assuming the group-level variance is notable in the first place.\n\n\nbayes_R2(model_baseline)                   # random effects included\n\n\n    Estimate  Est.Error     Q2.5     Q97.5\nR2 0.4823852 0.01829985 0.446667 0.5163177\n\nbayes_R2(model_baseline, re_formula = NA)  # random effects not included\n\n\n   Estimate  Est.Error       Q2.5     Q97.5\nR2 0.116369 0.01448987 0.08753375 0.1446644\n\n# performance::r2_bayes(model_baseline)    # performance package provides both\n\n\n\nTo show the limitation of R2, I rerun the model using a restrictive prior on the intercept. Intercepts for the resulting models are different but the other fixed effects are basically the same. The R2 suggests equal performance of both models.\n\n\n\n\n\nmodel\n\n\nR2\n\n\nEst.Error\n\n\nQ2.5\n\n\nQ97.5\n\n\nbaseline\n\n\n0.116\n\n\n0.014\n\n\n0.088\n\n\n0.145\n\n\nmodified\n\n\n0.116\n\n\n0.014\n\n\n0.089\n\n\n0.144\n\n\nHowever, a posterior predictive check shows clearly the failure of the modified model to capture the data.\n\n\n\nA variant of R2, the ‘LOO’ R2, is also available via the loo_R2 function. LOO stands for leave-one-out, as in leave-one-out cross-validation. It’s based on the residuals from the leave one out predictions. You can think of it as a better way to obtain an adjusted R2 in this setting. The results suggests that the LOO R2 actually picks up the difference in models, and would be lower for the modified model, even if we included the random effects.\n\n\n\nFor more on Bayesian R2, see the resources section\nPrediction & Model Comparison\nIn general, a model is judged most by whether it has practical value. Even if we think a model is effective, there still might be another model that can do better. So it can be a good idea to have a couple of models to compare with one another. And one of the best ways to compare them is via prediction, especially by predicting on data the model wasn’t trained on to begin with.\nFor our demonstration, we will add two new models. The first adds interactions, the second adds a nonlinear relationship for one of the variables to that model, and is the closest to the underlying data generating mechanism.\n\n\nmodel_interact = update(\n  model_baseline,\n  . ~ . + b1:b2 + b2:x1,\n  cores = 4,\n  seed  = 1234\n)\n\nmodel_interact_nonlin = update(\n  model_interact,\n  . ~ . + s(x3),\n  cores = 4,\n  seed  = 1234\n)\n\n\n\nBasic prediction\nWith models in hand, let’s look at our basic predictive capabilities. We can get fitted values which include ‘confidence’ intervals, or predictions, which include ‘prediction’ intervals that include the uncertainty for a new observation. We can specify these as follows. First we create a small data set to make some predictions on. It will include both values for of the binary covariates, and the means of the numeric covariates (0).\n\n\nprediction_data = crossing(\n  b1 = 0:1,\n  b2 = 0:1,\n  x1 = 0,\n  x2 = 0,\n  x3 = 0\n)\n\n# fitted values\nhead(fitted(model_baseline))  \n\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 3.510943 0.3515038 2.817057 4.201287\n[2,] 3.784720 0.3517490 3.070184 4.497029\n[3,] 3.854369 0.3461804 3.164223 4.550382\n[4,] 4.351891 0.3518805 3.651440 5.097923\n[5,] 3.171170 0.3473440 2.513093 3.888691\n[6,] 3.900029 0.3440698 3.233128 4.602300\n\n# new predictions\ndata.frame(\n  prediction_data,\n  predict(model_baseline, newdata = prediction_data, re_formula = NA)\n)\n\n\n  b1 b2 x1 x2 x3 Estimate Est.Error      Q2.5    Q97.5\n1  0  0  0  0  0 2.702195  1.062798 0.6371021 4.726339\n2  0  1  0  0  0 2.549677  1.015919 0.5020679 4.586656\n3  1  0  0  0  0 3.472134  1.060215 1.4655644 5.618881\n4  1  1  0  0  0 3.338356  1.011263 1.3089615 5.320062\n\nIn general, we’d always like to visualize the predictions. We can do so as we did before with the conditional_effects function, which would also allow us to set specific covariate values. For the third plot of the nonlinear effect below, I modify the basic conditional effects plot that brms provides for a slightly cleaner visualization.\n\n\nconditional_effects(model_baseline, effects = 'x2', conditions = prediction_data[1,])\n\n\n\nconditional_effects(model_interact, effects = 'x1:b2')\n\n\n\ninit = conditional_effects(model_interact_nonlin, effects = 'x3', spaghetti = T)\n\n\n\n\n\n\nExpanding your story through prediction is essential to helping your audience understand the model on a practical level. You would do well to spend time looking at specific data scenarios, especially in the case of nonlinear models (e.g. GLM) and models with interactions.\nModel Comparison\nIn typical situations it is good to have competing models, and having additional models allows us to see if improvements can be made in one way or another, both to our models, and potentially to our way of thinking about them. In a general sense, we will go about things very similarly in the Bayesian context that we would elsewhere. However, we’ll also more easily apply other approaches that are not so commonly used (even if they can be).\nChoosing a model\nIn traditional contexts, we can use a specific approach to pit competing models against one another, selecting the ‘best’ model based on a particular metric, for example, AIC, cross-validation error, etc. With ‘error metrics’, the model with the lowest value is the winner. In this case, nothing is new in the Bayesian world. Here, we can use estimates like WAIC and LOOIC for model comparison, much like you would AIC to compare models in traditional frameworks. The values themselves don’t tell us much, but in comparing models, lower means less predictive error for these ‘information criteria’ metrics, which is what we want4, and since we’re Bayesian, we will even have estimates of uncertainty for these values as well. We also have cross-validation approaches (which IC metrics approximate), which we will demonstrate later.\nWith our new models added to the mix, we can now make some comparisons using loo_compare. First, we’ll add LOOIC estimates to our models, which are not estimated by default.\n\n\nmodel_baseline = add_criterion(model_baseline,  'loo')\nmodel_interact = add_criterion(model_interact, 'loo')\nmodel_interact_nonlin = add_criterion(model_interact_nonlin, 'loo')\n\n\n\nTo start, we’ll show the LOOIC result for the baseline model. We have the total expected log probability (elpd_loo) for the leave-one-out observations. We also get stuff like p_loo, which is the effective number of parameters. For those familiar with penalized maximum likelihood, these are familiar analogues. However we also get a summary regarding Pareto k values, which we’ll talk about soon.\n\n\n# example\nloo(model_baseline)\n\n\n\nComputed from 1000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -1500.3 22.0\np_loo        88.1  3.8\nlooic      3000.7 44.0\n------\nMonte Carlo SE of elpd_loo is 0.4.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     993   99.3%   298       \n (0.5, 0.7]   (ok)         7    0.7%   186       \n   (0.7, 1]   (bad)        0    0.0%   <NA>      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nLet’s now compare the baseline model to the others models using loo_compare. It shows the ‘best’ (lowest-valued) model first, followed by the others. We get the difference of each elpd vs. the lowest, also get a standard error for this difference, which you could use to help assess how different the values are statistically. Just by this standard, the model that is based on the underlying data generating mechanism is the clear winner, as we would expect.\n\n\nloo_compare(\n  model_baseline, \n  model_interact,\n  model_interact_nonlin\n)\n\n\n                      elpd_diff se_diff\nmodel_interact_nonlin   0.0       0.0  \nmodel_interact        -44.4       9.3  \nmodel_baseline        -45.7       9.6  \n\nNow let’s compare several metrics available to us. In this particular setting, all are generally in agreement in the rank order of the models, though there appears to be no meaningful difference between the baseline and interaction models.\n\n\nmodel\n\n\nR2\n\n\nloo_R2\n\n\nWAIC\n\n\nLOOIC\n\n\nELPD\n\n\nweight\n\n\nbaseline\n\n\n0.48\n\n\n0.43\n\n\n2998.65\n\n\n3000.65\n\n\n-1500.33\n\n\n1.496607e-05\n\n\ninteract\n\n\n0.49\n\n\n0.43\n\n\n2995.76\n\n\n2998.06\n\n\n-1499.03\n\n\n3.436191e-07\n\n\ninteract_nonlin\n\n\n0.53\n\n\n0.48\n\n\n2906.68\n\n\n2909.16\n\n\n-1454.58\n\n\n9.999847e-01\n\n\nFor our ultimate model comparison we want to stick to using the IC values. As far as choosing between WAIC vs. LOOIC, the latter has better diagnostics for noting whether there are potential problems in using it. In practice, they will almost always agree with one another. As we noted previously, LOOIC reflects the ELPD, and this value is used in constructing the model weights shown in the last column5. The model weights can then be used in making final predictions (i.e. model averaging), or just providing a different way for your audience to gauge which model might be preferred.\nProblems at the loo\nAfter the model issues discussed in Part I, the next most common point of confusion I see people have is with model comparison, and using LOO in particular. Part of the reason is that this is an area of ongoing research and development, and most of the tools and documentation are notably technical. Another reason is that these are not perfect tools. They can fail to show notable problems for models that are definitely misspecified, and flag models that are essentially okay. Sometimes they flag models that other indicators may suggest are better models relatively speaking, which actually isn’t a contradiction, but which may indicate an overfit situation.\nSo in general, no tool is perfect, but in the real world we have to get things, so let’s address a couple issues.\nNot so different models\nLet’s start with the case where models do not appear to perform very differently. If two models aren’t very different from one another, the usual response is to go with the simpler model. For example, if we were only comparing the baseline model vs. the interaction model, there really isn’t much difference in terms of LOOIC/ELPD. However, we will have to consider things a little differently in the Bayesian context. Consider the following two thoughts.\n\nThe general issue is that with unregularized estimation such as least squares or maximum likelihood, adding parameters to a model (or making a model more complex) leads to overfitting. With regularized estimation such as multilevel modeling, Bayesian inference, lasso, deep learning, etc., the regularization adds complexity but in a way that reduces the problem of overfitting. So traditional notions of model complexity and tradeoffs are overturned. ~ Andrew Gelman\n\n\nSometimes a simple model will outperform a more complex model… Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well. ~ Radford Neal\n\nThe take-home message here is that simpler is not always better. And to be frank, using penalized (a.k.a. regularized) approaches (e.g. lasso, ridge, mixed models) should probably be our default model in the non-Bayesian context, and it turns out that such approaches actually approximate a Bayesian one with specific priors. In the end, you may have to think about things a little more carefully, and given that you are using methods that can help avoid overfitting, you may instead lean on a more complex model with otherwise similar performing models. And that would be closer to how nature works anyway, which is always more complex than our brains can easily understand.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPareto values\nLet’s look again at the basic result from using the loo function.\n\n\nloo(model_interact)\n\n\n\nComputed from 1000 by 1000 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -1499.0 22.0\np_loo        89.9  4.0\nlooic      2998.1 44.0\n------\nMonte Carlo SE of elpd_loo is 0.4.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     995   99.5%   161       \n (0.5, 0.7]   (ok)         5    0.5%   376       \n   (0.7, 1]   (bad)        0    0.0%   <NA>      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nWe haven’t yet discussed Pareto values, but it is not uncommon to get a result with some values that are not ‘good’ or ‘ok’. If you happen to see Pareto values in the ‘bad’ or ‘very bad’ group, what does it mean? You can read the definition provided here, but it may not help many due to the background knowledge needed to parse it. However, you can just understand it as an (leave-one-out) extreme value diagnostic, and if it is a problem, it mostly means your LOOIC may not be good for comparing models.\nAs in the standard model setting, ‘outliers’ indicate model incompetence, or in other words, the model is unable to understand such observations. Unless you have reason to suspect something inherently wrong in the data (e.g. an incorrect value/typo), an outlier is a sign that your model is not able to capture the data fully. It definitely is not a reason to remove the observation!\nIf you have Pareto values > .7, you may recalculate LOOIC with the options provided by the loo function or use the reloo function, getting a better estimate that could then be used in, for example, model stacking for prediction. If you don’t discover many outliers, it probably won’t make much difference in your final estimates and conclusions, and so probably isn’t worth the trouble pursing much further. The output for Pareto values doesn’t even save the row identifying information that would make it easy to find which observations are the problem, but you can do something like the following if you need to.\n\n\npareto = loo(model_interact_nonlin)\n\nproblems = pareto$pointwise %>% \n  data.frame() %>% \n  rowid_to_column() %>% \n  filter(influence_pareto_k > .5) %>% \n  pull(rowid)\n\nmodel_interact_nonlin$data %>% \n  mutate(rank = rank(y)) %>% \n  slice(problems)\n\n\n           y b1 b2         x1         x2          x3 group rank\n1  4.8358455  0  0  1.6595205 -0.1663858  0.03627905     6  902\n2  3.9232800  0  1  0.2235292  0.0699854  0.46457410    37  748\n3  6.7825956  0  1  1.0725661 -0.6440937  1.72027097    37  999\n4  2.3935576  0  1 -0.6799355  0.2057128 -1.12517628    45  366\n5  3.0636922  1  1  0.5484042 -0.8082104 -0.48645929    64  544\n6  8.1985399  1  1  0.3636720  0.0652158  1.37603123    68 1000\n7  2.1391887  0  1 -0.8143745  0.5552497  2.07192410    69  301\n8  0.1456969  1  1 -1.0297148 -0.8834361 -1.24675264    84   31\n9 -1.1238591  1  0  0.8573758  0.6268734 -0.33549539    97    3\n\nAs we might have expected, the observations with the more extreme target values are likely to be problems (rank closer to 1 or 1000), but for some of these, there is nothing to suggest why they might be difficult, and it’s even harder to speculate in typical modeling situations with more predictors and complexity. Furthermore, outside of additional model complexity, which might then hamper interpretation, there is often little we can do about this, or at least, what we can do is generally not obvious in applied settings.\nModel Averaging\nWith the previous statistics for model comparison we can obtain relative model weights, using the model_weights function. This essentially spreads the total probability of the models across all those being compared. These weights in turn allow us to obtain (weighted) average predictions. The key idea being that we do not select a ‘best’ model, but rather combine their results for predictive purposes6.\nWe can start by comparing the first two models. Adding the interactions helped, and comparing the weights suggests that the interaction model would be contributing most to the averaged predictions.\n\nMethod: stacking\n------\n               weight\nmodel_baseline 0.288 \nmodel_interact 0.712 \n\nIf we compare the baseline to our most complex model, almost the entirety of the weight is placed on the latter.\n\n\nloo_model_weights(model_baseline, model_interact_nonlin)\n\n\nMethod: stacking\n------\n                      weight\nmodel_baseline        0.000 \nmodel_interact_nonlin 1.000 \n\nNow we compare all three, with roughly the same conclusion.\n\n\nmodel_baseline\n\n\nmodel_interact\n\n\nmodel_interact_nonlin\n\n\n1e-05\n\n\n0\n\n\n0.99998\n\n\nNow what about those average predictions? Let’s create a data frame that sets the continuous covariates at their means, and at each level of the categorical covariates. For our purposes here, we will also ignore group effects7. We then will make average predictions for those observations using pp_average.\n\n\nprediction_data = crossing(\n  b1 = 0:1,\n  b2 = 0:1,\n  x1 = 0,\n  x2 = 0,\n  x3 = 0\n)\n\naverage_predictions = pp_average(\n  model_baseline,\n  model_interact,\n  model_interact_nonlin,\n  newdata = prediction_data,\n  re_formula = NA\n)\n\n\n\n\n\nb1\n\n\nb2\n\n\nx1\n\n\nx2\n\n\nx3\n\n\nEstimate\n\n\nEst.Error\n\n\nQ2.5\n\n\nQ97.5\n\n\nBaseline Estimate\n\n\nModel Nonlin Est.\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2.71\n\n\n1.03\n\n\n0.66\n\n\n4.60\n\n\n2.69\n\n\n2.74\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n2.32\n\n\n0.96\n\n\n0.40\n\n\n4.28\n\n\n2.55\n\n\n2.30\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2.97\n\n\n1.03\n\n\n1.09\n\n\n4.97\n\n\n3.49\n\n\n2.95\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n3.07\n\n\n1.03\n\n\n1.05\n\n\n5.05\n\n\n3.35\n\n\n3.08\n\n\nAs expected, we can see that the averaged predictions are essentially the same as what we would get from the model with all the weight. In other scenarios, you may be dealing with a more nuanced result.\nCross-Validation\nIn machine learning contexts, cross-validation is the default approach to considerations of model performance. We can do so easily within the Bayesian context as well. I go ahead and do so for a single model, as well as all three models, so we can see how our previous performance metrics might change. In general, prediction on a validation set will be expected to be worse than on the training data8, but it is the better estimate of prediction error.\n\n\nlibrary(future)\n\nplan(multisession)\n\nmodel_interact_nonlin_cv = kfold(\n  model_interact_nonlin,\n  K = 5,\n  chains = 1,\n  save_fits = TRUE\n)\n\nmodel_all_cv = kfold(\n  model_baseline,\n  model_interact,\n  model_interact_nonlin,\n  K = 5,\n  chains = 1,\n  save_fits = TRUE\n)\n\nplan(sequential)\n\n\n\nWith a single cross-validation model in place, we can then make predictions with it to get the test error or other metrics of interest. As we expect, the training error, i.e. that on the original/full data is better than the test error, but the latter is the better estimate of our model error, and thus a better metric for comparing models.\n\n\ntest_predictions = kfold_predict(model_interact_nonlin_cv)\n\n\ntrain_error = yardstick::rmse_vec(truth    = model_interact_nonlin$data$y,\n                                  estimate = fitted(model_interact_nonlin)[, 1])\n\ntest_error  = yardstick::rmse_vec(truth    = test_predictions$y,\n                                  estimate = colMeans(test_predictions$yrep))\n\n\n\n\n\ntrain_error\n\n\ntest_error\n\n\n0.934\n\n\n1.036\n\n\nNow let’s revisit our LOOIC comparison, only now it is based on LOOIC via the cross-validation process. We would come to the same conclusions, but we can see that the differences, while still substantial, are not as great. In addition, other standard metrics can help validate the Bayesian-specific metrics, as RMSE does here.\n\n\n## model_all_cv$diffs\n\n\n\nmodel\n\n\nelpd_diff\n\n\nse_diff\n\n\nelpd_kfold\n\n\nrmse\n\n\nmodel_interact_nonlin\n\n\n0.00\n\n\n0.00\n\n\n-1482.69\n\n\n1.07\n\n\nmodel_interact\n\n\n-22.67\n\n\n11.30\n\n\n-1505.36\n\n\n1.09\n\n\nmodel_baseline\n\n\n-28.48\n\n\n11.71\n\n\n-1511.17\n\n\n1.09\n\n\nVariable Selection\nIf desired, we can use cross-validation to help with feature selection. We’ve already discussed why this really shouldn’t be a concern, namely because there rarely is a reason to throw out variables regardless of how minimally important they might be. Furthermore, interactions among variables are the norm, not the exception. So while a variable might not do well on its own, it can be extremely important in how it interacts with another feature.\nIn any case, one can use the projpred package to get a sense of this, and also why it can be problematic. For starters, we cannot test our nonlinear model due to its complexity9. But we can also see that we would not choose the true underlying model using this approach. In addition, for expediency I had to turn off the random effects, otherwise this would take more time than I wanted to spend for this demo (the group effect would have been the first selected). In short, be prepared for issues that might accompany the complexities in your model.\n\n\nlibrary(projpred)\n\nmodel_feature_select_cv = update(model_interact, .~. - (1|group), cores = 4)\nref_model = get_refmodel(model_feature_select_cv)  # reference model structure\n\noptions(mc.cores = parallel::detectCores())\nvar_select = cv_varsel(ref_model)   # will take a long time\n\n\n\nWith results in place we can summarize and visualize our results, similar to how we have done before.\n\n\n\n\nsize\n\n\nsolution_terms\n\n\nelpd\n\n\nelpd.se\n\n\n2\n\n\n0\n\n\nNA\n\n\n-1779.673\n\n\n21.586\n\n\n3\n\n\n1\n\n\nb1\n\n\n-1736.588\n\n\n21.959\n\n\n4\n\n\n2\n\n\nx3\n\n\n-1714.826\n\n\n21.397\n\n\n5\n\n\n3\n\n\nb1:b2\n\n\n-1715.755\n\n\n21.552\n\n\n6\n\n\n4\n\n\nb2:x1\n\n\n-1715.668\n\n\n21.700\n\n\n7\n\n\n5\n\n\nx1\n\n\n-1715.799\n\n\n21.709\n\n\n8\n\n\n6\n\n\nx2\n\n\n-1716.550\n\n\n21.714\n\n\n9\n\n\n7\n\n\nb2\n\n\n-1716.550\n\n\n21.714\n\n\n# plot predictive performance on training data\nplot(var_select, stats = c('elpd', 'rmse'))\n\n\n\n\n\n\nsolution_terms(var_select)\n\n\n[1] \"b1\"    \"x3\"    \"b1:b2\" \"b2:x1\" \"x1\"    \"x2\"    \"b2\"   \n\nmcmc_areas(\n  as.matrix(ref_model$fit),\n  pars = c('b_b11', 'b_x3', 'b_x1')\n  ) +\n  coord_cartesian(xlim = c(-2, 2))\n\n\n\n\nThe rabbit hole of model comparsion\nIf you start to look more into this, there are numerous technical articles, whole websites, and various discussions regarding how to go about it. I’m guessing many do not want to try and parse highly technical information, only to still feel confused about what to actually do. Many suggestions amount to ‘your model is probably misspecified’, but without additional thoughts on how to proceed. Some of the data issues that lead to problems are just the reality of data doing what data does. There are also suggestions that posterior predictive checks (PPCs) can be used to detect the problem. But a difficulty here is that these don’t detect anything by themselves without very specific directed action, nor do they typically have a standard metric to report, so the practical utility does have its limits. In addition, it’s not clear to me that issues or problems regarding specific statistics for model comparison (e.g. LOOIC estimation) should be a basis for altering a model, unless there is an obvious path for doing so. And let’s face it, if there was, you’d probably already be taking it.\nFor those that do want to go down the rabbit hole, I have numerous links in the resources section.\nSolutions for Model Comparison\nWhen doing model comparison, the following summarizes some basic steps you can take.\nDon’t assume you’ll have any certainty about some model being ‘best’.\nUse the metrics noted above, e.g. LOOIC, when making comparisons (not R2).\nAvoid the problem and fit the model that includes everything of interest, assuming you have a decent data size to do so. It is likely you can still learn some things about the model by comparing it to others.\nVariable selection is typically just a model comparison problem restated differently, and in a lot of cases I’ve come across, a misguided endeavor. If something is even minimally important, there is no reason to throw it out, as you’d just have worse predictions doing so. With complex models, you can’t assess one variable without consideration of others, so trying to say that one is more important than the others doesn’t really make sense.\nIf some application performance measure is obvious and available to assess, pick a model that does best in that setting.\nIf trying to select among many competing models, e.g. feature selection, you should consider why you are in this situation. If you don’t have much data, then the usual model selection criteria may lead you notably astray. If you have a lot of data, consider why you need to select a subset of predictors and not use all available. If you are somewhere in between, note that you’ll likely spend a lot more time here and still not be confident in the results. However, there are approaches, such as those in the projpred package, that might be useful, but likely will only work for simpler models.\nSummary: The Practical Approach to Bayesian Models\nFor applied analysts, just a few steps can go a long way toward making you feel better about your model. You can assess your priors in a straightforward fashion before starting, and that will not only help you have more confidence in your results, but likely also help you convince others of the results as well. Once you run the model, explore it in-depth, and take advantage of taking the Bayesian approach that allows you to more easily explore it. Many tools are available for you to asses model effectiveness, use them!\nKey to understanding any model is through prediction, which can make even complex settings more relatable. You can pit models against one another to see which performs best in a predictive sense, but note that more complexity is more realistic, and in the Bayesian world, you don’t have to choose a ‘best’ model.\nYou can use the diagnostics to further understand why and how the model isn’t perfect, which might give you some ideas on how you might do things differently in the future. However, the diagnostic criteria and other statistics may themselves have problems, the solutions of which are difficult. You should be okay with leaving some aspects of your model imperfect. There’s always next time!\nAll in all, any modeling endeavor done well, Bayesian or otherwise, will take time, and often encounter difficulties. With the right tools, working through these difficulties can lead to a better understanding of both the data (i.e. the world around us), and how we think about it!\nResources\nPrior Checks\nGelman’s prior check approach\nStan Group’s Prior Choice Recommendations\nR2\nBayesian R2 and LOO-R2 Vehtari et al. \nAndrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari (2018). R-squared for Bayesian regression models. The American Statistician, doi:10.1080/00031305.2018.1549100. Online Preprint.\nModel Comparison\nThe LOO glossary\nLOO model weights\n16 What to do if I have many high Pareto k’s?\nStan Forum Threads\nUnderstanding LOOIC\nCan WAIC/LOOIC be used to compare models with different likelihoods?\nA quick note what I infer from p_loo and Pareto km values\nRecommendations for what to do when k exceeds 0.5 in the loo package?\nImprove model with some observations Pareto >0.7\nPareto K for outlier detection 1\nPossibly incompatible metrics\nPareto values\nVehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2019). Pareto smoothed importance sampling. preprint arXiv:1507.02646\nAki Vehtari’s A quick note what I infer from p_loo and Pareto k values\nWhat to do if I have many high Pareto k?\nModel Averaging\nVehtari, A., Gelman, A., and Gabry, J. (2017a). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. 27(5), 1413–1432. doi:10.1007/s11222-016-9696-4 (journal version, preprint arXiv:1507.04544).\nYao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018) Using stacking to average Bayesian predictive distributions. Bayesian Analysis, advance publication, doi:10.1214/17-BA1091. (online).\nCross-Validation\nCV FAQ\nProjpred Quickstart\nBayesian data analysis - roaches cross-validation demo\nMisc\nProblematic Posteriors\nGabry, J. , Simpson, D. , Vehtari, A. , Betancourt, M. and Gelman, A. (2019), Visualization in Bayesian workflow. J. R. Stat. Soc. A, 182: 389-402. doi:10.1111/rssa.12378. (journal version, arXiv preprint, code on GitHub)\nUse CmdStan to save memory\nJeffrey Arnold’s Bayesian Notes has nice examples of many models and good summaries otherwise\n\nIf you were coding in Stan directly, you can run a single iteration to see if your code compiles at all.↩︎\nDaniel Lakeland proposes (as a comment in the discussion of the 10% approach outlined) an alternative approach is whether the posterior estimate falls within the 95% highest density interval of the prior. This is available via the method argument in the demonstrated function (method = 'lakeland').↩︎\nAs in the text of the output, this is the same as testing whether abs(b1) + abs(b2) - 2*x1 > 0. In this case the resulting value is greater than zero with high probability.↩︎\nSimilar to AIC, LOOIC is ~ -2*expected log posterior density (ELPD), similar to how we use -2*log likelihood (a.k.a. deviance) in standard approaches for AIC. We don’t add a penalty for parameters here, and I think this is because the regularization is already built in to the modeling process, and the number of parameters might be more difficult to define in the Bayesian context with priors.↩︎\nTechnically we can use WAIC to produce weights like we do with AIC, e.g. exp(waic) / sum(exp(all_waics)), but this isn’t recommended. The stacking approach allows similar models to share their weight, while more unique models will mostly keep their weight as additional models are added.↩︎\nSome might be familiar with Bayesian model averaging. Conceptually we aren’t changing much, but BMA assumes that one of our models is the true model, while the stacking approach underlying these weights does not. It is also different from conventional stacking in machine learning in that we are trying to average posterior predictive distributions, rather than point estimates.↩︎\nIn other words, for prediction we set re_formula = NA.↩︎\nAt the time of this writing, the underlying use of the furrr package defaults to not using a seed in it’s parallelization process, and then warns you that a seed has not been set for each repeated use of a cluster. Passing a seed through the seed argument won’t actually do anything presently here, so one will hope that furrr will change their default behavior. It’s a nuisance that can be ignored though.↩︎\nPerhaps this might be possible in a future release, but there are other complications that might make it problematic still.↩︎\n",
    "preview": "posts/2021-02-28-practical-bayes-part-ii/../../img/r_and_stan.png",
    "last_modified": "2021-03-04T12:18:09-05:00",
    "input_file": {},
    "preview_width": 816,
    "preview_height": 303
  },
  {
    "path": "posts/2020-11-30-models-by-example/",
    "title": "Models by Example",
    "description": "Roll your own to understand more.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-11-30",
    "categories": [
      "regression",
      "machine learning"
    ],
    "contents": "\nNew Book\nI’ve completed a new bookdown document, Models by\nExample, that converts most of the code from my Miscellaneous R\nrepo. I initially just wanted to update the code, but decided to use a\nmore formal approach to make it cleaner and more accessible. It’s mostly\ncomplete, though may be added to on rare occasion, and further cleaned\nas I find annoying bits here and there. Each topic contains ‘by-hand’\ndemonstration, such that you can see conceptually how a model is\nestimated, or technique employed. This can help those that want to dive\na little deeper to get a peek behind the curtain of the functions and\npackages they use, hopefully empowering them to go further with such\nmodels.\nTopics covered include the following, and I plan to post a sample\nchapter soon.\nModels\nLinear Regression\nLogistic Regression\nOne-factor Mixed Model\nTwo-factor Mixed Model\nMixed Model via ML\nProbit & Bivariate Probit\nHeckman Selection\nMarginal Structural Model\nTobit\nCox Survival\nHurdle Model\nZero-Inflated Model\nNaive Bayes\nMultinomial\nOrdinal\nMarkov Model\nHidden Markov Model\nQuantile Regression\nCubic Spline Model\nGaussian Processes\nNeural Net\nExtreme Learning Machine\nReproducing Kernel Hilbert Space Regression\nConfirmatory Factor Analysis\nBayesian\nBasics\nBayesian t-test\nBayesian Linear Regression\nBayesian Beta Regression\nBayesian Mixed Model\nBayesian Multilevel Mediation\nBayesian IRT\nBayesian CFA\nBayesian Nonparametric Models\nBayesian Stochastic Volatility Model\nBayesian Multinomial Models\nVariational Bayes Regression\nTopic Model\nEstimation\nMaximum Likelihood\nPenalized Maximum Likelihood\nL1 (lasso) regularization\nL2 (ridge) regularization\nNewton and IRLS\nNelder Mead\nExpectation-Maximization\nGradient Descent\nStochastic Gradient Descent\nMetropolis Hastings\nHamiltonian Monte Carlo\n\n\n\n",
    "preview": "posts/2020-11-30-models-by-example/../../img/gp.svg",
    "last_modified": "2022-08-07T17:44:32-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-31-micro-macro-mlm/",
    "title": "Micro-macro models",
    "description": "An analysis in the wrong direction? Predicting group level targets with lower level covariates.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-08-31",
    "categories": [
      "mixed models",
      "SEM",
      "regression",
      "factor analysis"
    ],
    "contents": "\n\nContents\nIntroduction\nPredicting Group-Level Outcomes\nIssues with Aggregate Approaches\nIssues with Adjustment\nMy Perspective\nModel Setup\nData Setup\nResults\nReliability\nSummary\n\n\n\n\nIntroduction\nEvery once in a while, it comes up that someone has clustered data, with covariates that vary at different levels, and where mixed models or similar would normally be implemented, but in which the target variable only varies at the cluster level (or ‘group’ level- I will use the terms interchangeably). Though the outcome is at the cluster level, the individual may still want to use information from lower-level/within-cluster variables. Such situations are generically referred to as micro-macro models, to distinguish between the standard setting where the target varies at the lower level (which does not require a special name). An example might be using team member traits to predict team level scores. While conceptually one wants to use all available information in a model, normally we just run a model at the cluster (team) level using summaries of variables that would otherwise vary within the cluster, for example, using mean scores or proportions. Not only is it natural, it makes conceptual sense, and as such it is the default approach. Alternatives include using the within cluster variables as predictors, but this wouldn’t be applicable except in balanced settings where they would represent the same thing for each group, and even in the balanced settings collinearity might be a notable issue. So how would we deal with this?\nPrequsites\nFor the following you should have familiarity with mixed/multilevel models, and it would help to have an understanding of factor analysis and structural equation modeling.\nPredicting Group-Level Outcomes\nCroon and van Veldhoven Croon and Veldhoven (2007) (CV) present a group-level regression model (e.g. a basic linear model) as follows.\n\\[y_g = \\beta_0 + \\xi_g\\beta_1 + Z_g\\beta_2 + \\epsilon_g\\]\nIn this depiction, \\(y_g\\) is the group level target variable, the \\(Z_g\\) represent the typical observed group-level covariates and corresponding coefficients (\\(\\beta_2\\)). If this were the entirety of the model, there would be no ‘levels’ to consider and we could use a standard model, say OLS regression. In the case we are interested in, some variables vary within these clusters, while others do not. Again, normally we might do a mixed model, but remember, \\(y_g\\) only varies at the group level, so that won’t really work.\nIn this setting then, \\(\\xi_g\\) represents an aggregated effect of the lower level variables. In standard practice it would just be the calculated mean, proportion, or some other metric with values for each cluster. In the CV depiction however, it is a latent (or perhaps several) latent variables and their corresponding effects \\(\\beta_1\\).\nIf we assume a single \\(\\xi_g\\) variable, the model for the underlying within-cluster variables is the standard latent variable model, a.k.a factor analysis. With an observed multivariate \\(x\\), e.g. repeated observations of some measure for an individual or, as before, team member scores, we have the latent linear model as follows:\n\\[\\textbf{x}_{ig} = \\xi_g\\lambda + v_{ig}\\]\nwhere \\(x_{ig}\\) are the (possibly repeated) observations \\(i\\) for a group/individual \\(g\\), \\(\\lambda\\) are the factor loadings and variances are constant. We can now see the full model as a structural equation model as follows for a situation with five observations per group.\n\n\n\n\nFigure 1: The structural equation model\n\n\n\nIssues with Aggregate Approaches\nCV suggest that simple aggregation, e.g. using a group mean, will result in problems, specifically biased estimates. They simulate data that varies the number of groups/clusters, the number of observations within groups, the intraclass correlation of observations within a group. In most of the cases they explore, the bias for the aggregate mean effect is notable, and there is sometimes small bias for the group level covariates, if they are collinear with the aggregate covariate. We will duplicate this approach later.\nAn approach to adjusting the group mean is offered by CV, with the structural model implied. These adjusted group means, or in their parlance, best linear unbiased predictors (BLUPs), result in a bias-free result. The notion of a BLUP will be familiar to those who use mixed models, as that is what the random effects are for a standard linear mixed model. As such, later on we’ll take a look at using a mixed model as a possible solution. In any case, once the adjusted means are calculated, you can then run your standard regression with the bias mostly eliminated.\nIssues with Adjustment\nIt turns out the the weighting calculation proffered by CV is somewhat complicated, not easily implemented, and rarely used. Foster-Johnson & Kromrey Foster-Johnson and Kromrey (2018) (FJK) looked further into its utility, as well as other possible solutions that might be easier to implement. As far as type I error rate goes, FJK demonstrated that using the CV adjusted group means offers no advantage over unadjusted, and even showed less statistical power. They suggested that a standard correction for heteroscedasticity (White’s) might be enough. In applying corrected standard errors for both unadjusted and adjusted group means, FJK found there to be additional power for both approaches, but if anything still favored the standard group mean. What’s more, while the bias remained, there was actually notable variability in the adjusted mean results. FJK’s final recommendation was to use the usual group means with robust standard errors, easily implemented in any statistical package.\nI will add that the adjustment still uses an underlying factor model of equal loadings and variances across the observations. For notably reliable scales this might be reasonable, but it isn’t a necessity. In repeated measures settings for example, we might see decreased variability across time, or practice effects, which might make the assumption more tenuous.\nMy Perspective\nMy first glance at the issue raised by CV immediately called to mind the standard measurement model typically employed for factor analysis, i.e. a latent linear model. So my interpretation was that we are simply talking about a well known fact in measurement: that reliability of the measure is key in using a mean or sum score, and decreased reliability attenuates the correlation among the variables in question. I even did a simulation demonstrating the problem a while back. So in this case, I’m interested in the issue from a reliability perspective.\nIt turns out that factor models and mixed models share a lot in common. Those familiar with growth curve models know that they are equivalent to mixed models, but the comparison is a more general one of random effects methods. To demonstrate the equivalence, I’ll use a cleaned up version of the Big 5 data in the psych package. Specifically, we’ll use the five items that belong to the Agreeableness measure.\nFirst we make the data in both wide and long. The former makes it amenable to factor analysis, while the latter is what we need for a mixed model.\n\n\n# data prep for long and wide format\n\nagree_df = noiris::big_five %>% \n  select(A1:A5) %>% \n  drop_na()\n\nagree_long = agree_df %>% \n  mutate(id = factor(row_number())) %>% \n  pivot_longer(-id, names_to = 'variable', values_to = 'value')\n\n\n\nThe standard factor model will have to be constrained to have equal loadings and item variances. In addition, we’ll estimate the intercepts, but otherwise this is your basic factor analysis.\n\n\n # or use growth() to save some of the model tedium\ncfa_model_agree = \"\n  agree =~ a*A1 + a*A2 + a*A3 + a*A4 + a*A5\n  \n  A1 ~~ var*A1\n  A2 ~~ var*A2\n  A3 ~~ var*A3\n  A4 ~~ var*A4\n  A5 ~~ var*A5\n\"\n\nlibrary(lavaan)\n\ncfa_fit_agree = cfa(cfa_model_agree, data = agree_df, meanstructure = T) \n\nsummary(cfa_fit_agree)\n\n\nlavaan 0.6-7 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of free parameters                         11\n  Number of equality constraints                     4\n                                                      \n  Number of observations                          2709\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               744.709\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  agree =~                                            \n    A1         (a)    1.000                           \n    A2         (a)    1.000                           \n    A3         (a)    1.000                           \n    A4         (a)    1.000                           \n    A5         (a)    1.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .A1               -2.412    0.026  -94.340    0.000\n   .A2                4.797    0.026  187.611    0.000\n   .A3                4.599    0.026  179.859    0.000\n   .A4                4.682    0.026  183.107    0.000\n   .A5                4.551    0.026  177.982    0.000\n    agree             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .A1       (var)    1.201    0.016   73.607    0.000\n   .A2       (var)    1.201    0.016   73.607    0.000\n   .A3       (var)    1.201    0.016   73.607    0.000\n   .A4       (var)    1.201    0.016   73.607    0.000\n   .A5       (var)    1.201    0.016   73.607    0.000\n    agree             0.571    0.022   25.621    0.000\n\nWhen we run the mixed model, we get the same variance and intercept estimates.\n\n\nlibrary(lme4)\nlibrary(mixedup) # for post-processing\n\nmixed_fit = lmer(value ~ -1 + variable + (1 |id), data = agree_long,  REML = FALSE)\nsummarise_model(mixed_fit, digits = 3)\n\n\nComputing profile confidence intervals ...\n\nVariance Components:\n    Group    Effect Variance    SD SD_2.5 SD_97.5 Var_prop\n       id Intercept    0.571 0.755  0.727   0.785    0.322\n Residual              1.201 1.096  1.081   1.111    0.678\n\nFixed Effects:\n       Term  Value    SE       t P_value Lower_2.5 Upper_97.5\n variableA1 -2.412 0.026 -94.340   0.000    -2.462     -2.362\n variableA2  4.797 0.026 187.611   0.000     4.747      4.847\n variableA3  4.599 0.026 179.859   0.000     4.549      4.649\n variableA4  4.682 0.026 183.107   0.000     4.632      4.732\n variableA5  4.551 0.026 177.982   0.000     4.501      4.601\n\nWe can also see that the estimated factor scores agree with the estimated random effects.\n\n\nindex\n\n\nEstimated.Factor.Scores\n\n\nEstimated.Random.Effects\n\n\n1\n\n\n-0.453\n\n\n-0.453\n\n\n2\n\n\n-0.312\n\n\n-0.312\n\n\n3\n\n\n-0.594\n\n\n-0.594\n\n\n4\n\n\n-0.031\n\n\n-0.031\n\n\n5\n\n\n-0.453\n\n\n-0.453\n\n\n6\n\n\n-0.031\n\n\n-0.031\n\n\n2704\n\n\n-0.453\n\n\n-0.453\n\n\n2705\n\n\n-1.720\n\n\n-1.720\n\n\n2706\n\n\n-0.312\n\n\n-0.312\n\n\n2707\n\n\n-0.453\n\n\n-0.453\n\n\n2708\n\n\n-1.297\n\n\n-1.297\n\n\n2709\n\n\n-1.157\n\n\n-1.157\n\n\nUsually when the term BLUP comes up it is in reference to the random effects estimated from a linear mixed model. As such, I thought it might be interesting to see how a mixed or factor model might be used to deal with the bias. I also thought it was a bit odd that neither CV nor FJK actually conduct the implied SEM (but see the paper co-authored by the lavaan package author Devlieger, Mayer, and Rosseel (2016)), so I wanted to look at that too.\nModel Setup\nFor our demonstration, I will create some data as CV did and run a variety of models to see what we get. My focus is on bias, not coverage or power, as I think FJK covered those aspects plenty. The models in particular are:\nStandard linear model: a basic group level analysis using unadjusted means.\nRandom effects: a group level model using estimated factor scores using lavaan, or the BLUPs from lme4, or those with heterogeneous variance via glmmTMB1. These involved a two-step approach, with the factor/mixed model followed by the standard linear model.\nStructural equation model: A full, single-step SEM via lavaan. This model has the ability to account for the correlation of the Z and latent variable. It is exactly as CV depict in their Figure 1 and Figure 1 above.\nAdjusted means: Use CV’s approach\nData Setup\nI made a function2 to create data with the values shown in CV (p. 52) for a single aggregate \\(X\\) and single group-level covariate \\(Z\\). Using their notation, the model that generates the data is the following:\n\\[y_g = .3 + .3Z_g + .3\\xi_g + \\epsilon_g\\] \\[x_{ig} = \\xi + \\nu_g\\]\nAs there, \\(\\sigma^2_\\epsilon\\) is .35. While they look at a variety of situations, I’ll just consider a single scenario for our purposes, where the correlation of the \\(Z\\) and \\(\\xi\\) was .3, the intraclass correlation of the observed \\(x_{ig}\\) was .1 (i.e. \\(\\sigma^2_\\nu\\) = 9), the number of groups was 100 and the number of observations per group was balanced at 10 (row 16 of their table 1). I simulated 1000 such data sets so that we could examine the mean value of the estimated coefficients. I first started by analyzing the result with a factor analysis, and if there are any problems such as negative variances or lack of convergence, the data is regenerated, as that will also help with any issues the mixed model would have. So the final 1000 data sets don’t have convergence issues or other problems that might make the results a little wonky.\n\n\n\n\n\n\nResults\nHere are the results. We can first take a peek at the estimated scores from the two-step approaches. The CV adjustment appears closely matched to the true score at first, but we see it’s range is very wild, which is what FJK found also. Interestingly, the BLUPs from the mixed models have less variance than the true scores. The factor score is in keeping with the BLUPs, but appears also to have notable extremes, but far less than the CV adjustment. We’ll talk about why these extremes may arise later.\n\n\nTable 1: Estimated scores\n\n\nVariable\n\n\nN\n\n\nMean\n\n\nSD\n\n\nMin\n\n\nQ1\n\n\nMedian\n\n\nQ3\n\n\nMax\n\n\n% Missing\n\n\nTrue\n\n\n1e+05\n\n\n0.01\n\n\n1.00\n\n\n-4.31\n\n\n-0.67\n\n\n0\n\n\n0.68\n\n\n4.44\n\n\n0\n\n\nCV Adj\n\n\n1e+05\n\n\n0.01\n\n\n1.07\n\n\n-22.97\n\n\n-0.65\n\n\n0\n\n\n0.66\n\n\n25.64\n\n\n0\n\n\nUnadjusted\n\n\n1e+05\n\n\n0.00\n\n\n1.39\n\n\n-6.32\n\n\n-0.93\n\n\n0\n\n\n0.94\n\n\n6.96\n\n\n0\n\n\nBLUP_mixed\n\n\n1e+05\n\n\n0.00\n\n\n0.74\n\n\n-4.20\n\n\n-0.48\n\n\n0\n\n\n0.48\n\n\n3.98\n\n\n0\n\n\nBLUP_mixed_hetvar\n\n\n1e+05\n\n\n0.00\n\n\n0.74\n\n\n-4.18\n\n\n-0.48\n\n\n0\n\n\n0.47\n\n\n3.97\n\n\n0\n\n\nFactor Score\n\n\n1e+05\n\n\n0.00\n\n\n0.87\n\n\n-7.29\n\n\n-0.47\n\n\n0\n\n\n0.46\n\n\n6.54\n\n\n0\n\n\nNow let’s look at the bias in the estimates.\n\n\nTable 2: Percent bias\n\n\nModel\n\n\nIntercept\n\n\nZ\n\n\nX\n\n\nUnadjusted\n\n\n0.149\n\n\n14.494\n\n\n-49.877\n\n\nBLUP_mixed\n\n\n0.051\n\n\n14.494\n\n\n-1.828\n\n\nBLUP_mixed_hetvar\n\n\n0.053\n\n\n14.647\n\n\n-1.673\n\n\nFactor Score\n\n\n0.063\n\n\n16.502\n\n\n11.968\n\n\nCV Adj\n\n\n-0.663\n\n\n-2.368\n\n\n7.992\n\n\nSEM\n\n\n-0.017\n\n\n-1.777\n\n\n4.744\n\n\nTrue\n\n\n-0.567\n\n\n-0.380\n\n\n0.513\n\n\nThe results suggest a couple things. First, the results of CV were duplicated for the unadjusted setting, where the group level covariate has a slight bias upward, but the aggregate is severely downwardly biased3. We can also see that a two-step approach using BLUPs from a mixed model (with or without heterogeneous variances), or factor scores, either eliminate or notably reduce the bias for the aggregate score, but still have issue with the group level covariate. This is because of the correlation between the group level and lower level covariates, which if zero, would result in no bias, and has long been a known issue with mixed models. The factor scores had some very wild results at times, even after overcoming basic inadmissible results. In the end, we see that the calculated adjustment and SEM both essentially eliminate the bias by practical standards. It is worth noting that the bias for either the factor analysis or SEM would be completely eliminated if the model adds a regression of the latent variable onto the group level covariate \\(Z\\).\nNote that in practice, a two-step approach, such as using the mixed model BLUPs or factor scores, comes with the same issue of using an estimate rather than observed score that we have using the mean. Even if there is no bias, the estimated uncertainty would be optimistic as it doesn’t take into account the estimation process. This uncertainty decreases with the number of observations per group (or number of items from the factor analytic perspective), but would technically need to be dealt with, e.g. using ‘factor score regression’ Devlieger, Mayer, and Rosseel (2016) or more simply, just doing the SEM.\n\n\n\nReliability\n\n\n\n\n\n\nInterestingly, if we look at the reliability of the measure, we shouldn’t be surprised at the results. Reliability may be thought of as the amount of variance in an observed score that is true score variance Revelle and Condon (2019). Since the underlying construct is assumed unidimensional, we can examine something like coefficient \\(\\alpha\\), which gives a sense of how reliable the mean or total score would be. Doing so reveals a fairly poor measure for 10 observations per group under the CV settings. The mean coefficient \\(\\alpha\\) is 0.52, the max of which is 0.74, which, from a measurement model perspective, would be unacceptable4. This is all to say that we have rediscovered attenuation in correlation due to (lack of) reliability, something addressed by Spearman over a century ago5.\nIn actual repeated measures, or with constructed scales, it’s probably unlikely we would have this poor of a measure. Indeed, if we think a mean is appropriate in the first place, we are probably assuming that the scores are something that can be meaningfully combined in the first place, because if a latent construct doesn’t actually explain the observations well, then what is the point of estimating it?\n\n\n\n\n\n\nIn our current context, we can create a more reliable measure by decreasing the variance value for \\(\\sigma^2_\\nu\\) which is the residual variance for the observed items at the lower level. Decreasing it from 9 to 1, puts the observed scores in a notably better place (\\(\\alpha\\) = 0.91), and if we actually have a reliable measure (or even just increase the number of observations per group, as noted by CV), the results show hardly any bias for the group level effect and a near negligible one for the mean effect.\n\n\nTable 3: Percent bias\n\n\nModel\n\n\nIntercept\n\n\nZ\n\n\nX\n\n\nUnadjusted\n\n\n-0.011\n\n\n2.698\n\n\n-9.651\n\n\n\n\n\nSummary\nIn the end we relearn a valuable, but very old lesson. The take home story here, at least to me, is to have a reliable measure and/or get more observations per group if you can, which would be the same advice for any clustered data situation. If you do have a reliable measure, such as a proportion of simple counts, or a known scale with good properties, using the mean should not give you too much pause. As a precaution, you might go ahead and use White’s correction as suggested by FJK. If you have enough data and the model isn’t overly complicated, consider doing the SEM.\nlast updated: 2021-03-13\n\n\nCroon, Marcel A, and Marc JPM van Veldhoven. 2007. “Predicting Group-Level Outcome Variables from Variables Measured at the Individual Level: A Latent Variable Multilevel Model.” Psychological Methods 12 (1): 45.\n\n\nDevlieger, Ines, Axel Mayer, and Yves Rosseel. 2016. “Hypothesis Testing Using Factor Score Regression: A Comparison of Four Methods.” Educational and Psychological Measurement 76 (5): 741–70.\n\n\nFoster-Johnson, Lynn, and Jeffrey D Kromrey. 2018. “Predicting Group-Level Outcome Variables: An Empirical Comparison of Analysis Strategies.” Behavior Research Methods 50 (6): 2461–79.\n\n\nRevelle, William, and David M Condon. 2019. “Reliability from \\(\\alpha\\) to \\(\\omega\\): A Tutorial.” Psychological Assessment 31 (12): 1395.\n\n\nI wasn’t sure in the mixed model whether to include the item and or group level Z as fixed effects. Results did not change much, so I went with a mixed model with no fixed effects to make them closer to the scale of the mean scores.↩︎\nAll code is contained within the R markdown file that produced this post.↩︎\nFor those who may not have access to the article, the values for percentage bias in CV were as follows: for the unadjusted model, the bias for the coefficients under these conditions was 0.6, 15.3, -50.4, and for the adjusted model, -1.1, -1.3, 5.0.\n\n↩︎\nA typical cutoff for coefficient \\(\\alpha\\) for a good measure is .8. We can actually use a ‘G-theory’ approach and calculate this by hand \\(\\frac{1}{1+9/10}\\), where 1 is the variance CV fixed for the true score, and 9 is residual variance. \\(\\frac{1}{1+9}\\) is the \\(\\rho_x\\), i.e. intraclass correlation, that they have in Table 1. In the better scenario \\(\\rho_x\\) = \\(\\frac{1}{1+4}\\) = .2 and the reliability is \\(\\frac{1}{1+4/10}\\) = .71, which is notably better, though still substandard. Even then we can see from their table dramatic decreases in bias from that improvement in reliability.↩︎\nThe lack of reliability is likely the culprit behind the wider range in the estimated factor scores as well.↩︎\n",
    "preview": "posts/2020-08-31-micro-macro-mlm/../../img/micromacro/multilevel.png",
    "last_modified": "2021-04-13T14:19:58-04:00",
    "input_file": {},
    "preview_width": 2252,
    "preview_height": 1112
  },
  {
    "path": "posts/2020-07-10-eda/",
    "title": "Exploratory Data Analysis",
    "description": "Recently, Staniak & Biecek (2019) wrote an article in the R Journal exploring several of such packages, so I thought I'd try them out for myself, and take others along with me for that ride.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-07-10",
    "categories": [
      "exploratory data analysis"
    ],
    "contents": "\n\nContents\nIntroduction\n\nIntroduction\nIn R there are many tools available to help you dive in and explore your data. However, in consulting I still see a lot of people using base R’s table and summary functions, followed by a lot of work to get the result into a more presentable format. My own frustrations led to me creating a package (tidyext) for personal use in this area. While that suits me fine, there are tools that can go much further with little effort. Recently, Staniak & Biecek @staniak2019landscape wrote an article in the R Journal exploring several of such packages, so I thought I’d try them out for myself, and take others along with me for that ride.\nAs this will be a workshop/demo, I’ve created a separate repo and document to make it easier to find, so here is the link: https://m-clark.github.io/exploratory-data-analysis-tools/\nThe packages demoed are:\narsenal\nDataExplorer\ndataMaid\ngtsummary\njanitor (not explored in the previous article)\nSmartEDA\nsummarytools\nvisdat\n\n\n\n\n\n\n",
    "preview": "posts/2020-07-10-eda/../../img/eda.png",
    "last_modified": "2021-07-16T08:59:33-04:00",
    "input_file": {},
    "preview_width": 1340,
    "preview_height": 953
  },
  {
    "path": "posts/2020-06-15-predict-with-offset/",
    "title": "Predictions with an offset",
    "description": "Reconciling R and Stata Approaches",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-06-16",
    "categories": [
      "regression",
      "GLM"
    ],
    "contents": "\nTable of Contents\nIntroduction\nGet some data\nModel\nemmeans\nStata: basic marginsR by hand\nemmeans\nmargins\n\nStata: overR by hand\nemmeans\nmargins\n\nStata: dydxCategorical Covariate\nContinuous predictor\nR by hand\nemmeans\nmargins\n\nOther complications\nR Packages\nSummary\nResourcesReference\nNotes\n\nAppendix\nIntroduction\nGetting predictions in R is and always has been pretty easy for the vast majority of packages providing modeling functions, as they also provide a predict method for the model objects. For those in the Stata world, they typically use margins for this, but when they come to R, there is no obvious option for how to go about it in the same way1. Likewise, some in the R world catch a whiff of Stata’s margins and would want something similar, but may not be sure where to turn.\nA little digging will reveal there are several packages that will provide the same sort of thing. In addition, there are numerous resources for both R and Stata for getting marginal results (i.e. predictions). However, here we note the issues that arise when models include an offset. Offsets are commonly used to model rates when the target variable is a count, but are used in other contexts as well. The Stata documentation for the margins command offers no specific details of how the offset/exposure is treated, and some R packages appear not to know what to do with it, or offer few options to deal with it. So even when the models are identical, marginal estimates might be different in R and Stata. Here we’ll try to sort some of this out.\nGet some data\nWe will use the Insurance data from the MASS package which most with R will have access to. From the helpfile:\n\nThe data given in data frame Insurance consist of the numbers of policyholders of an insurance company who were exposed to risk, and the numbers of car insurance claims made by those policyholders.\n\nDistrict: district of residence of policyholder (1 to 4): 4 is major cities.\nGroup: group of car with levels <1 litre, 1–1.5 litre, 1.5–2 litre, >2 litre\nAge: the age of the insured in 4 groups labelled <25, 25–29, 30–35, >35\nHolders: number of policyholders\nClaims: number of claims\nWe do a bit of minor processing, and I save the data as a Stata file in case anyone wants to play with it in that realm.\n\n\nlibrary(tidyverse)\n\nset.seed(123)\n\ninsurance = MASS::Insurance %>%\n  rename_all(tolower) %>%\n  mutate(\n    # create standard rather than ordered factors for typical output\n    age   = factor(age, ordered = FALSE),\n    group = factor(group, ordered = FALSE),\n    \n    # create a numeric age covariate for later\n    age_num = case_when(\n      age == '<25'   ~ sample(18:25, n(), replace = T),\n      age == '25-29' ~ sample(25:29, n(), replace = T),\n      age == '30-35' ~ sample(30:35, n(), replace = T),\n      age == '>35'   ~ sample(36:75, n(), replace = T),\n    ),\n    \n    # for stata consistency\n    ln_holders = log(holders)\n  )\n\n\nhaven::write_dta(insurance, 'data/insurance.dta')\n\nLet’s take a quick peek to get our bearings.\n\n\n$`Numeric Variables`\n    Variable  N   Mean     SD  Min    Q1 Median     Q3     Max Missing\n1    holders 64 364.98 622.77  3.0 46.75 136.00 327.50 3582.00       0\n2     claims 64  49.23  71.16  0.0  9.50  22.00  55.50  400.00       0\n3    age_num 64  35.23  15.83 18.0 25.00  29.50  35.50   75.00       0\n4 ln_holders 64   4.90   1.48  1.1  3.84   4.91   5.79    8.18       0\n\n$`Categorical Variables`\n# A tibble: 12 x 4\n   Variable Group  Frequency   `%`\n   <chr>    <chr>      <int> <dbl>\n 1 district 1             16    25\n 2 district 2             16    25\n 3 district 3             16    25\n 4 district 4             16    25\n 5 group    <1l           16    25\n 6 group    >2l           16    25\n 7 group    1-1.5l        16    25\n 8 group    1.5-2l        16    25\n 9 age      <25           16    25\n10 age      >35           16    25\n11 age      25-29         16    25\n12 age      30-35         16    25\n\nModel\nStarting out, we run a model in as simple a form as possible. I use just a standard negative binomial with a single covariate age, so we can clearly see how the ouptut is being produced. Note that age has four categories as seen above.\n\n\nnb_glm_offset = MASS::glm.nb(claims ~  age + offset(ln_holders), data = insurance)\n\nsummary(nb_glm_offset)\n\nCall:\nMASS::glm.nb(formula = claims ~ age + offset(ln_holders), data = insurance, \n    init.theta = 28.40119393, link = log)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.29753  -0.79833  -0.04613   0.82465   3.00825  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.59233    0.09071 -17.554  < 2e-16 ***\nage25-29    -0.12697    0.11743  -1.081   0.2796    \nage30-35    -0.25340    0.11558  -2.193   0.0283 *  \nage>35      -0.41940    0.10583  -3.963  7.4e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(28.4012) family taken to be 1)\n\n    Null deviance: 86.761  on 63  degrees of freedom\nResidual deviance: 67.602  on 60  degrees of freedom\nAIC: 444.38\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  28.40 \n          Std. Err.:  9.80 \n\n 2 x log-likelihood:  -434.385 \n\nNow we run it with Stata. We get the same result, so this means we can’t get different predictions if we do the same thing in both R or Stata2.\n\n\nnbreg claims i.age, offset(ln_holders) nolog\n\nNegative binomial regression                    Number of obs     =         64\n                                                LR chi2(3)        =      15.73\nDispersion     = mean                           Prob > chi2       =     0.0013\nLog likelihood =  -217.1925                     Pseudo R2         =     0.0349\n\n------------------------------------------------------------------------------\n      claims |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         age |\n      25-29  |  -.1269666   .1182871    -1.07   0.283    -.3588049    .1048718\n      30-35  |  -.2533991    .116535    -2.17   0.030    -.4818034   -.0249948\n        >35  |  -.4193953   .1068907    -3.92   0.000    -.6288973   -.2098933\n             |\n       _cons |  -1.592325   .0913742   -17.43   0.000    -1.771415   -1.413235\n  ln_holders |          1  (offset)\n-------------+----------------------------------------------------------------\n    /lnalpha |  -3.346431   .3544212                     -4.041084   -2.651778\n-------------+----------------------------------------------------------------\n       alpha |   .0352098   .0124791                      .0175784    .0705257\n------------------------------------------------------------------------------\nLR test of alpha=0: chibar2(01) = 38.33                Prob >= chibar2 = 0.000\n\nemmeans\nFirst let’s use emmeans, a very popular package for getting estimated marginal means, to get the predicted counts for each age group.\n\n\nlibrary(emmeans)\n\nemmeans(\n  nb_glm_offset,\n  ~ age,\n  type = \"response\",\n  offset = mean(insurance$ln_holders)  #  default\n)\n\n\n\nage\n\n\nresponse\n\n\nSE\n\n\ndf\n\n\nasymp.LCL\n\n\nasymp.UCL\n\n\n<25\n\n\n27.437\n\n\n2.489\n\n\nInf\n\n\n22.968\n\n\n32.776\n\n\n25-29\n\n\n24.166\n\n\n1.802\n\n\nInf\n\n\n20.880\n\n\n27.969\n\n\n30-35\n\n\n21.295\n\n\n1.525\n\n\nInf\n\n\n18.507\n\n\n24.505\n\n\n>35\n\n\n18.038\n\n\n0.983\n\n\nInf\n\n\n16.211\n\n\n20.072\n\n\nHow is this result obtained? It is just the prediction at each value of the covariate, with the offset held at its mean. We can duplicate this result by using the predict method and specifying a data frame with the values of interest.\n\n\nnd = data.frame(\n  age = levels(insurance$age),\n  ln_holders = mean(insurance$ln_holders)\n)\n\npredict(\n  nb_glm_offset,\n  newdata = nd,\n  type = 'response'\n)\n\n\n\nprediction\n\n\n27.437\n\n\n24.166\n\n\n21.295\n\n\n18.038\n\n\nAs an explicit comparison, the intercept represents group ‘<25’, and if we exponentiate and add the mean offset we get:\n\\(exp(Intercept + \\overline{ln\\_holders}) = e^{-1.59+4.90} = \\qquad\\) 27.437\nStata: basic margins\nNow let’s look at Stata. First we want just the basic margins output.\n\n\nmargins age \n\nAdjusted predictions                            Number of obs     =         64\nModel VCE    : OIM\n\nExpression   : Predicted number of events, predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         age |\n        <25  |   74.25681   6.785153    10.94   0.000     60.95815    87.55546\n      25-29  |   65.40266   4.936493    13.25   0.000     55.72731    75.07801\n      30-35  |   57.63502   4.195113    13.74   0.000     49.41275    65.85729\n        >35  |   48.81971   2.747836    17.77   0.000     43.43405    54.20537\n------------------------------------------------------------------------------\n\nThese values, while consistent in pattern, are much different than the emmeans output, so what is going on?\nR by hand\nIn this model, we only have the age covariate and the offset, so there really isn’t much to focus on besides the latter. To replicate the Stata output in R, we will use all values of the offset for every level of age, and subsequently get an average prediction for each age group. First, we create a data frame for prediction using expand.grid, get the predictions for all those values, then get mean prediction per group.\n\n\npredictions = \n  expand.grid(\n    age        = levels(insurance$age), \n    ln_holders = insurance$ln_holders\n  ) %>% \n  mutate(prediction = predict(nb_glm_offset, newdata = ., type = 'response')) %>% \n  group_by(age) %>% \n  summarise(avg_prediction = mean(prediction)) \n\n\n\nage\n\n\navg_prediction\n\n\n<25\n\n\n74.257\n\n\n25-29\n\n\n65.403\n\n\n30-35\n\n\n57.635\n\n\n>35\n\n\n48.820\n\n\nemmeans\nThe emmeans doesn’t appear to allow one to provide all values of the offset, as adding additional values just applies them to each group and then recycles. In this case, it would just use the first four values of ln_holders for each age group respectively, which is not what we want.\n\n\nemmeans(\n  nb_glm_offset,\n  ~ age,\n  type = \"response\",\n  offset = insurance$ln_holders\n)\n\n age   response    SE  df asymp.LCL asymp.UCL\n <25       40.1  3.64 Inf      33.6      47.9\n 25-29     47.3  3.53 Inf      40.9      54.8\n 30-35     38.8  2.78 Inf      33.8      44.7\n >35      224.7 12.25 Inf     201.9     250.0\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\ninsurance$ln_holders[1:4]\n\n[1] 5.283204 5.575949 5.505332 7.426549\n\nIf we add the offset to the spec argument, it still just fixes it at the mean (and I tried variations on the spec). So at least using the standard approaches with this model does not appear to give you the same thing as Stata.\n\n\nemmeans(\n  nb_glm_offset,\n  ~ age + offset(ln_holders),\n  type = \"response\"\n)\n\n age   ln_holders response    SE  df asymp.LCL asymp.UCL\n <25          4.9     27.4 2.489 Inf      23.0      32.8\n 25-29        4.9     24.2 1.802 Inf      20.9      28.0\n 30-35        4.9     21.3 1.525 Inf      18.5      24.5\n >35          4.9     18.0 0.983 Inf      16.2      20.1\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\nUnfortunately Stata has the opposite issue. Trying to set the offset to the mean results in an error, and using atmeans doesn’t change the previous result.\n\n\nmargins age, atmeans\nmargins age, at(ln_holders = 10.27)\n\nAdjusted predictions                            Number of obs     =         64\nModel VCE    : OIM\n\nExpression   : Predicted number of events, predict()\nat           : 1.age           =         .25 (mean)\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         age |\n        <25  |   74.25681   6.785153    10.94   0.000     60.95815    87.55546\n      25-29  |   65.40266   4.936493    13.25   0.000     55.72731    75.07801\n      30-35  |   57.63502   4.195113    13.74   0.000     49.41275    65.85729\n        >35  |   48.81971   2.747836    17.77   0.000     43.43405    54.20537\n------------------------------------------------------------------------------\n\nvariable ln_holders not found in list of covariates\nr(322);\n\nend of do-file\nr(322);\n\nmargins\nThe margins package explicitly attempts to duplicate Stata’s margins command, but here we can see it has an issue with the offset.\n\n\nlibrary(margins)\n\nmargins(\n  nb_glm_offset,\n  variables = 'age',\n  type = \"response\"  # default\n)\n\nError in offset(ln_holders): could not find function \"offset\"\n\nThe offset function is part of the stats package of the base R installation, so I tried rerunning the model using stats::offset, but this makes the offset just like any other covariate, i.e. it did not have a fixed coefficient of 1. Changing the model to a standard glm class with poisson and moving the offset to the offset argument did work, and produces the results for the differences in predictions for each group from the reference group (dydx in Stata), but we’ll visit this type of result later3. However, the offset argument is not available to glm.nb, so we’re stuck for now.\n\n\npois_glm_offset = glm(\n  claims ~  age,\n  data   = insurance,\n  family = 'poisson',\n  offset = ln_holders\n)\n\nmargins(\n  pois_glm_offset,\n  variables = 'age',\n  type = \"response\"  # default\n)\n\n age25-29 age30-35 age>35\n   -10.32   -18.46 -28.79\n\nexpand.grid(\n    age        = levels(insurance$age), \n    ln_holders = insurance$ln_holders\n  ) %>% \n  mutate(prediction = predict(pois_glm_offset, newdata = ., type = 'response')) %>% \n  group_by(age) %>% \n  summarise(avg_prediction = mean(prediction)) %>% \n  mutate(diff = avg_prediction - avg_prediction[1]) \n\n# A tibble: 4 x 3\n  age   avg_prediction  diff\n  <fct>          <dbl> <dbl>\n1 <25             73.4   0  \n2 25-29           63.1 -10.3\n3 30-35           55.0 -18.5\n4 >35             44.7 -28.8\n\nStata: over\nIn Stata, with categorical values we can also use the over approach. What do we get in this case?\n\n\nmargins, over(age)\n\nPredictive margins                              Number of obs     =         64\nModel VCE    : OIM\n\nExpression   : Predicted number of events, predict()\nover         : age\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         age |\n        <25  |   14.47052   1.322232    10.94   0.000       11.879    17.06205\n      25-29  |   26.16218   1.974682    13.25   0.000     22.29188    30.03249\n      30-35  |   29.67738   2.160145    13.74   0.000     25.44358    33.91119\n        >35  |   141.0984   7.941774    17.77   0.000     125.5328    156.6639\n------------------------------------------------------------------------------\n\nThese are very different from our previous results for Stata, so what’s happening here?\nR by hand\nThis can be duplicated with the predict function as follows. While similar to the previous approach, here only the observed values of the offset for each group are used. We then make predictions for all values of the data and average them by group.\n\n\npredictions_over = insurance %>%\n  group_by(age) %>%\n  group_modify(\n    ~ data.frame(\n        prediction = mean(\n          predict(nb_glm_offset, newdata = ., type = 'response')\n        )\n      ),\n    keep = TRUE\n  ) \n\n\n\nage\n\n\nprediction\n\n\n<25\n\n\n14.471\n\n\n25-29\n\n\n26.162\n\n\n30-35\n\n\n29.677\n\n\n>35\n\n\n141.098\n\n\nThe pattern is actually in the opposite direction, which is unexpected, but probably just reflects the fact that we just don’t have much data. However, it’s good to note that these respective approaches would not necessarily tell you the same thing.\nemmeans\nI currently don’t know of an equivalence for emmeans in this offset case, and initial searches didn’t turn up much, though it is hard to distinguish specific ‘average predictions’ from many other similar scenarios. I attempted the following, which keeps the values of ln_holders, but it only keeps unique ones, and it’s not reproducing what I would expect, although it implies that it is averaging over the offset values.\n\n\nrg = ref_grid(nb_glm_offset, cov.keep = c('ln_holders'))\n\n# type is ignored for some reason, so we exponentiate after\nem_over = emmeans(rg, ~age, type = 'response')  \n\ndata.frame(em_over) %>% \n  mutate(count = exp(emmean))\n\n    age   emmean         SE  df asymp.LCL asymp.UCL     count\n1   <25 2.055962 0.09071069 Inf  1.878173  2.233752  7.814354\n2 25-29 2.835734 0.07457136 Inf  2.689577  2.981892 17.042911\n3 30-35 3.010866 0.07161826 Inf  2.870497  3.151236 20.304984\n4   >35 4.574641 0.05450461 Inf  4.467813  4.681468 96.993166\n\nmargins\nThe over approach for the margins package is not explicitly supported. The package author states:\n\nAt present, margins() does not implement the over option. The reason for this is also simple: R already makes data subsetting operations quite simple using simple [ extraction. If, for example, one wanted to calculate marginal effects on subsets of a data frame, those subsets can be passed directly to margins() via the data argument (as in a call to prediction()).\n\nIt would look something like the following, but we still have the offset problem for this negative binomial class, so I don’t show a result.\n\n\ninsurance %>% \n  group_by(age) %>% \n  group_map(~margins(nb_glm_offset, .), keep = TRUE) \n\nStata: dydx\nCategorical Covariate\nSometimes people want differences as you move from one level (e.g. the reference level) to the next for some covariate, the ‘average marginal effect’. In Stata this is obtained with the dydx option.\n\n\nmargins, dydx(age)\n\nConditional marginal effects                    Number of obs     =         64\nModel VCE    : OIM\n\nExpression   : Predicted number of events, predict()\ndy/dx w.r.t. : 2.age 3.age 4.age\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         age |\n      25-29  |  -8.854149   8.375143    -1.06   0.290    -25.26913     7.56083\n      30-35  |  -16.62179   7.959339    -2.09   0.037     -32.2218   -1.021769\n        >35  |   -25.4371   7.297716    -3.49   0.000    -39.74036   -11.13383\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nIn R, we can get this from our initial predictions that used all offset values by just taking the differences in the predicted values.\n\n\npredictions %>% \n  mutate(dydx = avg_prediction - avg_prediction[1])\n\n\n\nage\n\n\navg_prediction\n\n\ndydx\n\n\n<25\n\n\n74.257\n\n\n0.000\n\n\n25-29\n\n\n65.403\n\n\n-8.854\n\n\n30-35\n\n\n57.635\n\n\n-16.622\n\n\n>35\n\n\n48.820\n\n\n-25.437\n\n\nContinuous predictor\nNow we’ll consider a continuous covariate. Here we’ll again just focus on a simple example where we rerun the model, but with age as numeric rather than binned4. For comparison we’ll set the numeric age values at roughly the midpoint of the binned categories. We can do this using the at option.\n\n\nmargins, at(age_num = (21, 27, 32, 50))\n\nNegative binomial regression                    Number of obs     =         64\n                                                LR chi2(1)        =      12.58\nDispersion     = mean                           Prob > chi2       =     0.0004\nLog likelihood = -218.76542                     Pseudo R2         =     0.0280\n\n------------------------------------------------------------------------------\n      claims |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n     age_num |  -.0079258   .0020657    -3.84   0.000    -.0119746   -.0038771\n       _cons |  -1.521278   .0919801   -16.54   0.000    -1.701556   -1.341001\n  ln_holders |          1  (offset)\n-------------+----------------------------------------------------------------\n    /lnalpha |  -3.191873   .3194707                     -3.818025   -2.565722\n-------------+----------------------------------------------------------------\n       alpha |   .0410948   .0131286                      .0219712    .0768636\n------------------------------------------------------------------------------\nLR test of alpha=0: chibar2(01) = 69.85                Prob >= chibar2 = 0.000\n\n\nAdjusted predictions                            Number of obs     =         64\nModel VCE    : OIM\n\nExpression   : Predicted number of events, predict()\n\n1._at        : age_num         =          21\n\n2._at        : age_num         =          27\n\n3._at        : age_num         =          32\n\n4._at        : age_num         =          50\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         _at |\n          1  |   67.50043     3.7343    18.08   0.000     60.18133    74.81952\n          2  |   64.36558   3.025254    21.28   0.000     58.43619    70.29497\n          3  |   61.86471     2.5763    24.01   0.000     56.81526    66.91417\n          4  |   53.63947   2.270244    23.63   0.000     49.18988    58.08907\n------------------------------------------------------------------------------\n\nAgain, we can duplicate this with the basic predict function. We just predict at that value of the covariate for all values of the offset, and get the average prediction as we did before.\n\n\nnb_glm_offset_cont = MASS::glm.nb(claims ~  age_num + offset(ln_holders), data = insurance)\n\npredictions = expand.grid(\n  age_num = c(21, 27, 32, 50),\n  ln_holders = insurance$ln_holders\n) %>% \n  mutate(pred = \n           predict(\n             nb_glm_offset_cont,\n             newdata = .,\n             type = 'response'\n           )\n  ) %>% \n  group_by(age_num) %>% \n  summarise(average_prediction = mean(pred))\n\n\n\nage_num\n\n\naverage_prediction\n\n\n21\n\n\n67.500\n\n\n27\n\n\n64.366\n\n\n32\n\n\n61.865\n\n\n50\n\n\n53.639\n\n\nWe can also get the dydx for the continuous covariate, which is the derivative of the target with respect to the covariate. In linear models, this is just the regression coefficient, but here we have to do things a little differently.\n\n\nmargins, dydx(age_num)\n\nAverage marginal effects                        Number of obs     =         64\nModel VCE    : OIM\n\nExpression   : Predicted number of events, predict()\ndy/dx w.r.t. : age_num\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n     age_num |  -.4255242   .1050737    -4.05   0.000    -.6314649   -.2195836\n------------------------------------------------------------------------------\n\nAs noted for the categorical case, this value is the average marginal effect. As the Stata reference describes:\n\nIt is not necessarily true that dydx() = 0.5 means that “y increases by 0.5 if x increases by 1”. It is true that “y increases with x at a rate such that, if the rate were constant, y would increase by 0.5 if x increased by 1”\n\nThis qualified interpretation may not be of much value in contexts where the rate is not constant, but we can still see what Stata is doing.\nR by hand\nFor dydx, when it comes to continuous covariates, there isn’t an obvious change in the covariate to use (i.e. the dx) to evaluate at each point, as is the case with categorical variables, which can use a reference group. So what we do is use a small arbitrary difference (\\(\\epsilon\\)) for the covariate at its observed values, get the predictions for the values above and below the observed value, and then average those differences in predicted values. For comparison to Stata, I set \\(\\epsilon\\) to the value used by the margins command. Note that we are only using the observed values for the offset.\n\n\nh = function(x, epsilon = 1e-5) max(abs(x), 1) * sqrt(epsilon)\n\nage_dx_plus = insurance %>% \n  select(age_num, ln_holders) %>% \n  mutate(age_num = age_num + h(age_num))\n\nage_dx_minus = insurance %>% \n  select(age_num, ln_holders) %>% \n  mutate(age_num = age_num - h(age_num))\n\npredictions_dydx = \n  tibble(\n    dy = \n      predict(nb_glm_offset_cont, age_dx_plus,  type = 'response') -\n      predict(nb_glm_offset_cont, age_dx_minus, type = 'response'), \n    dx   = age_dx_plus$age_num - age_dx_minus$age_num,\n    dydx = dy/dx\n  )\n\n# summarise(predictions_dydx, ame = mean(dydx))\n\n\n\name\n\n\n-0.42552\n\n\nSo we just get predictions for a small difference in age for each value of age, and average that difference in predictions.\nemmeans\nThe emmeans package is primarily geared toward factor variables, but does have support for numeric variables interacting with factors. However, this isn’t what we’re really looking for here.\nmargins\nWe can however use the margins package for this, and it provides the same result as before. For whatever reason, it doesn’t have an issue with the offset if we use the lower level dydx function.\n\n\ndydx(\n  insurance,\n  nb_glm_offset_cont,\n  variable = 'age_num',\n  eps = 1e-5\n) %>%\n  summarise(ame = mean(dydx_age_num))\n\n\n\name\n\n\n-0.42552\n\n\nFor more on the dydx case for continuous variables in general, see the resources.\nOther complications\nObviously models will have more than one covariate, and in the particular case that was brought to my attention, there were also random effects. I may explore more in the future, but the general result should hold in those circumstances. As a quick example5, we can get the same age results for both, by getting the age group predictions with all values of the dataset (not just the offset).\n\n\nnb_glm_offset_full = MASS::glm.nb(\n  claims ~  age + group + district + offset(ln_holders), \n  data = insurance\n)\n\nsummary(nb_glm_offset_full)\n\nCall:\nMASS::glm.nb(formula = claims ~ age + group + district + offset(ln_holders), \n    data = insurance, init.theta = 449932.3841, link = log)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.46551  -0.50795  -0.03196   0.55554   1.94022  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.82174    0.07679 -23.723  < 2e-16 ***\nage25-29    -0.19101    0.08286  -2.305 0.021155 *  \nage30-35    -0.34495    0.08138  -4.239 2.25e-05 ***\nage>35      -0.53667    0.06996  -7.671 1.70e-14 ***\ngroup1-1.5l  0.16133    0.05054   3.192 0.001412 ** \ngroup1.5-2l  0.39281    0.05500   7.142 9.23e-13 ***\ngroup>2l     0.56341    0.07232   7.791 6.67e-15 ***\ndistrict2    0.02587    0.04302   0.601 0.547637    \ndistrict3    0.03853    0.05052   0.763 0.445686    \ndistrict4    0.23421    0.06168   3.797 0.000146 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(449932.4) family taken to be 1)\n\n    Null deviance: 236.212  on 63  degrees of freedom\nResidual deviance:  51.416  on 54  degrees of freedom\nAIC: 390.74\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  449932 \n          Std. Err.:  4185430 \nWarning while fitting theta: iteration limit reached \n\n 2 x log-likelihood:  -368.745 \n\n\n\nnbreg claims i.age i.group i.district, offset(ln_holders) nolog\nmargins age\n\nNegative binomial regression                    Number of obs     =         64\n                                                LR chi2(9)        =      81.37\nDispersion     = mean                           Prob > chi2       =     0.0000\nLog likelihood = -184.37077                     Pseudo R2         =     0.1808\n\n------------------------------------------------------------------------------\n      claims |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         age |\n      25-29  |  -.1910093   .0828564    -2.31   0.021    -.3534049   -.0286137\n      30-35  |  -.3449496   .0813741    -4.24   0.000    -.5044399   -.1854592\n        >35  |  -.5366602   .0699556    -7.67   0.000    -.6737706   -.3995497\n             |\n       group |\n     1-1.5l  |   .1613446   .0505323     3.19   0.001     .0623031     .260386\n        >2l  |   .5634109   .0723153     7.79   0.000     .4216755    .7051462\n             |\n    district |\n          2  |   .0258648   .0430156     0.60   0.548    -.0584443    .1101739\n          3  |   .0385174   .0505114     0.76   0.446    -.0604832     .137518\n          4  |   .2341969   .0616732     3.80   0.000     .1133197    .3550742\n             |\n       _cons |  -1.821741   .0767876   -23.72   0.000    -1.972242    -1.67124\n  ln_holders |          1  (offset)\n-------------+----------------------------------------------------------------\n    /lnalpha |  -19.19245   546.8434                     -1090.986    1052.601\n-------------+----------------------------------------------------------------\n       alpha |   4.62e-09   2.53e-06                             0           .\n------------------------------------------------------------------------------\nLR test of alpha=0: chibar2(01) = 7.1e-06              Prob >= chibar2 = 0.499\n\n\nPredictive margins                              Number of obs     =         64\nModel VCE    : OIM\n\nExpression   : Predicted number of events, predict()\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         age |\n        <25  |   76.39696   5.069345    15.07   0.000     66.46123    86.33269\n      25-29  |   63.11343   3.143376    20.08   0.000     56.95253    69.27433\n      30-35  |   54.10861   2.544524    21.26   0.000     49.12143    59.09579\n        >35  |   44.66913   .9835578    45.42   0.000      42.7414    46.59687\n------------------------------------------------------------------------------\n\nTo do this with predict, we make predictions for all observed values as if they were at each level of age. Then we average them for each age group, just like we did before.\n\n\npredictions_full_model =  \n  map_df(1:4, function(i) mutate(insurance, age = levels(age)[i])) %>% \n  mutate(\n    age = factor(age, levels(insurance$age)),   # convert back to factor\n    prediction = predict(nb_glm_offset_full, newdata = ., type = 'response')\n  )  %>% \n  group_by(age) %>% \n  summarise(avg_prediction = mean(prediction)) \n\n\n\nage\n\n\navg_prediction\n\n\n<25\n\n\n76.397\n\n\n25-29\n\n\n63.113\n\n\n30-35\n\n\n54.109\n\n\n>35\n\n\n44.669\n\n\n\n\n\nR Packages\nTo summarize R’s capabilities with Stata-like margins with models using an offset, we have a few options we can note. First, we can get the values using the predict method. Then there are the packages to help with getting these types of predictions. margins explicitly attempts to replicate Stata-like margins for standard and some more complex models, but there doesn’t appear to be documentation on how the offset is dealt with by default. Furthermore, care must be taken if it isn’t an explicitly supported model. As we have also seen, emmeans provides many predictions of the sort discussed here, supports many more models, produces results in a nice format, and has plotting capabilities. However, it’s mostly suited toward factor variables.\nBeyond those, ggeffects uses predict and emmeans under the hood, so offers a nice way to do the same sorts of things, but with a more viable plot as a result. Other packages and functions are available for specific settings. For example, conditional_effects in the brms package provides predictions and visualization for the bayesian setting.\nSummary\nHopefully this will clarify the discrepancies between R and Stata with models using an offset. Honestly, I pretty much always use the predict function with my specified data values because I know what it’s doing and I can understand the results without hesitation regardless of model or package used. Furthermore, if one knows their data at all, it should be possible to specify covariate values that are meaningful pretty easily. On the other hand, getting predictions at averages can cause conceptual issues with categorical variables in many settings, and getting average effects often also can be hard to interpret (e.g. nonlinear relationships).\nOne thing you don’t get with some of the averaged predictions using the predict function are interval estimates, but this could be obtained via bootstrapping. Otherwise, most predict methods provide the standard error for a prediction with an additional argument (e.g. se.fit = TRUE), so if you getting predictions at key values of the variables it is trivial to get the interval estimate. In general, most R packages are just using predict under the hood, so being familiar with it will likely get you what you need on its own.\nResources\nReference\nStata reference for margins\nemmeans\nmargins\nggeffects\nNotes\nMarginal Effects- Rich Williams notes- 1\nMarginal Effects- Rich Williams notes- 2\nMarginal Effects- Rich Williams notes- 3\nMarginal Effects Stata Article by Rich Williams\nJosh Errickson’s comparisons of Stata, emmeans, and ggeffects\nUCLA IDRE FAQ (Margins command section)\nStata FAQ (based on old mfx command)\nAppendix\nJust for giggles, I did an average marginal effect for a GAM, though I find little utility in it for the relationship shown. Confirmed via gratia and margins.\n\n\nlibrary(mgcv)\nset.seed(2)\ndat <- gamSim(1, n = 400, dist = \"normal\", scale = 2)\n\nGu & Wahba 4 term additive model\n\nb <- gam(y ~ s(x2), data = dat)\n\nvisibly::plot_gam(b)\n\n\n# set change step\nh = 1e-5\n\nb_dx_plus = dat %>% \n  select(x2) %>% \n  mutate(x2 = x2 + h)\n\nb_dx_minus = dat %>% \n  select(x2) %>% \n  mutate(x2 = x2 - h)\n\npredictions_dydx = \n  tibble(\n    x2 = dat$x2,\n    dy = \n      predict(b, b_dx_plus,  type = 'response') -\n      predict(b, b_dx_minus, type = 'response'), \n    dx   = b_dx_plus$x2 - b_dx_minus$x2,\n    dydx = dy/dx\n  ) \n\ngratia_result  = gratia::derivatives(b, newdata = dat, eps = h)\nmargins_result = margins::dydx(dat, b, variable = 'x2', eps = h^2)  # note that margins uses the h function specified previously\n\nall.equal(as.numeric(predictions_dydx$dydx), gratia_result$derivative)\n\n[1] \"Mean relative difference: 5.596277e-05\"\n\nall.equal(as.numeric(predictions_dydx$dydx), as.numeric(margins_result$dydx_x2))\n\n[1] TRUE\n\nsummarise(\n  predictions_dydx, \n  ame = mean(dydx), \n  ame_gratia = mean(gratia_result$derivative),\n  ame_margins = mean(margins_result$dydx_x2)\n) \n\n# A tibble: 1 x 3\n    ame ame_gratia ame_margins\n  <dbl>      <dbl>       <dbl>\n1  1.76       1.76        1.76\n\nNot the least of which is that most outside of econometrics don’t call predictions margins, since these days we aren’t adding results to the margin of a hand-written table.↩︎\nFor those in the R world, the i.age tells Stata to treat the age factor as, well, a factor. Stata’s alpha is 1/theta from R’s output.↩︎\nThe margins package does do predictions rather than the marginal effects, but it, like others, is just a wrapper for the predict method, and doesn’t appear to average them, so I don’t demonstrate that.↩︎\nThere is rarely a justifiable reason to discretize age as near as I can tell, and doing so inevitably results in less satisfying and less reliable conclusions.↩︎\nThere is a weird print issue where the Stata output isn’t showing the coefficient for one of the levels of group, but the model is correct and was verified directly using Stata.↩︎\n",
    "preview": "posts/2020-06-15-predict-with-offset/../../img/margins/preview.png",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {},
    "preview_width": 3187,
    "preview_height": 2008
  },
  {
    "path": "posts/2020-03-23-covid/",
    "title": "Exploring the Pandemic",
    "description": "Processing and Visualizing Covid-19 Data",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-06-10",
    "categories": [
      "visualization"
    ],
    "contents": "\nTable of Contents\nIntroduction\nGetting StartedFunctions\nRead the data\n\nInitial LookWorld\nUnited States\nMichigan\nWashtenaw County\n\nCountry Level TrendsNY Times-style Daily Count\nPer Capita Country Trends\n\nUS TrendsState Totals\nTrends\nState Bins\nGeofacet\nDaily counts\nPer Capita State Trends\n\nCounty TrendsMichigan\nUS\n\nHospital Data\nModelsUS State Trends\nA Bayesian SIR Model for Washtenaw County\n\n\nFirst posted 2020-03-23.\nIntroduction\nThis is some code to get processed data and visualizations regarding the COVID-19 outbreak. The goal here is just to present some code that I’m playing with that may also make it easier for others to get their hands dirty. In other words, this is for enabling others to play with the data as well, while presenting some clean code and data. There are actually already some R packages doing some of this, but they are currently (in my opinion) problematic, slow to update, and or don’t really offer much than what you could just do yourself, as demonstrated below. I’ll likely be updating this daily for the time being.\nI actually started scraping Wikipedia’s WHO incidence report tables, but any cursory glance showed numerous issues and lots of cleaning, coupled with a format that changed almost daily. I then started playing with the data behind the Johns Hopkins dashboard, which was notably better, but didn’t have some info and there are issues there as well. I then settled on Open Covid, but now just use whatever source works easiest for a situation.\nI use some custom functions in the following, and I don’t show every bit of code, but most of these cases are inconsequential. The bulk of this code should be usable by anyone pretty easily if they are familiar with the tidyverse.\nResources:\nFor the country level plots:\nOpen COVID-19: https://github.com/open-covid-19/data\nOpen COVID categorical data: https://open-covid-19.github.io/data/data_categories.csv\nFor U.S. county level plots:\nJohns Hopkins: https://github.com/CSSEGISandData/COVID-19\nNY Times finally put their data up on March 27: https://github.com/nytimes/covid-19-data\nCheck out CSCAR chum Alex Cao’s more local efforts here:\nhttps://observablehq.com/@caocscar/covid-19-michigan-data\nGetting Started\nFunctions\nThe following are functions that process the data from different sources. I only show parts as they are a bit long1, but they can be found here. They all require tidyverse, or at least dplyr and readr.\nOpen-Covid Data\nThe Open Covid was one of the more usable data sets early on. A bonus with this data is that it already includes population, and as it’s one source, the function doesn’t take much code. The one drawback I’ve had with it was some unnecessary column changes, which effectively break the initial read as the function tries to verify column types. Other issues include misspelled names and other minor things. Full code.\n\n\nread_open_covid_data <- function(\n  country = NULL,     # Filter to specific country\n  current = FALSE,    # Only the current data\n  totals  = FALSE     # Only totals (no regional)\n) {\n  \n  if (current) {\n    data = read_csv('https://open-covid-19.github.io/data/data_latest.csv',\n                    col_types = 'Dcccccddddd')\n  }\n  else {\n    data = read_csv('https://open-covid-19.github.io/data/data.csv',\n                    col_types = 'Dcccccddddd')\n  }\n  \n  if (!is.null(country)) {\n     data = filter(data, CountryCode == country | CountryName == country)\n  }\n  \n  .\n  .\n  .\n\n\n\n\nJohns Hopkins Data\nHere is the start of the function for the Johns Hopkins data. This data is presumably used for the website that is so popular, but there are issues. For some bizarre reason, they appear to randomly not provide country totals for Australia, Canada and China, only the province level data, so for country totals this must be dealt with. In addition, confirmed cases, deaths, and recovered are needlessly separated. Full code.\n\n\nread_jh_covid_data <- function(\n  first_date = lubridate::mdy('01-22-2020'),\n  last_date  = Sys.Date() - 1,\n  country_state = NULL,     # country if global data, state if US\n  include_regions = FALSE,  # for world data, include province/state specific data?\n  us         = FALSE,\n  wider      = TRUE\n) {\n  \n  if (!us) {\n    cleanup_global = function(data) {\n    data = data %>% \n      pivot_longer(\n        -c(`Province/State`, `Country/Region`, Lat, Long),\n        names_to = 'date',\n        values_to = 'count'\n      ) %>% \n      mutate(date = lubridate::mdy(date)) %>%  \n      rename(\n        province_state = `Province/State`,\n        country_region = `Country/Region`,\n      ) %>% \n      rename_all(tolower)\n  }\n    \n    init_confirmed = readr::read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n    init_deaths    = readr::read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n    init_recovered = readr::read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n    \n    .\n    .\n    .\n\n\n\n\nNew York Times Data\nThe NYT data is clean and ‘tidy’, and so a great resource for starting out. There are still some issues however (e.g. Georgia suddenly having fewer cumulative cases from one day to the next). Full code.\n\n\nread_nyt_data <- function(states = TRUE) {\n  if (states) {\n    us_states0 <- readr::read_csv(\n      \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\",\n      col_types = 'Dccdd'\n    )\n    \n    data = us_states0 %>%\n      filter(!state %in% c(\n        'Puerto Rico',\n        'Guam',\n        'Northern Mariana Islands',\n        'Virgin Islands',\n        'American Samoa')\n      ) %>% \n      arrange(state, date) %>%\n      group_by(state) %>%\n      mutate(\n        daily_cases = cases - lag(cases, default = NA),\n        daily_cases = if_else(is.na(daily_cases), cases, daily_cases),\n        daily_deaths = deaths - lag(deaths, default = NA),\n        daily_deaths = if_else(is.na(daily_deaths), deaths, daily_deaths)\n      ) %>% \n      ungroup() %>% \n      left_join(tibble(state = state.name, state_abb = state.abb)) %>% \n      mutate(state_abb = as.factor(state_abb))\n  }\n  .\n  .\n  .\n\n\n\n\nRead the data\n\n\n\n\n\nlibrary(tidyverse)  # required for function\n\ncountries =  read_jh_covid_data()    # country-level for all dates\n\nworld_totals = countries %>% \n  group_by(country_region) %>% \n  filter(date == max(date))\n\nus = read_nyt_data()  # all US data\n\nus_current = us %>% \n  filter(date == max(date))  # only current US state totals\n\ncounty_trends = read_nyt_data(states = FALSE)\n\n\n\n\nInitial Look\nWorld\nNow that we have the data, we can see what’s going on. These are posted as of 2020-06-10 15:43:43 UTC.\n\n\nTotal Confirmed\n\n\nTotal Deaths\n\n\nDeath Rate\n\n\n7,040,570\n\n\n398,560\n\n\n5.7%\n\n\nUnited States\n\n\nTotal Confirmed\n\n\nTotal Deaths\n\n\nDeath Rate\n\n\n1,973,230\n\n\n111,989\n\n\n5.7%\n\n\nMichigan\n\n\nTotal Confirmed\n\n\nTotal Deaths\n\n\nDeath Rate\n\n\n65,190\n\n\n5,946\n\n\n9.1%\n\n\nWashtenaw County\nThe following uses rvest to grab the current tally for this county from the county website.\n\n\nURL = 'https://www.washtenaw.org/3108/Cases'\n\nlibrary(rvest)\n\ninit = read_html(URL) %>% \n  html_table()\n\nwashtenaw_cases = as_tibble(init[[1]], .name_repair = 'unique')\n\n\n\nTotal Confirmed\n\n\nTotal Hospitalized\n\n\nTotal Recovered\n\n\nTotal Deaths\n\n\nDeath Rate\n\n\n1,637\n\n\n377\n\n\n1,425\n\n\n104\n\n\n6.4%\n\n\nCountry Level Trends\nThe following does some additional processing before going to a plot of the trends. I pick some countries to highlight (not necessarily the ones with the most cases), and I treat the world total as a separate addition to the plot.\n\n\nhighlight = c(\n  'US',\n  'China',\n  'Japan',\n  'Korea, South',\n  'Italy',\n  'Iran',\n  'United Kingdom',\n  'France',\n  'Germany',\n  'Spain'\n)\n\nworld = countries %>%\n  group_by(date) %>%\n  summarise(confirmed = sum(confirmed, na.rm = T))\n\nNow we can just do a basic plot. I use ggrepel to see more clearly where the highest cases are currently.\n\n\nlibrary(ggrepel)\np = countries %>% \n  ggplot(aes(x = date, y = confirmed)) +\n  geom_path(aes(group = country_region), alpha = .005) +\n  geom_point(\n    aes(),\n    size  = 5,\n    alpha = .1,\n    data  = world\n  ) +\n  geom_point(\n    aes(color = country_region),\n    size  = 1,\n    alpha = .5,\n    data  = filter(countries, country_region %in% highlight)\n  ) +\n  geom_text_repel(\n    aes(label = country_region, color = country_region),\n    size  = 2,\n    alpha = .85,\n    data  = filter(countries, country_region %in% highlight, date == max(date)-1),\n    show.legend = FALSE\n  ) +\n  scale_color_scico_d(begin = .1, end = .9) +\n  scale_x_date(\n    date_breaks = '2 weeks',\n    labels = function(x) format(x, format = \"%m/%d\")\n  ) +\n  scale_y_continuous(\n    position = \"right\",\n    trans    = 'log',\n    breaks   = c(50, unlist(map(c(1,5), function(x) x*10^(2:6)))), \n    labels   = scales::comma\n  ) +\n  visibly::theme_clean() + \n  labs(\n    x = '', \n    y = '',\n    subtitle =  'Total Confirmed Cases',\n    caption  = 'Dark large dot is world total'\n    ) +\n  theme(\n    axis.text.x  = element_text(size = 6),\n    axis.text.y  = element_text(size = 6),\n    axis.title.y = element_text(size = 6),\n    axis.ticks.y = element_blank(),\n    legend.title       = element_blank(),\n    legend.key.size    = unit(.25, 'cm'),\n    legend.text        = element_text(size = 6),\n    legend.box.spacing = unit(0, 'mm'),\n    legend.box.margin  = margin(0),\n    legend.position    = 'left',\n    title = element_text(size = 12)\n  )\n\np\n\n\n\n\n\nThe following animates the previous via gganimate.\n\n\n# be moved to the directory of the post to see\nlibrary(gganimate)\n\np_anim = p +\n  transition_reveal(date) +\n  shadow_wake(wake_length = 1/3, falloff = \"cubic-in-out\")\n\np_animate = animate(\n  p_anim,\n  nframes = 120,\n  fps = 10,\n  start_pause = 5,\n  end_pause   = 15,\n  width  = 800,\n  height = 600,\n  device =  'png',\n  res    = 144\n)\n\n\n\np_animate\n\n\n\n\n\nNY Times-style Daily Count\nThe following is similar to the plot shown on the New York Times daily count, just without the unnecessary discretizing of the color (the legend already does that for you). I chose what I thought was a similar palette, but obviously you can play around with that. Here is the ggplot code, but I show an interactive result via highcharter.\n\n\n# reduce to just the 20 countries with the most cases\ntop_20 = world_totals  %>% \n  ungroup() %>% \n  top_n(20, confirmed) %>% \n  arrange(desc(confirmed))\n \n# this is to create a ruled effect, not necessary\nplot_data = countries %>%\n  filter(country_region %in% top_20$country_region) %>%\n  group_by(country_region) %>% \n  mutate(\n    daily_confirmed = confirmed - lag(confirmed),\n    daily_confirmed = if_else(is.na(daily_confirmed), confirmed, daily_confirmed)\n  ) %>% \n  ungroup() %>% \n  mutate(\n    country_region   = ordered(country_region, levels = rev(top_20$country_region)),\n    line_positions = as.numeric(country_region) + .5,\n    line_positions = ifelse(line_positions == max(line_positions), NA, line_positions)\n  ) \n  \nplot_data %>% \n  ggplot(aes(x = date, y = country_region)) +\n  geom_tile(\n    aes(\n      fill   = daily_confirmed,\n      width  = .9,\n      height = 0.5\n    ),\n    na.rm = T,\n    size  = 2\n  ) +\n  geom_hline(\n    aes(yintercept = line_positions),\n    color = 'gray92',\n    size  = .25\n  ) +\n  scale_fill_scico(\n    end = .75,\n    na.value = 'gray98',\n    palette  = 'lajolla',\n    trans    = 'log',\n    breaks   = c(5, 25, 100, 500, 2500)\n  ) +\n  labs(x = '', y = '') +\n  guides(fill = guide_legend(title = 'New Cases')) +\n  visibly::theme_clean() +\n  theme(\n    axis.ticks.y = element_blank(),\n    legend.text  = element_text(size = 6),\n    legend.title = element_text(size = 10)\n  )\n\n\n\n\nThe following shows the percentage increase in cases over the previous day (capped at 100 or greater). Early on this isn’t very useful (e.g. going from 5 to 10 is a 100% increase). We can see that China and South Korea have minimal to no increase day to day at present, while other countries are continuing to see relatively large increases.\n\n\n\nPer Capita Country Trends\nThe following shows the number of confirmed cases per 10000 people using the Open Covid data, which includes population. However, this obviously doesn’t account for the lack of testing, country ineptitude in counting, country discretion in counting, and many other things. To keep things clean, we focus on the ten countries with the most cases.\n\n\ncountries_oc = read_open_covid_data() %>% \n  filter(is.na(region_code))\n\n\n\n\nUS Trends\nState Totals\nHere is a data table for easy look-up of current U.S. It is based on the New York Times database.\n\n\n\nTrends\nWe can visualize the state level data in numerous ways. Here is the world trend plot previously seen, now applied just to the U.S. I used plotly to make it interactive.\n\n\nhighlight = us_current %>% \n  top_n(10, cases) %>% \n  pull(state_abb)\n\nus_total = countries %>% \n  filter(country_region == 'US') %>% \n  rename(cases = confirmed)\n\np = us %>% \n  ggplot(aes(x = date, y = cases)) +\n  geom_path(aes(group = state), alpha = .01) +\n  geom_point(\n    aes(),\n    size  = 6,\n    alpha = .1,\n    data  = us_total\n  ) +\n  geom_point(\n    aes(color = state),\n    size  = .5,\n    alpha = .5,\n    data  = filter(us, state_abb %in% highlight)\n  ) +\n  geom_text_repel(\n    aes(label = state_abb, color = state),\n    size  = 2,\n    alpha = .85,\n    data  = filter(us_current, state_abb %in% highlight)\n  ) +\n  scale_color_scico_d(begin = .1, end = .9) +\n  scale_x_date(date_breaks = '2 weeks') +\n  scale_y_continuous(\n    position = \"right\",\n    trans    = 'log',\n    breaks   = c(50, unlist(map(c(1,5), function(x) x*10^(2:6)))), \n    labels   = scales::comma\n    ) +\n  visibly::theme_clean() + \n  labs(\n    x = '', \n    y = '',\n    subtitle =  'Total Confirmed Cases',\n    caption = 'Dark large dot is U.S. total'\n    ) +\n  theme(\n    axis.text.x  = element_text(size = 6),\n    axis.text.y  = element_text(size = 6),\n    axis.title.y = element_text(size = 6),\n    axis.ticks.y = element_blank(),\n    legend.title       = element_blank(),\n    legend.key.size    = unit(.25, 'cm'),\n    legend.text        = element_text(size = 6),\n    legend.box.spacing = unit(0, 'mm'),\n    legend.box.margin  = margin(0),\n    legend.position = 'left',\n    title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\nState Bins\nHere we look at counts and death rates via a ‘binned’ map. There are numerous issues with trying to depict numeric information on a map. This at least tries to solve the issue of state size by making them all equal while retaining the basic shape of the U.S.\n\n\nlibrary(statebins)\n\nus_current %>% \n  filter(state != 'District of Columbia') %>%\n  statebins(\n    state_col = 'state',\n    value_col = \"log(cases)\",\n    palette   = \"OrRd\", \n    direction = 1,\n    name = \"Covid Counts (log)\"\n  ) +\n  theme_statebins(base_size = 8)\n\n\nus_current %>% \n  mutate(death_rate = deaths/cases) %>% \n  statebins(\n    state_col = 'state',\n    value_col = \"death_rate\",\n    palette   = \"OrRd\", \n    direction = 1,\n    name = \"Death Rate\"\n  ) +\n  theme_statebins(base_size = 8)\n\n\nGeofacet\nWe can plot trends in a map-like fashion too, and the following uses the geofacet package. In early April, this is one of the scarier graphics, with no flattening in sight at the state level. By June, you can see the upticks in various states due to repopening.\n\n\nlibrary(geofacet)\n\nus %>% \n  filter(cases != 0, state != 'District of Columbia') %>%\n  ggplot(aes(date, cases, group = state)) +\n  geom_path(color = '#ff550080') +\n  labs(y = '', x = '') +\n  facet_geo(~state, scales = 'free') +\n  visibly::theme_clean() +\n  theme(\n    axis.text.x  = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y  = element_text(size = 4),\n    strip.text   = element_text(size = 4),\n  )\n\n\nDaily counts\nWe can do the daily counts as before.\n\n\nlevs = us_current %>% \n  arrange(cases) %>% \n  pull(state)\n\nplot_data = us %>%\n  mutate(\n    state    = ordered(state, levels = levs),\n    line_positions = as.numeric(state) + .5,\n    line_positions = ifelse(line_positions == max(line_positions), NA, line_positions)\n  ) \n  \nplot_data %>% \n  ggplot(aes(x = date, y = state)) +\n  geom_tile(\n    aes(\n      fill   = daily_cases,\n      width  = .9,\n      height = 0.5\n    ),\n    na.rm = T,\n    size  = 2\n  ) +\n  geom_hline(\n    aes(yintercept = line_positions),\n    color = 'gray92',\n    size  = .25\n  ) +\n  scale_fill_scico(\n    end = .75,\n    na.value = 'gray98',\n    palette  = 'lajolla',\n    trans    = 'log',\n    breaks   = c(5, 25, 100, 500, 2500)\n  ) +\n  labs(x = '', y = '') +\n  guides(fill = guide_legend(title = 'New Cases')) +\n  visibly::theme_clean() +\n  theme(\n    axis.text.y  = element_text(size = 6),\n    axis.ticks.y = element_blank(),\n    legend.text  = element_text(size = 6),\n    legend.title = element_text(size = 10)\n  )\n\n\nPer Capita State Trends\nThe following shows the number of confirmed cases per 10000 people. Top 10 states are highlighted.\n\n\n\nTrends since May\nSince states began opening in May we can look at what’s happened from that point on. Highlighted are the 10 states with the largest and smallest percentage increase since the beginning of May.\n\n\n\nCounty Trends\nLet’s start to look at more specific results, such as county level counts.\nMichigan\nThe following creates an animated plot of Michigan counties over time.\n\n\nlibrary(sf)\n\nmi_counties = county_trends %>% \n  filter(state == 'Michigan')\n\nall_counties_sf = st_as_sf(maps::map(\"county\", plot = FALSE, fill = TRUE))\n\nprep_counties = mi_counties %>% \n  mutate(county = str_remove(tolower(county), '[[:punct:]]'), \n         state = tolower(state)) %>% \n  droplevels() %>% \n  arrange(county)\n\nmi_counties_sf = all_counties_sf %>% \n  filter(grepl(\"michigan\", ID)) %>% \n  separate(ID, into = c('state', 'county'), remove = F, sep = ',') %>%\n  select(-state) %>%\n  left_join(prep_counties) %>% \n  mutate(cases = if_else(is.na(cases), 0, cases))\n\nmi_plot =  mi_counties_sf %>% \n  arrange(date) %>% \n  ggplot(aes(group = date)) +\n  geom_sf(aes(fill = cases), color = NA) +\n  coord_sf(xlim   = c(-91,-82),\n           ylim   = c(41, 48.5),\n           expand = FALSE) +\n  scale_fill_scico(\n    trans    = 'log',\n    breaks   = c(1, 10, 50, 250, 1250, 5000),\n    palette  = 'lajolla',\n    na.value = 'gray95'\n  ) +\n  guides(fill = guide_legend(title = '')) +\n  theme_void() +\n  theme(\n    legend.key.size = unit(.25, 'cm'),\n    legend.text     =  element_text(size = 6),\n    legend.title    = element_text(size = 6)\n  )\n\nmi_plot\n\n\nNow let’s show it over time with gganimate.\n\n\nlibrary(gganimate)\n\nmi_plot_anim = mi_plot +\n  labs(title = '{closest_state}') +\n  transition_states(date, transition_length = 2) +\n  theme(title = element_text(color ='gray50'))\n\nmi_plot_anim\n\n\n\n\n\n\n\nInteractive County Map\nI thought I would try an interactive map for Michigan counties using highcharts via the highcharter package. This requires creating a unique county code that is nothing more than a FIPS code (but oddly doesn’t simply use the actual FIPS code).\n\n\nlibrary(highcharter)\n\nplot_data = mi_counties %>%\n  filter(date == max(date)) %>%\n  mutate(code = paste0('us-mi-', str_sub(fips, start = 3)))\n\nhcmap(\n  \"countries/us/us-mi-all\",\n  data = plot_data,\n  name = \"Michigan\",\n  value = \"cases\",\n  joinBy = c(\"hc-key\", \"code\"),\n  borderColor = \"#fffff8\",\n  borderWidth = .1\n) %>%\n  hc_colorAxis(\n    dataClasses = color_classes(\n      c(0, 10, 100, 500, 1000, 2000, 5000, 1e4, 2e4, 5e4),\n      colors = scico(6, end = .9, palette = 'lajolla')\n    )\n  ) %>%\n  hc_legend(\n    layout = \"vertical\",\n    align = \"right\",\n    floating = TRUE,\n    valueDecimals = 0#,\n    # valueSuffix = \"%\"\n  ) %>% \n  hc_credits(enabled = FALSE)\n\n\nWashtenaw County\nWe can also do the trend for Washtenaw county here by just filtering the county trends data. There is a note at the county website:\n\nMost of the diagnoses in the 3/31 spike on the New COVID-19 Cases Reported per Day chart represent a backlog of labs that have been pending for 1-2 weeks. This means most of these individuals got sick and were tested at least a week ago, but are receiving test results now because labs are just now catching up on tests.\n\nNote the error in the NY Times data base starting in June (as of June 10th). It’s not clear why or how they would have gotten this from any county or state source, but they added a couple hundred cases for Washtenaw.\n\n\n\nUS\nFor the following map of the whole US, I wanted to do a point plot since counties are arbitrarily shaped, making it easy to think things are going weird in random places. We can use the county centroid locations, make very faint the county lines, and not use a ridiculous point range size to get a better sense of the overall pattern. I’m not used to the sf package, so it took a while to get what I wanted beyond a simple county map. Adding points was not very intuitive, but I got it sorted out in the end, so you get the benefit of the cleaned up code.\n\n\nlibrary(sf)\n\nprep_counties = county_trends %>% \n  mutate(\n    county = str_remove(tolower(county), '[[:punct:]]'),\n    state  = tolower(state),\n    cases = if_else(is.na(cases), 0, cases)\n  ) %>% \n  filter(\n    !state %in% c(\n      'alaska',\n      'hawaii'\n    )\n  ) \n\nall_counties_sf2 = all_counties_sf %>% \n  separate(ID, into = c('state', 'county'), remove = F, sep = ',') %>%\n  left_join(prep_counties) %>% \n  select(ID, geom, everything())\n\ncounty_point_plot = \n  ggplot(data = all_counties_sf2) +\n  geom_sf(fill = 'gray99', color = 'gray97', size = .1) +\n  geom_sf(\n    aes(\n      color = cases,\n      size  = cases\n    ),\n    data = st_centroid(all_counties_sf2),\n    alpha = .05,\n    show.legend = F\n  ) +\n  scale_color_scico(\n    trans    = 'log',\n    breaks   = c(1, 10, 50, 250, 1250, 5000),\n    na.value = 'gray99'\n  ) +\n  labs(title = 'Confirmed cases') +\n  scale_size_continuous(range =  c(.1, 10)) +\n  theme_void()  +\n  theme(title = element_text(color = 'gray50'))\n\ncounty_point_plot\n\n\nWhat’s interesting is if we do a similar plot, but actually look for more anomalous results, rather than just calling more largely populated areas hotspots. To do this we need either a per capita rate, a percentage change or similar, or even adjusting the size range, while making the assumption that populated areas are already the ones that are going to have the most cases.\nI grabbed population values with tidycensus (not shown), then removed the top 20 most populated counties from consideration. You can get the data here.\nThe visualization shows numerous hotspots across the deep south, and others not considered in the Midwest, e.g. the Indianapolis area, Rocky Mountain areas, and more. It definitely makes the spread more apparent than the simple counts might suggest.\n\n\n\n\n\n\n\n\n\nInteractive County Map\nIf you do want an actual county map, here is an interactive one as before. To make it easier I add state abbreviations to the original data before creating codes for every county. A value is initially missing for Oglala county South Dakota, because highcharts hasn’t updated its map data source in several years, and it has since been renamed from Shannon county. Likewise, in Alaska, highcharts still refers to Wade Hampton rather than Kusilvak.\n\n\nstates = tibble(state = state.name, state_abb = state.abb)\n\nplot_data = county_trends %>%\n  filter(date == max(date)) %>% \n  left_join(states) %>% \n  mutate(\n    code = paste0('us-', tolower(state_abb), '-', str_sub(fips, start = 3)),\n    code = if_else(code == 'us-sd-102', 'us-sd-113', code),  # Oglala\n    code = if_else(code == 'us-ak-158', 'us-ak-270', code),  # Kusilvak\n  )\n  \n\n# plot_data\n\nhcmap(\n  \"countries/us/us-all-all\",\n  data = plot_data,\n  name = \"Confirmed cases:\",\n  value = \"cases\",\n  joinBy = c(\"hc-key\", \"code\"),\n  borderColor = \"#fffff8\",\n  borderWidth = .1,\n  download_map_data = T\n) %>%\n  hc_colorAxis(\n    dataClasses = color_classes(\n      c(0, sort(unlist(map(c(1, 5), function(x)  x * 10^(1:5))))),\n      colors = scico(6, end = .9, palette = 'lajolla')\n    )\n  ) %>%\n  hc_legend(\n    layout = \"vertical\",\n    align = \"right\",\n    floating = TRUE,\n    valueDecimals = 0#,\n    # valueSuffix = \"%\"\n  ) %>% \n  hc_credits(enabled = FALSE)\n\n\nHospital Data\nI’ve started playing with some hospital data for a dashboard. The (still pretty messy) source code is available on a link there as well, so feel free to play with it for your own needs.\nModels\nThe following models U.S. state daily trends. First we’ll use data from NY Times, which seems to be cleaner than some other sources, but there are still random issues (e.g. Georgia suddenly having fewer cumulative cases from one day to the next). I add a bit of mild cleanup to provide each state a common starting point at the beginning of March, and do some other things specific to package requirements.\nUS State Trends\n\n\n\n\n\nus_for_model = us %>% \n  tidyr::expand(date, state) %>% \n  filter(date >= '2020-02-14') %>% \n  left_join(us) %>% \n  arrange(state, date) %>% \n  group_by(state) %>% \n  fill(fips, state_abb, .direction = 'up')  %>% \n  mutate_at(vars(cases:daily_deaths), function(x) ifelse(is.na(x), 0, x)) %>% \n  ungroup() %>% \n  mutate(\n    state_abb = fct_explicit_na(state_abb, na_level = 'DC'),\n    date_num  = as.numeric(date)\n    ) %>% \n  filter(daily_cases >= 0)  # because some states evidently can't count\n\n# us_for_model %>% filter(state == 'Michigan')  # check\n\nTo explore the trends, I will use a generalized additive model with mgcv, with a separate smooth for each state. It’s not clear to me whether a specific count distribution is really required given the very high number of daily counts, but we’ll go ahead and use the negative binomial. I also use a specific function to speed up processing, but this is not required for those willing to wait. This is a fairly naive model, and, as implemented, has no natural way to project the eventual downward trend to zero. However it should be better than deterministic models for very short-term prediction.\n\n\nlibrary(mgcv)\n\nmodel_state = bam(\n  daily_cases ~ s(date_num, state_abb, bs = 'fs'),\n  family   = nb,\n  data     = us_for_model,\n  nthreads = 10\n)\n\n# summary(model_state)\n\n\n\n\nWe can visualize the estimated trends. Looks to be fairly on point.\n\n\n\nThe following plots the derivatives of each state curve using gratia. Each dot reflects a state’s peak positive rate of change. Most peaks are past at this point. You can double click on a state in the legend to focus on that state.\n\n\n\n\n\n\n\n\n\nThe following predicts the next 4 days for the ten states with the most confirmed cases.\n\n\n\nA Bayesian SIR Model for Washtenaw County\nThis model is based on the article and code from:\nContemporary statistical inference for infectious disease models using Stan. Chatzilenaa, van Leeuwen,Ratmann, Baguelin, & Demiris (2019) (Chatzilena et al. 2019)\nThe following is only a very slightly revised version of the code they made available in GitHub. In essence it is a standard SIR epidemiological model, put into a probabilistic framework, to model infections. They have code for another more complicated model, but I’ve had trouble getting that to converge.\n\n\nfunctions {\n  real[] SIR(real t,  // time\n             real[] y,            // system state {susceptible,infected,recovered}\n             real[] theta,        // parameters\n             real[] x_r,\n             int[]  x_i) {\n    \n    real dy_dt[3];\n    \n    dy_dt[1] = -theta[1] * y[1] * y[2];                     // S\n    dy_dt[2] =  theta[1] * y[1] * y[2] - theta[2] * y[2];   // I\n    dy_dt[3] =                           theta[2] * y[2];   // R\n    \n    return dy_dt;\n  }\n}\n\ndata {\n  int<lower = 1> n_obs;           // number of days observed\n  int<lower = 1> n_theta;         // number of model parameters\n  int<lower = 1> n_difeq;         // number of differential equations\n  int<lower = 1> n_pop;           // population \n  int y[n_obs];                   // data, total number of infected individuals each day\n  real t0;                        // initial time point (zero)\n  real ts[n_obs];                 // time points observed\n}\n\ntransformed data {\n  real x_r[0];\n  int x_i[0];\n}\n\nparameters {\n  real<lower = 0> theta[n_theta]; // model parameters {beta, gamma}\n  real<lower = 0, upper = 1> S0;  // initial fraction of susceptible individuals\n}\n\ntransformed parameters{\n  real y_hat[n_obs, n_difeq];     // solution from the ODE solver\n  real y_init[n_difeq];           // initial conditions for both fractions of S and I\n  \n  y_init[1] = S0;\n  y_init[2] = 1 - S0;\n  y_init[3] = 0;\n  \n  y_hat = integrate_ode_rk45(SIR, y_init, t0, ts, theta, x_r, x_i);\n}\n\nmodel {\n  real lambda[n_obs];             // poisson parameter\n  \n  //priors\n  theta[1] ~ lognormal(0,1);\n  theta[2] ~ gamma(0.004, 0.02);  // Assume mean infectious period = 5 days \n  S0 ~ beta(99, 1);\n  \n  //likelihood\n  for (i in 1:n_obs) {\n    lambda[i] = y_hat[i, 2] * n_pop;\n  }\n  y ~ poisson(lambda);\n}\n\ngenerated quantities {\n  real R_0;                       // Basic reproduction number\n  \n  R_0 = theta[1]/theta[2];\n\n\n\n\n\n\n\nHere are the data steps. For ease we grab county info from the NY Times database. Beyond that we set things up for the Stan.\n\n\nwashtenaw <- readr::read_csv(\n  \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\"\n) %>%\n  filter(county == 'Washtenaw')\n\nwashtenaw = washtenaw %>% \n  mutate(day = 0:(n()-1)) %>% \n  mutate(infected = cases - lag(cases),\n         infected = ifelse(is.na(infected), 0, infected))\n\nN = nrow(washtenaw)    # Number of days observed throughout the outbreak\npop = 370963           # Population (from Census/Wikipedia)\nsample_time = 1:N\n\n# Specify parameters to monitor\nparameters = c(\"y_hat\", \"y_init\", \"theta\",  \"R_0\")  # SIR deterministic model parameters\n\n# Modify data into a form suitable for Stan\ncovid_data = list(\n  n_obs   = N,\n  n_theta = 2,\n  n_difeq = 3,\n  n_pop   = pop,\n  y       = washtenaw$infected,\n  t0      = 0,\n  ts      = sample_time\n)\n\n\n# Set initial values:\nset.seed(123)\n\ninitial_values = function() {\n  list(\n    theta = c(runif(1, .5, 3), runif(1, 0.2, 0.4)),\n    S0    = runif(1, (pop - 3) / pop, (pop - 1) / pop)\n    )\n}\n\n\n\nlibrary(rstan)\n\nsir_model_fit = \n  stan(\n    model_code = sir_model_code,\n    data       = covid_data,\n    pars       = parameters,\n    init       = initial_values,\n    cores      = 4,\n    warmup     = 2000,\n    iter       = 4000,\n    thin       = 8,\n    seed       = 123,\n    control    = list(\n      adapt_delta   = .99,\n      max_treedepth = 15\n    )\n)\n\n\n\n\nThe current \\(R_0\\) produced by the model is 1.01, which would suggest we’re getting close to a good state in this county (negative \\(R_0\\) would suggest the infection is no longer spreading). However, given that there is no vaccine, almost the entire population is still at risk and susceptible to the disease.\nNow we are ready to plot the result. The following uses tidybayes package for the visualization of the predictions. We can see that even adding a probabilistic approach to this deterministic model, it misses the mark, and notably undercounted some dates2. But in the end it does not miss it by too much except in one crucial aspect- determining the drop off.\n\n\nlibrary(tidybayes)\n\nsir_y_hat = gather_draws(sir_model_fit,  y_hat[day, type]) %>% \n  ungroup() %>% \n  mutate(type = factor(type, labels = c('S', 'I', 'R')))\n\nsamples_infected = sir_y_hat %>% \n  filter(type == 'I') %>% \n  select(day, type, .draw, .value) %>% \n  left_join(washtenaw %>% select(date, day)) %>% \n  rename(prop_infected = .value)  \n\nbilbao = scico::scico(10)\n\nsamples_infected %>% \n  mutate(pred_infected = prop_infected*pop) %>% \n  ggplot(aes(x = date, y = pred_infected)) +\n  stat_lineribbon(\n    aes(y = pred_infected),\n    color = bilbao[10],\n    size = 1,\n    .width = c(.99, .95, .8, .5),\n    alpha = 0.25\n  ) +\n  geom_point(aes(y = infected), color = bilbao[10], alpha=.5, data = washtenaw) +\n  guides(x = guide_axis(n.dodge = 2)) +\n  labs(x = '', y = '', title = 'Bayesian SIR model for daily infected counts', subtitle = 'Washtenaw County') +\n  scico::scale_fill_scico_d(begin = .25, palette = \"bilbao\") +\n  scale_x_date(date_breaks = '3 days', date_labels = '%b-%d') +\n  visibly::theme_clean()\n\n\nWe can do a posterior predictive check of our expected vs. observed counts. Not perfect, and definitely could be better. It’s actually been getting worse as time has progressed, and the counts and reporting times get more erratic.\n\n\n\n\n\nChatzilena, Anastasia, Edwin [van Leeuwen], Oliver Ratmann, Marc Baguelin, and Nikolaos Demiris. 2019. “Contemporary Statistical Inference for Infectious Disease Models Using Stan.” Epidemics 29: 100367. https://doi.org/https://doi.org/10.1016/j.epidem.2019.100367.\n\n\nI use Distill for this website, and strangely it does not have basic code folding.↩︎\nThere was a backlog of tests that came through on April 1st, so I don’t fault the model for missing that outlier.↩︎\n",
    "preview": "posts/2020-03-23-covid/../../img/covid_preview.gif",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-10-psych-explained/",
    "title": "Factor Analysis with the psych package",
    "description": "Making sense of the results",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-04-10",
    "categories": [
      "factor analysis",
      "reliability"
    ],
    "contents": "\n\nContents\nIntroduction\nDemonstration\nLoadings\nVariance accounted for\nFactor correlations\nModel test results\nNumber of observations\nFit indices\nMeasures of factor score adequacy\nMiscellaneous results\n\nAdditional Notes for Factor Analysis\nOther Functions\nAlpha\nOmega\nUnidimensionality\nMediate\n\nConclusion\n\n\nLast updated October 22, 2020.\nPrerequisites: familiarity with factor analysis\nIntroduction\nThe psych package is a great tool for assessing underlying latent structure. It can provide reliability statistics, do cluster analysis, principal components analysis, mediation models, and, of course factor analysis. However, it’s been around a very long time, and many things have added to, subtracted, renamed, debugged, etc. And while the package author and noted psychometrician William Revelle even provides a freely available book on the details, it can still be difficult for many to ‘jump right in’ with the package. This is because it provides so much more than other tools, which is great, but which also can be overwhelming. Even I don’t recall what some of the output regards for factor analysis, and I use the package often. While a lot of it doesn’t matter for most use, it’d be nice to have a clean reference, so here it is.\nWhat follows is an explanation of the factor analysis results from the psych package, but much of it carries over into printed results for principal components via principal, reliability via omega, very simple structure via vss and others. Note that this is not an introduction to factor analysis, reliability, and related. It’s assumed you are already familiar with the techniques to some extent, and are interested in using the package for those analyses.\nDemonstration\nWe will use the classic big-five personality measures, which comes with the package, but for our purposes, we’re just going to look at the agreeableness and neuroticism items. See ?bfi for details. With data in place we run a standard factor analysis, in this case, assuming two factors.\n\n\nlibrary(tidyverse)\nlibrary(psych)\n\ndata(bfi)\n\nbfi_trim = bfi %>% select(matches('^A[1-5]|^N'))\n\nmodel = fa(bfi_trim, 2)\nmodel\n\n\nFactor Analysis using method =  minres\nCall: fa(r = bfi_trim, nfactors = 2)\nStandardized loadings (pattern matrix) based upon correlation matrix\n     MR1   MR2   h2   u2 com\nA1  0.07 -0.36 0.14 0.86 1.1\nA2  0.05  0.69 0.47 0.53 1.0\nA3  0.03  0.76 0.56 0.44 1.0\nA4 -0.05  0.47 0.24 0.76 1.0\nA5 -0.12  0.60 0.39 0.61 1.1\nN1  0.78 -0.03 0.61 0.39 1.0\nN2  0.76 -0.02 0.58 0.42 1.0\nN3  0.77  0.05 0.58 0.42 1.0\nN4  0.58 -0.08 0.36 0.64 1.0\nN5  0.54  0.08 0.29 0.71 1.0\n\n                       MR1  MR2\nSS loadings           2.44 1.78\nProportion Var        0.24 0.18\nCumulative Var        0.24 0.42\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\n With factor correlations of \n      MR1   MR2\nMR1  1.00 -0.19\nMR2 -0.19  1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  45  and the objective function was  2.82 with Chi Square of  7880.99\nThe degrees of freedom for the model are 26  and the objective function was  0.23 \n\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic number of observations is  2759 with the empirical chi square  396.78  with prob <  5.7e-68 \nThe total number of observations was  2800  with Likelihood Chi Square =  636.27  with prob <  1.6e-117 \n\nTucker Lewis Index of factoring reliability =  0.865\nRMSEA index =  0.092  and the 90 % confidence intervals are  0.085 0.098\nBIC =  429.9\nFit based upon off diagonal values = 0.98\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.92 0.88\nMultiple R square of scores with factors          0.84 0.77\nMinimum correlation of possible factor scores     0.68 0.54\n\nLoadings\nThat’s a lot of stuff to work though. Let’s go through each part of the printed output.\nWhat’s MR, ML, PC etc.? These are factors, and the name merely reflects the fitting method, e.g. minimum residual, maximum likelihood, principal components. The default is minimum residual, so in this case MR.\nWhy are they ‘out of order’? the number assigned is arbitrary, but this has to do with a rotated solution. See the help file for more details, otherwise they are numbered in terms of variance accounted for.\nh2: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. communality.\nu2: 1 - h2. residual variance, a.k.a. uniqueness\ncom: Item complexity. Specifically it is “Hoffman’s index of complexity for each item. This is just \\({(Σ λ_i^2)^2}/{Σ λ_i^4}\\) where \\(λ_i\\) is the factor loading on the ith factor. From Hofmann (1978), MBR. See also Pettersson and Turkheimer (2010).” It equals one if an item loads only on one factor, 2 if evenly loads on two factors, etc. Basically it tells you how much an item reflects a single construct. It will be lower for relatively lower loadings.\nVariance accounted for\n                       MR1  MR2\nSS loadings           2.44 1.78\nProportion Var        0.24 0.18\nCumulative Var        0.24 0.42\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\nThe variance accounted for portion of the output can be explained as follows:\nSS loadings: These are the eigenvalues, the sum of the squared loadings. In this case where we are using a correlation matrix, summing across all factors would equal the number of variables used in the analysis.\nProportion Var: tells us how much of the overall variance the factor accounts for out of all the variables.\nCumulative Var: the cumulative sum of Proportion Var.\nProportion Explained: The relative amount of variance explained- Proportion Var/sum(Proportion Var).\nCumulative Proportion: the cumulative sum of Proportion Explained.\nThese are contained in model$Vaccounted.\nFactor correlations\n With factor correlations of \n      MR1   MR2\nMR1  1.00 -0.19\nMR2 -0.19  1.00\nWhether you get this part of the analysis depends on whether or not these are estimated. You have to have multiple factors and a rotation that allows for the correlations.\nfactor correlations: the correlation matrix for the factors. \\(\\phi\\) (phi)\nMean item complexity: the mean of com.\nThese are contained in model$Phi.\nModel test results\nTest of the hypothesis that 2 factors are sufficient.\n\nThe degrees of freedom for the null model are  45  and the objective function was  2.82 with Chi Square of  7880.99\nThe degrees of freedom for the model are 26  and the objective function was  0.23 \nnull model: The degrees of freedom for the null model that assumes no correlation structure.\nobjective function: The value of the function that is minimized by a specific procedure.\nmodel: The one you’re actually interested in. Where p = Number of items, nf = number of factors then: degrees of freedom = \\[p * (p-1)/2 - p * nf + nf*(nf-1)/2\\] For the null model this is \\(p * (p-1)/2\\).\nChi-square: If f is the objective function value. Then \\[\\chi^2 = (n.obs - 1 - (2 * p + 5)/6 - (2 * nf)/3)) * f\\]\nStrangely this is reported for the null but not the primary model result, which comes later.\nNumber of observations\nThe harmonic number of observations is  2759 with the empirical chi square  396.78  with prob <  5.7e-68 \nThe total number of observations was  2800  with Likelihood Chi Square =  636.27  with prob <  1.6e-117 \ntotal: the number of rows in the data you supplied for analysis\nharmonic: while one would assume it is the harmonic mean of the number of observations across items, it’s not this exactly, but is instead the harmonic mean of all the pairwise counts of observations (see ?pairwiseCount).\nThe \\(\\chi^2\\) reported here regards the primary model. So for your model you can report model$STATISTIC, model$dof, model$PVAL, which is what you see in the printed output for the total number of observations. As this regards the residual correlation matrix, a smaller value is better, as in SEM. The empirical chi-square is based on the harmonic sample size, so might be better, but I’ve never seen it reported.\nFit indices\nThe root mean square of the residuals (RMSR) is  0.04 \nThe df corrected root mean square of the residuals is  0.05 \n\n...\n\nTucker Lewis Index of factoring reliability =  0.865\nRMSEA index =  0.092  and the 90 % confidence intervals are  0.085 0.098\nBIC =  429.9\nFit based upon off diagonal values = 0.98\nThe nice thing about the psych package is that it reports SEM-style fit indices for standard factor analysis. You can find some more information via ?factor.stats.\nTLI: Tucker Lewis fit index, typically reported in SEM. Generally want > .9\nRMSEA: Root mean square error of approximation. Also reported is the so-called ‘test of close fit’.\nRMSR: The (standardized) root mean square of the residuals. Also provided is a ‘corrected’ version, but I doubt this is reported by many.\nFit based upon off diagonal values: This is not documented anywhere I can find. However, you can think of it as 1 - resid^2 / cor^2, or a kind of \\(R^2\\) applied to a correlation matrix instead of raw data. It is calculated via factor.stats.\nBIC: Useful for model comparison purposes only.\nMeasures of factor score adequacy\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.92 0.88\nMultiple R square of scores with factors          0.84 0.77\nMinimum correlation of possible factor scores     0.68 0.54\nUnfortunately these are named in such a way as to be nearly indistinguishable, but there is some documentation for them in ?factor.stats. In general, these tell us how representative the factor score estimates are of the underlying constructs, and can be called indeterminancy indices. Indeterminancy refers to the fact that an infinite number of factor scores can be derived that would be consistent with a given set of loadings. In Revelle’s text, chapter 6.9 goes into detail, while Grice (2001) is a thorough review of the problem.\nCorrelation of (regression) scores with factors: square root of the Multiple R square. These can be seen as upper bounds of the determinancy of the factor score estimates that can be computed based on the model. It is essentially the (multiple) correlation of the factor and the observed data, as the name now more clearly suggests.\nMultiple R square...: “The multiple R square between the factors and factor score estimates, if they were to be found. (From Grice, 2001).” Computationally, it is roughly t(model$weights) %*% model$loadings, where the weights are the factor score coefficients, and can be seen as the maximum proportion of determinancy (higher is better). One way you can think about this is as an \\(R^2\\) for a regression model of the items predicting the estimated factor score.\nMinimum correlation...: Not documented, and is only shown as part of the print method, as it is not calculated as part of the factor analysis. But it is \\(2 \\cdot R^2 - 1\\), and so ranges from -1 to +1. If your \\(R^2\\) is less than .5, it will be negative, which is not good.\nMiscellaneous results\nThere is a lot of other stuff in these objects, like a sample size corrected BIC, Grice’s validity coefficients, the actual residuals for the correlation matrix and more.\n\n\nstr(model, 1)\n\n\nList of 51\n $ residual     : num [1:10, 1:10] 0.8556 -0.0899 0.0122 0.0377 0.0573 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ dof          : num 26\n $ chi          : num 397\n $ nh           : num 2759\n $ rms          : num 0.04\n $ EPVAL        : num 5.71e-68\n $ crms         : num 0.0526\n $ EBIC         : num 191\n $ ESABIC       : num 273\n $ fit          : num 0.789\n $ fit.off      : num 0.981\n $ sd           : num 0.0382\n $ factors      : num 2\n $ complexity   : Named num [1:10] 1.09 1.01 1 1.02 1.08 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ n.obs        : int 2800\n $ objective    : num 0.228\n $ criteria     : Named num [1:3] 0.228 NA NA\n  ..- attr(*, \"names\")= chr [1:3] \"objective\" \"\" \"\"\n $ STATISTIC    : num 636\n $ PVAL         : num 1.6e-117\n $ Call         : language fa(r = bfi_trim, nfactors = 2)\n $ null.model   : num 2.82\n $ null.dof     : num 45\n $ null.chisq   : num 7881\n $ TLI          : num 0.865\n $ RMSEA        : Named num [1:4] 0.0916 0.0855 0.0978 0.9\n  ..- attr(*, \"names\")= chr [1:4] \"RMSEA\" \"lower\" \"upper\" \"confidence\"\n $ BIC          : num 430\n $ SABIC        : num 513\n $ r.scores     : num [1:2, 1:2] 1 -0.227 -0.227 1\n $ R2           : num [1:2] 0.842 0.768\n $ valid        : num [1:2] 0.905 0.852\n $ score.cor    : num [1:2, 1:2] 1 -0.185 -0.185 1\n $ weights      : num [1:10, 1:2] 0.00559 0.00986 -0.00452 -0.01416 -0.03449 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ rotation     : chr \"oblimin\"\n $ communality  : Named num [1:10] 0.144 0.467 0.563 0.237 0.395 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ communalities: Named num [1:10] 0.144 0.467 0.563 0.237 0.395 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ uniquenesses : Named num [1:10] 0.856 0.533 0.437 0.763 0.605 ...\n  ..- attr(*, \"names\")= chr [1:10] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ values       : num [1:10] 2.6714 1.5564 0.2727 0.1283 0.0455 ...\n $ e.values     : num [1:10] 3.185 2.11 0.938 0.768 0.71 ...\n $ loadings     : 'loadings' num [1:10, 1:2] 0.0744 0.0541 0.0311 -0.0519 -0.1191 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ model        : num [1:10, 1:10] 0.144 -0.25 -0.277 -0.184 -0.239 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ fm           : chr \"minres\"\n $ rot.mat      : num [1:2, 1:2] 0.857 0.549 -0.38 0.944\n $ Phi          : num [1:2, 1:2] 1 -0.186 -0.186 1\n  ..- attr(*, \"dimnames\")=List of 2\n $ Structure    : 'loadings' num [1:10, 1:2] 0.1413 -0.0748 -0.1098 -0.1402 -0.23 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ method       : chr \"regression\"\n $ scores       : num [1:2800, 1:2] -0.224 0.2186 0.532 -0.2429 -0.0564 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ R2.scores    : Named num [1:2] 0.842 0.768\n  ..- attr(*, \"names\")= chr [1:2] \"MR1\" \"MR2\"\n $ r            : num [1:10, 1:10] 1 -0.34 -0.265 -0.146 -0.181 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ np.obs       : num [1:10, 1:10] 2784 2757 2759 2767 2769 ...\n  ..- attr(*, \"dimnames\")=List of 2\n $ fn           : chr \"fa\"\n $ Vaccounted   : num [1:5, 1:2] 2.443 0.244 0.244 0.578 0.578 ...\n  ..- attr(*, \"dimnames\")=List of 2\n - attr(*, \"class\")= chr [1:2] \"psych\" \"fa\"\n\nIn turn, these are:\nresidual: The residual correlation matrix\ndof: The model degrees of freedom\nchi: The empirical model \\(X^2\\)\nnh: The harmonic sample size\nrms: Root mean square residual\nEPVAL: p-value for the empirical chi-square\ncrms: a ‘corrected’ rms\nEBIC: BIC for the empirical model\nESABIC: sample-size corrected BIC for the empirical model\nfit: Similar to fit.off. General fit index (how well is the observed correlation reproduced)\nfit.off: Fit based on off diagonals. Reported in the output. See above.\nsd: standard deviation of the off-diagonals of the residual correlation matrix.\nfactors: the number of factors\ncomplexity: The complexity scores. See above.\nn.obs: The number of observations (assuming complete data)\nobjective: The objective function for the model\ncriteria: Along with the objective function, additional fitting results\nSTATISTIC: The model-based \\(X^2\\)\nPVAL: The p-value for the model-based \\(X^2\\)\nCall: The function call\nnull.model: \\(X^2\\) test results for the null model\nnull.dof: \\(X^2\\) test results for the null model\nnull.chisq: \\(X^2\\) test results for the null model\nTLI: Tucker-Lewis fit index\nRMSEA: Root mean square error of approximation with upper and lower bounds\nBIC: Bayesian Information Criterion for the model\nSABIC: sample-size corrected BIC for the model\nr.scores: The correlations of the factor score estimates using the specified model, if they were to be found. Comparing these correlations with that of the scores themselves will show, if an alternative estimate of factor scores is used (e.g., the tenBerge method), the problem of factor indeterminacy. For these correlations will not necessarily be the same.\nR2: correlation of factors and estimated factor scores (squared)\nvalid: validity coefficients\nscore.cor: The correlation matrix of course coded (unit weighted) factor score estimates (i.e. sum scores), if they were to be found, based upon the loadings matrix rather than the weights matrix.\nweights: weights used to construct factor scores\nrotation: rotation used\ncommunality: communality scores h2\ncommunalities: So nice they put them twice (actually not entirely equal)\nuniquenesses: Uniquenesses u2\nvalues eigenvalues of the model implied correlation matrix\ne.values: eigenvalues of the correlation matrix\nloadings: the factor loadings\nmodel: the model-implied correlation matrix\nfm: the estimation approach\nrot.mat: matrix used in the rotated solution\nPhi: factor score correlation matrix\nStructure: this is just the loadings (pattern) matrix times the factor intercorrelation matrix (Phi).\nmethod: method used to construct the factor scores\nscores: estimated factor scores\nR2.scores: estimated correlation of factor scores with the factor (squared)\nr: the (possibly smoothed) correlation matrix of the observation\nnp.obs: pairwise sample sizes (used to get the harmonic mean)\nfn: the function used\nVaccounted: the SS loadings output.\nAdditional Notes for Factor Analysis\nMost of the above would apply to other versions of fa and principal for principal components analysis.\nThough rarely done, if you only provide a correlation matrix as your data, you will not get a variety of metrics in the results, nor factor scores.\nCertain rotations will lead to differently named factors, and possibly lacking some output (e.g. varimax won’t have factor correlations).\nOther Functions\nWhile the previous will help explain factor analysis and related models, a similar issue arises elsewhere with other package functions that might be of interest. I’ll explain some of those here as interest and personal use dictates.\nAlpha\nThis is a quick reminder for the results of the reliability coefficient \\(\\alpha\\). For this we’ll just use the agreeableness items to keep things succinct.\nBasic Results\n\n\nagreeableness = bfi_trim[,1:5]\n\n# check.keys will rescale negatively scored items\nalpha_results = alpha(agreeableness, check.keys = TRUE, n.iter=10) \nalpha_results\n\n\n\nReliability analysis   \nCall: alpha(x = agreeableness, check.keys = TRUE, n.iter = 10)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n       0.7      0.71    0.68      0.33 2.5 0.009  4.7 0.9     0.34\n\n lower alpha upper     95% confidence boundaries\n0.69 0.7 0.72 \n\n lower median upper bootstrapped confidence intervals\n 0.68 0.7 0.71\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nA1-      0.72      0.73    0.67      0.40 2.6   0.0087 0.0065  0.38\nA2       0.62      0.63    0.58      0.29 1.7   0.0119 0.0169  0.29\nA3       0.60      0.61    0.56      0.28 1.6   0.0124 0.0094  0.32\nA4       0.69      0.69    0.65      0.36 2.3   0.0098 0.0159  0.37\nA5       0.64      0.66    0.61      0.32 1.9   0.0111 0.0126  0.34\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nA1- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nA2  2773  0.73  0.75  0.67   0.56  4.8 1.2\nA3  2774  0.76  0.77  0.71   0.59  4.6 1.3\nA4  2781  0.65  0.63  0.47   0.39  4.7 1.5\nA5  2784  0.69  0.70  0.60   0.49  4.6 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.29 0.14 0.12 0.08 0.03 0.01\nA2 0.02 0.05 0.05 0.20 0.37 0.31 0.01\nA3 0.03 0.06 0.07 0.20 0.36 0.27 0.01\nA4 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nA5 0.02 0.07 0.09 0.22 0.35 0.25 0.01\n\nraw_alpha: Raw estimate of alpha (based on covariances)\nstd.alpha: Standardized estimate. This value is what is typically reported (though most applied researchers would probably not be able to tell you which they reported). Identical to raw if data is already standardized.\nG6 (smc): Guttman’s \\(\\lambda_6\\), the amount of variance in each item that can be accounted for the linear regression of all of the other items\naverage_r: Average inter-item correlation among the items.\nS/N: Signal to noise ratio, \\(n \\cdot r/(1-r)\\) where r is the average_r\nase: standard error for raw \\(\\alpha\\) (used for the confidence interval, bootstrapped would be better)\nmean: the mean of the total/mean score of the items\nsd: the standard deviation of the total/mean score of the items\nmedian_r: median inter-item correlation\nAfter the initial statistics, the same stats are reported but for a result where a specific item is dropped. For example, if your \\(\\alpha\\) goes up when an item is dropped, it probably isn’t a good item. In this case, the negatively scored item is probably worst, which isn’t an uncommon result.\nItem statistics\nNext we get the item statistics, they are as follows.\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nA1- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nA2  2773  0.73  0.75  0.67   0.56  4.8 1.2\nA3  2774  0.76  0.77  0.71   0.59  4.6 1.3\nA4  2781  0.65  0.63  0.47   0.39  4.7 1.5\nA5  2784  0.69  0.70  0.60   0.49  4.6 1.3\nn: number of complete observations\nraw.r: correlation of each item with the total score\nstd.r: correlation of each item with the total score if the items were all standardized\nr.cor: item correlation corrected for item overlap and scale reliability\nr.drop item correlation for this item against the scale without this item\nmean: item mean\nsd: item standard deviation\nResponse frequency\nFinally we have information about the missingness of each item. The initial values show the proportion of each level observed, while the last column shows the percentage missing.\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.29 0.14 0.12 0.08 0.03 0.01\nA2 0.02 0.05 0.05 0.20 0.37 0.31 0.01\nA3 0.03 0.06 0.07 0.20 0.36 0.27 0.01\nA4 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nA5 0.02 0.07 0.09 0.22 0.35 0.25 0.01\nOther Output\nIn addition to these we have a bit more from the output.\n\n\nstr(alpha_results[5:length(alpha_results)])\n\n\nList of 10\n $ keys   : Named num [1:5] -1 1 1 1 1\n  ..- attr(*, \"names\")= chr [1:5] \"A1\" \"A2\" \"A3\" \"A4\" ...\n $ scores : Named num [1:2800] 4 4.2 3.8 4.6 4 4.6 4.6 2.6 3.6 5.4 ...\n  ..- attr(*, \"names\")= chr [1:2800] \"61617\" \"61618\" \"61620\" \"61621\" ...\n $ nvar   : int 5\n $ boot.ci: Named num [1:3] 0.678 0.697 0.708\n  ..- attr(*, \"names\")= chr [1:3] \"2.5%\" \"50%\" \"97.5%\"\n $ boot   : num [1:10, 1:10] 0.697 0.709 0.698 0.694 0.705 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:10] \"raw_alpha\" \"std.alpha\" \"G6(smc)\" \"average_r\" ...\n $ Unidim :List of 1\n  ..$ Unidim: num 0.668\n $ var.r  : num 0.0134\n $ Fit    :List of 1\n  ..$ Fit.off: num 0.974\n $ call   : language alpha(x = agreeableness, check.keys = TRUE, n.iter = 10)\n $ title  : NULL\n\nIn turn these are:\nkeys: how the items are score (-1 if reverse scored)\nscores: row means/sums depending on the cumulative argument\nnvar: the number of variables/items\nboot.ci: the bootstrapped confidence interval for \\(\\alpha\\) (if requested)\nboot: the bootstrapped values of \\(\\alpha\\) and other statistics (if requested)\nUnidim: index of unidimensionalty. \\(\\alpha\\) is a lower bound of a reliability estimate if the data is not unidimensional. See ?unidim for details.\nvar.r: This doesn’t appear to be documented anywhere, but it is depicted in the Reliability if item is dropped section. It is the variance of the values of the lower triangle of a correlation matrix.\nFit: see Fit based upon off diagonal values for the factor analysis section above.\ncall: the function call\ntitle: title of the results (if requested)\nOmega\nOmega is another reliability metric that finally has been catching on. The psych function omega requires a factor analysis to be run behind the scenes, specifically a bifactor model, so most of the output is the same as with other factor analysis. In addition, the results also provide coefficient \\(\\alpha\\) and Guttman’s \\(\\lambda_6\\) that were explained in the alpha section.\nHowever there is a little more to it, so we’ll explain those aspects. The key help files are ?omega and ?schmid.\n\n\nomega_result = omega(bfi[,1:15])\n\n\n\nomega_result\n\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.81 \nG.6:                   0.83 \nOmega Hierarchical:    0.54 \nOmega H asymptotic:    0.64 \nOmega Total            0.85 \n\nSchmid Leiman Factor loadings greater than  0.2 \n        g   F1*   F2*   F3*   h2   u2   p2\nA1-  0.24              0.30 0.16 0.84 0.36\nA2   0.52              0.44 0.47 0.53 0.58\nA3   0.59              0.45 0.56 0.44 0.63\nA4   0.39              0.28 0.25 0.75 0.62\nA5   0.56              0.31 0.44 0.56 0.70\nC1               0.53       0.31 0.69 0.11\nC2   0.23        0.60       0.42 0.58 0.12\nC3   0.21        0.51       0.32 0.68 0.14\nC4-  0.25        0.59       0.41 0.59 0.15\nC5-  0.28        0.51       0.34 0.66 0.23\nE1-  0.37  0.47             0.37 0.63 0.38\nE2-  0.48  0.54             0.53 0.47 0.43\nE3   0.47  0.36             0.37 0.63 0.61\nE4   0.54  0.46             0.51 0.49 0.57\nE5   0.41  0.32  0.25       0.33 0.67 0.51\n\nWith eigenvalues of:\n   g  F1*  F2*  F3* \n2.47 1.03 1.60 0.69 \n\ngeneral/max  1.54   max/min =   2.32\nmean percent general =  0.41    with sd =  0.21 and cv of  0.52 \nExplained Common Variance of the general factor =  0.43 \n\nThe degrees of freedom are 63  and the fit is  0.34 \nThe number of observations was  2800  with Chi Square =  961.82  with prob <  6.4e-161\nThe root mean square of the residuals is  0.04 \nThe df corrected root mean square of the residuals is  0.05\nRMSEA index =  0.071  and the 10 % confidence intervals are  0.067 0.075\nBIC =  461.77\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 90  and the fit is  1.58 \nThe number of observations was  2800  with Chi Square =  4407.94  with prob <  0\nThe root mean square of the residuals is  0.13 \nThe df corrected root mean square of the residuals is  0.14 \n\nRMSEA index =  0.131  and the 10 % confidence intervals are  0.128 0.134\nBIC =  3693.57 \n\nMeasures of factor score adequacy             \n                                                 g   F1*  F2*   F3*\nCorrelation of scores with factors            0.78  0.70 0.82  0.60\nMultiple R square of scores with factors      0.61  0.49 0.68  0.36\nMinimum correlation of factor score estimates 0.22 -0.01 0.36 -0.28\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*  F3*\nOmega total for total scores and subscales    0.85 0.77 0.73 0.73\nOmega general for total scores and subscales  0.54 0.40 0.11 0.46\nOmega group for total scores and subscales    0.25 0.36 0.62 0.27\n\nThe bifactor model requires a single general factor and minimally three specific factors, as the plot shows. However, you can run it on single factor just to get the omega total statistic, but any less than three factors will produce a warning and some metrics will either be unavailable or not make much sense.\nReliability\nAlpha:                 0.81 \nG.6:                   0.83 \nOmega Hierarchical:    0.54 \nOmega H asymptotic:    0.64 \nOmega Total            0.85 \nThe first two pieces of info are as in alpha, the next regard \\(\\omega\\) specifically. \\(\\omega\\) is based on the squared factor loadings. \\(\\omega_{hierarchical}\\) regards just the loadings of the general factor. The asymptotic is the same for a ‘test of infinite items’, and so can be seen as an upper bound. \\(\\omega_{total}\\) is based on all the general and specific factor loadings. I personally like to think of the ratio of \\(\\frac{\\omega_{hier}}{\\omega_{total}}\\), which if very high, say .9 or so, may suggest unidimensionality.\nLoadings\nSchmid Leiman Factor loadings greater than  0.2 \n        g   F1*   F2*   F3*   h2   u2   p2\nA1-  0.24              0.30 0.16 0.84 0.36\nA2   0.52              0.44 0.47 0.53 0.58\nA3   0.59              0.45 0.56 0.44 0.63\nA4   0.39              0.28 0.25 0.75 0.62\nA5   0.56              0.31 0.44 0.56 0.70\nC1               0.53       0.31 0.69 0.11\nC2   0.23        0.60       0.42 0.58 0.12\nC3   0.21        0.51       0.32 0.68 0.14\nC4-  0.25        0.59       0.41 0.59 0.15\nC5-  0.28        0.51       0.34 0.66 0.23\nE1-  0.37  0.47             0.37 0.63 0.38\nE2-  0.48  0.54             0.53 0.47 0.43\nE3   0.47  0.36             0.37 0.63 0.61\nE4   0.54  0.46             0.51 0.49 0.57\nE5   0.41  0.32  0.25       0.33 0.67 0.51\n\nWith eigenvalues of:\n   g  F1*  F2*  F3* \n2.47 1.03 1.60 0.69 \n\ngeneral/max  1.54   max/min =   2.32\nmean percent general =  0.41    with sd =  0.21 and cv of  0.52 \nExplained Common Variance of the general factor =  0.43 \nThe loadings are for the general and specific factors are provided, as well as the communalities and uniquenesses. In addition there is a column for p2, which is considered a diagnostic tool for the appropriateness of a hierarchical model. It is defined as “percent of the common variance for each variable that is general factor variance”, which is just g2/h2. The line of mean percent general... isn’t documented and is a result of the unexported print.psych.omega function. It wasn’t obvious to me, but these are merely statistics regarding the p2 column (cv is the coefficient of variation).\nNext you get eigenvalue/variance accounted as in standard factor analysis. Then general/max and max/min regard those ratios of the corresponding eigenvalues. Explained common variance is the percent of variance attributable to the general factor (g/sum(all eigenvalues))\nModel test results & fit\nThe degrees of freedom are 63  and the fit is  0.34 \nThe number of observations was  2800  with Chi Square =  961.82  with prob <  6.4e-161\nThe root mean square of the residuals is  0.04 \nThe df corrected root mean square of the residuals is  0.05\nRMSEA index =  0.071  and the 10 % confidence intervals are  0.067 0.075\nBIC =  461.77\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 90  and the fit is  1.58 \nThe number of observations was  2800  with Chi Square =  4407.94  with prob <  0\nThe root mean square of the residuals is  0.13 \nThe df corrected root mean square of the residuals is  0.14 \n\nRMSEA index =  0.131  and the 10 % confidence intervals are  0.128 0.134\nBIC =  3693.57 \nThe only thing different here relative to the standard factor analysis results is that there are two models considered- a model with general and specific factors and a model with no specific factors.\nMeasures of factor score adequacy\nMeasures of factor score adequacy             \n                                                 g   F1*  F2*   F3*\nCorrelation of scores with factors            0.78  0.70 0.82  0.60\nMultiple R square of scores with factors      0.61  0.49 0.68  0.36\nMinimum correlation of factor score estimates 0.22 -0.01 0.36 -0.28\nThis first part of the output is the same as standard factor analysis (see above).\nVariance accounted for by group and specific factors\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*  F3*\nOmega total for total scores and subscales    0.85 0.77 0.73 0.73\nOmega general for total scores and subscales  0.54 0.40 0.11 0.46\nOmega group for total scores and subscales    0.25 0.36 0.62 0.27\nThis part is explained in the ?omega helpfile as:\n\nThe notion of omega may be applied to the individual factors as well as the overall test. A typical use of omega is to identify subscales of a total inventory. Some of that variability is due to the general factor of the inventory, some to the specific variance of each subscale. Thus, we can find a number of different omega estimates: what percentage of the variance of the items identified with each subfactor is actually due to the general factor. What variance is common but unique to the subfactor, and what is the total reliable variance of each subfactor. These results are reported in omega.group object and in the last few lines of the normal output.\n\nAs noted, this is contained in omega_result$omega.group. For the unique factors, these sum very simply as total = general + group. The ones for unique factors pertain only to the loadings and part of the correlation matrix for those items specific to that factor. Take agreeableness for example, we are only concerned with the variance of those items. The ‘general’ part regards the loadings of g for the agreeableness items, the group part the loadings of the agreeableness items, and the ‘total’ is just their sum.\nThe first column, g, just regurgitates \\(\\omega\\) and \\(\\omega_h\\) from the beginning for the first two values, and adds yet another statistic, based only on the sum of variance attributable to each unique factor. Unlike the \\(\\omega_{total}\\), this calculation does not include off-loadings the unique factors have, only the items that are grouped with each factor. In pseudo-code:\nfor (i in specific) {\n specific_var[i] = sum(specific[i]$loadings[specific_items[i]])^2)\n}\n\nvalue = sum(specific_var) / total_var\nThat is the variance uniquely defined by the specific factors. Had it included all the loadings for each specific factor calculation, then group + general = total for g as well.\nUnidimensionality\nRevelle provides an ‘exploratory’ statistic of unidimensionality, or how well a set of variables may be explained by one construct. In practice, you may find multiple factors fit better, e.g. via BIC, but the resulting factors may be highly correlated, so you might still want to consider a single construct. Something like unidim will help make a decision on how viable using a sum score might be for regression or other models. It is explained in the help file as follows:\n\nThe fit FF’ (model implied correlation matrix based on a one factor model) should be identical to the (observed) correlation matrix minus the uniquenesses. unidim is just the ratio of these two estimates. The higher it is, the more the evidence for unidimensionality.\n\nI’ll run it for both the case where there is only a single construct vs. two underlying constructs.\n\n\nunidim(bfi[,1:5])\n\n\n\nA measure of unidimensionality \n Call: unidim(x = bfi[, 1:5])\n\nUnidimensionality index = \n       u av.r fit   fa.fit    alpha     av.r median.r Unidim.A \n    0.89     0.90     0.99     0.71     0.33     0.34     1.00 \n\nunidim adjusted index reverses negatively scored items.\nalpha    Based upon reverse scoring some items.\naverage and median  correlations are based upon reversed scored items\n\nunidim(bfi_trim)\n\n\n\nA measure of unidimensionality \n Call: unidim(x = bfi_trim)\n\nUnidimensionality index = \n       u av.r fit   fa.fit    alpha     av.r median.r Unidim.A \n    0.45     0.62     0.73     0.75     0.23     0.17     0.93 \n\nunidim adjusted index reverses negatively scored items.\nalpha    Based upon reverse scoring some items.\naverage and median  correlations are based upon reversed scored items\n\nThe values reported are as follows. In general, you’d pay attention to the adjusted results that are based on items that are reverse scored if needed. If there are no reverse scored items (which you generally should be doing), then these adjusted metrics will be identical to the raw metrics.\nRaw Unidim: The raw value of the unidimensional criterion\nAdjusted: The unidimensional criterion when items are keyed in positive direction.\nFit1: The off diagonal fit from fa. (explained above)\nalpha: Standardized \\(\\alpha\\) of the keyed items (after appropriate reversals)\nav.r: The average inter-item correlation of the keyed items.\noriginal model: The ratio of the FF’ (model implied correlation matrix based on the loadings) model to the sum(R).\nadjusted model: The ratio of the FF’ model to the sum(R) when items are flipped.\nraw.total: sum(R - uniqueness)/sum(R)\nadjusted total: raw.total ratio with flipped items\nMediate\nThe psych makes even somewhat complicated mediation models about as easily conducted as they can be, assuming you are only dealing with fully observed (no latent) variables and linear models with continuous endogenous variables that are assumed to be normally distributed. Though the output should be straightforward if one understands basic regression as well as the basics of mediation, we demonstrate it here.\nInitial Model\nGarcia, Schmitt, Branscome, and Ellemers (2010) report data for 129 subjects on the effects of perceived sexism on anger and liking of women’s reactions to in-group members who protest discrimination. We will predict liking (how much the individual liked the target) while using protest (prot2 yes or no) and sexism (a scale score based on multiple items) as predictors, and a scaled score of the appropriateness of the target’s response as the mediator (respappr).\n\n\ndata(GSBE)   # The Garcia et al data set; see ?GSBE for details\n\nmodel <- mediate(\n  liking ~  sexism + prot2 + (respappr),\n  data   = Garcia,\n  n.iter = 500,\n  plot   = FALSE\n)   \n\n\n\nVisual Results\nTo start, most of the output of psych is straightforward if you understand what mediation is, as it follows the same depiction and even uses the same labels as most initial demonstrations of mediation I’ve come across. So if it’s confusing, you probably need to review what such models are attempting to accomplish. The visualization it automatically produces is even clearer for storytelling. I reserved plotting for display here so as to make it easier to compare to the printed output.\n\n\nmediate.diagram(model, digits = 3)\n\n\n\n\nWe see the original effects of sexism and prot2 as c, and what they are after including the mediator c', where the difference between those values is equivalent to a * b, i.e. the indirect effect (a is the coefficient from the predictor to mediator, b is from the mediator to the outcome). The rest are standard path/regression coefficients as well.\nStatistical Results\nNow we will just print the result.\n\n\nprint(model, digits = 3)\n\n\n\nMediation/Moderation Analysis \nCall: mediate(y = liking ~ sexism + prot2 + (respappr), data = Garcia, \n    n.iter = 500, plot = FALSE)\n\nThe DV (Y) was  liking . The IV (X) was  sexism prot2 . The mediating variable(s) =  respappr .\n\nTotal effect(c) of  sexism  on  liking  =  0.111   S.E. =  0.116  t  =  0.956  df=  126   with p =  0.341\nDirect effect (c') of  sexism  on  liking  removing  respappr  =  0.096   S.E. =  0.104  t  =  0.923  df=  125   with p =  0.358\nIndirect effect (ab) of  sexism  on  liking  through  respappr   =  0.015 \nMean bootstrapped indirect effect =  0.016  with standard error =  0.055  Lower CI =  -0.096    Upper CI =  0.124\n\nTotal effect(c) of  prot2  on  liking  =  0.471   S.E. =  0.195  t  =  2.417  df=  126   with p =  0.0171\nDirect effect (c') of  prot2  on  liking  removing  respappr  =  -0.105   S.E. =  0.201  t  =  -0.522  df=  125   with p =  0.602\nIndirect effect (ab) of  prot2  on  liking  through  respappr   =  0.576 \nMean bootstrapped indirect effect =  0.576  with standard error =  0.156  Lower CI =  0.296    Upper CI =  0.916\nR = 0.501 R2 = 0.251   F = 13.967 on 3 and 125 DF   p-value:  1.903e-09 \n\n To see the longer output, specify short = FALSE in the print statement or ask for the summary\n\nThe output simply shows the same results as the graph. The total effect is the effect of a covariate on the outcome, without the mediator. For example, the effect of sexism on liking without the mediation is 0.111. We can reproduce it as follows. The statistical result is identical to the lm output.\n\n\nsummary(lm(liking ~  sexism + prot2, data = Garcia))\n\n\n\nCall:\nlm(formula = liking ~ sexism + prot2, data = Garcia)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3857 -0.6246  0.0599  0.7754  1.7954 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.7468     0.6110   7.768 2.41e-12 ***\nsexism        0.1111     0.1162   0.956   0.3410    \nprot2         0.4711     0.1949   2.417   0.0171 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.03 on 126 degrees of freedom\nMultiple R-squared:  0.0523,    Adjusted R-squared:  0.03726 \nF-statistic: 3.477 on 2 and 126 DF,  p-value: 0.03391\n\nThe direct effect is the effect of sexism with the mediator in the model. We can reproduce the effect here. However this version of the model printout currently has a bug where, after the coefficient, it is reporting SE t etc. from the intercept. If you do a summary(model), as we will shortly, you’ll get the correct statistical test until it is fixed.\n\n\nsummary(lm(liking ~  sexism + prot2 + respappr, data = Garcia))\n\n\n\nCall:\nlm(formula = liking ~ sexism + prot2 + respappr, data = Garcia)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0210 -0.5337  0.0874  0.6591  2.6760 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.26766    0.60281   5.421 2.94e-07 ***\nsexism       0.09584    0.10379   0.923    0.358    \nprot2       -0.10482    0.20065  -0.522    0.602    \nrespappr     0.40075    0.06958   5.760 6.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9193 on 125 degrees of freedom\nMultiple R-squared:  0.2511,    Adjusted R-squared:  0.2331 \nF-statistic: 13.97 on 3 and 125 DF,  p-value: 6.545e-08\n\nThe indirect effect coefficient is the product of the a and b paths: 0.038 * 0.401. Along with this, the bootstrapped interval estimate is provided (you can ignore the mean bootstrapped effect, which is equal to the effect with enough iterations). There is no p-value, but it’s not needed anyway for any of these results.\nAfter that, the same results are provided for the prot2 predictor. Finally, the \\(R^2\\) and F test for the overall model are reported, which are identical to the lm summary results that include all effects. I would suggest reporting the adjusted \\(R^2\\) from that instead.\nA much cleaner result that incorporates the lm results we did can be obtained by summarizing instead of printing the fitted model. All of this output is available as elements of the model object itself.\n\n\nsummary(model)\n\n\nCall: mediate(y = liking ~ sexism + prot2 + (respappr), data = Garcia, \n    n.iter = 500, plot = FALSE)\n\nDirect effect estimates (traditional regression)    (c') \n          liking   se     t  df     Prob\nIntercept   3.27 0.60  5.42 125 2.94e-07\nsexism      0.10 0.10  0.92 125 3.58e-01\nprot2      -0.10 0.20 -0.52 125 6.02e-01\nrespappr    0.40 0.07  5.76 125 6.18e-08\n\nR = 0.5 R2 = 0.25   F = 13.97 on 3 and 125 DF   p-value:  6.54e-08 \n\n Total effect estimates (c) \n          liking   se    t  df     Prob\nIntercept   4.75 0.61 7.77 126 2.41e-12\nsexism      0.11 0.12 0.96 126 3.41e-01\nprot2       0.47 0.19 2.42 126 1.71e-02\n\n 'a'  effect estimates \n          respappr   se    t  df     Prob\nIntercept     3.69 0.70 5.29 126 5.33e-07\nsexism        0.04 0.13 0.29 126 7.75e-01\nprot2         1.44 0.22 6.45 126 2.15e-09\n\n 'b'  effect estimates \n         liking   se    t  df     Prob\nrespappr    0.4 0.07 5.78 126 5.47e-08\n\n 'ab'  effect estimates (through mediators)\n       liking boot   sd lower upper\nsexism   0.02 0.02 0.06  -0.1  0.12\nprot2    0.58 0.58 0.16   0.3  0.92\n\nIf you are doing mediation with linear models only, you would be hard-pressed to find an easier tool to use than the psych package. It can incorporate multiple mediators and so-called ‘moderated mediation’ as well. However, just because it is easy to do a mediation model, doesn’t mean you should.\nConclusion\nThe psych package is very powerful and provides a lot of results from just a single line of code. However, the documentation, while excellent in general, fails to note many pieces of output, or clearly explain it, at least, not without consulting other references (which are provided). Hopefully this saves others some time when they use it. I may add some other functions to explain in time, so check back at some point.\n\n\n\n",
    "preview": "posts/2020-04-10-psych-explained/../../img/factor_analysis.png",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {},
    "preview_width": 2172,
    "preview_height": 1434
  },
  {
    "path": "posts/2020-03-16-convergence/",
    "title": "Convergence Problems",
    "description": "Lack of convergence got ya down? A plan of attack.",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-03-16",
    "categories": [
      "regression",
      "mixed models"
    ],
    "contents": "\n\nContents\nIntro\nData Setup\nInitial model\nStep back\nStart simply\nRescale variables\nRemove zero random effects\nTechnical options\nCheck singularity\nDouble-checking gradient calculations\nRestart the fit\nChange the optimizer\n\nFinal Step\nSummary\nReferences\n\n\nLast updated March 12, 2021.\nPrerequisite: Knowledge of regression modeling. Helpful would be to know something about mixed models and optimization.\nPrimary packages used:\n\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(mixedup)  # http://m-clark.github.io/mixedup\n\n\n\nIntro\nIt is not uncommon that complex models lead to difficulties with convergence. Though the following example is a demo with the R package lme4, most of it would potentially apply to any complex modeling situation where convergence problems arise. The goal is provide some steps one can take to get their models back on track. The running example is taken from the data posted at this stackoverflow question. Ben Bolker’s1 response there can be seen as the basis for this post, along with some extensions, updates and other organization.\nData Setup\nYou can download the data from here (RDS file), or go to the stackoverflow discussion and paste the code there. There isn’t any real explanation of the variables unfortunately, though you can get a sense of some of them (e.g. Day, replicate, temperature, etc.).\n\n\n\n\n\n# df = readRDS(\n#   gzcon(\n#     url('https://github.com/m-clark/m-clark.github.io/raw/master/data/convergence.RDS')\n#   )\n# )\n\ndf = readRDS('data/convergence.RDS')\n\ndf = df %>% \n  mutate(\n    SUR.ID = factor(SUR.ID),\n    replicate = factor(replicate),\n    Unit = factor(1:n())\n  )\n\n\n\n\n\n\nInitial model\nThe following is the model that led to the stackoverflow post. It’s fairly complicated with multiple interactions and random effects, modeling the proportion of valid detections via a binomial model. The Unit effect is used to account for overdispersion, a common issue in count modeling.\n\n\nmodel_mixed_0 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    tm:Area + tm:c.distance + c.distance:Area + c.tm.depth:Area + \n    c.receiver.depth:Area + c.temp:Area + c.wind:Area + c.tm.depth + \n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + replicate + \n    (1|SUR.ID) + (1|Day) + (1|Unit), \n  data = df, \n  family = binomial(link = logit)\n)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| =\n0.131568 (tol = 0.002, component 1)\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue\n - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\nThis gives several warnings, the more egregious of which is that the model has not converged, meaning the estimates may not be trustworthy. So what do we do?\nStep back\nThe first step is to step back and look at the data. Are there issues that can be spotted? Are some essentially collinear with others? In the following, we can see that some variables are only a few levels, and some, like Day, c.tm.depth and c.receiver.depth, are both notably correlated with each other and with other predictor variables.\n\n$`Numeric Variables`\n# A tibble: 9 x 10\n  Variable             N  Mean     SD     Min      Q1 Median    Q3    Max `% Missing`\n  <chr>            <dbl> <dbl>  <dbl>   <dbl>   <dbl>  <dbl> <dbl>  <dbl>       <dbl>\n1 ValidDetections    220  6.49  10.1     0       0      1     7     45              0\n2 CountDetections    220  6.85  10.1     0       0      2     7     45              0\n3 FalseDetections    220  0.36   0.73    0       0      0     0      4              0\n4 R.det              220  0.52   0.47    0       0      0.68  1      1              0\n5 c.receiver.depth   220 -0.06   0.31   -0.4    -0.34  -0.18  0.21   0.54           0\n6 c.tm.depth         220 -0.01   0.48   -0.62   -0.55   0.03  0.4    0.78           0\n7 c.temp             220 -0.46   2.38   -4.22   -3.62   0.54  1.55   2.84           0\n8 c.wind             220 -0.26   3.29   -2.97   -2.97  -2.94  1.28   5.88           0\n9 c.distance         220 -6.36 129.   -160    -110    -10    90    190              0\n\n$`Categorical Variables`\n# A tibble: 14 x 4\n   Variable  Group        Frequency   `%`\n   <chr>     <fct>            <int> <dbl>\n 1 SUR.ID    10185               74   34 \n 2 SUR.ID    10186               74   34 \n 3 SUR.ID    10250               72   33 \n 4 tm        CT                 110   50 \n 5 tm        PT-04              110   50 \n 6 replicate 1                  120   55.\n 7 replicate 2                  100   45 \n 8 Area      Drug Channel       120   55.\n 9 Area      Finger             100   45 \n10 Day       03/06/13            60   27 \n11 Day       2/22/13             60   27 \n12 Day       2/27/13             60   27 \n13 Day       3/14/13             28   13 \n14 Day       2/26/13             12    5 \n\n\n\n\nWe can obtain a rough metric of total correlation of a variable with the others by looking at the variance inflation factor (VIF)2. For this, we’ll treat any binary or ordered data as numeric, and we can use the car package to get the VIF. This requires running an arbitrary regression model that includes the covariates of interest, but the value is derived only from the predictor variables. If we just try it with a dummy model, we get an error, since the linear model has perfect collinearity. We find out that Day is probably causing issues, and we’ll see why later.\n\n\n# the model used to acquire the vif doesn't matter- anything could be used for\n# the target variable\n\n# this produces errors about aliased coefficients\n# car::vif(lm(CountDetections ~ . - Unit - FalseDetections - ValidDetections, dat = df))\n\nattributes(alias(\n  lm(\n    CountDetections ~ . - Unit - FalseDetections - ValidDetections,\n    dat = df\n  )\n)$Complete)$dimnames[[1]]\n\n\n[1] \"Day3/14/13\" \"c.wind\"    \n\ncar::vif(lm(CountDetections ~ . -Unit -Day -FalseDetections, dat = df))\n\n\n                      GVIF Df GVIF^(1/(2*Df))\nSUR.ID            1.200902  2        1.046832\ntm                1.066227  1        1.032583\nValidDetections   2.708644  1        1.645796\nreplicate         1.057540  1        1.028368\nArea              5.795510  1        2.407387\nR.det             2.393891  1        1.547220\nc.receiver.depth  8.674740  1        2.945291\nc.tm.depth       13.438407  1        3.665843\nc.temp            2.956315  1        1.719394\nc.wind            3.041363  1        1.743950\nc.distance        2.065335  1        1.437127\n\nIt looks like, along with Day, tm.depth is probably going to cause a problem, as more than 90% of its variance is accounted for by the other covariates. So at this point we can consider both it and Day as potential predictors to remove. Wind is less an issue once Day is removed.\nAn alternative approach I tried (not shown) was just running a PCA on the predictor variables. Only six components were needed to account for nearly almost 90% of the total variance, so I think it’s safe to say there is a notable amount of redundancy in this data.\nOne final note is that the target variable is a potential problem as well. More than half of the data is a 1.0 proportion of Valid Detections, and it also turns out this doesn’t appear to be proportional data per se, as sometimes both Valid and False Detections can be zero, meaning such observations don’t contribute meaningfully to the model. This isn’t necessarily a problem as these observations are basically dropped dropped from the model, and you will note in the GLM results that follow that the residual degrees of freedom will be notably lower than the total number of observations (220). This is due to the dropout of those observations, which is about 37% of the data. Even so, the lack of variability may be an underlying issue in general. Especially in the case of interactions, it is likely that some cells have no variability in the outcome.\nStart simply\nSo we know there are some data issues, so let’s start with a model that’s relatively simple but still plausible. If we just look at a GLM without any random effects, can we spot any issues?\n\n\nmodel_glm_1 = glm(\n  cbind(ValidDetections, FalseDetections) ~\n    tm:Area + tm:c.distance + c.distance:Area + c.tm.depth:Area +\n    c.receiver.depth:Area + c.temp:Area + c.wind:Area + c.tm.depth +\n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + replicate +\n    SUR.ID + Day,\n  data = df,\n  family = binomial(link = logit)\n)\n\nsummary(model_glm_1)\n\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ tm:Area + \n    tm:c.distance + c.distance:Area + c.tm.depth:Area + c.receiver.depth:Area + \n    c.temp:Area + c.wind:Area + c.tm.depth + c.receiver.depth + \n    c.temp + c.wind + tm + c.distance + Area + replicate + SUR.ID + \n    Day, family = binomial(link = logit), data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-2.739   0.000   0.000   0.539   2.161  \n\nCoefficients: (3 not defined because of singularities)\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -1.125e+01  6.504e+00  -1.730  0.08372 .  \nc.tm.depth                  -8.406e-01  1.228e+00  -0.684  0.49376    \nc.receiver.depth             7.026e+00  7.852e+00   0.895  0.37095    \nc.temp                      -5.514e+00  2.538e+00  -2.172  0.02983 *  \nc.wind                      -6.256e+00  3.349e+00  -1.868  0.06179 .  \ntmPT-04                     -2.053e+00  4.961e-01  -4.139  3.5e-05 ***\nc.distance                  -3.685e-03  2.614e-03  -1.410  0.15864    \nAreaFinger                   5.901e+01  2.775e+01   2.127  0.03346 *  \nreplicate2                   2.700e+00  1.161e+00   2.326  0.02000 *  \nSUR.ID10186                 -2.227e-01  4.456e-01  -0.500  0.61721    \nSUR.ID10250                 -2.995e-01  4.469e-01  -0.670  0.50281    \nDay2/22/13                  -7.114e+01  3.463e+01  -2.054  0.03996 *  \nDay2/26/13                   3.306e+00  1.134e+03   0.003  0.99767    \nDay2/27/13                          NA         NA      NA       NA    \nDay3/14/13                          NA         NA      NA       NA    \ntmPT-04:AreaFinger           4.308e-01  6.066e-01   0.710  0.47759    \ntmPT-04:c.distance          -5.332e-03  3.186e-03  -1.674  0.09419 .  \nAreaFinger:c.distance        1.192e-02  3.855e-03   3.091  0.00199 ** \nAreaFinger:c.tm.depth       -2.815e+00  4.731e+00  -0.595  0.55189    \nAreaFinger:c.receiver.depth -3.211e+01  2.545e+01  -1.262  0.20708    \nAreaFinger:c.temp            2.377e+00  1.902e+00   1.250  0.21141    \nAreaFinger:c.wind                   NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 135.23  on 119  degrees of freedom\nAIC: 253.65\n\nNumber of Fisher Scoring iterations: 17\n\nSure enough, there are problems. What’s going on with Day and Area? One issue is that Area only couples with certain days, so having one already tells you a lot about what the other could tell you.\n\n\nwith(df, table(Day, Area))\n\n\n          Area\nDay        Drug Channel Finger\n  03/06/13           60      0\n  2/22/13             0     60\n  2/26/13             0     12\n  2/27/13            60      0\n  3/14/13             0     28\n\nYou could potentially create a combined type of variable to deal with this for example, but otherwise the problem will persist with both in the model. There are likely remaining collinearities besides, but let’s take Day out for example.\n\n\nmodel_glm_2 = update(model_glm_1, . ~ . -Day)\n\nsummary(model_glm_2)\n\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ c.tm.depth + \n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + \n    replicate + SUR.ID + tm:Area + tm:c.distance + Area:c.distance + \n    Area:c.tm.depth + Area:c.receiver.depth + Area:c.temp + Area:c.wind, \n    family = binomial(link = logit), data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7693   0.0000   0.0000   0.5335   2.1586  \n\nCoefficients:\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -10.980797   6.323591  -1.736  0.08248 .  \nc.tm.depth                   -0.842695   1.227579  -0.686  0.49242    \nc.receiver.depth              7.003401   7.846631   0.893  0.37211    \nc.temp                       -5.407822   2.465579  -2.193  0.02828 *  \nc.wind                       -6.125119   3.266203  -1.875  0.06075 .  \ntmPT-04                      -2.049088   0.495012  -4.139 3.48e-05 ***\nc.distance                   -0.003755   0.002586  -1.452  0.14644    \nAreaFinger                   11.163412   6.537806   1.708  0.08773 .  \nreplicate2                    2.646675   1.120176   2.363  0.01814 *  \nSUR.ID10186                  -0.223527   0.445491  -0.502  0.61584    \nSUR.ID10250                  -0.300337   0.446936  -0.672  0.50159    \ntmPT-04:AreaFinger            0.425446   0.605567   0.703  0.48233    \ntmPT-04:c.distance           -0.005304   0.003175  -1.670  0.09483 .  \nc.distance:AreaFinger         0.011937   0.003849   3.101  0.00193 ** \nc.tm.depth:AreaFinger        -3.137815   4.366782  -0.719  0.47241    \nc.receiver.depth:AreaFinger -35.342161  17.332674  -2.039  0.04145 *  \nc.temp:AreaFinger             2.222685   1.682491   1.321  0.18648    \nc.wind:AreaFinger             8.259864   3.700848   2.232  0.02562 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 135.27  on 120  degrees of freedom\nAIC: 251.69\n\nNumber of Fisher Scoring iterations: 10\n\nWell, at least we got rid of the complete collinearity- no NA remains. However, those familiar with such models can still see that some of these coefficients and their associated standard errors are exceedingly large for this setting, so we shouldn’t really be surprised there would still be issues with the more complicated mixed models. With binary logistic models, large absolute coefficients and their standard errors are usually a sign of collinearity/separation, and we now know from our previous exploration that something similar is going on in this proportional binomial model. Here is a table of the more egregious offenders and a plot of all coefficients, their standard errors, and the VIF (size).\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\nvif\n\n\nc.receiver.depth\n\n\n7.003\n\n\n7.847\n\n\n0.893\n\n\n0.372\n\n\n21.202\n\n\nc.wind\n\n\n-6.125\n\n\n3.266\n\n\n-1.875\n\n\n0.061\n\n\n88.124\n\n\nAreaFinger\n\n\n11.163\n\n\n6.538\n\n\n1.708\n\n\n0.088\n\n\n24.372\n\n\nc.receiver.depth:AreaFinger\n\n\n-35.342\n\n\n17.333\n\n\n-2.039\n\n\n0.041\n\n\n43.537\n\n\nc.wind:AreaFinger\n\n\n8.260\n\n\n3.701\n\n\n2.232\n\n\n0.026\n\n\n89.504\n\n\n\nAt this point I would not assume anything about the mixed model itself being a problem, and be leaning toward this being primarily a data issue. Or, at the very least, data issues will need to be sorted out. So let’s begin the restart of our mixed model effort by pulling out some of those variables we thought had some collinearity issues. You might have noticed from the GLM that SUR.ID was only 3 levels, so let’s move that to a fixed effect also. In keeping things simple, I’m not including any interactions.\n\n\nmodel_mixed_1 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance + c.distance:Area + #c.tm.depth:Area + \n    # c.receiver.depth:Area + c.temp:Area + c.wind:Area + #c.tm.depth + \n    c.receiver.depth + c.temp + c.wind + tm + c.distance + Area + replicate + \n    SUR.ID + \n    (1|Unit), #  + (1|Day)\n  data = df,\n  family = binomial(link = logit)\n)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| =\n0.117672 (tol = 0.002, component 1)\n\nsummarize_model(model_mixed_1, ci = FALSE)\n\n\n\nVariance Components:\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     0.90 0.95     1.00\n\nFixed Effects:\n             Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n        Intercept  2.39 0.73  3.26    0.00      0.95       3.82\n c.receiver.depth -3.80 1.08 -3.53    0.00     -5.91      -1.69\n           c.temp  0.25 0.19  1.28    0.20     -0.13       0.63\n           c.wind  0.13 0.15  0.88    0.38     -0.16       0.43\n          tmPT-04 -1.36 0.36 -3.77    0.00     -2.07      -0.65\n       c.distance -0.01 0.00 -5.09    0.00     -0.01      -0.01\n       AreaFinger  1.22 0.62  1.99    0.05      0.02       2.43\n       replicate2 -0.39 0.37 -1.05    0.29     -1.11       0.33\n      SUR.ID10186 -0.13 0.55 -0.23    0.82     -1.20       0.95\n      SUR.ID10250 -0.20 0.54 -0.36    0.72     -1.26       0.87\n\nWell, we still have issues, so what else can we try?\nRescale variables\nLet’s go ahead with the easy part and rescale our variables, which might as well be done with any model. I will standardize the numeric variables.\n\n\nsc = function(x) scale(x)[, 1]\n\ndf = df %>% \n  mutate(\n    c.receiver.depth_sc = sc(c.receiver.depth),\n    c.tm.depth_sc = sc(c.tm.depth),\n    c.temp_sc     = sc(c.temp),\n    c.wind_sc     = sc(c.wind),\n    c.distance_sc = sc(c.distance),\n  )\n\nmodel_mixed_2 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area + \n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc + c.temp_sc + c.wind_sc + tm + c.distance_sc + Area + replicate + \n    SUR.ID +\n    (1|Unit), \n  data = df,\n  family = binomial(link = logit)\n)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| =\n0.135829 (tol = 0.002, component 1)\n\nsummarize_model(model_mixed_2, ci = FALSE)\n\n\n\nVariance Components:\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     0.87 0.93     1.00\n\nFixed Effects:\n                Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n           Intercept  2.48 0.69  3.59    0.00      1.13       3.84\n c.receiver.depth_sc -1.18 0.33 -3.56    0.00     -1.83      -0.53\n           c.temp_sc  0.58 0.45  1.27    0.20     -0.31       1.47\n           c.wind_sc  0.42 0.49  0.86    0.39     -0.54       1.38\n             tmPT-04 -1.36 0.36 -3.78    0.00     -2.07      -0.66\n       c.distance_sc -1.19 0.23 -5.15    0.00     -1.64      -0.74\n          AreaFinger  1.23 0.61  2.01    0.04      0.03       2.43\n          replicate2 -0.39 0.37 -1.07    0.28     -1.11       0.33\n         SUR.ID10186 -0.11 0.54 -0.20    0.84     -1.17       0.95\n         SUR.ID10250 -0.19 0.54 -0.35    0.73     -1.25       0.87\n\nSo at least we have the rescaling taken care of, and while that got rid of one warning, we still have the convergence problem. What can we check for next? I looked to see if there was any further imbalance of categorical variables, didn’t spot much issue, but then discovered something else. A couple covariates - c.wind and c.distance - have only five unique values, and for the former, some of those values only occur a few times. In addition, c.wind was unique per day, so was essentially confounded with it. So we can feel fine with having previously removed Day.\n\n\ndf %>% \n  select(-ends_with('sc')) %>% \n  map_int(n_distinct)\n\n\n          SUR.ID               tm  ValidDetections  CountDetections  FalseDetections        replicate             Area \n               3                2               35               35                5                2                2 \n             Day            R.det c.receiver.depth       c.tm.depth           c.temp           c.wind       c.distance \n               5               21               30               31               37                5                5 \n            Unit \n             220 \n\ntable(df$c.wind, df$Day)\n\n\n              \n               03/06/13 2/22/13 2/26/13 2/27/13 3/14/13\n  -2.96855001         0      60       0       0       0\n  -2.939182972        0       0       0      60       0\n  1.27535159         60       0       0       0       0\n  4.71144999          0       0      12       0       0\n  5.88092439          0       0       0       0      28\n\n\nIf we treat these as categorical what happens? I’ll do it just for the GLM again.\n\n\nmodel_glm_3 = glm(\n  cbind(ValidDetections, FalseDetections) ~\n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area +\n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc + c.temp_sc + factor(c.wind) + tm + factor(c.distance) + Area + replicate +\n    SUR.ID ,\n  data = df,\n  family = binomial(link = logit)\n)\n\nsummary(model_glm_3)\n\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ c.receiver.depth_sc + \n    c.temp_sc + factor(c.wind) + tm + factor(c.distance) + Area + \n    replicate + SUR.ID, family = binomial(link = logit), data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.4641   0.0000   0.0000   0.5706   2.1259  \n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   7.8190     2.2057   3.545 0.000393 ***\nc.receiver.depth_sc           1.3883     2.1912   0.634 0.526349    \nc.temp_sc                    -2.3820     1.4315  -1.664 0.096116 .  \nfactor(c.wind)-2.939182972   -5.0028     4.0691  -1.229 0.218901    \nfactor(c.wind)1.27535159     -7.8429     2.9788  -2.633 0.008466 ** \nfactor(c.wind)4.71144999     17.9094  1180.6959   0.015 0.987898    \nfactor(c.wind)5.88092439     -9.1668     6.3781  -1.437 0.150654    \ntmPT-04                      -1.2459     0.2823  -4.413 1.02e-05 ***\nfactor(c.distance)-110       -0.3953     0.3908  -1.011 0.311786    \nfactor(c.distance)-10        -0.9887     0.4110  -2.405 0.016162 *  \nfactor(c.distance)90         -0.8547     0.5253  -1.627 0.103694    \nfactor(c.distance)190        -3.2681     0.7664  -4.264 2.01e-05 ***\nAreaFinger                        NA         NA      NA       NA    \nreplicate2                    0.7647     0.5249   1.457 0.145120    \nSUR.ID10186                  -0.1224     0.4384  -0.279 0.780057    \nSUR.ID10250                  -0.2700     0.4363  -0.619 0.536010    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 146.17  on 123  degrees of freedom\nAIC: 256.59\n\nNumber of Fisher Scoring iterations: 17\n\nAh! So now we see that Area is also accounted for by other factors. In addition, wind is still problematic. Let’s take out Area, while still treating wind and distance as categorical for our diagnostic adventure.\n\n\n\n\n\nmodel_glm_4 =  update(model_glm_3, . ~ . - Area)\n\nsummary(model_glm_4)\n\n\n\nCall:\nglm(formula = cbind(ValidDetections, FalseDetections) ~ c.receiver.depth_sc + \n    c.temp_sc + factor(c.wind) + tm + factor(c.distance) + replicate + \n    SUR.ID, family = binomial(link = logit), data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.4641   0.0000   0.0000   0.5706   2.1259  \n\nCoefficients:\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   7.8190     2.2057   3.545 0.000393 ***\nc.receiver.depth_sc           1.3883     2.1912   0.634 0.526349    \nc.temp_sc                    -2.3820     1.4315  -1.664 0.096116 .  \nfactor(c.wind)-2.939182972   -5.0028     4.0691  -1.229 0.218901    \nfactor(c.wind)1.27535159     -7.8429     2.9788  -2.633 0.008466 ** \nfactor(c.wind)4.71144999     17.9094  1180.6959   0.015 0.987898    \nfactor(c.wind)5.88092439     -9.1668     6.3781  -1.437 0.150654    \ntmPT-04                      -1.2459     0.2823  -4.413 1.02e-05 ***\nfactor(c.distance)-110       -0.3953     0.3908  -1.011 0.311786    \nfactor(c.distance)-10        -0.9887     0.4110  -2.405 0.016162 *  \nfactor(c.distance)90         -0.8547     0.5253  -1.627 0.103694    \nfactor(c.distance)190        -3.2681     0.7664  -4.264 2.01e-05 ***\nreplicate2                    0.7647     0.5249   1.457 0.145120    \nSUR.ID10186                  -0.1224     0.4384  -0.279 0.780057    \nSUR.ID10250                  -0.2700     0.4363  -0.619 0.536010    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 334.57  on 137  degrees of freedom\nResidual deviance: 146.17  on 123  degrees of freedom\nAIC: 256.59\n\nNumber of Fisher Scoring iterations: 17\n\nThe remaining collinearity is due to the relatively few observations for that value of wind, but at least most of the other covariates effects have settled down. Let’s try collapsing wind values and officially making it categorical.\n\n\ndf = df %>% \n  mutate(wind = fct_lump(factor(c.wind), 3, other_level = 'Higher'))\n\ntable(df$wind)\n\n\n\n -2.96855001 -2.939182972   1.27535159       Higher \n          60           60           60           40 \n\nmodel_mixed_3 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area + \n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc + c.temp_sc + wind + tm + factor(c.distance) + #Area + \n    replicate + SUR.ID +\n    (1|Unit), \n  data = df, \n  family = binomial(link = logit)\n)\n\nsummarize_model(model_mixed_3, ci = FALSE)\n\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     0.78 0.88     1.00\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  3.67 1.16  3.16    0.00      1.40       5.94\n  c.receiver.depth_sc -3.02 1.40 -2.16    0.03     -5.76      -0.28\n            c.temp_sc -1.72 1.70 -1.02    0.31     -5.05       1.60\n     wind-2.939182972  2.73 2.87  0.95    0.34     -2.89       8.35\n       wind1.27535159 -4.20 2.88 -1.46    0.14     -9.85       1.44\n           windHigher  4.41 2.81  1.57    0.12     -1.10       9.92\n              tmPT-04 -1.31 0.36 -3.65    0.00     -2.01      -0.60\n factorc.distance-110 -0.62 0.48 -1.31    0.19     -1.56       0.31\n  factorc.distance-10 -1.44 0.50 -2.87    0.00     -2.42      -0.45\n   factorc.distance90 -1.23 0.62 -1.97    0.05     -2.44      -0.01\n  factorc.distance190 -4.21 0.96 -4.39    0.00     -6.09      -2.33\n           replicate2  0.20 0.58  0.34    0.73     -0.93       1.33\n          SUR.ID10186 -0.06 0.54 -0.12    0.90     -1.12       0.99\n          SUR.ID10250 -0.16 0.54 -0.30    0.76     -1.21       0.89\n\nChecking VIF adjusted for the degrees of freedom associated with the covariate (which is greater for categorical variables), we still have some issues. Below I show VIF both with and without wind as an example.\n\n\ncovariate\n\n\nVIF_adj_orig\n\n\nVIF_adj\n\n\nc.receiver.depth_sc\n\n\n10.017\n\n\n1.277\n\n\nc.temp_sc\n\n\n9.437\n\n\n1.065\n\n\nwind\n\n\n3.914\n\n\nNA\n\n\ntm\n\n\n1.059\n\n\n1.047\n\n\nfactor(c.distance)\n\n\n1.080\n\n\n1.062\n\n\nreplicate\n\n\n1.769\n\n\n1.101\n\n\nSUR.ID\n\n\n1.075\n\n\n1.029\n\n\nLet’s see what happens if we remove wind from the model.\n\n\nmodel_mixed_4 = glmer(\n  cbind(ValidDetections, FalseDetections) ~ \n    # tm:Area + tm:c.distance_sc + c.distance_sc:Area + \n    # c.receiver.depth_sc:Area + c.temp_sc:Area + c.wind_sc:Area +\n    c.receiver.depth_sc +  c.temp_sc  + tm + factor(c.distance) + #Area + \n    replicate + SUR.ID +\n    (1|Unit), \n  data = df,\n  family = binomial(link = logit)\n)\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| =\n0.0547464 (tol = 0.002, component 1)\n\nsummarize_model(model_mixed_4, ci = FALSE)\n\n\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     1.45 1.20     1.00\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  5.62 0.74  7.63    0.00      4.18       7.06\n  c.receiver.depth_sc -1.08 0.19 -5.57    0.00     -1.46      -0.70\n            c.temp_sc  0.50 0.21  2.34    0.02      0.08       0.92\n              tmPT-04 -1.28 0.40 -3.19    0.00     -2.08      -0.49\n factorc.distance-110 -0.99 0.53 -1.86    0.06     -2.04       0.05\n  factorc.distance-10 -1.87 0.55 -3.41    0.00     -2.95      -0.80\n   factorc.distance90 -1.87 0.66 -2.83    0.00     -3.17      -0.58\n  factorc.distance190 -5.60 1.04 -5.38    0.00     -7.65      -3.56\n           replicate2 -0.37 0.40 -0.94    0.35     -1.15       0.40\n          SUR.ID10186 -0.90 0.56 -1.60    0.11     -1.99       0.20\n          SUR.ID10250 -0.98 0.56 -1.74    0.08     -2.07       0.12\n\nWe’re doing better, as our max|grad| value is closer to the tolerance value, but we’re still not where we want to be. What else can we do?\nRemove zero random effects\nIf any variance components estimates are zero we could remove them. However, at this point we already have. Day was zero because it was already accounted for by other covariates. SUR.ID moved to a fixed effect, where it still appears to be a small effect, but at least won’t cause computational problems. The remaining Unit effect does appear to capture some overdispersion, so we can leave it for now.\nTechnical options\nAt this point, we have explored some of the more problematic aspects of the data. Some might feel that they are missing out on some of their theoretical priorities by removing some covariates, but if they are confounded with other covariates, their story can’t be easily disentangled anyway. As noted, we could still play around with creation of different categorical effects through further collapsing or combining, but a lot of that would be arbitrary, so must be done with caution.\nIn general, the problems with the model appear to actually be entirely with the data, but we can move on for demonstration. At some point in your own modeling adventure, you may exhaust what you can do data-wise, and still have convergence issues. This leaves exploration of the more technical side of things to see what tweaks can be made to help further. In order to best check these more technical aspects, it helps to know something about the underlying optimization algorithms, or at least optimization in general. And in general, I suggest refraining from this unless the previous steps have failed. It is very common that once the data has been sorted, convergence gets solved as well, so the data must be ruled out. In any case, let’s see what else we can do!\nCheck singularity\nThis is a mixed model-specific check3, and in general, checking singularity4 goes along with removing zero random effects. These days, you’ll usually get a singularity warning when it is likely the case. In the past, Bolker suggested checking this problem as follows, but for this example concluded the result wasn’t close enough to zero to be a real concern. The theta below are just our random effect standard deviations, and I would say that the ones besides Unit probably weren’t meaningfully different from zero.\n\n\nthetas = getME(model_mixed_0, \"theta\")\n\nthetas\n\n\n  Unit.(Intercept)    Day.(Intercept) SUR.ID.(Intercept) \n       0.680682401        0.009520505        0.007124172 \n\nll = getME(model_mixed_0, \"lower\") # lower bounds on model parameters (random effects parameters only)\n\nmin(thetas[ll == 0])\n\n\n[1] 0.007124172\n\nNowadays lme4 provides the function isSingular which uses the steps above to check the minimum value against some specified tolerance.\n\n\nisSingular(model_mixed_0, tol = 1e-5)\n\n\n[1] FALSE\n\n# rePCA(model_mixed_0)  # via PCA of the random-effects variance-covariance estimates\n\n\n\nDouble-checking gradient calculations\nFor the mixed model setting, Bolker notes the following:\n\nOne general problem is that large scaled gradients are often associated with small absolute gradients: we might decide that we’re more interested in testing the (parallel) minimum of these two quantities.\n\nWe can do this as follows for the initial mixed model.\n\n\nderivs_init  = model_mixed_0@optinfo$derivs\n\nsc_grad_init = with(derivs_init, solve(Hessian, gradient))\n\nmax(abs(sc_grad_init))\n\n\n[1] 1.518823\n\nmax(pmin(abs(sc_grad_init), abs(derivs_init$gradient)))\n\n\n[1] 0.1315685\n\nWe see that the unscaled gradient results in a lower maximum value, but is still large relative to the tolerance. That value is what is reported in the warning message.\n\n\nmodel_mixed_0@optinfo$conv$lme4$messages[[1]]\n\n\n[1] \"Model failed to converge with max|grad| = 0.131568 (tol = 0.002, component 1)\"\n\nIt may be instructive to compare the result to the model where we scaled the inputs. In this case the scaled gradient results in the lower max value.\n\n\nderivs_model_mixed_2 = model_mixed_2@optinfo$derivs\n\nsc_grad_model_mixed_2 = with(derivs_model_mixed_2, solve(Hessian, gradient))\n\nmax(abs(sc_grad_model_mixed_2))\n\n\n[1] 0.01645039\n\nmax(pmin(abs(sc_grad_model_mixed_2), abs(derivs_model_mixed_2$gradient)))\n\n\n[1] 0.01645039\n\nBolker also suggests checking if the result varies from using a different calculation, but it’s not clear what we’d do if this was the case. In any event, the results would be similar.\n\n\ndevfun_init  = update(model_mixed_0, devFunOnly = TRUE)\npars_init    = unlist(getME(model_mixed_0, c(\"theta\", \"fixef\")))\ngrad_init    = numDeriv::grad(devfun_init, pars_init)\nhess_init    = numDeriv::hessian(devfun_init, pars_init)\nsc_grad_init = solve(hess_init, grad_init)\n\nmax(pmin(abs(sc_grad_init), abs(grad_init)))\n\n\n[1] 0.1315678\n\nmax(pmin(abs(sc_grad_init), abs(derivs_init$gradient)))\n\n\n[1] 0.1315685\n\nRestart the fit\nAs another step along our technical travails, we can just let the optimizer keep going until it does converge. Many R modeling packages allow for you to access the optimizer and change various settings. Most optimizers have a maxit type of argument to let you set the number of iterations, and we can use update to continue where we left off. Unfortunately there is no standard argument name for the total number of iterations, or even if you do happen to remember, guessing is required as to what we should set it at. So instead, we can just call update iteratively until there is no convergence warning. I check this by seeing if there is any output to @optinfo$conv$lme4, as it will only be there if it doesn’t converge5.\n\n\nmodel_mixed_5 = model_mixed_4\n\nwhile (length(model_mixed_5@optinfo$conv$lme4) > 0) {\n  pars = getME(model_mixed_5,c(\"theta\",\"fixef\"))\n  model_mixed_5 <-\n    update(model_mixed_5,\n           start = pars,\n           control = glmerControl(optCtrl = list(maxfun = 2e5)))\n}\n\nmax(\n  abs(\n    with(\n      model_mixed_5@optinfo$derivs, solve(Hessian, gradient)\n    )\n  )\n)  \n\n\n[1] 0.0008423291\n\n# we win!\n\n\n\nSo at this point we have converged with no warnings. Hooray for us! However, the following shows us that we were pretty close anyway. The estimates from the last model with warnings and the converged model are nearly identical.\n\n\nsummarize_model(model_mixed_4, ci = 0)\n\n\n\nVariance Components:\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     1.45 1.20     1.00\n\nFixed Effects:\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  5.62 0.74  7.63    0.00      4.18       7.06\n  c.receiver.depth_sc -1.08 0.19 -5.57    0.00     -1.46      -0.70\n            c.temp_sc  0.50 0.21  2.34    0.02      0.08       0.92\n              tmPT-04 -1.28 0.40 -3.19    0.00     -2.08      -0.49\n factorc.distance-110 -0.99 0.53 -1.86    0.06     -2.04       0.05\n  factorc.distance-10 -1.87 0.55 -3.41    0.00     -2.95      -0.80\n   factorc.distance90 -1.87 0.66 -2.83    0.00     -3.17      -0.58\n  factorc.distance190 -5.60 1.04 -5.38    0.00     -7.65      -3.56\n           replicate2 -0.37 0.40 -0.94    0.35     -1.15       0.40\n          SUR.ID10186 -0.90 0.56 -1.60    0.11     -1.99       0.20\n          SUR.ID10250 -0.98 0.56 -1.74    0.08     -2.07       0.12\n\nsummarize_model(model_mixed_5, ci = 0)\n\n\n\nVariance Components:\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     1.46 1.21     1.00\n\nFixed Effects:\n                 Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n            Intercept  5.63 0.74  7.62    0.00      4.18       7.08\n  c.receiver.depth_sc -1.07 0.19 -5.54    0.00     -1.45      -0.69\n            c.temp_sc  0.50 0.22  2.34    0.02      0.08       0.92\n              tmPT-04 -1.28 0.40 -3.18    0.00     -2.08      -0.49\n factorc.distance-110 -0.99 0.53 -1.85    0.06     -2.04       0.06\n  factorc.distance-10 -1.87 0.55 -3.39    0.00     -2.95      -0.79\n   factorc.distance90 -1.87 0.66 -2.82    0.00     -3.17      -0.57\n  factorc.distance190 -5.59 1.04 -5.37    0.00     -7.63      -3.55\n           replicate2 -0.38 0.40 -0.96    0.34     -1.15       0.40\n          SUR.ID10186 -0.91 0.56 -1.61    0.11     -2.01       0.20\n          SUR.ID10250 -0.99 0.56 -1.76    0.08     -2.09       0.11\n\nAs an additional point, one may provide starting estimates from the outset. For example you could run a a simpler model, e.g. standard GLM, and feed the estimates there for the fixed effects coefficients of the mixed model, or even run a model to obtain starting values for the random effects (see this example).\nChange the optimizer\nAs a last effort among the more technical knobs to turn, we can start fiddling with the optimizer options. The lme4 package has a nice function allFit that will search across several different optimizers (some may require additional packages to be installed). However, in any particular modeling setting you could potentially do this, though often you may not be able to without quite a bit of effort relative to what lme4 allows. I’ll do this with our last non-converged model.\n\n\nlibrary(optimx)  # required for some optimizers\n\nglmm_all = allFit(model_mixed_4)\n\n\nbobyqa : [OK]\nNelder_Mead : [OK]\nnlminbwrap : [OK]\nnmkbw : [OK]\noptimx.L-BFGS-B : [OK]\nnloptwrap.NLOPT_LN_NELDERMEAD : [OK]\nnloptwrap.NLOPT_LN_BOBYQA : [OK]\n\nglmm_all_summary = summary(glmm_all)\n\n\n\nThe ‘[OK]’ just means there wasn’t an error, however we can see that several have convergence problems, but even a couple of those are almost to the tolerance level. In the end though, the estimates and log likelihoods are not meaningfully different across the optimizers.\n\n\nopt\n\n\nOK\n\n\nMessage\n\n\nNelder_Mead\n\n\nTRUE\n\n\nModel failed to converge with max|grad| = 0.0062003 (tol = 0.002, component 1)\n\n\n\noptimizer\n\n\n(Intercept)\n\n\nc.receiver.depth_sc\n\n\nc.temp_sc\n\n\ntmPT-04\n\n\nfactor(c.distance)-110\n\n\nfactor(c.distance)-10\n\n\nfactor(c.distance)90\n\n\nfactor(c.distance)190\n\n\nreplicate2\n\n\nSUR.ID10186\n\n\nSUR.ID10250\n\n\nll\n\n\nbobyqa\n\n\n5.633\n\n\n-1.072\n\n\n0.503\n\n\n-1.284\n\n\n-0.987\n\n\n-1.867\n\n\n-1.867\n\n\n-5.591\n\n\n-0.379\n\n\n-0.905\n\n\n-0.989\n\n\n-121.842\n\n\nNelder_Mead\n\n\n5.632\n\n\n-1.073\n\n\n0.503\n\n\n-1.284\n\n\n-0.986\n\n\n-1.866\n\n\n-1.867\n\n\n-5.590\n\n\n-0.378\n\n\n-0.906\n\n\n-0.989\n\n\n-121.842\n\n\nnlminbwrap\n\n\n5.633\n\n\n-1.072\n\n\n0.503\n\n\n-1.284\n\n\n-0.987\n\n\n-1.867\n\n\n-1.867\n\n\n-5.591\n\n\n-0.379\n\n\n-0.905\n\n\n-0.989\n\n\n-121.842\n\n\nnmkbw\n\n\n5.633\n\n\n-1.072\n\n\n0.503\n\n\n-1.284\n\n\n-0.987\n\n\n-1.867\n\n\n-1.867\n\n\n-5.591\n\n\n-0.379\n\n\n-0.905\n\n\n-0.989\n\n\n-121.842\n\n\noptimx.L-BFGS-B\n\n\n5.632\n\n\n-1.072\n\n\n0.503\n\n\n-1.284\n\n\n-0.987\n\n\n-1.867\n\n\n-1.867\n\n\n-5.591\n\n\n-0.379\n\n\n-0.905\n\n\n-0.989\n\n\n-121.842\n\n\nnloptwrap.NLOPT_LN_NELDERMEAD\n\n\n5.633\n\n\n-1.072\n\n\n0.503\n\n\n-1.284\n\n\n-0.987\n\n\n-1.867\n\n\n-1.867\n\n\n-5.591\n\n\n-0.379\n\n\n-0.905\n\n\n-0.989\n\n\n-121.842\n\n\nnloptwrap.NLOPT_LN_BOBYQA\n\n\n5.633\n\n\n-1.072\n\n\n0.503\n\n\n-1.284\n\n\n-0.987\n\n\n-1.867\n\n\n-1.867\n\n\n-5.591\n\n\n-0.379\n\n\n-0.906\n\n\n-0.990\n\n\n-121.842\n\n\n\nThe conclusion would be that our estimates are probably okay regardless of chosen optimizer, but the data likely has issues that still need to be overcome.\nAlong with different optimizers comes trying different packages. Some packages, in this case like glmmTMB or brms, would be viable options. In my playing around with those, glmmTMB converged, but obviously doesn’t magically overcome the collinearity problems. Likewise, brms appeared to converge, but had other estimation issues specific to it. Unlike the others, brms also noted that the zero count observations could not be used.\nFinal Step\nThe final step would be to make some hard decisions about the model. Which predictors are most meaningful? Is this the best target variable available? Is this the right way to think about the distribution of the target? Is the research question clear enough? Is it strongly related to the available data, enough to be answerable?6\nHowever, these are more theoretical problems, not so much statistical ones, and there may be no right answer in the end. That is perfectly fine, and just what you’d report to others. The following model has no issues, but may leave more questions than answers.\n\n\nmodel_mixed_final = glmer(\n  ValidDetections ~ \n    c.receiver.depth_sc + c.temp_sc + tm + \n    replicate + SUR.ID + (1|Unit),\n    data = df,\n    family = poisson\n    )\n\nsummarize_model(model_mixed_final, ci = 0)\n\n\n\nVariance Components:\n Group    Effect Variance   SD Var_prop\n  Unit Intercept     2.96 1.72     1.00\n\nFixed Effects:\n                Term Value   SE     Z P_value Lower_2.5 Upper_97.5\n           Intercept -0.01 0.33 -0.02    0.98     -0.66       0.64\n c.receiver.depth_sc -0.86 0.14 -6.16    0.00     -1.13      -0.59\n           c.temp_sc  0.60 0.14  4.27    0.00      0.33       0.88\n             tmPT-04 -0.62 0.27 -2.27    0.02     -1.16      -0.08\n          replicate2 -0.11 0.27 -0.41    0.68     -0.65       0.42\n         SUR.ID10186  0.92 0.35  2.67    0.01      0.25       1.60\n         SUR.ID10250  1.11 0.35  3.18    0.00      0.42       1.79\n\nSummary\nWe can summarize our approach to convergence problems as follows:\nStep back and look at your data. Are there issues? Are some variables essentially collinear with others or otherwise can be practically reduced?\nStart with the simplest but still plausible model. Do you spot any additional issues? If there are, these probably need to be remedied before moving to more complex models.\nScale your continuous variables. You should be doing this anyway. If some categorical levels have very relatively few observations, consider collapsing.\nFor mixed models, if some of your random effects only have a few levels, treat them as fixed or see if they are even needed in the model.\nFor mixed models, are any random effect variance estimates zero or nearly zero? Remove.\nAfter some initial technical checks, if possible, restart the model using previous starting values, and run until convergence. Many packages provide some functionality in this regard, but what exactly is provided may be limited.\nChange the optimizer (if a small mixed model using lme4, compare several with allFit function).\nIf log-likelihood and parameter estimates are pretty much the same across optimizers (and they are rarely notably different in my experience), pick one and go with it.\n\nUse a different package. The different estimation approach may simply work better for the current problem or provide other opportunities for tweaks. For example, with mixed models one could use glmmTMB or brms (Bayesian) in lieu of lme4.\nIn my experience with many clients with many types of data coming across many fields of study, the usual problem with convergence and lme4 (and others) is typically a fixable data problem, or a problematically specified model. In this case, the lme4 developers have worked hard over a number of years to build this awesome tool, and it works very well, so if it is having problems, you should be inspecting your data closely and thinking hard about your model. If it is an lme4 problem, switching optimizers will likely get you to convergence, but going through technical solutions should be a last resort.\nReferences\nBates et al.(Bates et al. 2015)\n\nStackoverflow question(StackOverflow 2014)\n\nBolker’s answer given a little more cleanly(Bolker 2014?)\n\nHelp file for convergence(Bolker 2020)\n\n\nBates, Douglas, Reinhold Kliegl, Shravan Vasishth, and Harald Baayen. 2015. “Parsimonious Mixed Models.” arXiv Preprint arXiv:1506.04967. https://arxiv.org/abs/1506.04967.\n\n\nBolker, Ben. 2014? “Lme4 Convergence Warnings: Troubleshooting.” https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html.\n\n\n———. 2020. “Lme4 Help File for Convergence.” https://rdrr.io/cran/lme4/man/convergence.html.\n\n\nStackOverflow. 2014. “Warning Messages When Trying to Run Glmer in r.” https://stackoverflow.com/questions/23478792/warning-messages-when-trying-to-run-glmer-in-r.\n\n\nBen Bolker is one of the primary developers of the lme4 package.↩︎\nFor a basic linear model situation, \\(1-1/VIF = R^2\\), where \\(R^2\\) is a regression model where a covariate is predicted by all the other covariates. We wouldn’t typically use a standard linear regression for binary outcomes or other scenarios, but this provides a quick and rough metric. The car package actually also provides a ‘generalized’ VIF though. In terms of interpretation, it tells us how much the standard error increases relative to the covariate if it was independent of the others. We would be concerned with redundancy of anything of VIF > 10 / \\(R^2\\) > .90, but maybe even less. As a final note, collinearity is basically a sample size issue, as larger data would reduce the standard errors, and we’d likely get a fuller sense of true variability in the covariates.↩︎\nThis outline mostly follows the documentation in the help file for ?convergence for lme4.↩︎\nThink of doing a principal components analysis on the variance-covariance matrix for the random effects. If one of those components is essentially accounting for zero variance, it may suggest at least one of the estimated random effects is not needed.↩︎\nUnfortunately there doesn’t appear to be much documentation on what should be listed in optinfo.↩︎\nTo paraphrase Barthelme slightly: “What is wonderful? Are these results wonderful? Are they significant? Are they what I need?”\n\n↩︎\n",
    "preview": "posts/2020-03-16-convergence/../../img/convergence/warning.png",
    "last_modified": "2021-04-13T14:19:58-04:00",
    "input_file": {},
    "preview_width": 1779,
    "preview_height": 270
  },
  {
    "path": "posts/2020-03-01-random-categorical/",
    "title": "Categorical Effects as Random",
    "description": "Exploring random slopes for categorical covariates and similar models",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2020-03-01",
    "categories": [
      "mixed models"
    ],
    "contents": "\nTable of Contents\nIntroduction\nMachinesThe Data\nRandom Effects Models\nSummarize All the Models\n\nSimulationData Creation\nRun the Models & Summarize\nChange the model orientation\n\nSummary\n\nLast updated March 09, 2020.\nPrerequisites: familiarity with mixed models\nIntroduction\nIt’s often the case where, for mixed models, we want to look at random ‘slopes’ as well as random intercepts, such that coefficients for the fixed effects are expected to vary by group. This is very common in longitudinal settings, were we want to examine an overall trend, but allow the trend to vary by individual.\nIn such settings, when time is numeric, things are straightforward. The variance components are decomposed into parts for the intercept, the coefficient for the time indicator, and the residual variance (for linear mixed models). But what happens if we have only three time points? Does it make sense to treat it as numeric and hope for the best?\nThis came up in consulting because someone had a similar issue, and tried to keep the format for random slopes while treating the time indicator as categorical. This led to convergence issues, so we thought about what models might be possible. This post explores that scenario.\nPackages used:\n\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(mixedup)  # http://m-clark.github.io/mixedup\n\nMachines\nThe Data\nLet’s start with a very simple data set from the nlme package, which comes with the standard R installation. The reason I chose this is because Doug Bates has a good treatment on this topic using that example (starting at slide 85), which I just extend a bit.\nHere is the data description from the help file.\n\nData on an experiment to compare three brands of machines used in an industrial process are presented in Milliken and Johnson (p. 285, 1992). Six workers were chosen randomly among the employees of a factory to operate each machine three times. The response is an overall productivity score taking into account the number and quality of components produced.\n\nSo for each worker and each machine, we’ll have three scores. Let’s look.\n\n\nmachines = nlme::Machines\n\n# for some reason worker is an ordered factor.\nmachines = machines %>% \n  mutate(Worker = factor(Worker, levels = 1:6, ordered = FALSE))\n\n\n\nWorker\n\n\nMachine\n\n\nscore\n\n\n1\n\n\nA\n\n\n52.0\n\n\n1\n\n\nA\n\n\n52.8\n\n\n1\n\n\nA\n\n\n53.1\n\n\n1\n\n\nB\n\n\n62.1\n\n\n1\n\n\nB\n\n\n62.6\n\n\n1\n\n\nB\n\n\n64.0\n\n\nThis duplicates the plot in Bates’ notes and visually describes the entire data set. There likely is variability due to both workers and machines.\n\n\n\nRandom Effects Models\nThe random effects of potential interest are for worker and machine, so how do we specify this? Let’s try a standard approach. The following is the type of model tried by our client.\n\n\nmodel_m_slope = lmer(score ~ Machine + (1 + Machine | Worker), machines)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model failed to converge with max|grad| =\n0.00805193 (tol = 0.002, component 1)\n\nThis was exactly the same issue our client had- problematic convergence. This could be more of an issue with lme4, and we could certainly explore tweaks to make the problem go away (or use a different package like glmmTMB), but let’s go ahead and keep it.\n\n\nsummarize_model(model_m_slope, ci = FALSE, cor_re = TRUE)\n\nVariance Components:\n\n    Group    Effect Variance   SD Var_prop\n   Worker Intercept    16.68 4.08     0.25\n   Worker  MachineB    34.72 5.89     0.53\n   Worker  MachineC    13.62 3.69     0.21\n Residual               0.92 0.96     0.01\n\nCorrelation of Random Effects:\n\n          Intercept MachineB MachineC\nIntercept      1.00     0.49    -0.36\nMachineB       0.49     1.00     0.30\nMachineC      -0.36     0.30     1.00\n\n\nFixed Effects:\n\n      Term Value   SE     t P_value Lower_2.5 Upper_97.5\n Intercept 52.36 1.68 31.12    0.00     49.06      55.65\n  MachineB  7.97 2.43  3.28    0.00      3.21      12.72\n  MachineC 13.92 1.54  9.03    0.00     10.90      16.94\n\nWe get the variance components we expect, i.e. the variance attributable to the intercept (i.e. Machine A), as well as for the slopes for the difference in machine B vs. A, and C vs. A. We also see the correlations among the random effects. It’s this part that Bates acknowledges is hard to estimate, and incurs estimating potentially notably more parameters than typical random effects models. We have different options that will be available to us though, so let’s try some.\nLet’s start with the simplest, most plausible models. The first would be to have at least a worker effect. The next baseline model could be if we only had a machine by worker effect, i.e. a separate effect of each machine for each worker, essentially treating the interaction term as the sole clustering unit.\n\n\nmodel_base_w  = lmer(score ~ Machine + (1 | Worker), machines)\nmodel_base_wm = lmer(score ~ Machine + (1 | Worker:Machine), machines)\n\nExamining the random effects makes clear the difference between the two models. For our first baseline model, we only have 6 effects, one for each worker. For the second we have an effect of each machine for each worker.\n\n\nextract_random_effects(model_base_w)  # only 6 effects\nextract_random_effects(model_base_wm) # 6 workers by 3 machines = 18 effects\n\n\n\ngroup_var\n\n\neffect\n\n\ngroup\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nWorker\n\n\nIntercept\n\n\n1\n\n\n1.210\n\n\n1.032\n\n\n-0.813\n\n\n3.234\n\n\nWorker\n\n\nIntercept\n\n\n2\n\n\n-1.594\n\n\n1.032\n\n\n-3.618\n\n\n0.429\n\n\nWorker\n\n\nIntercept\n\n\n3\n\n\n6.212\n\n\n1.032\n\n\n4.188\n\n\n8.235\n\n\nWorker\n\n\nIntercept\n\n\n4\n\n\n-0.069\n\n\n1.032\n\n\n-2.093\n\n\n1.954\n\n\nWorker\n\n\nIntercept\n\n\n5\n\n\n2.949\n\n\n1.032\n\n\n0.925\n\n\n4.972\n\n\nWorker\n\n\nIntercept\n\n\n6\n\n\n-8.707\n\n\n1.032\n\n\n-10.731\n\n\n-6.684\n\n\ngroup_var\n\n\neffect\n\n\ngroup\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:A\n\n\n0.275\n\n\n0.553\n\n\n-0.808\n\n\n1.359\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:B\n\n\n2.556\n\n\n0.553\n\n\n1.473\n\n\n3.640\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:C\n\n\n0.920\n\n\n0.553\n\n\n-0.164\n\n\n2.004\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:A\n\n\n0.209\n\n\n0.553\n\n\n-0.874\n\n\n1.293\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:B\n\n\n-0.749\n\n\n0.553\n\n\n-1.833\n\n\n0.334\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:C\n\n\n-4.402\n\n\n0.553\n\n\n-5.486\n\n\n-3.318\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:A\n\n\n7.118\n\n\n0.553\n\n\n6.035\n\n\n8.202\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:B\n\n\n7.647\n\n\n0.553\n\n\n6.563\n\n\n8.731\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:C\n\n\n4.490\n\n\n0.553\n\n\n3.407\n\n\n5.574\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:A\n\n\n-1.113\n\n\n0.553\n\n\n-2.196\n\n\n-0.029\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:B\n\n\n2.391\n\n\n0.553\n\n\n1.307\n\n\n3.475\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:C\n\n\n-1.493\n\n\n0.553\n\n\n-2.577\n\n\n-0.409\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:A\n\n\n-0.981\n\n\n0.553\n\n\n-2.064\n\n\n0.103\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:B\n\n\n4.705\n\n\n0.553\n\n\n3.621\n\n\n5.789\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:C\n\n\n5.416\n\n\n0.553\n\n\n4.332\n\n\n6.499\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:A\n\n\n-5.509\n\n\n0.553\n\n\n-6.593\n\n\n-4.426\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:B\n\n\n-16.550\n\n\n0.553\n\n\n-17.634\n\n\n-15.467\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:C\n\n\n-4.931\n\n\n0.553\n\n\n-6.014\n\n\n-3.847\n\n\nAs a next step, we’ll essentially combine our two baseline models.\n\n\nmodel_w_wm = lmer(score ~ Machine + (1 | Worker) + (1 | Worker:Machine), machines)\n\nNow we have 6 worker effects plus 18 machine within worker effects1.\n\n\nextract_random_effects(model_w_wm)\n\n\n\ngroup_var\n\n\neffect\n\n\ngroup\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:A\n\n\n-0.750\n\n\n2.015\n\n\n-4.699\n\n\n3.198\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:B\n\n\n1.500\n\n\n2.015\n\n\n-2.449\n\n\n5.449\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:C\n\n\n-0.114\n\n\n2.015\n\n\n-4.063\n\n\n3.834\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:A\n\n\n1.553\n\n\n2.015\n\n\n-2.396\n\n\n5.501\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:B\n\n\n0.607\n\n\n2.015\n\n\n-3.342\n\n\n4.555\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:C\n\n\n-2.997\n\n\n2.015\n\n\n-6.945\n\n\n0.952\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:A\n\n\n1.778\n\n\n2.015\n\n\n-2.171\n\n\n5.726\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:B\n\n\n2.299\n\n\n2.015\n\n\n-1.649\n\n\n6.248\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:C\n\n\n-0.815\n\n\n2.015\n\n\n-4.763\n\n\n3.134\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:A\n\n\n-1.039\n\n\n2.015\n\n\n-4.988\n\n\n2.909\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:B\n\n\n2.417\n\n\n2.015\n\n\n-1.531\n\n\n6.366\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:C\n\n\n-1.414\n\n\n2.015\n\n\n-5.363\n\n\n2.534\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:A\n\n\n-3.457\n\n\n2.015\n\n\n-7.405\n\n\n0.492\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:B\n\n\n2.152\n\n\n2.015\n\n\n-1.796\n\n\n6.101\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:C\n\n\n2.853\n\n\n2.015\n\n\n-1.095\n\n\n6.802\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:A\n\n\n1.916\n\n\n2.015\n\n\n-2.032\n\n\n5.865\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:B\n\n\n-8.976\n\n\n2.015\n\n\n-12.924\n\n\n-5.027\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:C\n\n\n2.487\n\n\n2.015\n\n\n-1.462\n\n\n6.435\n\n\nWorker\n\n\nIntercept\n\n\n1\n\n\n1.045\n\n\n1.981\n\n\n-2.839\n\n\n4.928\n\n\nWorker\n\n\nIntercept\n\n\n2\n\n\n-1.376\n\n\n1.981\n\n\n-5.259\n\n\n2.507\n\n\nWorker\n\n\nIntercept\n\n\n3\n\n\n5.361\n\n\n1.981\n\n\n1.478\n\n\n9.244\n\n\nWorker\n\n\nIntercept\n\n\n4\n\n\n-0.060\n\n\n1.981\n\n\n-3.943\n\n\n3.823\n\n\nWorker\n\n\nIntercept\n\n\n5\n\n\n2.545\n\n\n1.981\n\n\n-1.339\n\n\n6.428\n\n\nWorker\n\n\nIntercept\n\n\n6\n\n\n-7.514\n\n\n1.981\n\n\n-11.397\n\n\n-3.631\n\n\nIf you look closely at these effects, and add them together, you will get a value similar to our second baseline model, which is probably not too surprising. For example in the above model 1:B + 1 = 1.5 + 1.045. Looking at the initial model, the estimated random effect for 1:B was 2.556. Likewise if we look at the variance components, we can see that the sum of the non-residual effect variances for model_w_wm equals the variance of model_base_wm (36.8). So this latest model allows us to disentangle the worker and machine effects, where our baseline models did not.\n\n\n\nNext we’ll do the ‘vector-valued’ model Bates describes. This removes the intercept portion of the formula in the original random slopes model, but is otherwise the same. We can look at the results here, but I will hold off description for comparing it to other models. Note that at least have no convergence problem.\n\n\nmodel_m_vv = lmer(score ~ Machine + (0 + Machine | Worker), machines)\n\nsummarize_model(model_m_vv, ci = 0, cor_re = TRUE)\n\nVariance Components:\n\n    Group   Effect Variance   SD Var_prop\n   Worker MachineA    16.67 4.08     0.15\n   Worker MachineB    74.30 8.62     0.67\n   Worker MachineC    19.27 4.39     0.17\n Residual              0.93 0.96     0.01\n\nCorrelation of Random Effects:\n\n         MachineA MachineB MachineC\nMachineA     1.00     0.80     0.62\nMachineB     0.80     1.00     0.77\nMachineC     0.62     0.77     1.00\n\n\nFixed Effects:\n\n      Term Value   SE     t P_value Lower_2.5 Upper_97.5\n Intercept 52.36 1.68 31.12    0.00     49.06      55.65\n  MachineB  7.97 2.42  3.30    0.00      3.23      12.70\n  MachineC 13.92 1.54  9.05    0.00     10.90      16.93\n\n\n\n\nSummarize All the Models\nNow let’s extract the fixed effect and variance component summaries for all the models.\n\n\nmodel_list = mget(ls(pattern = 'model_'))\n\nfe = map_df(model_list, extract_fixed_effects, .id = 'model')\n\nvc = map_df(model_list, extract_vc, ci_level = 0, .id = 'model')\n\nFirst, let’s look at the fixed effects. We see that there are no differences in the coefficients for the fixed effect of machine, which is our only covariate in the model. However, there are notable differences for the estimated standard errors. Practically we’d come to no differences in our conclusions, but the uncertainty associated with them would be different.\n\n\nmodel\n\n\nterm\n\n\nvalue\n\n\nse\n\n\nt\n\n\np_value\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nmodel_base_w\n\n\nIntercept\n\n\n52.356\n\n\n2.229\n\n\n23.485\n\n\n0.000\n\n\n47.986\n\n\n56.725\n\n\nmodel_base_w\n\n\nMachineB\n\n\n7.967\n\n\n1.054\n\n\n7.559\n\n\n0.000\n\n\n5.901\n\n\n10.032\n\n\nmodel_base_w\n\n\nMachineC\n\n\n13.917\n\n\n1.054\n\n\n13.205\n\n\n0.000\n\n\n11.851\n\n\n15.982\n\n\nmodel_base_wm\n\n\nIntercept\n\n\n52.356\n\n\n2.486\n\n\n21.062\n\n\n0.000\n\n\n47.483\n\n\n57.228\n\n\nmodel_base_wm\n\n\nMachineB\n\n\n7.967\n\n\n3.516\n\n\n2.266\n\n\n0.023\n\n\n1.076\n\n\n14.857\n\n\nmodel_base_wm\n\n\nMachineC\n\n\n13.917\n\n\n3.516\n\n\n3.959\n\n\n0.000\n\n\n7.026\n\n\n20.807\n\n\nmodel_m_slope\n\n\nIntercept\n\n\n52.356\n\n\n1.683\n\n\n31.116\n\n\n0.000\n\n\n49.058\n\n\n55.653\n\n\nmodel_m_slope\n\n\nMachineB\n\n\n7.967\n\n\n2.427\n\n\n3.283\n\n\n0.001\n\n\n3.210\n\n\n12.723\n\n\nmodel_m_slope\n\n\nMachineC\n\n\n13.917\n\n\n1.541\n\n\n9.033\n\n\n0.000\n\n\n10.897\n\n\n16.936\n\n\nmodel_m_vv\n\n\nIntercept\n\n\n52.356\n\n\n1.682\n\n\n31.122\n\n\n0.000\n\n\n49.058\n\n\n55.653\n\n\nmodel_m_vv\n\n\nMachineB\n\n\n7.967\n\n\n2.416\n\n\n3.297\n\n\n0.001\n\n\n3.230\n\n\n12.703\n\n\nmodel_m_vv\n\n\nMachineC\n\n\n13.917\n\n\n1.537\n\n\n9.053\n\n\n0.000\n\n\n10.904\n\n\n16.930\n\n\nmodel_w_wm\n\n\nIntercept\n\n\n52.356\n\n\n2.486\n\n\n21.062\n\n\n0.000\n\n\n47.483\n\n\n57.228\n\n\nmodel_w_wm\n\n\nMachineB\n\n\n7.967\n\n\n2.177\n\n\n3.660\n\n\n0.000\n\n\n3.700\n\n\n12.233\n\n\nmodel_w_wm\n\n\nMachineC\n\n\n13.917\n\n\n2.177\n\n\n6.393\n\n\n0.000\n\n\n9.650\n\n\n18.183\n\n\nHere are the variance components, there are definitely some differences here, but, as we’ll see, maybe not as much as we suspect.\n\n\nmodel\n\n\ngroup\n\n\neffect\n\n\nvariance\n\n\nsd\n\n\nvar_prop\n\n\nmodel_base_w\n\n\nWorker\n\n\nIntercept\n\n\n26.487\n\n\n5.147\n\n\n0.726\n\n\nmodel_base_w\n\n\nResidual\n\n\n\n\n9.996\n\n\n3.162\n\n\n0.274\n\n\nmodel_base_wm\n\n\nWorker:Machine\n\n\nIntercept\n\n\n36.768\n\n\n6.064\n\n\n0.975\n\n\nmodel_base_wm\n\n\nResidual\n\n\n\n\n0.925\n\n\n0.962\n\n\n0.025\n\n\nmodel_m_slope\n\n\nWorker\n\n\nIntercept\n\n\n16.679\n\n\n4.084\n\n\n0.253\n\n\nmodel_m_slope\n\n\nWorker\n\n\nMachineB\n\n\n34.717\n\n\n5.892\n\n\n0.526\n\n\nmodel_m_slope\n\n\nWorker\n\n\nMachineC\n\n\n13.625\n\n\n3.691\n\n\n0.207\n\n\nmodel_m_slope\n\n\nResidual\n\n\n\n\n0.924\n\n\n0.961\n\n\n0.014\n\n\nmodel_m_vv\n\n\nWorker\n\n\nMachineA\n\n\n16.672\n\n\n4.083\n\n\n0.150\n\n\nmodel_m_vv\n\n\nWorker\n\n\nMachineB\n\n\n74.302\n\n\n8.620\n\n\n0.668\n\n\nmodel_m_vv\n\n\nWorker\n\n\nMachineC\n\n\n19.270\n\n\n4.390\n\n\n0.173\n\n\nmodel_m_vv\n\n\nResidual\n\n\n\n\n0.925\n\n\n0.962\n\n\n0.008\n\n\nmodel_w_wm\n\n\nWorker:Machine\n\n\nIntercept\n\n\n13.909\n\n\n3.730\n\n\n0.369\n\n\nmodel_w_wm\n\n\nWorker\n\n\nIntercept\n\n\n22.858\n\n\n4.781\n\n\n0.606\n\n\nmodel_w_wm\n\n\nResidual\n\n\n\n\n0.925\n\n\n0.962\n\n\n0.025\n\n\nWe can see that the base_wm model has (non-residual) variance 36.768. This equals the total of the two (non-residual) variance components of the w_wm model 13.909 + 22.858, which again speaks to the latter model decomposing a machine effect into worker + machine effects. This value also equals the variance of the vector-valued model divided by the number of groups (16.672 + 74.302 + 19.27) / 3.\nWe can see that the estimated random effects from the vector-valued model (m_vv) are essentially the same as from the baseline, interaction-only model. However, the way it is estimated allows for incorporation of correlations among the machine random effects, so they are not identical (but pretty close).\n\n\nextract_random_effects(model_m_vv)\n\nextract_random_effects(model_base_wm) \n\n\n\ngroup_var\n\n\neffect\n\n\ngroup\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nWorker\n\n\nMachineA\n\n\n1\n\n\n0.312\n\n\n0.541\n\n\n-0.749\n\n\n1.373\n\n\nWorker\n\n\nMachineB\n\n\n1\n\n\n2.553\n\n\n0.551\n\n\n1.474\n\n\n3.632\n\n\nWorker\n\n\nMachineC\n\n\n1\n\n\n0.930\n\n\n0.545\n\n\n-0.137\n\n\n1.998\n\n\nWorker\n\n\nMachineA\n\n\n2\n\n\n0.183\n\n\n0.541\n\n\n-0.878\n\n\n1.245\n\n\nWorker\n\n\nMachineB\n\n\n2\n\n\n-0.803\n\n\n0.551\n\n\n-1.882\n\n\n0.276\n\n\nWorker\n\n\nMachineC\n\n\n2\n\n\n-4.282\n\n\n0.545\n\n\n-5.350\n\n\n-3.214\n\n\nWorker\n\n\nMachineA\n\n\n3\n\n\n6.969\n\n\n0.541\n\n\n5.908\n\n\n8.031\n\n\nWorker\n\n\nMachineB\n\n\n3\n\n\n7.779\n\n\n0.551\n\n\n6.700\n\n\n8.858\n\n\nWorker\n\n\nMachineC\n\n\n3\n\n\n4.474\n\n\n0.545\n\n\n3.406\n\n\n5.542\n\n\nWorker\n\n\nMachineA\n\n\n4\n\n\n-1.024\n\n\n0.541\n\n\n-2.085\n\n\n0.037\n\n\nWorker\n\n\nMachineB\n\n\n4\n\n\n2.328\n\n\n0.551\n\n\n1.249\n\n\n3.408\n\n\nWorker\n\n\nMachineC\n\n\n4\n\n\n-1.415\n\n\n0.545\n\n\n-2.482\n\n\n-0.347\n\n\nWorker\n\n\nMachineA\n\n\n5\n\n\n-0.849\n\n\n0.541\n\n\n-1.910\n\n\n0.212\n\n\nWorker\n\n\nMachineB\n\n\n5\n\n\n4.726\n\n\n0.551\n\n\n3.647\n\n\n5.805\n\n\nWorker\n\n\nMachineC\n\n\n5\n\n\n5.323\n\n\n0.545\n\n\n4.255\n\n\n6.390\n\n\nWorker\n\n\nMachineA\n\n\n6\n\n\n-5.592\n\n\n0.541\n\n\n-6.653\n\n\n-4.531\n\n\nWorker\n\n\nMachineB\n\n\n6\n\n\n-16.583\n\n\n0.551\n\n\n-17.662\n\n\n-15.504\n\n\nWorker\n\n\nMachineC\n\n\n6\n\n\n-5.030\n\n\n0.545\n\n\n-6.098\n\n\n-3.963\n\n\ngroup_var\n\n\neffect\n\n\ngroup\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:A\n\n\n0.275\n\n\n0.553\n\n\n-0.808\n\n\n1.359\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:B\n\n\n2.556\n\n\n0.553\n\n\n1.473\n\n\n3.640\n\n\nWorker:Machine\n\n\nIntercept\n\n\n1:C\n\n\n0.920\n\n\n0.553\n\n\n-0.164\n\n\n2.004\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:A\n\n\n0.209\n\n\n0.553\n\n\n-0.874\n\n\n1.293\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:B\n\n\n-0.749\n\n\n0.553\n\n\n-1.833\n\n\n0.334\n\n\nWorker:Machine\n\n\nIntercept\n\n\n2:C\n\n\n-4.402\n\n\n0.553\n\n\n-5.486\n\n\n-3.318\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:A\n\n\n7.118\n\n\n0.553\n\n\n6.035\n\n\n8.202\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:B\n\n\n7.647\n\n\n0.553\n\n\n6.563\n\n\n8.731\n\n\nWorker:Machine\n\n\nIntercept\n\n\n3:C\n\n\n4.490\n\n\n0.553\n\n\n3.407\n\n\n5.574\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:A\n\n\n-1.113\n\n\n0.553\n\n\n-2.196\n\n\n-0.029\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:B\n\n\n2.391\n\n\n0.553\n\n\n1.307\n\n\n3.475\n\n\nWorker:Machine\n\n\nIntercept\n\n\n4:C\n\n\n-1.493\n\n\n0.553\n\n\n-2.577\n\n\n-0.409\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:A\n\n\n-0.981\n\n\n0.553\n\n\n-2.064\n\n\n0.103\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:B\n\n\n4.705\n\n\n0.553\n\n\n3.621\n\n\n5.789\n\n\nWorker:Machine\n\n\nIntercept\n\n\n5:C\n\n\n5.416\n\n\n0.553\n\n\n4.332\n\n\n6.499\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:A\n\n\n-5.509\n\n\n0.553\n\n\n-6.593\n\n\n-4.426\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:B\n\n\n-16.550\n\n\n0.553\n\n\n-17.634\n\n\n-15.467\n\n\nWorker:Machine\n\n\nIntercept\n\n\n6:C\n\n\n-4.931\n\n\n0.553\n\n\n-6.014\n\n\n-3.847\n\n\nEven the default way that the extracted random effects are structured implies this difference. In the vector-valued model we have a multivariate normal draw for 3 machines (i.e. 3 variances and 3 covariances) for each of six workers. In the baseline model, we do not estimate any covariances and assume equal variance to draw for 18 groups (1 variance).\n\n\nranef(model_m_vv)\n\n$Worker\n    MachineA    MachineB   MachineC\n1  0.3121470   2.5531279  0.9302595\n2  0.1833294  -0.8032619 -4.2820004\n3  6.9694168   7.7792265  4.4740418\n4 -1.0238949   2.3283434 -1.4146891\n5 -0.8487803   4.7258593  5.3227211\n6 -5.5922179 -16.5832953 -5.0303328\n\nwith conditional variances for \"Worker\" \n\nranef(model_base_wm)\n\n$`Worker:Machine`\n    (Intercept)\n1:A   0.2754687\n1:B   2.5563491\n1:C   0.9200653\n2:A   0.2093562\n2:B  -0.7492747\n2:C  -4.4019891\n3:A   7.1181101\n3:B   7.6470099\n3:C   4.4901391\n4:A  -1.1128934\n4:B   2.3910679\n4:C  -1.4930401\n5:A  -0.9806684\n5:B   4.7050047\n5:C   5.4157138\n6:A  -5.5093731\n6:B -16.5501569\n6:C  -4.9308890\n\nwith conditional variances for \"Worker:Machine\" \n\nNow let’s compare the models directly via AIC. As we would expect if we dummy coded a predictor vs. running a model without the intercept (e.g. lm(score ~ machine), vs. lm(score ~ -1 + machine)), the random slope model and vector-valued models are identical and produce the same AIC. Likewise the intercept variance of the former is equal to the first group variance of the vector-valued model.\n\n\nmodel_base_w\n\n\nmodel_base_wm\n\n\nmodel_m_slope\n\n\nmodel_m_vv\n\n\nmodel_w_wm\n\n\n296.878\n\n\n231.256\n\n\n228.311\n\n\n228.311\n\n\n227.688\n\n\nWhile such a model is doing better than either of our baseline models, it turns out that our other approach is slightly better, as the additional complexity of estimating the covariances and separate variances wasn’t really worth it.\nAt this point we’ve seen a couple of ways of doing a model in this situation. Some may be a little too simplistic for a given scenario, others may not capture the correlation structure the way we’d want. In any case, we have options to explore.\n\nSimulation\nThe following is a simplified approach to creating data in this scenario, and allows us to play around with the settings to see what happens.\nData Creation\nFirst we need some data. The following creates a group identifier similar to Worker in our previous example, a cat_var like our Machine, and other covariates just to make it interesting.\n\n\n# for simplicity keeping to 3 cat levels\nset.seed(1234)\nng = 5000     # n groups\ncat_levs = 3  # n levels per group\nreps = 4      # number of obs per level per cat\n\nid = rep(1:ng, each = cat_levs * reps)           # id indicator (e.g. like Worker)\ncat_var = rep(1:cat_levs, times = ng, e = reps)  # categorical variable (e.g. Machine)\nx = rnorm(ng * cat_levs * reps)                  # continuous covariate\nx_c = rep(rnorm(ng), e = cat_levs * reps)        # continuous cluster level covariate\n\nSo we have the basic data in place, now we need to create the random effects. There are several ways we could do this, including more efficient ones, but this approach focuses on a conceptual approach and on the model that got us here, i.e. something of the form (1 + cat_var | group). In this case we assume this model is ‘correct’, so we’re going to create a multivariate normal draw of random effects for each level of the cat_var, which is only 3 levels. The correlations depicted are the estimates we expect from our model for the random effects2.\n\n\n# as correlated  (1, .5, .5) var, (1, .25, .25) sd \ncov_mat = lazerhawk::create_corr(c(.1, .25, .25), diagonal = c(1, .5, .5))\n\ncov2cor(cov_mat)  # these will be the estimated correlations for the random_slope model\n\n          [,1]      [,2]      [,3]\n[1,] 1.0000000 0.1414214 0.3535534\n[2,] 0.1414214 1.0000000 0.5000000\n[3,] 0.3535534 0.5000000 1.0000000\n\nNow we create the random effects by drawing an effect for each categorical level for each group.\n\n\n# take a multivariate normal draw for each of the groups in `id`\nre_id_cat_lev = mvtnorm::rmvnorm(ng, mean = rep(0, 3), sigma = cov_mat) %>% \n  data.frame()\n\nhead(re_id_cat_lev)\n\n          X1          X2          X3\n1  0.5618465  0.62780841  1.15175459\n2  0.6588685  0.78045939  0.25942427\n3  0.2680315 -0.06403496  1.30173301\n4 -0.3711184  0.37579392 -0.03242486\n5 -1.1306064 -0.08450038 -0.38165685\n6 -0.7537223  0.65320469  0.33269260\n\nNow that we have the random effects, we can create our target variable. We do this by adding our first effect to the intercept, and the others to their respective coefficients.\n\n\ny = \n  # fixed effect = (2, .5, -.5)\n  2  + .5*x - .5*x_c +   \n  # random intercept\n  rep(re_id_cat_lev[, 1], each = cat_levs * reps) +                           \n  # .25 is the fixef for group 2 vs. 1\n  (.25 + rep(re_id_cat_lev[, 2], each = cat_levs * reps)) * (cat_var == 2) +  \n  # .40 is the fixef for group 3 vs. 1\n  (.40 + rep(re_id_cat_lev[, 3], each = cat_levs * reps)) * (cat_var == 3) +  \n  rnorm(ng * cat_levs * reps, sd = .5)\n\nNow we create a data frame so we can see everything together.\n\n\ndf = tibble(\n    id,\n    cat_var,\n    x,\n    x_c,\n    y,\n    re_id = rep(re_id_cat_lev[, 1], each = cat_levs*reps),\n    re_id_cat_lev2 = rep(re_id_cat_lev[, 2], each = cat_levs*reps),\n    re_id_cat_lev3 = rep(re_id_cat_lev[, 3], each = cat_levs*reps)\n  ) %>% \n  mutate(\n    cat_var = factor(cat_var),\n    cat_as_num = as.integer(cat_var),\n    id = factor(id)\n  )\n\ndf %>% print(n = 30)\n\n# A tibble: 60,000 x 9\n   id    cat_var       x    x_c     y re_id re_id_cat_lev2 re_id_cat_lev3 cat_as_num\n   <fct> <fct>     <dbl>  <dbl> <dbl> <dbl>          <dbl>          <dbl>      <int>\n 1 1     1       -1.21   -1.43  3.04  0.562         0.628           1.15           1\n 2 1     1        0.277  -1.43  3.35  0.562         0.628           1.15           1\n 3 1     1        1.08   -1.43  3.46  0.562         0.628           1.15           1\n 4 1     1       -2.35   -1.43  2.64  0.562         0.628           1.15           1\n 5 1     2        0.429  -1.43  4.59  0.562         0.628           1.15           2\n 6 1     2        0.506  -1.43  3.61  0.562         0.628           1.15           2\n 7 1     2       -0.575  -1.43  3.54  0.562         0.628           1.15           2\n 8 1     2       -0.547  -1.43  2.98  0.562         0.628           1.15           2\n 9 1     3       -0.564  -1.43  5.28  0.562         0.628           1.15           3\n10 1     3       -0.890  -1.43  4.06  0.562         0.628           1.15           3\n11 1     3       -0.477  -1.43  4.31  0.562         0.628           1.15           3\n12 1     3       -0.998  -1.43  4.70  0.562         0.628           1.15           3\n13 2     1       -0.776   0.126 2.25  0.659         0.780           0.259          1\n14 2     1        0.0645  0.126 2.48  0.659         0.780           0.259          1\n15 2     1        0.959   0.126 2.99  0.659         0.780           0.259          1\n16 2     1       -0.110   0.126 2.55  0.659         0.780           0.259          1\n17 2     2       -0.511   0.126 3.94  0.659         0.780           0.259          2\n18 2     2       -0.911   0.126 3.22  0.659         0.780           0.259          2\n19 2     2       -0.837   0.126 3.98  0.659         0.780           0.259          2\n20 2     2        2.42    0.126 4.59  0.659         0.780           0.259          2\n21 2     3        0.134   0.126 3.58  0.659         0.780           0.259          3\n22 2     3       -0.491   0.126 3.31  0.659         0.780           0.259          3\n23 2     3       -0.441   0.126 2.66  0.659         0.780           0.259          3\n24 2     3        0.460   0.126 3.42  0.659         0.780           0.259          3\n25 3     1       -0.694   0.437 0.985 0.268        -0.0640          1.30           1\n26 3     1       -1.45    0.437 2.08  0.268        -0.0640          1.30           1\n27 3     1        0.575   0.437 2.62  0.268        -0.0640          1.30           1\n28 3     1       -1.02    0.437 1.24  0.268        -0.0640          1.30           1\n29 3     2       -0.0151  0.437 2.24  0.268        -0.0640          1.30           2\n30 3     2       -0.936   0.437 2.57  0.268        -0.0640          1.30           2\n# … with 5.997e+04 more rows\n\nRun the Models & Summarize\nWith everything in place, let’s run four models similar to our previous models from the Machine example:\nThe baseline model that does not distinguish the id from cat_var variance.\nThe random slope approach (data generating model)\nThe vector valued model (equivalent to #2)\nThe scalar model that does not estimate the random effect correlations\n\n\nm_interaction_only = lmer(y ~ x + x_c + cat_var + (1 | id:cat_var), df)\nm_random_slope = lmer(y ~ x + x_c + cat_var + (1 + cat_var | id), df)    \nm_vector_valued = lmer(y ~ x + x_c + cat_var + (0 + cat_var | id), df)\nm_separate_re = lmer(y ~ x + x_c + cat_var + (1 | id) + (1 | id:cat_var), df)\n\n\n\nmodel_mixed = list(\n  m_interaction_only = m_interaction_only,\n  m_random_slope = m_random_slope,\n  m_vector_valued = m_vector_valued,\n  m_separate_re = m_separate_re\n)\n\n# model summaries if desired\n# map(model_mixed, summarize_model, ci = 0, cor_re = TRUE)\nfe = map_df(model_mixed, extract_fixed_effects, .id = 'model') \nvc = map_df(model_mixed, extract_vc, ci_level = 0, .id = 'model')\n\nLooking at the fixed effects, we get what we should but, as before, we do see differences in the standard errors.\n\n\nmodel\n\n\nterm\n\n\nvalue\n\n\nse\n\n\nt\n\n\np_value\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nm_interaction_only\n\n\nIntercept\n\n\n2.006\n\n\n0.018\n\n\n111.051\n\n\n0\n\n\n1.971\n\n\n2.042\n\n\nm_interaction_only\n\n\nx\n\n\n0.497\n\n\n0.002\n\n\n212.962\n\n\n0\n\n\n0.492\n\n\n0.501\n\n\nm_interaction_only\n\n\nx_c\n\n\n-0.489\n\n\n0.010\n\n\n-46.776\n\n\n0\n\n\n-0.509\n\n\n-0.469\n\n\nm_interaction_only\n\n\ncat_var2\n\n\n0.256\n\n\n0.026\n\n\n10.026\n\n\n0\n\n\n0.206\n\n\n0.306\n\n\nm_interaction_only\n\n\ncat_var3\n\n\n0.389\n\n\n0.026\n\n\n15.228\n\n\n0\n\n\n0.339\n\n\n0.439\n\n\nm_random_slope\n\n\nIntercept\n\n\n2.006\n\n\n0.015\n\n\n136.942\n\n\n0\n\n\n1.977\n\n\n2.035\n\n\nm_random_slope\n\n\nx\n\n\n0.497\n\n\n0.002\n\n\n217.676\n\n\n0\n\n\n0.493\n\n\n0.502\n\n\nm_random_slope\n\n\nx_c\n\n\n-0.495\n\n\n0.014\n\n\n-34.636\n\n\n0\n\n\n-0.523\n\n\n-0.467\n\n\nm_random_slope\n\n\ncat_var2\n\n\n0.256\n\n\n0.011\n\n\n23.038\n\n\n0\n\n\n0.234\n\n\n0.278\n\n\nm_random_slope\n\n\ncat_var3\n\n\n0.389\n\n\n0.011\n\n\n34.969\n\n\n0\n\n\n0.367\n\n\n0.411\n\n\nm_vector_valued\n\n\nIntercept\n\n\n2.006\n\n\n0.015\n\n\n136.944\n\n\n0\n\n\n1.977\n\n\n2.035\n\n\nm_vector_valued\n\n\nx\n\n\n0.497\n\n\n0.002\n\n\n217.676\n\n\n0\n\n\n0.493\n\n\n0.502\n\n\nm_vector_valued\n\n\nx_c\n\n\n-0.495\n\n\n0.014\n\n\n-34.636\n\n\n0\n\n\n-0.523\n\n\n-0.467\n\n\nm_vector_valued\n\n\ncat_var2\n\n\n0.256\n\n\n0.011\n\n\n23.038\n\n\n0\n\n\n0.234\n\n\n0.278\n\n\nm_vector_valued\n\n\ncat_var3\n\n\n0.389\n\n\n0.011\n\n\n34.969\n\n\n0\n\n\n0.367\n\n\n0.411\n\n\nm_separate_re\n\n\nIntercept\n\n\n2.006\n\n\n0.018\n\n\n111.045\n\n\n0\n\n\n1.971\n\n\n2.042\n\n\nm_separate_re\n\n\nx\n\n\n0.497\n\n\n0.002\n\n\n216.586\n\n\n0\n\n\n0.493\n\n\n0.502\n\n\nm_separate_re\n\n\nx_c\n\n\n-0.489\n\n\n0.017\n\n\n-28.883\n\n\n0\n\n\n-0.522\n\n\n-0.456\n\n\nm_separate_re\n\n\ncat_var2\n\n\n0.256\n\n\n0.011\n\n\n23.077\n\n\n0\n\n\n0.234\n\n\n0.278\n\n\nm_separate_re\n\n\ncat_var3\n\n\n0.389\n\n\n0.011\n\n\n35.050\n\n\n0\n\n\n0.367\n\n\n0.411\n\n\nThe variance components break down as before.\n\n\nmodel\n\n\ngroup\n\n\neffect\n\n\nvariance\n\n\nsd\n\n\nvar_prop\n\n\nm_interaction_only\n\n\nid:cat_var\n\n\nIntercept\n\n\n1.570\n\n\n1.253\n\n\n0.864\n\n\nm_interaction_only\n\n\nResidual\n\n\n\n\n0.247\n\n\n0.497\n\n\n0.136\n\n\nm_random_slope\n\n\nid\n\n\nIntercept\n\n\n1.011\n\n\n1.006\n\n\n0.450\n\n\nm_random_slope\n\n\nid\n\n\ncat_var2\n\n\n0.494\n\n\n0.703\n\n\n0.220\n\n\nm_random_slope\n\n\nid\n\n\ncat_var3\n\n\n0.495\n\n\n0.704\n\n\n0.220\n\n\nm_random_slope\n\n\nResidual\n\n\n\n\n0.247\n\n\n0.497\n\n\n0.110\n\n\nm_vector_valued\n\n\nid\n\n\ncat_var1\n\n\n1.011\n\n\n1.006\n\n\n0.204\n\n\nm_vector_valued\n\n\nid\n\n\ncat_var2\n\n\n1.709\n\n\n1.307\n\n\n0.345\n\n\nm_vector_valued\n\n\nid\n\n\ncat_var3\n\n\n1.990\n\n\n1.411\n\n\n0.401\n\n\nm_vector_valued\n\n\nResidual\n\n\n\n\n0.247\n\n\n0.497\n\n\n0.050\n\n\nm_separate_re\n\n\nid:cat_var\n\n\nIntercept\n\n\n0.246\n\n\n0.496\n\n\n0.135\n\n\nm_separate_re\n\n\nid\n\n\nIntercept\n\n\n1.324\n\n\n1.151\n\n\n0.728\n\n\nm_separate_re\n\n\nResidual\n\n\n\n\n0.247\n\n\n0.497\n\n\n0.136\n\n\nIn this case, we know the model with correlated random effects is the more accurate model, and this is born out via AIC.\n\n\nm_interaction_only\n\n\nm_random_slope\n\n\nm_vector_valued\n\n\nm_separate_re\n\n\n135592.8\n\n\n122051.9\n\n\n122051.9\n\n\n123745.2\n\n\nChange the model orientation\nNow I will make the vector_valued model reduce to the separate_re model. First, we create a covariance matrix that has equal variances/covariances (i.e. compound symmetry), and for demonstration, we will apply the random effects a little differently. So, when we create the target variable, we make a slight alteration to apply it to the vector valued model instead.\n\n\nset.seed(1234)\n\ncov_mat = lazerhawk::create_corr(c(0.1, 0.1, 0.1), diagonal = c(.5, .5, .5))\n\ncov2cor(cov_mat)  # these will now be the estimated correlations for the vector_valued model\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.2  0.2\n[2,]  0.2  1.0  0.2\n[3,]  0.2  0.2  1.0\n\nre_id_cat_lev = mvtnorm::rmvnorm(ng, mean = rep(0, 3), sigma = cov_mat) %>% \n  data.frame()\n\ny = 2  + .5*x - .5*x_c +   # fixed effect = (2, .5, -.5)\n  rep(re_id_cat_lev[, 1], each = cat_levs * reps) * (cat_var == 1) +     # added this\n  rep(re_id_cat_lev[, 2], each = cat_levs * reps) * (cat_var == 2) +\n  rep(re_id_cat_lev[, 3], each = cat_levs * reps) * (cat_var == 3) +\n  .25 * (cat_var == 2) +  # .25 is the fixef for group 2 vs. 1\n  .40 * (cat_var == 3) +  # .40 is the fixef for group 3 vs. 1\n  rnorm(ng * cat_levs * reps, sd = .5)\n\n\ndf = tibble(\n    id,\n    cat_var  = factor(cat_var),\n    x,\n    x_c,\n    y\n  )\n\nRerun the models.\n\n\nm_random_slope = lmer(y ~ x + x_c + cat_var + (1 + cat_var | id), df)  # still problems!\nm_vector_valued = lmer(y ~ x + x_c + cat_var + (0 + cat_var | id), df)   \nm_separate_re = lmer(y ~ x + x_c + cat_var + (1 | id) + (1 | id:cat_var), df)\n\nExamine the variance components.\n\n\nmodel_mixed = list(\n  m_random_slope = m_random_slope,\n  m_vector_valued = m_vector_valued,\n  m_separate_re = m_separate_re\n)\n\n# model summaries if desired\n# map(model_mixed, summarize_model, ci = 0, cor_re = TRUE)\n\n# fixed effects if desired\n# fe = map_df(model_mixed, extract_fixed_effects, .id = 'model')\n\nvc = map_df(model_mixed, extract_vc, ci_level = 0, .id = 'model')\n\n\n\nmodel\n\n\ngroup\n\n\neffect\n\n\nvariance\n\n\nsd\n\n\nvar_prop\n\n\nm_random_slope\n\n\nid\n\n\nIntercept\n\n\n0.491\n\n\n0.700\n\n\n0.213\n\n\nm_random_slope\n\n\nid\n\n\ncat_var2\n\n\n0.786\n\n\n0.886\n\n\n0.341\n\n\nm_random_slope\n\n\nid\n\n\ncat_var3\n\n\n0.778\n\n\n0.882\n\n\n0.337\n\n\nm_random_slope\n\n\nResidual\n\n\n\n\n0.251\n\n\n0.501\n\n\n0.109\n\n\nm_vector_valued\n\n\nid\n\n\ncat_var1\n\n\n0.491\n\n\n0.700\n\n\n0.283\n\n\nm_vector_valued\n\n\nid\n\n\ncat_var2\n\n\n0.497\n\n\n0.705\n\n\n0.287\n\n\nm_vector_valued\n\n\nid\n\n\ncat_var3\n\n\n0.492\n\n\n0.702\n\n\n0.284\n\n\nm_vector_valued\n\n\nResidual\n\n\n\n\n0.251\n\n\n0.501\n\n\n0.145\n\n\nm_separate_re\n\n\nid:cat_var\n\n\nIntercept\n\n\n0.395\n\n\n0.628\n\n\n0.530\n\n\nm_separate_re\n\n\nid\n\n\nIntercept\n\n\n0.099\n\n\n0.314\n\n\n0.133\n\n\nm_separate_re\n\n\nResidual\n\n\n\n\n0.251\n\n\n0.501\n\n\n0.337\n\n\nIn this case, we know the true case regards zero correlations and equal variances, so estimating them is adding complexity we don’t need, thus our simpler model wins (log likelihoods are essentially the same).\n\n\nparameter\n\n\nm_random_slope\n\n\nm_vector_valued\n\n\nm_separate_re\n\n\nLL\n\n\n-59818.9\n\n\n-59818.9\n\n\n-59819.72\n\n\nAIC\n\n\n119661.8\n\n\n119661.8\n\n\n119655.45\n\n\nSummary\nHere we’ve demonstrated a couple of different ways to specify a particular model with random slopes for a categorical covariate. Intuition may lead to a model that is not easy to estimate, often leading to convergence problems. Sometimes, this model may be overly complicated, and a simpler version will likely have less estimation difficulty. Try it out if you run into trouble!\nThough I use the word ‘within’, do not take it to mean we have nested data.↩︎\nI use a personal package to create the matrix where I can just specify the lower diagonal, as this comes up a lot for simulation. Feel free to just create the matrix directly given it’s just a 3x3.↩︎\n",
    "preview": "posts/2020-03-01-random-categorical/../../img/cat_ran/machines.svg",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-20-big-mixed-models/",
    "title": "Mixed Models for Big Data",
    "description": "Explorations of a fast penalized regression approach with mgcv",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2019-10-20",
    "categories": [
      "GAM",
      "mixed models",
      "big data",
      "bayesian"
    ],
    "contents": "\n\nContents\nIntroduction\nR Packages for Mixed Models with Large Data\nAdditive Models as Mixed Models\nComparison of GAM to the Mixed Model\nThe bam approach\nFixed effects comparison\nVariance components comparison\nEstimated random effects\nComparisons to Bayesian Estimates\n\nBack to the initial problem\nWhen to use bam\nLinear Mixed Models\nGeneralized Linear Mixed Models\nOther options\n\nLimitations\nSummary\nSupplemental\nSimulation Settings\n\n\n\nLast updated March 13, 2021. Timings based on original date of posting.\nIntroduction\nWith mixed models, it is easy to run into data that is larger in size than some more typical data scenarios. Consider a cross-sectional data set with 200 individuals. This is fairly small data. Now, if we observe them each five times, as in a longitudinal setting, we suddenly have 1000 observations. There may be less than 200 countries in the world, but if we survey 100s or 1000s of people in many of them, we suddenly have a notable data set size, and still would potentially like to model a country-level random effect. What are our options when dealing with possibly gigabytes of data?\nThis post will demonstrate an approach that can be used with potentially millions of data points, multiple random effects, and possibly other complexities. First we’ll demonstrate how to get typical mixed model results using the approach used for generalized additive models. We’ll compare the output of the GAM, lme4, and even fully Bayesian mixed models. Then we’ll show some timings to compare the speed of the different approaches of common tools, and summarize some findings from other places.\nBackground required:\nFor the following you should have familiarity with mixed models. Knowledge of the lme4 package would be useful but isn’t required. Likewise, knowledge of generalized additive models and mgcv would be helpful, but I don’t think it’s required to follow the demonstration.\nR Packages for Mixed Models with Large Data\nWhile many tools abound to conduct mixed models for larger data sizes, their limitations can be found pretty quickly. R’s lme4 is a standard, but powerful mixed model tool. More to the point, it is computationally efficient, such that it can handle very large sample sizes for simpler mixed models. For linear mixed models this can include hundreds of thousands of observations with possibly multiple random effects, still running on a basic laptop. For such models, it’s still largely the tool of choice, and its approach has even been copied/ported into other statistical packages.\nWe’ll first create some data to model. This is just a simple random intercepts setting.\n\n\nset.seed(12358)\nN = 1e6                                  # total sample size\nn_groups = 1000                          # number of groups\ng = rep(1:n_groups, e = N/n_groups)      # the group identifier\n\nx = rnorm(N)                             # an observation level continuous variable\nb = rbinom(n_groups, size = 1, prob=.5)  # a cluster level categorical variable\nb = b[g]\n\nsd_g = .5     # standard deviation for the random effect\nsigma = 1     # standard deviation for the observation\n\nre0 = rnorm(n_groups, sd = sd_g)  # random effects\nre  = re0[g]\n\nlp = 0 + .5*x + .25*b + re        # linear predictor \n\ny = rnorm(N, mean = lp, sd = sigma)               # create a continuous target variable\ny_bin = rbinom(N, size = 1, prob = plogis(lp))    # create a binary target variable\n\nd = tibble(x, b, y, y_bin, g = factor(g))\n\n\n\n\nLet’s take a look at the data first.\n\n\nindex\n\n\nx\n\n\nb\n\n\ny\n\n\ny_bin\n\n\ng\n\n\n1\n\n\n-0.378\n\n\n0\n\n\n-0.278\n\n\n1\n\n\n1\n\n\n2\n\n\n-0.812\n\n\n0\n\n\n-0.343\n\n\n0\n\n\n1\n\n\n3\n\n\n0.218\n\n\n0\n\n\n-0.810\n\n\n1\n\n\n1\n\n\n4\n\n\n1.529\n\n\n0\n\n\n0.465\n\n\n1\n\n\n1\n\n\n5\n\n\n-1.877\n\n\n0\n\n\n-1.570\n\n\n0\n\n\n1\n\n\n6\n\n\n-0.427\n\n\n0\n\n\n0.047\n\n\n0\n\n\n1\n\n\n999995\n\n\n-1.181\n\n\n1\n\n\n-1.111\n\n\n0\n\n\n1000\n\n\n999996\n\n\n-1.487\n\n\n1\n\n\n0.563\n\n\n1\n\n\n1000\n\n\n999997\n\n\n-1.236\n\n\n1\n\n\n-0.603\n\n\n0\n\n\n1000\n\n\n999998\n\n\n0.412\n\n\n1\n\n\n0.736\n\n\n0\n\n\n1000\n\n\n999999\n\n\n-0.644\n\n\n1\n\n\n-1.257\n\n\n1\n\n\n1000\n\n\n1000000\n\n\n0.409\n\n\n1\n\n\n0.520\n\n\n1\n\n\n1000\n\n\nNow with the data in place, let’s try lme4 to model the continuous outcome.\n\nThe time to focus on is elapsed, which is the number of seconds the function took to run.\n\n\nlibrary(lme4)\n\nsystem.time({\n  mixed_big = lmer(y ~ x + b + (1|g))\n})\n\n\n   user  system elapsed \n  6.306   0.228   7.196 \n\nsummary(mixed_big, cor = FALSE)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + b + (1 | g)\n\nREML criterion at convergence: 2841256\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6066 -0.6743 -0.0004  0.6744  5.0367 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n g        (Intercept) 0.2509   0.5009  \n Residual             0.9978   0.9989  \nNumber of obs: 1000000, groups:  g, 1000\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 0.0364754  0.0223332   1.633\nx           0.5017616  0.0009987 502.409\nb           0.1904534  0.0317430   6.000\n\nThis is great! We just ran a mixed model for 1,000,000 observations and 1,000 groups for our random effect in just a few seconds.\nBut one problem comes as soon as you move to the generalized mixed model, e.g. having a binary outcome, or include additional complexity while still dealing with large data. The following is essentially the same model, but for a binary outcome.\n\n\nsystem.time({\n  mixed_big_glmm = glmer(y_bin ~ x + b + (1|g), family = binomial)\n})\n\n\n   user  system elapsed \n 85.741   0.774  89.480 \n\nTo begin with, you shouldn’t be worried about models taking a few minutes to run, or even a couple hours. Once you have your model(s) squared away, the testing of which can be done on a smaller sample of the data set, there is no need to repeatedly run it. But in this case we had a greater than 15 fold increase in time for a very simple data scenario. So it’s good to have options when you need them. Let’s turn to those.\nAdditive Models as Mixed Models\nSimon Wood’s wonderful work on generalized additive models (GAM) and the mgcv package make it one of the better modeling tools in the R kingdom. As his text(S. N. Wood 2017) and other work shows, additive models constructed be posited in a similar way as mixed models, and he exploits this by providing numerous ways to include and explore random effects in the GAM approach. One key difference between the GAM and a standard linear mixed model approach is the way parameters are estimated. For the GAM, the random effects are estimated as are other fixed effect coefficients. Those random effects are penalized, in a similar way as L2/ridge regression. The ‘fixed effects’ are not penalized, and so that part is basically just a generalized linear model. As we will see though, the results will be nearly the same between mgcv and lme4.\nThe following demonstrates the link between the approaches by showing a model that includes a random intercept and slope. We will use the standard mgcv approach for specifying a smooth term, but alternatives are shown for those familiar with the package.\n\nIf you just use coef on the following gam objects, you will see that the random effects are lumped in with the other estimated coefficients.\n\n\nlibrary(lme4)\nlibrary(mgcv)\n\nmixed_model = lmer(\n  Reaction ~ Days + (1 | Subject) + (0 + Days | Subject),\n  data = sleepstudy\n)\n\nga_model = gam(\n  Reaction ~  Days + s(Subject, bs = 're') + s(Days, Subject, bs = 're'),\n  data = sleepstudy,\n  method = 'REML'\n)\n\n# Using gamm and gamm4 for the same model\n# ga_model = gamm(\n#   Reaction ~  Days ,\n#   random = list(Subject = ~ 0 + Days),\n#   data = sleepstudy,\n#   method = 'REML'\n# )\n# \n# ga_model = gamm4::gamm4(\n#   Reaction ~  Days,\n#   random =  ~ (Days||Subject),\n#   data = sleepstudy,\n#   REML = TRUE\n# )\n\n\n\nNote that we use s to denote a smooth term in the parlance of additive models, and the bs = 're' specifies that we want it as a random effect (as opposed to a spline or other basis function). The second smooth term s(Days, Subject, bs = 're') denotes random coefficients for the Days covariate.\n\nAs shown, one could use the gamm function for the nlme style, or Wood’s gamm4 package to use the lme4 syntax. These alternate approaches allow for more flexibility in some ways, but will not be useful to us for big data.\nComparison of GAM to the Mixed Model\nAside from the syntax, the underlying model between the two is the same, and the following shows that we obtain the same results for both lme4 and mgcv.\n\n\nsummary(mixed_model, cor = FALSE)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9626 -0.4625  0.0204  0.4653  5.1860 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n Subject   (Intercept) 627.57   25.051  \n Subject.1 Days         35.86    5.988  \n Residual              653.58   25.565  \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.885  36.513\nDays          10.467      1.560   6.712\n\nsummary(ga_model)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nReaction ~ Days + s(Subject, bs = \"re\") + s(Days, Subject, bs = \"re\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  251.405      6.885  36.513  < 2e-16 ***\nDays          10.467      1.560   6.712 3.67e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                  edf Ref.df      F  p-value    \ns(Subject)      12.94     17  89.29 1.09e-06 ***\ns(Days,Subject) 14.41     17 104.56  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.794   Deviance explained = 82.7%\n-REML = 871.83  Scale est. = 653.58    n = 180\n\nI don’t want to go into the details of the printout for mgcv, but it is worth noting that the parametric part is equivalent to the fixed effects portion of the lme4 output. Likewise the smooth terms output is related to the random effects, but we’ll extract them in a manner more suited to typical mixed model output instead. So let’s compare the variance components, and get them ready for later comparison to bam results. Note, I’ve use several packages for mixed models, so I created a package called mixedup to provide tidier and consistent output, and which is more similar to lme4. I note the corresponding mgcv function where appropriate.\n\nThe mixedup package is available on GitHub.\n\n\nlibrary(mixedup)\n\n# extract just the fixed effects for later.\nmixed_fe = extract_fixed_effects(mixed_model, digits = 5)\ngam_fe   = extract_fixed_effects(ga_model, digits = 5)  # coefs with se and confidence interval\n\n# variance components\nlmer_vcov = extract_vc(mixed_model, digits = 5)\ngam_vcov  = extract_vc(ga_model, digits = 5)    # cleaner gam.vcomp\n\n\n\n\n\nTable 1: LME Result\n\n\n\n\ngroup\n\n\neffect\n\n\nvariance\n\n\nsd\n\n\nsd_2.5\n\n\nsd_97.5\n\n\nvar_prop\n\n\nsd_(Intercept)|Subject\n\n\nSubject\n\n\nIntercept\n\n\n627.569\n\n\n25.051\n\n\n15.259\n\n\n37.786\n\n\n0.477\n\n\nsd_Days|Subject\n\n\nSubject.1\n\n\nDays\n\n\n35.858\n\n\n5.988\n\n\n3.964\n\n\n8.769\n\n\n0.027\n\n\nsigma\n\n\nResidual\n\n\n\n\n653.584\n\n\n25.565\n\n\n22.881\n\n\n28.788\n\n\n0.496\n\n\n\n\nTable 2: GAM Result\n\n\n\n\ngroup\n\n\neffect\n\n\nvariance\n\n\nsd\n\n\nsd_2.5\n\n\nsd_97.5\n\n\nvar_prop\n\n\ns(Subject)\n\n\nSubject\n\n\nIntercept\n\n\n627.571\n\n\n25.051\n\n\n16.085\n\n\n39.015\n\n\n0.477\n\n\ns(Days,Subject)\n\n\nSubject\n\n\nDays\n\n\n35.858\n\n\n5.988\n\n\n4.025\n\n\n8.908\n\n\n0.027\n\n\nscale\n\n\nResidual\n\n\n\n\n653.582\n\n\n25.565\n\n\n22.792\n\n\n28.676\n\n\n0.496\n\n\n\nThe penalty parameter in the GAM model is inversely related to the variance estimate of the random effects. See this demo.\nThe bam approach\nFor large data, mgcv provides the bam function. For this small data setting we don’t really need it, but we can establish that we would get similar results using it without having to wait. We will see the benefits when we apply bam to large data later. None of our syntax changes, just the function.\n\n\nba_model = bam(\n  Reaction ~  Days + s(Subject, bs='re') + s(Days, Subject, bs='re'), \n  data = sleepstudy\n)\n\nbam_fe   = extract_fixed_effects(ba_model, digits = 5)\nbam_vcov = extract_vc(ba_model, digits = 5)\n\n\n\nHow does it work? The function uses a parallelized approach where possible, essentially working on subsets of the model matrices simultaneously. Details can be found in the references(Li and Wood 2019)(S. N. Wood, Goude, and Shaw 2015a)(S. N. Wood et al. 2017), but basically mgcv parallelizes the parts that can be, and additionally provides an option to discretize the data to work with the minimal information necessary to produce viable estimates. The following uses the discrete option. As there isn’t really anything to discretize with so little data, this is just to demonstrate the syntax.\n\n\nba_d_model = bam(\n  Reaction ~  Days + s(Subject, bs='re') + s(Days, Subject, bs='re'), \n  data = sleepstudy,\n  discrete = TRUE\n)\n\nbam_d_fe   = extract_fixed_effects(ba_d_model, digits = 5)\nbam_d_vcov = extract_vc(ba_d_model, digits = 5)\n\n\n\nFixed effects comparison\nWe start by comparing the fixed effects of all models run thus far. No surprises here, the results are the same.\n\n\nTable 3: Fixed Effects Estimates\n\n\nterm\n\n\nmixed\n\n\ngam\n\n\nbam\n\n\nbam_d\n\n\nIntercept\n\n\n251.405\n\n\n251.405\n\n\n251.405\n\n\n251.405\n\n\nDays\n\n\n10.467\n\n\n10.467\n\n\n10.467\n\n\n10.467\n\n\nLet’s examine the standard errors. Note that there are options for the GAM models for standard error estimation, including a Bayesian one. For more details, see ?gamObject, but I will offer the summary:\nVe\nfrequentist estimated covariance matrix for the parameter estimators. Particularly useful for testing whether terms are zero. Not so useful for CI’s as smooths are usually biased.\nVp\nestimated covariance matrix for the parameters. This is a Bayesian posterior covariance matrix that results from adopting a particular Bayesian model of the smoothing process. Particularly useful for creating credible/confidence intervals.\nVc\nUnder ML or REML smoothing parameter estimation it is possible to correct the covariance matrix Vp for smoothing parameter uncertainty. This is the corrected version.\nWe will use the Bayesian estimates (Vp), but for this setting there are no appreciable differences. I expand the digits to show they are in fact different to some decimal place.\n\n\nTable 4: Fixed Effects Standard Errors\n\n\nterm\n\n\nmixed\n\n\ngam\n\n\nbam\n\n\nbam_d\n\n\nIntercept\n\n\n6.88538\n\n\n6.88540\n\n\n6.88538\n\n\n6.88538\n\n\nDays\n\n\n1.55957\n\n\n1.55956\n\n\n1.55957\n\n\n1.55957\n\n\nVariance components comparison\nNow we move to the variance component estimates. Reported are the standard deviations for subject level random effects for intercept, Days coefficient, and residual.\n\n\nTable 5: Variance Components Estimates\n\n\n\n\nIntercept\n\n\nDays\n\n\nResidual\n\n\nmixed\n\n\n25.051\n\n\n5.988\n\n\n25.565\n\n\ngam\n\n\n25.051\n\n\n5.988\n\n\n25.565\n\n\nbam\n\n\n25.051\n\n\n5.988\n\n\n25.565\n\n\nbam_d\n\n\n25.051\n\n\n5.988\n\n\n25.565\n\n\nWe can also look at their interval estimates. We use the profile likelihood for the lme4 mixed model. In this case we can see slightly wider and somewhat different boundary estimates for the variance components, but not too dissimilar.\n\n\nTable 6: Interval Estimates for Variance Components\n\n\nModel\n\n\ngroup\n\n\neffect\n\n\nvariance\n\n\nsd\n\n\nsd_2.5\n\n\nsd_97.5\n\n\nwidth\n\n\nmixed\n\n\nSubject\n\n\nIntercept\n\n\n627.5690\n\n\n25.0513\n\n\n15.2586\n\n\n37.7865\n\n\n22.5278\n\n\nDays\n\n\n35.8584\n\n\n5.9882\n\n\n3.9641\n\n\n8.7692\n\n\n4.8051\n\n\nResidual\n\n\n\n\n653.5835\n\n\n25.5653\n\n\n22.8805\n\n\n28.7876\n\n\n5.9071\n\n\ngam\n\n\nSubject\n\n\nIntercept\n\n\n627.5712\n\n\n25.0514\n\n\n16.0854\n\n\n39.0150\n\n\n22.9297\n\n\nDays\n\n\n35.8580\n\n\n5.9882\n\n\n4.0252\n\n\n8.9083\n\n\n4.8830\n\n\nResidual\n\n\n\n\n653.5822\n\n\n25.5652\n\n\n22.7918\n\n\n28.6763\n\n\n5.8845\n\n\nbam\n\n\nSubject\n\n\nIntercept\n\n\n627.5691\n\n\n25.0513\n\n\n16.0853\n\n\n39.0150\n\n\n22.9297\n\n\nDays\n\n\n35.8582\n\n\n5.9882\n\n\n4.0253\n\n\n8.9083\n\n\n4.8830\n\n\nResidual\n\n\n\n\n653.5838\n\n\n25.5653\n\n\n22.7918\n\n\n28.6763\n\n\n5.8845\n\n\nbam_d\n\n\nSubject\n\n\nIntercept\n\n\n627.5691\n\n\n25.0513\n\n\n16.0853\n\n\n39.0150\n\n\n22.9297\n\n\nDays\n\n\n35.8582\n\n\n5.9882\n\n\n4.0253\n\n\n8.9083\n\n\n4.8830\n\n\nResidual\n\n\n\n\n653.5838\n\n\n25.5653\n\n\n22.7918\n\n\n28.6763\n\n\n5.8845\n\n\nEstimated random effects\nNow let’s look at the random effect estimates.\n\n\nmixed_re_init = extract_ranef(mixed_model, digits = 5)\ngam_re_init   = extract_ranef(ga_model, digits = 5)\nbam_re_init   = extract_ranef(ba_model, digits = 5)\nbam_d_re_init = extract_ranef(ba_d_model, digits = 5)\n\n\n\nWe’ll start with the random effects for the intercept. To several decimal places, we start to see differences, so again we know they aren’t doing exactly the same thing, but they are coming to the same conclusion.\n\n\nTable 7: Estimated Random Intercepts\n\n\nbam_d_re_init\n\n\nbam_re_init\n\n\ngam_re_init\n\n\nmixed_re_init\n\n\n1.51270\n\n\n1.51270\n\n\n1.51272\n\n\n1.51266\n\n\n-40.37390\n\n\n-40.37390\n\n\n-40.37397\n\n\n-40.37387\n\n\n-39.18104\n\n\n-39.18104\n\n\n-39.18111\n\n\n-39.18103\n\n\n24.51890\n\n\n24.51890\n\n\n24.51893\n\n\n24.51892\n\n\n22.91443\n\n\n22.91443\n\n\n22.91446\n\n\n22.91445\n\n\n9.22197\n\n\n9.22197\n\n\n9.22199\n\n\n9.22198\n\n\n17.15612\n\n\n17.15612\n\n\n17.15614\n\n\n17.15612\n\n\n-7.45173\n\n\n-7.45173\n\n\n-7.45174\n\n\n-7.45174\n\n\n0.57872\n\n\n0.57872\n\n\n0.57870\n\n\n0.57876\n\n\n34.76793\n\n\n34.76793\n\n\n34.76800\n\n\n34.76790\n\n\n-25.75432\n\n\n-25.75432\n\n\n-25.75436\n\n\n-25.75433\n\n\n-13.86504\n\n\n-13.86504\n\n\n-13.86504\n\n\n-13.86506\n\n\n4.91598\n\n\n4.91598\n\n\n4.91598\n\n\n4.91599\n\n\n20.92904\n\n\n20.92904\n\n\n20.92908\n\n\n20.92903\n\n\n3.25865\n\n\n3.25865\n\n\n3.25865\n\n\n3.25864\n\n\n-26.47583\n\n\n-26.47583\n\n\n-26.47585\n\n\n-26.47585\n\n\n0.90565\n\n\n0.90565\n\n\n0.90565\n\n\n0.90565\n\n\n12.42176\n\n\n12.42176\n\n\n12.42178\n\n\n12.42175\n\n\nRandom effects for the Days coefficient.\n\n\nTable 8: Estimated Random Intercepts\n\n\nbam_d_re_init\n\n\nbam_re_init\n\n\ngam_re_init\n\n\nmixed_re_init\n\n\n9.32349\n\n\n9.32349\n\n\n9.32348\n\n\n9.32350\n\n\n-8.59917\n\n\n-8.59917\n\n\n-8.59916\n\n\n-8.59918\n\n\n-5.38779\n\n\n-5.38779\n\n\n-5.38778\n\n\n-5.38779\n\n\n-4.96865\n\n\n-4.96865\n\n\n-4.96865\n\n\n-4.96865\n\n\n-3.19393\n\n\n-3.19393\n\n\n-3.19394\n\n\n-3.19394\n\n\n-0.30849\n\n\n-0.30849\n\n\n-0.30850\n\n\n-0.30849\n\n\n-0.28721\n\n\n-0.28721\n\n\n-0.28721\n\n\n-0.28721\n\n\n1.11599\n\n\n1.11599\n\n\n1.11599\n\n\n1.11599\n\n\n-10.90597\n\n\n-10.90597\n\n\n-10.90596\n\n\n-10.90598\n\n\n8.62762\n\n\n8.62762\n\n\n8.62760\n\n\n8.62762\n\n\n1.28069\n\n\n1.28069\n\n\n1.28069\n\n\n1.28069\n\n\n6.75640\n\n\n6.75640\n\n\n6.75640\n\n\n6.75641\n\n\n-3.07513\n\n\n-3.07513\n\n\n-3.07513\n\n\n-3.07514\n\n\n3.51221\n\n\n3.51221\n\n\n3.51220\n\n\n3.51221\n\n\n0.87305\n\n\n0.87305\n\n\n0.87305\n\n\n0.87305\n\n\n4.98379\n\n\n4.98379\n\n\n4.98379\n\n\n4.98379\n\n\n-1.00529\n\n\n-1.00529\n\n\n-1.00529\n\n\n-1.00529\n\n\n1.25840\n\n\n1.25840\n\n\n1.25840\n\n\n1.25840\n\n\nStandard errors for the random effects. In the balanced design these are essentially constant across clusters. We can see that the Bayesian estimates from mgcv reflect greater uncertainty.\n\nThe bam results may actually be slightly different for some clusters.\n\n\n\nTable 9: Standard Errors of the Random Coefficients\n\n\nModel\n\n\nIntercepts\n\n\nDays\n\n\nmixed\n\n\n12.239\n\n\n2.335\n\n\ngam\n\n\n13.279\n\n\n2.673\n\n\nbam\n\n\n13.279\n\n\n2.673\n\n\nbam_discrete\n\n\n13.279\n\n\n2.673\n\n\nComparisons to Bayesian Estimates\nAs we have noted, one of the differences between lme4 and mgcv output is that the default uncertainty estimates for the GAM are Bayesian. As such, it might be interesting to compare these to a fully Bayes approach. We’ll use rstanarm, which uses the lme4 style syntax.\n\nFor those familiar with Bayesian models, the Stan group provides a vignette with information about the priors in this model and comparisons to lme4 and gamm4.\n\n\nlibrary(rstanarm)\nbayes = stan_lmer(Reaction ~ Days + (1|Subject) + (0 + Days|Subject), \n                  data = sleepstudy,\n                  cores = 4)\n\nbayes_fe = extract_fixed_effects(bayes)\nbayes_vc = extract_vc(bayes)\nbayes_re = extract_random_effects(bayes)\n\n\n\n\n\nTable 10: Bayesian fixed effects\n\n\nterm\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nIntercept\n\n\n251.461\n\n\n7.172\n\n\n237.372\n\n\n265.477\n\n\nDays\n\n\n10.524\n\n\n1.678\n\n\n7.333\n\n\n13.972\n\n\nTable 10: Bayesian variance components\n\n\ngroup\n\n\neffect\n\n\nvariance\n\n\nsd\n\n\nsd_2.5\n\n\nsd_97.5\n\n\nvar_prop\n\n\nSubject\n\n\nIntercept\n\n\n701.430\n\n\n26.485\n\n\n15.604\n\n\n39.465\n\n\n0.498\n\n\nSubject\n\n\nDays\n\n\n44.073\n\n\n6.639\n\n\n4.219\n\n\n9.964\n\n\n0.031\n\n\nResidual\n\n\n\n\n662.244\n\n\n25.734\n\n\n4.799\n\n\n5.378\n\n\n0.470\n\n\nTable 10: Bayesian random effects\n\n\neffect\n\n\ngroup\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nIntercept\n\n\n308\n\n\n1.035\n\n\n12.347\n\n\n-23.165\n\n\n25.235\n\n\nIntercept\n\n\n309\n\n\n-39.030\n\n\n12.347\n\n\n-63.230\n\n\n-14.831\n\n\nIntercept\n\n\n310\n\n\n-38.225\n\n\n12.347\n\n\n-62.425\n\n\n-14.026\n\n\nIntercept\n\n\n330\n\n\n23.639\n\n\n12.347\n\n\n-0.561\n\n\n47.838\n\n\nIntercept\n\n\n331\n\n\n22.412\n\n\n12.347\n\n\n-1.787\n\n\n46.612\n\n\nIntercept\n\n\n332\n\n\n8.914\n\n\n12.347\n\n\n-15.285\n\n\n33.114\n\n\nIntercept\n\n\n333\n\n\n16.892\n\n\n12.347\n\n\n-7.308\n\n\n41.091\n\n\nIntercept\n\n\n334\n\n\n-7.233\n\n\n12.347\n\n\n-31.432\n\n\n16.967\n\n\nIntercept\n\n\n335\n\n\n0.390\n\n\n12.347\n\n\n-23.810\n\n\n24.589\n\n\nIntercept\n\n\n337\n\n\n34.017\n\n\n12.347\n\n\n9.818\n\n\n58.217\n\n\nIntercept\n\n\n349\n\n\n-25.220\n\n\n12.347\n\n\n-49.420\n\n\n-1.021\n\n\nIntercept\n\n\n350\n\n\n-13.650\n\n\n12.347\n\n\n-37.850\n\n\n10.549\n\n\nIntercept\n\n\n351\n\n\n4.664\n\n\n12.347\n\n\n-19.536\n\n\n28.863\n\n\nIntercept\n\n\n352\n\n\n19.978\n\n\n12.347\n\n\n-4.222\n\n\n44.178\n\n\nIntercept\n\n\n369\n\n\n3.150\n\n\n12.347\n\n\n-21.050\n\n\n27.350\n\n\nIntercept\n\n\n370\n\n\n-25.832\n\n\n12.347\n\n\n-50.031\n\n\n-1.632\n\n\nIntercept\n\n\n371\n\n\n0.719\n\n\n12.347\n\n\n-23.481\n\n\n24.918\n\n\nIntercept\n\n\n372\n\n\n12.066\n\n\n12.347\n\n\n-12.133\n\n\n36.266\n\n\nDays\n\n\n308\n\n\n9.262\n\n\n2.417\n\n\n4.525\n\n\n14.000\n\n\nDays\n\n\n309\n\n\n-8.800\n\n\n2.417\n\n\n-13.538\n\n\n-4.062\n\n\nDays\n\n\n310\n\n\n-5.531\n\n\n2.417\n\n\n-10.269\n\n\n-0.793\n\n\nDays\n\n\n330\n\n\n-4.860\n\n\n2.417\n\n\n-9.598\n\n\n-0.123\n\n\nDays\n\n\n331\n\n\n-3.225\n\n\n2.417\n\n\n-7.962\n\n\n1.513\n\n\nDays\n\n\n332\n\n\n-0.350\n\n\n2.417\n\n\n-5.087\n\n\n4.388\n\n\nDays\n\n\n333\n\n\n-0.325\n\n\n2.417\n\n\n-5.063\n\n\n4.412\n\n\nDays\n\n\n334\n\n\n1.043\n\n\n2.417\n\n\n-3.694\n\n\n5.781\n\n\nDays\n\n\n335\n\n\n-10.978\n\n\n2.417\n\n\n-15.715\n\n\n-6.240\n\n\nDays\n\n\n337\n\n\n8.693\n\n\n2.417\n\n\n3.956\n\n\n13.431\n\n\nDays\n\n\n349\n\n\n1.127\n\n\n2.417\n\n\n-3.611\n\n\n5.864\n\n\nDays\n\n\n350\n\n\n6.666\n\n\n2.417\n\n\n1.929\n\n\n11.404\n\n\nDays\n\n\n351\n\n\n-3.042\n\n\n2.417\n\n\n-7.779\n\n\n1.696\n\n\nDays\n\n\n352\n\n\n3.496\n\n\n2.417\n\n\n-1.241\n\n\n8.234\n\n\nDays\n\n\n369\n\n\n0.799\n\n\n2.417\n\n\n-3.939\n\n\n5.537\n\n\nDays\n\n\n370\n\n\n4.823\n\n\n2.417\n\n\n0.086\n\n\n9.561\n\n\nDays\n\n\n371\n\n\n-1.036\n\n\n2.417\n\n\n-5.773\n\n\n3.702\n\n\nDays\n\n\n372\n\n\n1.215\n\n\n2.417\n\n\n-3.523\n\n\n5.952\n\n\nTable 10: Random effect standard errors\n\n\neffect\n\n\ngroup\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nIntercept\n\n\n308\n\n\n1.035\n\n\n12.347\n\n\n-23.165\n\n\n25.235\n\n\nIntercept\n\n\n309\n\n\n-39.030\n\n\n12.347\n\n\n-63.230\n\n\n-14.831\n\n\nIntercept\n\n\n310\n\n\n-38.225\n\n\n12.347\n\n\n-62.425\n\n\n-14.026\n\n\nIntercept\n\n\n330\n\n\n23.639\n\n\n12.347\n\n\n-0.561\n\n\n47.838\n\n\nIntercept\n\n\n331\n\n\n22.412\n\n\n12.347\n\n\n-1.787\n\n\n46.612\n\n\nIntercept\n\n\n332\n\n\n8.914\n\n\n12.347\n\n\n-15.285\n\n\n33.114\n\n\nIntercept\n\n\n333\n\n\n16.892\n\n\n12.347\n\n\n-7.308\n\n\n41.091\n\n\nIntercept\n\n\n334\n\n\n-7.233\n\n\n12.347\n\n\n-31.432\n\n\n16.967\n\n\nIntercept\n\n\n335\n\n\n0.390\n\n\n12.347\n\n\n-23.810\n\n\n24.589\n\n\nIntercept\n\n\n337\n\n\n34.017\n\n\n12.347\n\n\n9.818\n\n\n58.217\n\n\nIntercept\n\n\n349\n\n\n-25.220\n\n\n12.347\n\n\n-49.420\n\n\n-1.021\n\n\nIntercept\n\n\n350\n\n\n-13.650\n\n\n12.347\n\n\n-37.850\n\n\n10.549\n\n\nIntercept\n\n\n351\n\n\n4.664\n\n\n12.347\n\n\n-19.536\n\n\n28.863\n\n\nIntercept\n\n\n352\n\n\n19.978\n\n\n12.347\n\n\n-4.222\n\n\n44.178\n\n\nIntercept\n\n\n369\n\n\n3.150\n\n\n12.347\n\n\n-21.050\n\n\n27.350\n\n\nIntercept\n\n\n370\n\n\n-25.832\n\n\n12.347\n\n\n-50.031\n\n\n-1.632\n\n\nIntercept\n\n\n371\n\n\n0.719\n\n\n12.347\n\n\n-23.481\n\n\n24.918\n\n\nIntercept\n\n\n372\n\n\n12.066\n\n\n12.347\n\n\n-12.133\n\n\n36.266\n\n\nDays\n\n\n308\n\n\n9.262\n\n\n2.417\n\n\n4.525\n\n\n14.000\n\n\nDays\n\n\n309\n\n\n-8.800\n\n\n2.417\n\n\n-13.538\n\n\n-4.062\n\n\nDays\n\n\n310\n\n\n-5.531\n\n\n2.417\n\n\n-10.269\n\n\n-0.793\n\n\nDays\n\n\n330\n\n\n-4.860\n\n\n2.417\n\n\n-9.598\n\n\n-0.123\n\n\nDays\n\n\n331\n\n\n-3.225\n\n\n2.417\n\n\n-7.962\n\n\n1.513\n\n\nDays\n\n\n332\n\n\n-0.350\n\n\n2.417\n\n\n-5.087\n\n\n4.388\n\n\nDays\n\n\n333\n\n\n-0.325\n\n\n2.417\n\n\n-5.063\n\n\n4.412\n\n\nDays\n\n\n334\n\n\n1.043\n\n\n2.417\n\n\n-3.694\n\n\n5.781\n\n\nDays\n\n\n335\n\n\n-10.978\n\n\n2.417\n\n\n-15.715\n\n\n-6.240\n\n\nDays\n\n\n337\n\n\n8.693\n\n\n2.417\n\n\n3.956\n\n\n13.431\n\n\nDays\n\n\n349\n\n\n1.127\n\n\n2.417\n\n\n-3.611\n\n\n5.864\n\n\nDays\n\n\n350\n\n\n6.666\n\n\n2.417\n\n\n1.929\n\n\n11.404\n\n\nDays\n\n\n351\n\n\n-3.042\n\n\n2.417\n\n\n-7.779\n\n\n1.696\n\n\nDays\n\n\n352\n\n\n3.496\n\n\n2.417\n\n\n-1.241\n\n\n8.234\n\n\nDays\n\n\n369\n\n\n0.799\n\n\n2.417\n\n\n-3.939\n\n\n5.537\n\n\nDays\n\n\n370\n\n\n4.823\n\n\n2.417\n\n\n0.086\n\n\n9.561\n\n\nDays\n\n\n371\n\n\n-1.036\n\n\n2.417\n\n\n-5.773\n\n\n3.702\n\n\nDays\n\n\n372\n\n\n1.215\n\n\n2.417\n\n\n-3.523\n\n\n5.952\n\n\nWe can see that the mgcv estimates for standard errors of the random effects are close to the average standard errors from the fully Bayesian approach. For the Bayesian result we have 12.347 and 2.417 for Intercept and Days coefficient respectively, while for mgcv this is 13.27913 and 2.67273.\nBack to the initial problem\nSo we’ve established that both default gam and bam functions are providing what we want. However, the reason we’re here is to use demonstrate the speed gain we’ll get with big data using mgcv for mixed models. So let’s return to the binary outcome example that took over a minute for lme4 to run.\n\n\nsystem.time({\n  bam_big <- bam(\n    y_bin ~ x + b + s(g, bs='re'), \n    data = d,\n    nthreads = 8,\n    family = binomial\n  )\n})\n\n\n\n\n\n\n\n\n    user   system  elapsed \n8164.817  120.570 1298.584 \n\nThat didn’t actually improve our situation, and was much worse in time- more than 20 minutes! Remember though, that the mgcv approach has to estimate all those random effect coefficients, while lme4 is able to take advantage of design for mixed models among other things.\nHowever, even here we haven’t used all our secret weapons. Another option with bam works on a modified data set using binned/rounded values for continuous covariates, and working with only the minimum data necessary to estimate the coefficients(S. N. Wood, Goude, and Shaw 2015a). With large enough data, as is the case here, the estimated parameters might not be different at all, while the efficiency gains could be tremendous. Let’s add discrete = TRUE and see what happens.\n\nWe just need the distinct set of values after rounding.\n\n\nsystem.time({\n  bam_big_d <- bam(\n    y_bin ~ x + b + s(g, bs='re'), \n    data = d,\n    nthreads = 8,\n    family = binomial, \n    discrete = TRUE\n  )\n})\n\n\n\n\n\n   user  system elapsed \n 43.542   2.649  12.387 \n    \n\nWow! That was almost as fast as lme4 with the linear mixed model! Let’s check the results. We’ll start with the fixed effects. I add some digits to the result so we can see the very slight differences.\n\n\n\n\n\nTable 11: Fixed Effects\n\n\nModel\n\n\nterm\n\n\nvalue\n\n\nse\n\n\nlower_2.5\n\n\nupper_97.5\n\n\nTrue\n\n\n(Intercept)\n\n\n0.00000000\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nTrue\n\n\nx\n\n\n0.50000000\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nTrue\n\n\nb\n\n\n0.25000000\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nbam_big\n\n\nIntercept\n\n\n0.03730496\n\n\n0.02257804\n\n\n-0.00694725\n\n\n0.08155716\n\n\nbam_big\n\n\nx\n\n\n0.50087021\n\n\n0.00223195\n\n\n0.49649565\n\n\n0.50524476\n\n\nbam_big\n\n\nb\n\n\n0.19083417\n\n\n0.03209232\n\n\n0.12793430\n\n\n0.25373405\n\n\nbam_big_d\n\n\nIntercept\n\n\n0.03730460\n\n\n0.02257842\n\n\n-0.00694835\n\n\n0.08155755\n\n\nbam_big_d\n\n\nx\n\n\n0.50087441\n\n\n0.00223195\n\n\n0.49649987\n\n\n0.50524895\n\n\nbam_big_d\n\n\nb\n\n\n0.19083450\n\n\n0.03209286\n\n\n0.12793357\n\n\n0.25373543\n\n\nmixed_big_glmm\n\n\nIntercept\n\n\n0.03734366\n\n\n0.02254863\n\n\n-0.00685085\n\n\n0.08153816\n\n\nmixed_big_glmm\n\n\nx\n\n\n0.50136904\n\n\n0.00223341\n\n\n0.49699163\n\n\n0.50574645\n\n\nmixed_big_glmm\n\n\nb\n\n\n0.19105107\n\n\n0.03204661\n\n\n0.12824086\n\n\n0.25386127\n\n\nNow for the variance components.\n\n\nTable 12: Variance Components\n\n\nModel\n\n\nsd\n\n\nvariance\n\n\ntrue\n\n\n0.500\n\n\n0.250\n\n\nbam_big\n\n\n0.503\n\n\n0.253\n\n\nbam_big_d\n\n\n0.503\n\n\n0.253\n\n\nlme4\n\n\n0.503\n\n\n0.253\n\n\nAnd finally, let’s look at the estimated random effects for the first 5 clusters.\n\nJust a note, unless you have very many observations per cluster, you should not expect to get very close to the true values of the random effects except on average, which should serve as a caution for any 2-step approach one might undertake using the estimates. The rank correlations of the estimates vs. the true values in this example are 1.0.\n\n\nTable 13: Estimated Random Effects\n\n\ncluster\n\n\ntrue\n\n\nbam_big\n\n\nbam_big_d\n\n\nlme4\n\n\n1\n\n\n0.0912404\n\n\n-0.036\n\n\n-0.036\n\n\n-0.036\n\n\n2\n\n\n0.1310768\n\n\n0.148\n\n\n0.148\n\n\n0.148\n\n\n3\n\n\n-0.0562572\n\n\n-0.142\n\n\n-0.143\n\n\n-0.143\n\n\n4\n\n\n0.6194238\n\n\n0.556\n\n\n0.556\n\n\n0.556\n\n\n5\n\n\n-0.5022289\n\n\n-0.415\n\n\n-0.415\n\n\n-0.415\n\n\n\n\n\n\n\n\nSo we’re getting what we should in general.\nWhen to use bam\nThe following are some guidelines for when bam might be preferable compared to other mixed modeling tools. To help with this, I’ve conducted my own examinations on very large data sets of up to one million observations, and included timing results from other relevant studies, which will be presented here.\nLinear Mixed Models\nAs we’ll see, in general you’ll probably need very large data for bam to be preferred to lme4 for linear mixed models unless:\nYou have complicated structure that begins to bog down lme4\nYou want to add smooth terms1\nYou have memory issues\nYou have a computing setup that can take advantage of bam\nThe following shows some timings for lme4, glmmTMB, and mgcv for the linear mixed model case under a variety of settings with large data. In some sense, this is not exactly a fair comparison as mgcv parallelizes computations while lme4 and glmmTMB do not. However, this is also exactly the point of the demonstration - those who can, do. In general though, the lme4 advantage holds until around 500k observations. We can see that the main issue for bam is not so much the sample size, but the number of parameters to estimate.\n\nFor lme4, I set at least one argument to possibly improve speed/performance for both lme and glmm models, though this only shaved a few seconds for the largest sample size settings for the linear mixed model. For mgcv I only used 12 cores for parallelization so as to be similar to what is common on modern machines (8-12), but anyone with access to a better machine or cluster computing environment would see even more speed gain by utilizing additional cores, so I also looked at 16 cores. For glmmTMB, settings were left at defaults, as I’ve not come across any specific speed recommendations. See Brooks et al.(Brooks et al. 2017) (Brooks et al. (2017)) for more speed comparisons of glmmTMB, mgcv, lme4, and others, as well as the glmmTMB vignette.\n\n\n\n\n\n\nGeneralized Linear Mixed Models\nFor the generalized setting with binary, count, and other outcomes:\nlme4, at least at the time of this writing, will almost certainly start giving convergence warnings even in well-behaved data settings, and as such, will require tweaking to mitigate.\nglmmTMB is probably viable up to 100k and one or two random effects, but may generally be a slower option.\nUse mgcv for same reasons as with linear mixed models, but here it potentially becomes an advantage with as few as 100k.\n\n\n\n\nI have also done some timings on a local machine with as many as 10000 levels for one of the random effects, 1000 for the other, and 5 million observations. Depending on the computational setup, this could take 30 minutes for a linear mixed model and 24 cores, to 2-3 hours for a logistic mixed model using 12 cores.\nOther options\nWhen looking into mixed models for big data, you typically won’t find much in the way of options. I’ve seen some packages or offerings for some machine learning approaches like random forests2, but this doesn’t address the issue of large data. A Spark module provided by LinkedIn is available, photonML, but it’s not clear how easy it is to implement. Julia has recently made multithreading a viable option for any function. This is notable since Doug Bates, one of the lme4 authors, develops the MixedModels module for Julia. Should multithreading functionality be added, it could be a very powerful tool3.\nAmong proprietary options, SAS and Stata are the more commonly used tools. SAS PROC HPMIXED essentially uses the lme4 approach, but can be faster for well-behaved data. Stata, while commonly used for mixed models, is generally slower than the lme4 even for standard settings, and is likely prohibitively slow for settings above4.\n\nSAS uses disk rather than RAM for processing, so may be preferred for low RAM devices.\nHere is a summary of other timings of various tools for mixed models.\nMcCoach et al. 2018\nThese are the results from McCoach et al.(McCoach et al. 2018) with a standard linear mixed model. Sample size fixed at 10000, with a single grouping factor with only 50 levels. Models included five covariates each with a random slope. In the first five cases, a true variance parameter was set to zero, a situation lme4 handles well5. SAS is very speedy for such settings if data is well-behaved.\n\n\n\nBrooks et al. 2018 timing as a function of sample size\nBrooks et al. uses the Salamander data from the glmmTMB package. It has a single grouping factor for the random effect with 23 levels. Starting sample size is 644, which is then replicated to produce larger data. This is a negative binomial count model. In this particular setting glmmTMB has an advantage.\n\n\n\nBrooks et al. 2018 timing as a function of number of levels\nThis data is simulated based on models from the previous, and adds increasing numbers of (balanced) levels to the random effect. This shows a similar effect of the number of levels on mgcv as the simulation presented in this post, though they are not using the functionality of bam.\n\n\n\nglmmTMB timings\nAs previously noted, depending on the data, and whether the target is assumed gaussian or not, glmmTMB might be preferable. For the following, in the first case a small data set was replicated to create larger data, and in the second, a larger data set was sub-sampled6. The advantage is to glmmTMB in the first case, and lme4 in the second.\n\n\n\nLimitations\nThere are limitations to the use of the mgcv approach.\nThe number of parameters to estimate increases with the number of random effect levels, which may void any gains until very large data with complex models\nNo estimation of random effect correlations, e.g. between slopes and intercepts\nAll in all, these are pretty minor, and the last one likely will be remedied in a future release.\nSummary\nThe take home point here is that you now have viable tools to run mixed models on even very large data with millions of observations. This doesn’t mean you won’t have to wait for it, especially for more complicated models, but you may even be able to run some of these on standard machines in reasonable times. The alternative estimation procedures may even make otherwise problematic models more feasible in smaller data settings. Good luck!\nSupplemental\nSimulation Settings\nI will set up a repo with the simulation code at some point and link it here. But the settings for the timings can be summarized as follows.\nLinear mixed model\nThe following are for the linear mixed model. N is the sample size, balanced refers to whether a random 75% sample was taken with proportion equivalent to the group index (first grouping variable for both 1 and 2 random effect settings), and tau_2 is zero if there is only one random effect, or refers to the standard deviation of the second random effect. Each of these settings was run 5 times, and the previous visualizations display the average timing of those.\n\n\nN\n\n\nbalanced\n\n\ntau_2\n\n\n1e+04\n\n\n0.75\n\n\n0.0\n\n\n1e+04\n\n\n0.75\n\n\n0.5\n\n\n1e+04\n\n\n1.00\n\n\n0.0\n\n\n1e+04\n\n\n1.00\n\n\n0.5\n\n\n5e+04\n\n\n0.75\n\n\n0.0\n\n\n5e+04\n\n\n0.75\n\n\n0.5\n\n\n5e+04\n\n\n1.00\n\n\n0.0\n\n\n5e+04\n\n\n1.00\n\n\n0.5\n\n\n1e+05\n\n\n0.75\n\n\n0.0\n\n\n1e+05\n\n\n0.75\n\n\n0.5\n\n\n1e+05\n\n\n1.00\n\n\n0.0\n\n\n1e+05\n\n\n1.00\n\n\n0.5\n\n\n5e+05\n\n\n0.75\n\n\n0.0\n\n\n5e+05\n\n\n0.75\n\n\n0.5\n\n\n5e+05\n\n\n1.00\n\n\n0.0\n\n\n5e+05\n\n\n1.00\n\n\n0.5\n\n\n1e+06\n\n\n0.75\n\n\n0.0\n\n\n1e+06\n\n\n0.75\n\n\n0.5\n\n\n1e+06\n\n\n1.00\n\n\n0.0\n\n\n1e+06\n\n\n1.00\n\n\n0.5\n\n\nHeld constant are:\nThe number of covariates: 20, all drawn from a standardized normal distribution\nFixed effect coefficients: drawn from a random uniform (-1, 1)\nResidual standard deviation: 1\nThe number of levels in each factor: 1000 for the first, 100 for the second\nThe standard deviations of the random effects: .5 for both\nGeneralized linear mixed model\nFor the generalized linear mixed model, the settings are the same but we also add a case where the outcome is rare or not in this binary setting (~ 10% prevalence or less).\n\n\nN\n\n\nbalanced\n\n\ntau_2\n\n\nrare\n\n\n1e+04\n\n\n0.75\n\n\n0.0\n\n\nFALSE\n\n\n1e+04\n\n\n0.75\n\n\n0.0\n\n\nTRUE\n\n\n1e+04\n\n\n0.75\n\n\n0.5\n\n\nFALSE\n\n\n1e+04\n\n\n0.75\n\n\n0.5\n\n\nTRUE\n\n\n1e+04\n\n\n1.00\n\n\n0.0\n\n\nFALSE\n\n\n1e+04\n\n\n1.00\n\n\n0.0\n\n\nTRUE\n\n\n1e+04\n\n\n1.00\n\n\n0.5\n\n\nFALSE\n\n\n1e+04\n\n\n1.00\n\n\n0.5\n\n\nTRUE\n\n\n5e+04\n\n\n0.75\n\n\n0.0\n\n\nFALSE\n\n\n5e+04\n\n\n0.75\n\n\n0.0\n\n\nTRUE\n\n\n5e+04\n\n\n0.75\n\n\n0.5\n\n\nFALSE\n\n\n5e+04\n\n\n0.75\n\n\n0.5\n\n\nTRUE\n\n\n5e+04\n\n\n1.00\n\n\n0.0\n\n\nFALSE\n\n\n5e+04\n\n\n1.00\n\n\n0.0\n\n\nTRUE\n\n\n5e+04\n\n\n1.00\n\n\n0.5\n\n\nFALSE\n\n\n5e+04\n\n\n1.00\n\n\n0.5\n\n\nTRUE\n\n\n1e+05\n\n\n0.75\n\n\n0.0\n\n\nFALSE\n\n\n1e+05\n\n\n0.75\n\n\n0.0\n\n\nTRUE\n\n\n1e+05\n\n\n0.75\n\n\n0.5\n\n\nFALSE\n\n\n1e+05\n\n\n0.75\n\n\n0.5\n\n\nTRUE\n\n\n1e+05\n\n\n1.00\n\n\n0.0\n\n\nFALSE\n\n\n1e+05\n\n\n1.00\n\n\n0.0\n\n\nTRUE\n\n\n1e+05\n\n\n1.00\n\n\n0.5\n\n\nFALSE\n\n\n1e+05\n\n\n1.00\n\n\n0.5\n\n\nTRUE\n\n\n5e+05\n\n\n0.75\n\n\n0.0\n\n\nFALSE\n\n\n5e+05\n\n\n0.75\n\n\n0.0\n\n\nTRUE\n\n\n5e+05\n\n\n0.75\n\n\n0.5\n\n\nFALSE\n\n\n5e+05\n\n\n0.75\n\n\n0.5\n\n\nTRUE\n\n\n5e+05\n\n\n1.00\n\n\n0.0\n\n\nFALSE\n\n\n5e+05\n\n\n1.00\n\n\n0.0\n\n\nTRUE\n\n\n5e+05\n\n\n1.00\n\n\n0.5\n\n\nFALSE\n\n\n5e+05\n\n\n1.00\n\n\n0.5\n\n\nTRUE\n\n\n1e+06\n\n\n0.75\n\n\n0.0\n\n\nFALSE\n\n\n1e+06\n\n\n0.75\n\n\n0.0\n\n\nTRUE\n\n\n1e+06\n\n\n0.75\n\n\n0.5\n\n\nFALSE\n\n\n1e+06\n\n\n0.75\n\n\n0.5\n\n\nTRUE\n\n\n1e+06\n\n\n1.00\n\n\n0.0\n\n\nFALSE\n\n\n1e+06\n\n\n1.00\n\n\n0.0\n\n\nTRUE\n\n\n1e+06\n\n\n1.00\n\n\n0.5\n\n\nFALSE\n\n\n1e+06\n\n\n1.00\n\n\n0.5\n\n\nTRUE\n\n\nFunction arguments\nFor g/lmer I set check.derivatives = FALSE and for the GLMM I additionally set nAGQ = 0, as this is precisely the setting one would do so7. I did not mess with the optimizers but it is possible to get a speed gain there in some settings. See performance tips here and demonstrated here.\nFor bam I set the following.\ngc.level = 0\nuse.chol = TRUE\nnthreads = 12/16\nchunk.size = 1000\nsamfrac = .1\nglmmTMB was left at defaults as I’m not aware of a specific approach for speed gain.\n\n\nBrooks, Mollie E., Kasper Kristensen, Koen J. van Benthem, Arni Magnusson, Casper W. Berg, Anders Nielsen, Hans J. Skaug, Martin Mächler, and Benjamin M. Bolker. 2017. “glmmTMB Balances Speed and Flexibility Among Packages for Zero-Inflated Generalized Linear Mixed Modeling.” The R Journal 9 (2): 378–400. https://journal.r-project.org/archive/2017/RJ-2017-066.\n\n\nLi, Zheyuan, and Simon N. Wood. 2019. “Faster Model Matrix Crossproducts for Large Generalized Linear Models with Discretized Covariates.” Statistics and Computing, March. https://doi.org/10.1007/s11222-019-09864-2.\n\n\nMcCoach, D Betsy, Graham G Rifenbark, Sarah D Newton, Xiaoran Li, Janice Kooken, Dani Yomtov, Anthony J Gambino, and Aarti Bellara. 2018. “Does the Package Matter? A Comparison of Five Common Multilevel Modeling Software Packages.” Journal of Educational and Behavioral Statistics 43 (5): 594–627. https://journals.sagepub.com/doi/10.3102/1076998618776348.\n\n\nWood, Simon. 2012. “Mgcv: Mixed GAM Computation Vehicle with GCV/AIC/REML Smoothness Estimation,” October. https://researchportal.bath.ac.uk/en/publications/mgcv-mixed-gam-computation-vehicle-with-gcvaicreml-smoothness-est.\n\n\nWood, Simon N. 2017. Generalized Additive Models : An Introduction with r, Second Edition. Chapman; Hall/CRC. https://doi.org/10.1201/9781315370279.\n\n\nWood, Simon N., Yannig Goude, and Simon Shaw. 2015a. “Generalized Additive Models for Large Data Sets.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 64 (1): 139–55. https://doi.org/10.1111/rssc.12068.\n\n\n———. 2015b. “Generalized Additive Models for Large Data Sets.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 64 (1): 139–55. https://doi.org/10.1111/rssc.12068.\n\n\nWood, Simon N., Zheyuan Li, Gavin Shaddick, and Nicole H. Augustin. 2017. “Generalized Additive Models for Gigadata: Modeling the u.k. Black Smoke Network Daily Data.” Journal of the American Statistical Association 112 (519): 1199–1210. https://doi.org/10.1080/01621459.2016.1195744.\n\n\nYou could use construct the smooth with mgcv and add it to the model matrix for lme4.↩︎\nSee REEMtree, mixRF for example.↩︎\nWhile I haven’t seen it done, so it may have to serve as a later post, it should be possible to use deep learning tools like Keras or fastai by regularizing only weights associated with the random effects. If one takes an actual deep learning approach, then one can estimate functions of the ‘fixed’ covariates (like the smooth terms in typical GAM) and possibly get at correlations of the clusters themselves (a la spatial random effects).\n\n↩︎\nSee McCoach reference (McCoach et al. 2018). They also look at HLM and Mplus. However, I haven’t in years consulted with anyone across dozens of disciplines that was using HLM for mixed models. With Mplus, the verbosity of the syntax, plus additional data processing required, plus huge lack of post-processing of the model would negate any speed gain one might get from simply running the model. Couple this with the fact that campus-wide licenses are rare for either, neither could be recommended for mixed models. Note also, that one setting of lmer probably would have negated almost all their reported convergence issues.↩︎\nSee McCoach reference (McCoach et al. 2018). They also look at HLM and Mplus. However, I haven’t in years consulted with anyone across dozens of disciplines that was using HLM for mixed models. With Mplus, the verbosity of the syntax, plus additional data processing required, plus huge lack of post-processing of the model would negate any speed gain one might get from simply running the model. Couple this with the fact that campus-wide licenses are rare for either, neither could be recommended for mixed models. Note also, that one setting of lmer probably would have negated almost all their reported convergence issues.↩︎\n“In general, we expect glmmTMB‘s advantages over lme4 to be (1) greater flexibility (zero-inflation etc.); (2) greater speed for GLMMs, especially those with large number of ’top-level’ parameters (fixed effects plus random-effects variance-covariance parameters). In contrast, lme4 should be faster for LMMs.”↩︎\nSee this R user group thread for a discussion, this stackoverflow exchange involving one of the lme4 contributors, and Bates Julia notebook for more detail.↩︎\n",
    "preview": "posts/2019-10-20-big-mixed-models/../../img/bam/gam_timings.svg",
    "last_modified": "2021-04-13T14:19:58-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-20-fractional-regression/",
    "title": "Fractional Regression",
    "description": "A quick primer regarding data between zero and one, including zero and one",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2019-08-20",
    "categories": [
      "regression",
      "GLM",
      "mixed models",
      "fractional regression"
    ],
    "contents": "\nTable of Contents\nIntroduction\nStata example\nR GLM\nRobust standard errors\nQuasibinomial\nMixed model with per-observation random effect\nSummarized results\nConclusion\nReferences\nIntroduction\nIt is sometimes the case that you might have data that falls primarily between zero and one. For example, these may be proportions, grades from 0-100 that can be transformed as such, reported percentile values, and similar. If you had the raw counts where you also knew the denominator or total value that created the proportion, you would be able to just use standard logistic regression with the binomial distribution. Similarly, if you had a binary outcome (i.e. just zeros and ones), this is just a special case, so the same model would be applicable. Alternatively, if all the target variable values lie between zero and one, beta regression is a natural choice for which to model such data. However, if the variable you wish to model has values between zero and one, and additionally, you also have zeros or ones, what should you do?\nSome suggest adding a ‘fudge factor’ to the zeros or ones to put all values on the (0, 1) interval, so that beta regression could still be employed. Others might implement zero/one-inflated beta regression if a larger percentage of the observations are at the boundaries. However, as we will see, you already have more standard tools that are appropriate for this modeling situation, and this post will demonstrate some of them.\nRelated models\nBinomial logistic for binary and count/proportional data, i.e. \\(x\\) successes out of \\(n\\) trials (can use standard glm tools)\nBeta regression for (0, 1), i.e. only values between 0 and 1 (see betareg, DirichletReg, mgcv, brms packages)\nZero/One-inflated binomial or beta regression for cases including a relatively high amount of zeros and ones (brms, VGAM, gamlss)\nStata example\nIt might seem strange to start with an example using Stata1, but if you look this sort of thing up, you’ll almost certainly come across the Stata demonstration using the fracreg command. For comparison we’ll use the data in the corresponding documentation. The data regards the expected participation rate in 401(k) plans for a cross-section of firms2. They define participation rate (prate) as the fraction of eligible employees in a firm that participate in a 401(k) plan. This is modeled by the matching rate of employee 401(k) contributions (mrate), the (natural) log of the total number of employees (ltotemp), the age of the plan (age), and whether the 401(k) plan is the only retirement plan offered by the employer (sole). Here we do not use quadratic effects for ltotemp and age as in the Stata documentation, though we do use an additive modeling approach later that could be implemented for the same purpose instead3.\n\n\n\nThe following shows the distribution of the target variable. There are no zeroes in the participation rate, however the amount of ones is 33.2%.\n\n\n\n\nThe following specifies a fractional regression with logit link. Probit and heteroscedastic probit are also available.\n\n\nuse http://www.stata-press.com/data/r14/401k\n\nfracreg logit prate mrate c.ltotemp c.age i.sole\n\n\n. use http://www.stata-press.com/data/r14/4. \n. fracreg logit prate mrate c.ltotemp c.age i.sole\n\nIteration 0:   log pseudolikelihood = -1985.1469  \nIteration 1:   log pseudolikelihood = -1689.2659  \nIteration 2:   log pseudolikelihood = -1681.1055  \nIteration 3:   log pseudolikelihood = -1681.0263  \nIteration 4:   log pseudolikelihood = -1681.0263  \n\nFractional logistic regression                  Number of obs     =      4,075\n                                                Wald chi2(4)      =     685.26\n                                                Prob > chi2       =     0.0000\nLog pseudolikelihood = -1681.0263               Pseudo R2         =     0.0596\n\n------------------------------------------------------------------------------\n             |               Robust\n       prate |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n       mrate |   1.157832   .0749231    15.45   0.000     1.010986    1.304679\n     ltotemp |  -.2072429   .0141468   -14.65   0.000      -.23497   -.1795157\n         age |   .0345786   .0027604    12.53   0.000     .0291684    .0399888\n             |\n        sole |\n  only plan  |   .1655762   .0506445     3.27   0.001     .0663147    .2648377\n       _cons |   2.391717   .1061292    22.54   0.000     2.183707    2.599726\n------------------------------------------------------------------------------\n\n\n\n\nPerhaps not surprisingly, all of the covariates are statistically notable. With the logistic link, the coefficients can be exponentiated to provide odds ratios4. Stata’s is one of the few tools that is specifically advertised to model such outcomes, but as we’re about to see, you don’t need Stata’s command, or even a special package in R, once you know what’s going on.\nR GLM\nIt turns out that the underlying likelihood for fractional regression in Stata is the same as the standard binomial likelihood we would use for binary or count/proportional outcomes. In the following, \\(y\\) is our target variable, \\(X\\beta\\) is the linear predictor, and \\(g(.)\\) is the link function, for example, the logit.\n\\[\\mathcal{L} \\sim y(\\ln{g(X\\beta)}) + (1-y)(1-\\ln{g(X\\beta)})\\]\nAs such, we can just use glm like we would for count or binary outcomes. It will warn you that the outcome isn’t integer as it expects, but in this case we can just ignore the warning.\n\n\nd = haven::read_dta('data/401k.dta')\n\nmodel_glm = glm(\n  prate ~ mrate + ltotemp + age + sole,\n  data = d,\n  family = binomial\n)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n# summary(model_glm)\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n(Intercept)\n\n\n2.392\n\n\n0.227\n\n\n10.520\n\n\n0.000\n\n\nmrate\n\n\n1.158\n\n\n0.147\n\n\n7.887\n\n\n0.000\n\n\nltotemp\n\n\n-0.207\n\n\n0.029\n\n\n-7.143\n\n\n0.000\n\n\nage\n\n\n0.035\n\n\n0.006\n\n\n5.678\n\n\n0.000\n\n\nsole\n\n\n0.166\n\n\n0.104\n\n\n1.591\n\n\n0.112\n\n\n\n\n\nSo the model runs fine, and the coefficients are the same as the Stata example. The only difference regards the standard errors, but we can fix that.\nRobust standard errors\nThe difference in the standard errors is that, by default, Stata reports robust standard errors. We can use the sandwich package to get them in R. The lmtest package provides a nice summary table.\n\n\nlibrary(lmtest)\nlibrary(sandwich)\n\nse_glm_robust = coeftest(model_glm, vcov = vcovHC(model_glm, type=\"HC\"))\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n(Intercept)\n\n\n2.392\n\n\n0.106\n\n\n22.539\n\n\n0.000\n\n\nmrate\n\n\n1.158\n\n\n0.075\n\n\n15.455\n\n\n0.000\n\n\nltotemp\n\n\n-0.207\n\n\n0.014\n\n\n-14.651\n\n\n0.000\n\n\nage\n\n\n0.035\n\n\n0.003\n\n\n12.528\n\n\n0.000\n\n\nsole\n\n\n0.166\n\n\n0.051\n\n\n3.270\n\n\n0.001\n\n\nSo now we have the same result via a standard R generalized linear model and Stata. Likewise, you could just use the glm command in Stata with the vce(robust) option.\nQuasibinomial\nWe could also use the quasibinomial family. Quasi-likelihoods are similar to standard likelihood functions, but technically do not relate to any particular probability distribution5. Using this family would provide the same result as the previous glm, but without the warning.\nFrom the R help file for ?family: The quasibinomial and quasipoisson families differ from the binomial and poisson families only in that the dispersion parameter is not fixed at one, so they can model over-dispersion.\nAlso, as noted in the StackExchange link in the references, while by default the variance estimate is ‘robust’, possibly leading to standard errors that are similar, the basic result is not the same as using the robust standard errors.\n\n\nmodel_quasi = glm(\n  prate ~ mrate + ltotemp + age + sole,\n  data = d,\n  family = quasibinomial\n)\n\n# summary(model_quasi)\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n(Intercept)\n\n\n2.392\n\n\n0.105\n\n\n22.684\n\n\n0.000\n\n\nmrate\n\n\n1.158\n\n\n0.068\n\n\n17.006\n\n\n0.000\n\n\nltotemp\n\n\n-0.207\n\n\n0.013\n\n\n-15.402\n\n\n0.000\n\n\nage\n\n\n0.035\n\n\n0.003\n\n\n12.244\n\n\n0.000\n\n\nsole\n\n\n0.166\n\n\n0.048\n\n\n3.431\n\n\n0.001\n\n\nWe can get robust standard errors for the quasi-likelihood approach as well, but they were already pretty close.\n\n\n\nse_glm_robust_quasi = coeftest(model_quasi, vcov = vcovHC(model_quasi, type=\"HC\"))\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n(Intercept)\n\n\n2.392\n\n\n0.106\n\n\n22.539\n\n\n0.000\n\n\nmrate\n\n\n1.158\n\n\n0.075\n\n\n15.455\n\n\n0.000\n\n\nltotemp\n\n\n-0.207\n\n\n0.014\n\n\n-14.651\n\n\n0.000\n\n\nage\n\n\n0.035\n\n\n0.003\n\n\n12.528\n\n\n0.000\n\n\nsole\n\n\n0.166\n\n\n0.051\n\n\n3.270\n\n\n0.001\n\n\nMixed model with per-observation random effect\nIt turns out that we can also use a mixed model approach. For some distributions such as binomial and poisson, the variance is directly tied to the mean function, and so does not have to be estimated. In these scenarios, we can insert a per-observation random effect and estimate the associated variance. This extra source of variance can account for overdispersion, similar to what the scale parameter estimate does for the quasibinomial.\nI initially attempted to do so using the popular mixed model package lme4 and its glmer function, with an observation level random effect. While I’ve had success using this package with such models in the past, in this particular instance, all failed to converge with default optimization settings across multiple optimizers. As such, those results are not shown.\n\n\nd$id = 1:nrow(d)\n\n## model_glmm = lme4::glmer(\n##   prate ~ mrate + ltotemp + age + sole + (1 | id),\n##   data = d,\n##   family = binomial\n## )\n## \n## summary(model_glmm, cor=F)\n## \n## test_models = lme4::allFit(model_glmm)\n## \n## summary(test_models)\n\n\nWe have options though. The glmmTMB package was able to estimate the model.\n\n\n\nlibrary(glmmTMB)\n\nmodel_glmm = glmmTMB(\n  prate ~ mrate + ltotemp + age + sole + (1 | id),\n  data = d,\n  family = binomial,\n  REML = TRUE\n)\n\n# summary(model_glmm)\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n(Intercept)\n\n\n2.392\n\n\n0.227\n\n\n10.520\n\n\n0.000\n\n\nmrate\n\n\n1.158\n\n\n0.147\n\n\n7.887\n\n\n0.000\n\n\nltotemp\n\n\n-0.207\n\n\n0.029\n\n\n-7.143\n\n\n0.000\n\n\nage\n\n\n0.035\n\n\n0.006\n\n\n5.678\n\n\n0.000\n\n\nsole\n\n\n0.166\n\n\n0.104\n\n\n1.591\n\n\n0.112\n\n\nWe can maybe guess why glmer was struggling. The extra variance is estimated by glmmTMB to be basically zero.\n\n\n\nLately, I’ve been using mgcv to do most of my mixed models, so we can try a GAM instead. The following is equivalent to the glm-quasibinomial approach before.\n\n\nlibrary(mgcv)\n\nmodel_gam_std = gam(\n  prate ~ mrate + ltotemp + age + sole, \n  data = d, \n  family = quasibinomial\n)\n\n# summary(model_gam_std)\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n(Intercept)\n\n\n2.392\n\n\n0.104\n\n\n22.908\n\n\n0.000\n\n\nmrate\n\n\n1.158\n\n\n0.067\n\n\n17.174\n\n\n0.000\n\n\nltotemp\n\n\n-0.207\n\n\n0.013\n\n\n-15.554\n\n\n0.000\n\n\nage\n\n\n0.035\n\n\n0.003\n\n\n12.365\n\n\n0.000\n\n\nsole\n\n\n0.166\n\n\n0.048\n\n\n3.464\n\n\n0.001\n\n\n\n\n\nThe following adds the per observation random effect as with the mixed model. Unlike with lme4 or glmmTMB, you can technically use the quasi family here as well, but I will follow Bates’ thinking and avoid doing so6. I will also calculate the robust standard errors.\n\n\nmodel_gam_re = gam(\n  prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),\n  data = d,\n  family = binomial,\n  method = 'REML'\n)\n\n# summary(model_gam_re)\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n(Intercept)\n\n\n2.392\n\n\n0.227\n\n\n10.519\n\n\n0.000\n\n\nmrate\n\n\n1.158\n\n\n0.147\n\n\n7.887\n\n\n0.000\n\n\nltotemp\n\n\n-0.207\n\n\n0.029\n\n\n-7.143\n\n\n0.000\n\n\nage\n\n\n0.035\n\n\n0.006\n\n\n5.678\n\n\n0.000\n\n\nsole\n\n\n0.166\n\n\n0.104\n\n\n1.591\n\n\n0.112\n\n\n\n\n\n\n\n\nSummarized results\nThe following tables show the results of the models. The first table regards the estimated coefficients, the second the standard errors. There are no differences for the coefficients. For standard errors, some approaches are definitely working better than others.\n\n\nTable 1: Coefficients\n\n\nbaseline_glm\n\n\nstata\n\n\nglm_robust\n\n\nglm_robust_quasi\n\n\ngam_re_robust\n\n\ngam_std\n\n\nglmm_std\n\n\n2.3917\n\n\n-2.3917\n\n\n2.3917\n\n\n2.3917\n\n\n2.3917\n\n\n2.3917\n\n\n2.3917\n\n\n1.1578\n\n\n1.1578\n\n\n1.1578\n\n\n1.1578\n\n\n1.1578\n\n\n1.1578\n\n\n1.1578\n\n\n-0.2072\n\n\n-0.2072\n\n\n-0.2072\n\n\n-0.2072\n\n\n-0.2072\n\n\n-0.2072\n\n\n-0.2072\n\n\n0.0346\n\n\n0.0346\n\n\n0.0346\n\n\n0.0346\n\n\n0.0346\n\n\n0.0346\n\n\n0.0346\n\n\n0.1656\n\n\n0.1656\n\n\n0.1656\n\n\n0.1656\n\n\n0.1656\n\n\n0.1656\n\n\n0.1656\n\n\nTable 1: Standard Errors\n\n\nbaseline_glm\n\n\nstata\n\n\nglm_robust\n\n\nglm_robust_quasi\n\n\ngam_re_robust\n\n\ngam_std\n\n\nglmm_std\n\n\n0.2274\n\n\n0.1061\n\n\n0.1061\n\n\n0.1061\n\n\n0.1061\n\n\n0.1044\n\n\n0.2274\n\n\n0.1468\n\n\n0.0749\n\n\n0.0749\n\n\n0.0749\n\n\n0.0749\n\n\n0.0674\n\n\n0.1468\n\n\n0.0290\n\n\n0.0141\n\n\n0.0141\n\n\n0.0141\n\n\n0.0141\n\n\n0.0133\n\n\n0.0290\n\n\n0.0061\n\n\n0.0028\n\n\n0.0028\n\n\n0.0028\n\n\n0.0028\n\n\n0.0028\n\n\n0.0061\n\n\n0.1041\n\n\n0.0506\n\n\n0.0506\n\n\n0.0506\n\n\n0.0506\n\n\n0.0478\n\n\n0.1041\n\n\nIn addition, we can see what the frm package, which is specifically for fractional regression, would produce. Unless you really need this tool for the more complicated scenarios it has some functionality for (e.g. panel data), this isn’t as user friendly an approach as the others7. Like Stata’s specialized command, it is equivalent to using the quasibinomial family with robust standard errors.\n\n\nlibrary(frm)\n\nx_mat = model.matrix(prate ~ mrate + ltotemp + age + sole, d)\ny = d$prate\n\nmodel_frm = frm(\n  y,\n  x_mat,\n  linkfrac = 'logit',\n  intercept = FALSE  # included in model matrix\n)\n\n*** Fractional logit regression model ***\n\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.391717   0.106116  22.539    0.000 ***\nmrate        1.157832   0.074914  15.455    0.000 ***\nltotemp     -0.207243   0.014145 -14.651    0.000 ***\nage          0.034579   0.002760  12.528    0.000 ***\nsole         0.165576   0.050638   3.270    0.001 ***\n\nNote: robust standard errors\n\nNumber of observations: 4075 \nR-squared: 0.195 \n\nConclusion\nFractional data occurs from time to time. While Stata and R have specific functionality for such outcomes, more commonly used statistical tools can be used, which might provide additional means of model exploration. In the demo above, a standard glm with robust errors would be fine, and the simplest to pull off. With that as a basis, other complexities could be incorporated in more or less a standard fashion.\nReferences\nStata demo\nStata reference on fracreg command\nMcCullagh P. and Nelder, J. A. (1989) Generalized Linear Models. London: Chapman and Hall.\nPapke & Wooldridge. (1996) Econometric Methods For Fractional Response Variables With An Application To 401 (K) Plan Participation Rates. link\nRamalho, E., Ramalho, J. & Coelho, L. (2016) Exponential Regression of Fractional-Response Fixed-Effects Models with an Application to Firm Capital Structure. Journal of Econometric Methods. link\nRamalho, E., Ramalho, J. & Murteira, J. (2011) Alternative Estimating And Testing Empirical Strategies For Fractional Regression Models. link\nStackExchange has some more useful discussion, e.g. more on standard error differences between the approaches and other context link, link2\nGiven that I’m an avid R user. But if that was not apparent, then using Stata is possibly no surprise at all! 😄↩\nI added the original data, which has the raw values and many more observations, to my noiris package.↩\nI actually played with this a bit. The quadratic would be okay for age, but log firm size has a little more going on and mrate should also be allowed to wiggle. There would also be some interesting smooth interactions. In short, a generalized additive model is pretty much always a better option than trying to guess polynomials.↩\nIn Stata you can just add the option , or to the end of the model line.↩\nThis is in fact what fracreg in Stata is doing.↩\nFrom Doug Bates: In many application areas using ‘pseudo’ distribution families, such as quasibinomial and quasipoisson, is a popular and well-accepted technique for accommodating variability that is apparently larger than would be expected from a binomial or a Poisson distribution. This amounts to adding an extra parameter, like σ, the common scale parameter in a LMM, to the distribution of the response. It is possible to form an estimate of such a quantity during the IRLS algorithm but it is an artificial construct. There is no probability distribution with such a parameter. I find it difficult to define maximum likelihood estimates without a probability model. It is not clear how this ‘distribution which is not a distribution’ could be incorporated into a GLMM. This, of course, does not stop people from doing it but I don’t know what the estimates from such a model would mean.↩\nAs mentioned, the frm package may not be user friendly enough for many. It doesn’t use data frames, requires inputs that separate variables from the data matrix, lacks typical model methods (e.g. coef, predict), and I’m not sure it’s still being actively developed, among other things.↩\n",
    "preview": "posts/2019-08-20-fractional-regression/fracreg_files/figure-html5/prate-vis-1.svg",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-05-comparing-latent-variables/",
    "title": "Comparisons of the Unseen",
    "description": "Examining group differences across latent variables",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2019-08-05",
    "categories": [
      "SEM",
      "factor analysis"
    ],
    "contents": "\nTable of Contents\nIntroduction\nMultiple group analysis\nLatent variable intercepts\nObserved variable group differences\nStructural modelMultigroup as the structural model\nGroup difference as an indirect effect\n\nMore structural models\nSum/Factor score\nSummary of differences\nSupplemental: Measurement invariance\nIntroduction\nIn some cases we are interested in looking at group differences with regard to  latent variables. For example, social scientists are interested in race and sex differences on psychological measures, or educational scientists might want to create exams in different languages. We cannot measure many constructs directly, but can get reliable measures of them indirectly, e.g. by asking a series of questions, or otherwise observing multiple instances of activity thought to be related to some construct. There are a variety of ways to assess group differences across latent structure, such as anxiety or verbal ability, and this post provides a demo using lavaan.\nMy motivation for doing this is that it comes up from time to time in consulting, and I wanted a quick reminder for the syntax to refer back to. As a starting point though, you can find some demonstration on the lavaan website. For more on factor analysis, structural equation modeling, and more, see my document.\nMultiple group analysis\nA common way to assess group differences is via multiple group analysis, which amounts to doing separate structural equation models of some kind across the groups of interest. We will use a classic data set to demonstrate the approach. From the help file:\n\nThe Holzinger and Swineford (1939) dataset consists of mental ability test scores of seventh- and eighth-grade children from two different schools (Pasteur and Grant-White). In the original dataset, there are scores for 26 tests. However, a smaller subset with 9 variables is more widely used in the literature…\n\n\n\nlibrary(lavaan)\ndata(HolzingerSwineford1939)\n\nThe basic model is a factor analysis with three latent variables, with items for visual-spatial ability (x1-x3), verbal comprehension (x4-x6), and so-called ‘speed’ tests (x7-x9), e.g. for addition and counting, which might be thought of general cognitive processing.\nWith lavaan, we specify the model for three factor (or latent variables). After that, a simple group argument will allow the multigroup analysis, providing the factor analysis for both school groups.\n\n\nlibrary(tidyverse)\nlibrary(lavaan)\n\nhs_model_baseline <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n'\n\nfit_baseline <- cfa(\n  hs_model_baseline, \n  data = HolzingerSwineford1939, \n  group = \"school\"\n)\n\nsummary(fit_baseline)  \n\nlavaan 0.6-7 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of free parameters                         60\n                                                      \n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               115.851\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     64.309\n    Grant-White                                 51.542\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.394    0.122    3.220    0.001\n    x3                0.570    0.140    4.076    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                1.183    0.102   11.613    0.000\n    x6                0.875    0.077   11.421    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.125    0.277    4.057    0.000\n    x9                0.922    0.225    4.104    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal            0.479    0.106    4.531    0.000\n    speed             0.185    0.077    2.397    0.017\n  verbal ~~                                           \n    speed             0.182    0.069    2.628    0.009\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                4.941    0.095   52.249    0.000\n   .x2                5.984    0.098   60.949    0.000\n   .x3                2.487    0.093   26.778    0.000\n   .x4                2.823    0.092   30.689    0.000\n   .x5                3.995    0.105   38.183    0.000\n   .x6                1.922    0.079   24.321    0.000\n   .x7                4.432    0.087   51.181    0.000\n   .x8                5.563    0.078   71.214    0.000\n   .x9                5.418    0.079   68.440    0.000\n    visual            0.000                           \n    verbal            0.000                           \n    speed             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.298    0.232    1.286    0.198\n   .x2                1.334    0.158    8.464    0.000\n   .x3                0.989    0.136    7.271    0.000\n   .x4                0.425    0.069    6.138    0.000\n   .x5                0.456    0.086    5.292    0.000\n   .x6                0.290    0.050    5.780    0.000\n   .x7                0.820    0.125    6.580    0.000\n   .x8                0.510    0.116    4.406    0.000\n   .x9                0.680    0.104    6.516    0.000\n    visual            1.097    0.276    3.967    0.000\n    verbal            0.894    0.150    5.963    0.000\n    speed             0.350    0.126    2.778    0.005\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.736    0.155    4.760    0.000\n    x3                0.925    0.166    5.583    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                0.990    0.087   11.418    0.000\n    x6                0.963    0.085   11.377    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.226    0.187    6.569    0.000\n    x9                1.058    0.165    6.429    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal            0.408    0.098    4.153    0.000\n    speed             0.276    0.076    3.639    0.000\n  verbal ~~                                           \n    speed             0.222    0.073    3.022    0.003\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                4.930    0.095   51.696    0.000\n   .x2                6.200    0.092   67.416    0.000\n   .x3                1.996    0.086   23.195    0.000\n   .x4                3.317    0.093   35.625    0.000\n   .x5                4.712    0.096   48.986    0.000\n   .x6                2.469    0.094   26.277    0.000\n   .x7                3.921    0.086   45.819    0.000\n   .x8                5.488    0.087   63.174    0.000\n   .x9                5.327    0.085   62.571    0.000\n    visual            0.000                           \n    verbal            0.000                           \n    speed             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.715    0.126    5.676    0.000\n   .x2                0.899    0.123    7.339    0.000\n   .x3                0.557    0.103    5.409    0.000\n   .x4                0.315    0.065    4.870    0.000\n   .x5                0.419    0.072    5.812    0.000\n   .x6                0.406    0.069    5.880    0.000\n   .x7                0.600    0.091    6.584    0.000\n   .x8                0.401    0.094    4.249    0.000\n   .x9                0.535    0.089    6.010    0.000\n    visual            0.604    0.160    3.762    0.000\n    verbal            0.942    0.152    6.177    0.000\n    speed             0.461    0.118    3.910    0.000\n\nSo we’re left with visual inspection to note whether there are general differences on latent variables among the groups. This is all well and good, but given that none of the parameters will be identical from one group to the next, perhaps we want a more principled approach. Say our question specifically concerns a mean difference between schools on the visual latent variable. How do we go about it?\nNote that at this point the intercepts for the latent variables are zero. They have to be for the model to be identified, much in the same way that at least one factor loading (the first by default) has to be fixed to one. We only have so much information to estimate the parameters in a latent variable setting. Now let’s see how we might go about changing things to get a better understanding of group differences on the latent variables.\nLatent variable intercepts\n\n\n\nTo get around this limitation, we could try and fix some parameters, thereby freeing the intercepts to be estimated. For example, if we fix the mean of one of the observed variables to be zero instead, we would be able to estimate the intercept for the latent variable. In the following we’ll do this for the visuo-spatial ability construct, which will be our point of focus for group differences going forward.\n\nRecall that for the standard latent linear model, the observed variable is the dependent variable . For example, given an observed variable \\(X\\), a latent variable \\(F\\) and loading \\(\\lambda\\): \\[X = b_0 + \\lambda F \\]\nIn the model we we also identify a new parameter, which will be the differences in these latent variable intercepts, simply called diff. I have omitted some output for brevity of space.\n\n\nhs_model_1 <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  \n  # intercepts: in order to have an identified model, you would have to fix the\n  # intercepts of observed to 0, 1 represents the intercept, 0* fixes it to be 0\n  x1 ~ 0*1   \n\n  # intercept for Pasteur and Grant-White schools\n  visual ~  c(int_p, int_gw)*1    \n   \n  # comparisons\n  diff := int_p - int_gw\n'\n\nfit_1 <- cfa(hs_model_1, \n             data = HolzingerSwineford1939, \n             group = \"school\",\n             meanstructure = T)\nsummary(fit_1, header=F, nd=2)\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2                 0.39     0.12     3.22     0.00\n    x3                 0.57     0.14     4.08     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5                 1.18     0.10    11.61     0.00\n    x6                 0.87     0.08    11.42     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8                 1.12     0.28     4.06     0.00\n    x9                 0.92     0.22     4.10     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal             0.48     0.11     4.53     0.00\n    speed              0.19     0.08     2.40     0.02\n  verbal ~~                                           \n    speed              0.18     0.07     2.63     0.01\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.00                           \n    visual  (int_)     4.94     0.09    52.25     0.00\n   .x2                 4.04     0.61     6.61     0.00\n   .x3                -0.33     0.70    -0.47     0.64\n   .x4                 2.82     0.09    30.69     0.00\n   .x5                 4.00     0.10    38.18     0.00\n   .x6                 1.92     0.08    24.32     0.00\n   .x7                 4.43     0.09    51.18     0.00\n   .x8                 5.56     0.08    71.21     0.00\n   .x9                 5.42     0.08    68.44     0.00\n    verbal             0.00                           \n    speed              0.00                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.30     0.23     1.29     0.20\n   .x2                 1.33     0.16     8.46     0.00\n   .x3                 0.99     0.14     7.27     0.00\n   .x4                 0.43     0.07     6.14     0.00\n   .x5                 0.46     0.09     5.29     0.00\n   .x6                 0.29     0.05     5.78     0.00\n   .x7                 0.82     0.12     6.58     0.00\n   .x8                 0.51     0.12     4.41     0.00\n   .x9                 0.68     0.10     6.52     0.00\n    visual             1.10     0.28     3.97     0.00\n    verbal             0.89     0.15     5.96     0.00\n    speed              0.35     0.13     2.78     0.01\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2                 0.74     0.15     4.76     0.00\n    x3                 0.92     0.17     5.58     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5                 0.99     0.09    11.42     0.00\n    x6                 0.96     0.08    11.38     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8                 1.23     0.19     6.57     0.00\n    x9                 1.06     0.16     6.43     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal             0.41     0.10     4.15     0.00\n    speed              0.28     0.08     3.64     0.00\n  verbal ~~                                           \n    speed              0.22     0.07     3.02     0.00\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.00                           \n    visual  (int_)     4.93     0.10    51.70     0.00\n   .x2                 2.57     0.77     3.35     0.00\n   .x3                -2.56     0.82    -3.12     0.00\n   .x4                 3.32     0.09    35.63     0.00\n   .x5                 4.71     0.10    48.99     0.00\n   .x6                 2.47     0.09    26.28     0.00\n   .x7                 3.92     0.09    45.82     0.00\n   .x8                 5.49     0.09    63.17     0.00\n   .x9                 5.33     0.09    62.57     0.00\n    verbal             0.00                           \n    speed              0.00                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.71     0.13     5.68     0.00\n   .x2                 0.90     0.12     7.34     0.00\n   .x3                 0.56     0.10     5.41     0.00\n   .x4                 0.32     0.06     4.87     0.00\n   .x5                 0.42     0.07     5.81     0.00\n   .x6                 0.41     0.07     5.88     0.00\n   .x7                 0.60     0.09     6.58     0.00\n   .x8                 0.40     0.09     4.25     0.00\n   .x9                 0.53     0.09     6.01     0.00\n    visual             0.60     0.16     3.76     0.00\n    verbal             0.94     0.15     6.18     0.00\n    speed              0.46     0.12     3.91     0.00\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    diff               0.01     0.13     0.08     0.93\n\nFor clearer presentation, we’ll look at a table of the specific parameter estimates.\n\n\nterm\n\n\nop\n\n\nblock\n\n\ngroup\n\n\nlabel\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\nconf.low\n\n\nconf.high\n\n\nstd.lv\n\n\nstd.all\n\n\nstd.nox\n\n\nvisual ~1\n\n\n~1\n\n\n1\n\n\n1\n\n\nint_p\n\n\n4.941\n\n\n0.095\n\n\n52.249\n\n\n0.000\n\n\n4.756\n\n\n5.127\n\n\n4.718\n\n\n4.718\n\n\n4.718\n\n\nvisual ~1\n\n\n~1\n\n\n2\n\n\n2\n\n\nint_gw\n\n\n4.930\n\n\n0.095\n\n\n51.696\n\n\n0.000\n\n\n4.743\n\n\n5.117\n\n\n6.345\n\n\n6.345\n\n\n6.345\n\n\ndiff := int_p-int_gw\n\n\n:=\n\n\n0\n\n\n0\n\n\ndiff\n\n\n0.011\n\n\n0.134\n\n\n0.085\n\n\n0.933\n\n\n-0.252\n\n\n0.275\n\n\n-1.627\n\n\n-1.627\n\n\n-1.627\n\n\nThe above shows the schools to be not much different from one another on the visual-spatial ability latent variable. But compare this result to the intercepts for x1 in our baseline model. This model would would be identical to comparing the intercepts on whichever observed variable you had fixed to zero. Much like we must scale the latent variable to that of one of the observed variables by fixing the loading to be 1, we essentially come to the same type of issue by fixing its mean to be on that of the observed variable.\nTo make this more explicit, we’ll label the x1 intercepts in our baseline model and look at their difference. I won’t show the model out put and simply focus on the parameter table instead.\n\n\nhs_model_2 <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  x1 ~ c(a, b)*1   \n   \n  # comparisons\n   diff := a - b\n'\n\nfit_2 <- cfa(hs_model_2, \n           data = HolzingerSwineford1939, \n           group = \"school\",\n           meanstructure = T)\n\n# summary(fit_2)\n\n\n\nterm\n\n\nop\n\n\nblock\n\n\ngroup\n\n\nlabel\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\nconf.low\n\n\nconf.high\n\n\nstd.lv\n\n\nstd.all\n\n\nstd.nox\n\n\nx1 ~1\n\n\n~1\n\n\n1\n\n\n1\n\n\na\n\n\n4.941\n\n\n0.095\n\n\n52.249\n\n\n0.000\n\n\n4.756\n\n\n5.127\n\n\n4.941\n\n\n4.183\n\n\n4.183\n\n\nx1 ~1\n\n\n~1\n\n\n2\n\n\n2\n\n\nb\n\n\n4.930\n\n\n0.095\n\n\n51.696\n\n\n0.000\n\n\n4.743\n\n\n5.117\n\n\n4.930\n\n\n4.293\n\n\n4.293\n\n\ndiff := a-b\n\n\n:=\n\n\n0\n\n\n0\n\n\ndiff\n\n\n0.011\n\n\n0.134\n\n\n0.085\n\n\n0.933\n\n\n-0.252\n\n\n0.275\n\n\n0.011\n\n\n-0.110\n\n\n-0.110\n\n\nSame difference.\nObserved variable group differences\nThe following approach is not the same model, but as we’ll see, would also provide the same result. In this case, each observed variable is affected by the school grouping, and the path coefficient for x1 is the same difference in means as before.\n\n\nhs_model_3 <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  x1 ~ diff*school \n  x2 + x3 + x4 + x5 + x6 +  x7 + x8 + x9 ~ school\n'\n\nfit_3 <- cfa(hs_model_3, \n             data = HolzingerSwineford1939,\n             meanstructure = T)\n\n# summary(fit_3)\n\nA comparison of all three shows the same results, but that the third model has fewer parameters, as the loadings and latent variable variances are not changing across groups.\n\n\nmodel\n\n\nestimate\n\n\nstd.error\n\n\nconf.low\n\n\nconf.high\n\n\nfit_1\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\nfit_2\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\nfit_3\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\n\n\n(#tab:compare_123_Npar)Model N parameters\n\n\nfit_1\n\n\nfit_2\n\n\nfit_3\n\n\n60\n\n\n60\n\n\n39\n\n\n\n\n(#tab:compare_123_AIC)Model AIC\n\n\nfit_1\n\n\nfit_2\n\n\nfit_3\n\n\n7484.395\n\n\n7484.395\n\n\n7474.493\n\n\nStructural model\nIn the models I see, people would more commonly address such a theoretical question without a multigroup approach, simply regressing the latent variable of interest on the group factor. For lack of a better name, I’ll just call this a structural model in the sense we have an explicit regression model. We can do that here.\n\n\n# standard cfa with school predicting visual\nhs_model_4a <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  visual ~ diff*school\n  \n  visual ~~ speed + verbal  # lavaan will not estimate this by default\n'\n\nfit_4a = sem(hs_model_4a, data=HolzingerSwineford1939, meanstructure=T)\nsummary(fit_4a)\n\nlavaan 0.6-7 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of free parameters                         31\n                                                      \n  Number of observations                           301\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               161.444\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.570    0.100    5.723    0.000\n    x3                0.797    0.111    7.212    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                1.113    0.065   17.021    0.000\n    x6                0.928    0.055   16.741    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.194    0.168    7.124    0.000\n    x9                1.060    0.148    7.152    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~                                            \n    school  (diff)    0.287    0.110    2.612    0.009\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n .visual ~~                                           \n    speed             0.241    0.054    4.446    0.000\n    verbal            0.429    0.073    5.846    0.000\n  verbal ~~                                           \n    speed             0.172    0.049    3.483    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                4.500    0.180   24.997    0.000\n   .x2                5.840    0.122   47.989    0.000\n   .x3                1.903    0.151   12.631    0.000\n   .x4                3.061    0.067   45.694    0.000\n   .x5                4.341    0.074   58.452    0.000\n   .x6                2.186    0.063   34.667    0.000\n   .x7                4.186    0.063   66.766    0.000\n   .x8                5.527    0.058   94.854    0.000\n   .x9                5.374    0.058   92.546    0.000\n   .visual            0.000                           \n    verbal            0.000                           \n    speed             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.604    0.105    5.739    0.000\n   .x2                1.137    0.102   11.172    0.000\n   .x3                0.795    0.090    8.879    0.000\n   .x4                0.372    0.048    7.821    0.000\n   .x5                0.448    0.058    7.698    0.000\n   .x6                0.354    0.043    8.254    0.000\n   .x7                0.796    0.081    9.775    0.000\n   .x8                0.470    0.076    6.209    0.000\n   .x9                0.580    0.071    8.195    0.000\n   .visual            0.754    0.135    5.597    0.000\n    verbal            0.978    0.112    8.735    0.000\n    speed             0.387    0.087    4.462    0.000\n\nAt first blush, it would seem we are not getting the same result, as the mean difference is now estimated to be 0.287. Our difference is notably larger and significant. We can also see that the fit is different based on AIC and the number of parameters estimated.\n\n\nfit_1\n\n\nfit_2\n\n\nfit_3\n\n\nfit_4a\n\n\nfit_baseline\n\n\n7484.395\n\n\n7484.395\n\n\n7474.493\n\n\n7531.852\n\n\n7484.395\n\n\nfit_1\n\n\nfit_2\n\n\nfit_3\n\n\nfit_4a\n\n\nfit_baseline\n\n\n60\n\n\n60\n\n\n39\n\n\n31\n\n\n60\n\n\nStructural model as multigroup\nHowever, we can recover the previous multigroup results by regressing the other observed variables on school as well. This leaves the only group effect remaining to be the effect on x1.\n\n\nhs_model_4b <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  visual ~ diff*school\n  \n  x2 + x3 + x4 + x5 + x6 +  x7 + x8 + x9 ~ school\n'\n\nfit_4b = sem(hs_model_4b, data=HolzingerSwineford1939, meanstructure=T)\n\n\n\nfit_1\n\n\nfit_2\n\n\nfit_3\n\n\nfit_4a\n\n\nfit_4b\n\n\nfit_baseline\n\n\n7484.395\n\n\n7484.395\n\n\n7474.493\n\n\n7531.852\n\n\n7526.606\n\n\n7484.395\n\n\nfit_1\n\n\nfit_2\n\n\nfit_3\n\n\nfit_4a\n\n\nfit_4b\n\n\nfit_baseline\n\n\n60\n\n\n60\n\n\n39\n\n\n31\n\n\n37\n\n\n60\n\n\nmodel\n\n\nestimate\n\n\nstd.error\n\n\nconf.low\n\n\nconf.high\n\n\n1\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\n2\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\n3\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\n4b\n\n\n0.011\n\n\n0.134\n\n\nNA\n\n\nNA\n\n\nMultigroup as the structural model\nLet’s see if we can get the structural model result from our multigroup approach. In fact we can. The following produces the same coefficient by summing the differences on the observed items. As we will see later, the statistical result is essentially what we’d get by using a linear regression on a sum score of visual items.\n\n\nhs_model_baseline_2 <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n  \n  x1 ~ c(a1, a2)*1\n  x2 ~ c(b1, b2)*1\n  x3 ~ c(c1, c2)*1\n  \n  diff := (a1 - a2) + (b1 - b2) + (c1 - c2)\n'\n\nfit_baseline_2 <- cfa(\n  hs_model_baseline_2, \n  data = HolzingerSwineford1939, \n  group = \"school\"\n)\n\nsummary(fit_baseline_2)\n\nlavaan 0.6-7 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of free parameters                         60\n                                                      \n  Number of observations per group:                   \n    Pasteur                                        156\n    Grant-White                                    145\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                               115.851\n  Degrees of freedom                                48\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    Pasteur                                     64.309\n    Grant-White                                 51.542\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.394    0.122    3.220    0.001\n    x3                0.570    0.140    4.076    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                1.183    0.102   11.613    0.000\n    x6                0.875    0.077   11.421    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.125    0.277    4.057    0.000\n    x9                0.922    0.225    4.104    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal            0.479    0.106    4.531    0.000\n    speed             0.185    0.077    2.397    0.017\n  verbal ~~                                           \n    speed             0.182    0.069    2.628    0.009\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1        (a1)    4.941    0.095   52.249    0.000\n   .x2        (b1)    5.984    0.098   60.949    0.000\n   .x3        (c1)    2.487    0.093   26.778    0.000\n   .x4                2.823    0.092   30.689    0.000\n   .x5                3.995    0.105   38.183    0.000\n   .x6                1.922    0.079   24.321    0.000\n   .x7                4.432    0.087   51.181    0.000\n   .x8                5.563    0.078   71.214    0.000\n   .x9                5.418    0.079   68.440    0.000\n    visual            0.000                           \n    verbal            0.000                           \n    speed             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.298    0.232    1.286    0.198\n   .x2                1.334    0.158    8.464    0.000\n   .x3                0.989    0.136    7.271    0.000\n   .x4                0.425    0.069    6.138    0.000\n   .x5                0.456    0.086    5.292    0.000\n   .x6                0.290    0.050    5.780    0.000\n   .x7                0.820    0.125    6.580    0.000\n   .x8                0.510    0.116    4.406    0.000\n   .x9                0.680    0.104    6.516    0.000\n    visual            1.097    0.276    3.967    0.000\n    verbal            0.894    0.150    5.963    0.000\n    speed             0.350    0.126    2.778    0.005\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.736    0.155    4.760    0.000\n    x3                0.925    0.166    5.583    0.000\n  verbal =~                                           \n    x4                1.000                           \n    x5                0.990    0.087   11.418    0.000\n    x6                0.963    0.085   11.377    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.226    0.187    6.569    0.000\n    x9                1.058    0.165    6.429    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal            0.408    0.098    4.153    0.000\n    speed             0.276    0.076    3.639    0.000\n  verbal ~~                                           \n    speed             0.222    0.073    3.022    0.003\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1        (a2)    4.930    0.095   51.696    0.000\n   .x2        (b2)    6.200    0.092   67.416    0.000\n   .x3        (c2)    1.996    0.086   23.195    0.000\n   .x4                3.317    0.093   35.625    0.000\n   .x5                4.712    0.096   48.986    0.000\n   .x6                2.469    0.094   26.277    0.000\n   .x7                3.921    0.086   45.819    0.000\n   .x8                5.488    0.087   63.174    0.000\n   .x9                5.327    0.085   62.571    0.000\n    visual            0.000                           \n    verbal            0.000                           \n    speed             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.715    0.126    5.676    0.000\n   .x2                0.899    0.123    7.339    0.000\n   .x3                0.557    0.103    5.409    0.000\n   .x4                0.315    0.065    4.870    0.000\n   .x5                0.419    0.072    5.812    0.000\n   .x6                0.406    0.069    5.880    0.000\n   .x7                0.600    0.091    6.584    0.000\n   .x8                0.401    0.094    4.249    0.000\n   .x9                0.535    0.089    6.010    0.000\n    visual            0.604    0.160    3.762    0.000\n    verbal            0.942    0.152    6.177    0.000\n    speed             0.461    0.118    3.910    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    diff              0.287    0.297    0.965    0.335\n\nGroup difference as an indirect effect\nGoing back to the first structural model hs_model_4a, it might be interesting to some to see that the group difference still regards a difference on the observed x1 observed variable. We can see this more clearly if we set the x1 loading to be estimated rather than fixed at one, then use the product of coefficients approach (a la mediation) to estimate the group difference.\n\n\nhs_model_4c <- ' \n  visual =~ x2 + a*x1 + x3    # estimate x1 loading vs. scaling by it\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n\n  visual ~ b*school\n  \n  visual ~~ verbal + speed\n  \n  diff := a*b    # \"indirect\" effect of school on x1\n'\n\n# same as fit_4a\nfit_4c = cfa(hs_model_4c, data = HolzingerSwineford1939, meanstructure=T)\n\n\n\nmodel\n\n\nlabel\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\nconf.low\n\n\nconf.high\n\n\n4a\n\n\ndiff\n\n\n0.287\n\n\n0.11\n\n\n2.612\n\n\n0.009\n\n\n0.072\n\n\n0.503\n\n\n4c\n\n\ndiff\n\n\n0.287\n\n\n0.11\n\n\n2.612\n\n\n0.009\n\n\n0.072\n\n\n0.503\n\n\nAnd what is this value of 0.287? We see it as the group difference on the latent construct rather than simply being a mean difference on x1. However, in fit_4a it is estimated on the metric of x1, which had it’s loading fixed to 1. We will see another interpretation later.\n\nYou might be thinking, why does my effect on the latent variable depend on a specific item? Well it shouldn’t. If the items are random observations measured in the same way of the same underlying construct, then the loadings will essentially be equal, and so it wouldn’t matter which one is chosen as a default.\nMore structural models\nThe following is equivalent to the result one would get from group.equal = c('loadings', 'intercepts'), but to make things more clear, I show the explicit syntax (commented out are other options one could potentially play with). The first group would have latent variable means at zero, while the second group would be allowed to vary. This is more or less what is desired if we want to know a group difference on the latent structure. The first group mean is arbitrarily set to zero, so the estimated intercept for the second group tells us the relative difference, much like when we are dummy coding with standard regression models.\n\n\nhs_model_4d <- ' \n  # make loadings equal across groups\n  \n  visual =~ c(1, 1)*x1 + c(v_x2, v_x2)*x2 + c(v_x3, v_x3)*x3\n  verbal =~ c(1, 1)*x4 + c(v_x5, v_x5)*x5 + c(v_x6, v_x6)*x6\n  speed  =~ c(1, 1)*x7 + c(v_x8, v_x8)*x8 + c(v_x9, v_x9)*x9 \n  \n  # make intercepts equal across groups\n  \n  x1 ~ c(0, 0) * 1\n  x2 ~ c(int_x2, int_x2) * 1\n  x3 ~ c(int_x3, int_x3) * 1\n  x4 ~ c(0, 0) * 1\n  x5 ~ c(int_x5, int_x5) * 1\n  x6 ~ c(int_x6, int_x6) * 1\n  x7 ~ c(0, 0) * 1\n  x8 ~ c(int_x8, int_x8) * 1\n  x9 ~ c(int_x9, int_x9) * 1\n  \n  # make covariances equal across groups\n  \n  # visual ~~ c(cov_vv, cov_vv) * verbal + c(cov_visp, cov_visp) * speed\n  # verbal ~~ c(cov_vesp, cov_vesp) * speed\n  \n  # make variances equal\n  \n  # visual ~~ c(vvar, vvar) * visual\n  # verbal ~~ c(tvar, tvar) * verbal\n  # speed  ~~ c(svar, svar) * speed\n  \n  # x1 ~~ c(x1var, x1var) * x1\n  # x2 ~~ c(x2var, x2var) * x2\n  # x3 ~~ c(x3var, x3var) * x3\n  # x4 ~~ c(x4var, x4var) * x4\n  # x5 ~~ c(x5var, x5var) * x5\n  # x6 ~~ c(x6var, x6var) * x6\n  # x7 ~~ c(x7var, x7var) * x7\n  # x8 ~~ c(x8var, x8var) * x8\n  # x9 ~~ c(x9var, x9var) * x9\n  \n  \n  visual ~ c(vis_int_p, vis_int_gw)*1\n  verbal ~ c(verb_int_p, verb_int_gw)*1\n  speed  ~ c(speed_int_p, speed_int_gw)*1\n  \n  \n   \n  # comparisons\n   diff := vis_int_p - vis_int_gw\n'\n\nfit_4d  = sem(hs_model_4d, \n                data=HolzingerSwineford1939, \n                group = 'school',\n                meanstructure=T)\n\nsummary(fit_4d, header=F, nd=2)\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\n\nGroup 1 [Pasteur]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2      (v_x2)     0.58     0.10     5.71     0.00\n    x3      (v_x3)     0.80     0.11     7.15     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5      (v_x5)     1.12     0.07    16.97     0.00\n    x6      (v_x6)     0.93     0.06    16.61     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8      (v_x8)     1.13     0.15     7.79     0.00\n    x9      (v_x9)     1.01     0.13     7.67     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal             0.41     0.10     4.29     0.00\n    speed              0.18     0.07     2.69     0.01\n  verbal ~~                                           \n    speed              0.18     0.06     2.90     0.00\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.00                           \n   .x2      (in_2)     3.27     0.50     6.54     0.00\n   .x3      (in_3)    -1.72     0.55    -3.11     0.00\n   .x4                 0.00                           \n   .x5      (in_5)     0.92     0.21     4.36     0.00\n   .x6      (in_6)    -0.66     0.18    -3.75     0.00\n   .x7                 0.00                           \n   .x8      (in_8)     0.84     0.60     1.39     0.17\n   .x9      (in_9)     1.18     0.55     2.16     0.03\n    visual  (vs__)     5.00     0.09    55.76     0.00\n    verbal  (vr__)     2.78     0.09    31.95     0.00\n    speed   (sp__)     4.24     0.07    57.97     0.00\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.56     0.14     3.98     0.00\n   .x2                 1.30     0.16     8.19     0.00\n   .x3                 0.94     0.14     6.93     0.00\n   .x4                 0.45     0.07     6.43     0.00\n   .x5                 0.50     0.08     6.14     0.00\n   .x6                 0.26     0.05     5.26     0.00\n   .x7                 0.89     0.12     7.42     0.00\n   .x8                 0.54     0.09     5.71     0.00\n   .x9                 0.65     0.10     6.80     0.00\n    visual             0.80     0.17     4.64     0.00\n    verbal             0.88     0.13     6.69     0.00\n    speed              0.32     0.08     3.91     0.00\n\n\nGroup 2 [Grant-White]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual =~                                           \n    x1                 1.00                           \n    x2      (v_x2)     0.58     0.10     5.71     0.00\n    x3      (v_x3)     0.80     0.11     7.15     0.00\n  verbal =~                                           \n    x4                 1.00                           \n    x5      (v_x5)     1.12     0.07    16.97     0.00\n    x6      (v_x6)     0.93     0.06    16.61     0.00\n  speed =~                                            \n    x7                 1.00                           \n    x8      (v_x8)     1.13     0.15     7.79     0.00\n    x9      (v_x9)     1.01     0.13     7.67     0.00\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  visual ~~                                           \n    verbal             0.43     0.10     4.42     0.00\n    speed              0.33     0.08     4.01     0.00\n  verbal ~~                                           \n    speed              0.24     0.07     3.22     0.00\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.00                           \n   .x2      (in_2)     3.27     0.50     6.54     0.00\n   .x3      (in_3)    -1.72     0.55    -3.11     0.00\n   .x4                 0.00                           \n   .x5      (in_5)     0.92     0.21     4.36     0.00\n   .x6      (in_6)    -0.66     0.18    -3.75     0.00\n   .x7                 0.00                           \n   .x8      (in_8)     0.84     0.60     1.39     0.17\n   .x9      (in_9)     1.18     0.55     2.16     0.03\n    visual  (vs__)     4.85     0.09    52.96     0.00\n    verbal  (vr__)     3.35     0.09    38.16     0.00\n    speed   (sp__)     4.06     0.08    50.75     0.00\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                 0.65     0.13     5.09     0.00\n   .x2                 0.96     0.12     7.81     0.00\n   .x3                 0.64     0.10     6.32     0.00\n   .x4                 0.34     0.06     5.53     0.00\n   .x5                 0.38     0.07     5.13     0.00\n   .x6                 0.44     0.07     6.56     0.00\n   .x7                 0.63     0.10     6.57     0.00\n   .x8                 0.43     0.09     4.91     0.00\n   .x9                 0.52     0.09     6.10     0.00\n    visual             0.71     0.16     4.42     0.00\n    verbal             0.87     0.13     6.66     0.00\n    speed              0.51     0.12     4.38     0.00\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    diff               0.15     0.12     1.21     0.23\n\nThe fit is now estimated to be 0.148, as we are now taking into account the intercorrelations of the latent variables. However, there is a more simple and obvious way to do this model. We simply regress all latent variables on school.\n\n\nhs_model_4e <- ' \n  visual =~ x1 + x2 + x3    # estimate x1 loading vs. scaling by it\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9 \n\n  visual ~ diff*school\n  \n  verbal + speed ~ school\n'\n\nfit_4e = cfa(hs_model_4e, data = HolzingerSwineford1939, meanstructure=T)\n# summary(fit_4e)\n\n\n\nmodel\n\n\nestimate\n\n\nstd.error\n\n\nconf.low\n\n\nconf.high\n\n\nObserved value differences\n\n\n1\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\n2\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\n3\n\n\n0.011\n\n\n0.134\n\n\n-0.252\n\n\n0.275\n\n\n4b\n\n\n0.011\n\n\n0.134\n\n\nNA\n\n\nNA\n\n\nLatent variable differences\n\n\n4a\n\n\n0.287\n\n\n0.110\n\n\n0.072\n\n\n0.503\n\n\n4c\n\n\n0.287\n\n\n0.110\n\n\n0.072\n\n\n0.503\n\n\n4d\n\n\n0.148\n\n\n0.122\n\n\nNA\n\n\nNA\n\n\n4e\n\n\n0.147\n\n\n0.122\n\n\nNA\n\n\nNA\n\n\nThe primary differences we’ve seen thus far can be summarized as follows:\n1:3 and 4b: These models are focusing on observed variable differences, specifically on x1.\n4a and 4c: These models are latent variable differences on the visual factor, but do not control for indirect (or backdoor) effect school has on visual through speed and verbal. In that light, we might consider this the total effect of school on the visual factor. Had we regressed the verbal and speed factors on school also, thereby decomposing the total effect into those different paths, we’d get the same result for the school difference on the visual factor as we do in 4d and e\n4d and 4e: This is generally what we want. A simple group difference on a latent variable(s) with other parameters assumed (relatively) equal across groups.\nSum/Factor score\nWhat would happen if we look at the structural/regression model with the estimated latent variable scores1? How about we go even simpler, by not even running an SEM and simply using sum scores? Let’s see what results.\nWe’ll start with the estimated factor scores.\n\n\nhs_model_5 <- ' \n  visual =~ x1 + x2 + x3\n  verbal =~ x4 + x5 + x6\n  speed  =~ x7 + x8 + x9\n'\n\nfit_5 = cfa(hs_model_5, data = HolzingerSwineford1939, meanstructure=T)\n\nHolzingerSwineford1939 = HolzingerSwineford1939 %>% \n  mutate(\n    visual = lavPredict(fit_5)[,'visual'],\n    verbal = lavPredict(fit_5)[,'verbal'],\n    speed  = lavPredict(fit_5)[,'speed']\n  )\n\nlm_1 = lm(visual ~ school + verbal + speed, data = HolzingerSwineford1939)\ncoef(lm_1)  \n\n  (Intercept) schoolPasteur        verbal         speed \n  -0.07231513    0.13953111    0.34744541    0.63626370 \n\nThe estimated coefficient is pretty close to that estimated by the SEM when we regressed all the factors on school. Interestingly, if we fix the loadings to be constant, we recover the initial multigroup estimates.\n\n\nhs_model_5b <- ' \n  visual =~ x1 + l1*x2 + l1*x3\n  verbal =~ x4 + l2*x5 + l2*x6\n  speed  =~ x7 + l3*x8 + l3*x9 \n'\n\nfit_5b = cfa(hs_model_5b,\n             data = lavaan::HolzingerSwineford1939,\n             meanstructure = T)\n\nHolzingerSwineford1939 = HolzingerSwineford1939 %>%\n  mutate(\n    visual = lavPredict(fit_5b)[, 'visual'],\n    verbal = lavPredict(fit_5b)[, 'verbal'],\n    speed  = lavPredict(fit_5b)[, 'speed']\n  )\n\nlm_2 = lm(visual ~ school, data = HolzingerSwineford1939)\ncoef(lm_2)  # same as x1 diffs\n\n  (Intercept) schoolPasteur \n -0.005790171   0.011172061 \n\nlm_3 = lm(x1 ~ school, data = HolzingerSwineford1939)\ncoef(lm_3)\n\n  (Intercept) schoolPasteur \n   4.92988506    0.01135425 \n\n\n\n\n\n\n\nNow lets do a sum score. It may not be obvious, but a sum score can be seen as assuming a latent variable model where there is only a single construct and loadings and variances are equal for each item. As such, it is a natural substitute for a latent variable if we don’t want to use SEM, especially if we’re dealing with a notably reliable measure.\n\n\nHolzingerSwineford1939 = HolzingerSwineford1939 %>%\n  rowwise() %>%\n  mutate(\n    visual_sum = sum(x1, x2, x3),\n    verbal_sum = sum(x4, x5, x6),\n    speed_sum  = sum(x7, x8, x9)\n  ) %>%\n  ungroup()\n\nlm_sum = lm(visual_sum ~ school, HolzingerSwineford1939)\n\ncoef(lm_sum)  # same as structural diffs\n\n  (Intercept) schoolPasteur \n   13.1255747     0.2868184 \n\nThat coefficient representing the group difference looks familiar- it’s the same value as we had for models 4a and 4c, where we looked at a group difference only for the visual factor. As with those, this can be seen as a total effect of school on the visual ‘factor’.\nSummary of differences\nWe can summarize our results as follows. The ?? multigroup approach can be seen as an interaction of everything with the grouping variable. In some measurement scenarios, for example, the development of a nationwide achievement exam, this might be desirable as a means to establish measurement invariance (see below). However, I think this is probably rarely a theoretical goal for most applied researchers using SEM. Furthermore, group sizes may be prohibitively small given the number of parameters that need to be estimated. And if we consider other modeling contexts outside of SEM, it is exceedingly rare to interact every covariate with a moderator.\nIn general though, we may very well be interested in a specific group difference on some latent variable, possibly controlling for other effects in some fashion. It is far simpler to specify such a model as above by regressing the latent variable on the group indicator as in the demonstration above, and it is a notably simpler model as well.\nSupplemental: Measurement invariance\nAs a final note, in some cases we are instead looking for similarities across groups among the latent constructs, rather than differences. This is especially the case in scale development, where one would like a measure to be consistent across groups of individuals (e.g. sex, age, race, etc.).\nAside from general problems of ‘accepting the null hypothesis’, the basic idea is to test a restricted model (e.g. loadings, intercepts, etc. are equal) vs. the less restrictive one that assumes the differences across groups exist, and if the general fit of the models is not appreciably different, then one can claim equivalence across groups. As a starting point, we assume configural equivalence, or in other words, that the factor structure is the same. There is no point in testing measurement equivalence if there is not a similar factor structure. The first more restricted model is that the loadings are equivalent. The next is that observed variable intercepts are equivalent, followed by latent variable means, and finally residual variances/covariances.\nI find in consulting and in published reports that researchers think that because they are interested in group differences that they are required to take a measurement invariance approach. This is not the case at all, as our previous models have shown. However, below is a demonstration using semTools. The package used to have a simple function that did exactly what most users want in a way easier than any other SEM package I’ve come across. In an effort to add flexibility and accommodate other data scenarios, they’ve made it much more complicated to do the default scenario, and have unfortunately deprecated the simple approach. I demonstrate both below.\n\n\nhs_model_4 <- ' \n  visual  =~ x1 + x2 + x3\n  verbal  =~ x4 + x5 + x6\n  speed   =~ x7 + x8 + x9 \n'\n\nsemTools::measurementInvariance(\n  model = hs_model_4, \n  data  = HolzingerSwineford1939, \n  group = \"school\"\n)\n\n\n\ncat(\"\nMeasurement invariance models:\n\nModel 1 : fit.configural\nModel 2 : fit.loadings\nModel 3 : fit.intercepts\nModel 4 : fit.means\n\nChi-Squared Difference Test\n\n               Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)    \nfit.configural 48 7484.4 7706.8 115.85                                  \nfit.loadings   54 7480.6 7680.8 124.04      8.192       6     0.2244    \nfit.intercepts 60 7508.6 7686.6 164.10     40.059       6  4.435e-07 ***\nfit.means      63 7543.1 7710.0 204.61     40.502       3  8.338e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nFit measures:\n\n                 cfi rmsea cfi.delta rmsea.delta\nfit.configural 0.923 0.097        NA          NA\nfit.loadings   0.921 0.093     0.002       0.004\nfit.intercepts 0.882 0.107     0.038       0.015\nfit.means      0.840 0.122     0.042       0.015\n    \")\n\nMeasurement invariance models:\n\nModel 1 : fit.configural\nModel 2 : fit.loadings\nModel 3 : fit.intercepts\nModel 4 : fit.means\n\nChi-Squared Difference Test\n\n               Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)    \nfit.configural 48 7484.4 7706.8 115.85                                  \nfit.loadings   54 7480.6 7680.8 124.04      8.192       6     0.2244    \nfit.intercepts 60 7508.6 7686.6 164.10     40.059       6  4.435e-07 ***\nfit.means      63 7543.1 7710.0 204.61     40.502       3  8.338e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nFit measures:\n\n                 cfi rmsea cfi.delta rmsea.delta\nfit.configural 0.923 0.097        NA          NA\nfit.loadings   0.921 0.093     0.002       0.004\nfit.intercepts 0.882 0.107     0.038       0.015\nfit.means      0.840 0.122     0.042       0.015\n    \n\nNow for the new approach. From a single line of code, we now have to do the following to produce the same result. Great if you need that additional functionality, not so much if you don’t. If you look at the visual latent variable intercepts model , their difference would equal that seen in models 4d/e.\n\n\ntest.seq <- c(\"loadings\",\"intercepts\",\"means\",\"residuals\")\n\nmeq.list <- list()\n\nfor (i in 0:length(test.seq)) {\n  if (i == 0L) {\n    meq.label <- \"configural\"\n    group.equal <- \"\"\n  } else {\n    meq.label <- test.seq[i]\n    group.equal <- test.seq[1:i]\n  }\n  \n  meq.list[[meq.label]] <- \n    semTools::measEq.syntax(\n      configural.model = hs_model_baseline,\n      data = lavaan::HolzingerSwineford1939,\n      ID.fac = \"auto.fix.first\",\n      group = \"school\",\n      group.equal = group.equal,\n      return.fit = TRUE\n  )\n}\n\nsemTools::compareFit(meq.list)\n\n################### Nested Model Comparison #########################\nChi-Squared Difference Test\n\n                    Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)    \nmeq.list.configural 48 7484.4 7706.8 115.85                                  \nmeq.list.loadings   54 7480.6 7680.8 124.04      8.192       6    0.22436    \nmeq.list.intercepts 60 7508.6 7686.6 164.10     40.059       6  4.435e-07 ***\nmeq.list.means      63 7543.1 7710.0 204.61     40.502       3  8.338e-09 ***\nmeq.list.residuals  72 7541.9 7675.3 221.34     16.730       9    0.05312 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                       chisq df pvalue   cfi   tli       aic       bic rmsea  srmr\nmeq.list.configural 115.851† 48   .000 .923† .885  7484.395  7706.822  .097  .068†\nmeq.list.loadings   124.044  54   .000 .921  .895† 7480.587† 7680.771  .093† .072 \nmeq.list.intercepts 164.103  60   .000 .882  .859  7508.647  7686.588  .107  .082 \nmeq.list.means      204.605  63   .000 .840  .817  7543.149  7709.969  .122  .109 \nmeq.list.residuals  221.335  72   .000 .831  .831  7541.879  7675.335† .117  .114 \n\n################## Differences in Fit Indices #######################\n                                        df    cfi    tli    aic     bic  rmsea  srmr\nmeq.list.loadings - meq.list.configural  6 -0.002  0.009 -3.808 -26.050 -0.004 0.004\nmeq.list.intercepts - meq.list.loadings  6 -0.038 -0.036 28.059   5.817  0.015 0.011\nmeq.list.means - meq.list.intercepts     3 -0.042 -0.042 34.502  23.381  0.015 0.026\nmeq.list.residuals - meq.list.means      9 -0.009  0.014 -1.270 -34.634 -0.005 0.005\n\nThis last one just compares the means. We see that assuming no group difference results in a worse model all around.\n\n\n# means_only\ntest.seq <- c(\"means\")\n\nmeq.list <- list()\n\nfor (i in 0:length(test.seq)) {\n  if (i == 0L) {\n    meq.label <- \"configural\"\n    group.equal <- \"\"\n  } else {\n    meq.label <- test.seq[i]\n    group.equal <- test.seq[1:i]\n  }\n  \n  meq.list[[meq.label]] <- \n    semTools::measEq.syntax(\n      configural.model = hs_model_baseline,\n      data = lavaan::HolzingerSwineford1939,\n      ID.fac = \"auto.fix.first\",\n      group = \"school\",\n      group.equal = group.equal,\n      return.fit = TRUE\n  )\n}\n\nsemTools::compareFit(meq.list)\n\n################### Nested Model Comparison #########################\nChi-Squared Difference Test\n\n                    Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)    \nmeq.list.configural 48 7484.4 7706.8 115.85                                  \nmeq.list.means      51 7515.7 7727.0 153.19     37.343       3  3.893e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                       chisq df pvalue   cfi   tli       aic       bic rmsea  srmr\nmeq.list.configural 115.851† 48   .000 .923† .885† 7484.395† 7706.822† .097† .068†\nmeq.list.means      153.195  51   .000 .885  .837  7515.738  7727.044  .115  .089 \n\n################## Differences in Fit Indices #######################\n                                     df    cfi    tli    aic    bic rmsea  srmr\nmeq.list.means - meq.list.configural  3 -0.039 -0.048 31.343 20.222 0.018 0.021\n\n\n\n\nNote that lavaan allows one to take this two step approach while estimating proper standard errors given the measurement error associated with the latent variable scores. The function is fsr, but as of this writing, it is undergoing development, has been hidden from the user, and was not working.↩︎\n",
    "preview": "posts/2019-08-05-comparing-latent-variables/../../img/compare_group_latent.png",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {},
    "preview_width": 1288,
    "preview_height": 862
  },
  {
    "path": "posts/2019-06-21-empirical-bayes/",
    "title": "Empirical Bayes",
    "description": "Revisiting an old post",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2019-06-21",
    "categories": [
      "empirical bayes",
      "regression",
      "mixed models",
      "bayesian"
    ],
    "contents": "\n\nContents\nIntroduction\nData Setup\nModels\nBayesian\nModel\nMixed Model\nFiltered Data Models\nPrediction Comparisons\nTop and bottom predictions\nExtreme\npredictions\n\nVisualizing the Results\nSummary\nAddendum\nBeta-binomial\n\n\nIntroduction\nA couple of folks I work with in different capacities independently\ncame across an\narticle by Data Camp’s David\nRobinson1 demonstrating empirical bayes. It provides a nice and simple\nexample of how to create a prior from the observed data, allowing it to\ninduce shrinkage in estimates, in that case, career batting averages of\nMajor League Baseball players. This would better allow one to compare\nsomeone that had only a relatively few at-bats to those that had longer\ncareers.\nIt is a simple and straightforward demo, and admits that it doesn’t\naccount for many other things that could be brought into the model, but\nthat’s also why it’s effective at demonstrating the technique. However,\nshrinkage of parameter estimates can be accomplished in other ways, so I\nthought I’d compare it to two of my preferred ways to do so - a fully\nBayesian approach and a random effects/mixed-model approach.\nI demonstrate shrinkage in mixed models in more detail here\nand here,\nand I’m not going to explain Bayesian analysis in general, but feel free to see\nmy doc on it. This post is just to provide a quick comparison of\ntechniques.\nData Setup\nWe’ll start as we typically do, with the data. The following just\nduplicates David’s code from the article. Nothing new here. If you want\nthe details, please\nread it.\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(Lahman)\n\ncareer <- Batting %>%\n  filter(AB > 0) %>%\n  anti_join(Pitching, by = \"playerID\") %>%  # This removes Babe Ruth!\n  group_by(playerID) %>%\n  summarize(H = sum(H), AB = sum(AB)) %>%\n  mutate(average = H / AB)\n\n# use names along with the player IDs\ncareer <- People %>%\n  as_tibble() %>%\n  select(playerID, nameFirst, nameLast) %>%\n  unite(name, nameFirst, nameLast, sep = \" \") %>%\n  inner_join(career, by = \"playerID\") \n\ncareer_filtered <- career %>%\n  filter(AB >= 500)\n\n\n\nWith data in place, we can get the empirical bayes estimates. Again,\nthis is just the original code. As a reminder, we assume a beta distribution for batting average, and the mean\nof the filtered data is 0.258. This finds the corresponding \\(\\alpha\\) and \\(\\beta\\) values for the beta distribution\nusing MASS.\n\nThe beta distribution can be reparameterized as having a mean and\nvariance: \\[\\mu = \\frac{\\alpha}{\\alpha +\n\\beta}\\] \\[\\sigma^2 =\n\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\]\n\n\nm <- MASS::fitdistr(career_filtered$average, \n                    dbeta,\n                    start = list(shape1 = 1, shape2 = 10))\n\nalpha0 <- m$estimate[1]\nbeta0 <- m$estimate[2]\n\ncareer_eb <- career %>%\n  mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))\n\n\n\n\nWe use the estimated parameters as input for the beta prior. Let’s\nexamine what we’ve got.\n\n\n\n\nJust to refresh, we can see how the EB estimates are able to guess\nsomething more meaningful for someone with just a few at-bats than say,\na 0 batting average. Even for Ody Abbot there, we would guess something\ncloser to the overall average than their .186 average after 70 plate\nappearances. With Frank Abercrombie, who had no hits in a measly 4 at\nbats, with so little information, we’d give him the benefit of the doubt\nof being average.\nFrom Wikipedia:\n\nFrancis Patterson Abercrombie (January 2, 1851 – November 11, 1939)\nwas an American professional baseball player who played in the National\nAssociation for one game as a shortstop in 1871. Born in Fort Towson,\nOklahoma, then part of Indian Territory, he played for the Troy\nHaymakers. He died at age 88 in Philadelphia, Pennsylvania.\n\nPretty sure that does not qualify for Wikipedia’s notability standards,\nbut oh well.\nModels\nAs mentioned, I will compare the empirical bayes results to those of\na couple of other approaches. They are:\nBayesian mixed model on full data (using brms)\nStandard mixed model on full data (using lme4)\nBayesian mixed model on filtered data (at bats greater than\n500)\nStandard mixed model on filtered data\nThe advantages to these are that using a fully Bayesian approach\nallows us to not approximate the Bayesian and just do it. In the other\ncase, the standard mixed model provides shrinkage with a penalized\nregression approach which also approximates the Bayesian, but doesn’t\nrequire any double dipping of the data to get at a prior, or any\nadditional steps aside from running the model.\nIn both cases, we can accomplish the desired result with just a\nstandard R modeling approach. In particular, the model is a standard\nbinomial model for counts. With base R glm, we\nwould do something like the following:\n\n\nglm(cbind(H, AB-H) ~ ..., data = career_eb, family = binomial)\n\n\n\nThe model is actually for the count of successes out of the total,\nwhich R has always oddly done in glm as\ncbind(# successes, # failures) rather than the more\nintuitive route (my opinion). The brms package\nwill make it more obvious, but glmer uses the\nglm approach. The key difference for both\nmodels relative to the standard binomial is that we add a\nper-observation random effect for\nplayerID2.\nBayesian Model\nWe’ll start with the full Bayesian approach using brms. This model will struggle a bit3,\nand takes a while to run, as it’s estimating 9864 parameters. But in the\nend we get what we want.\n\nI later learned the Bayesian model’s depicted here are essentially the\nsame as in the example for one of the Stan\nvignettes.\n\n\n# in case anyone wants to use rstanarm I show it here\n# library(rstanarm)\n# bayes_full = stan_glmer(cbind(H, AB-H) ~ 1 + (1|playerID),\n#                         data = career_eb,\n#                         family = binomial)\n\nlibrary(brms)\nbayes_full = brm(H|trials(AB) ~ 1 + (1|playerID), \n                 data = career_eb,\n                 family = binomial,\n                 seed = 1234,\n                 iter = 1000,\n                 thin = 4,\n                 cores = 4)\n\n\n\n\n\n\nWith the posterior predictive check we can see right off the bat4 that this approach estimates the\ndata well. Our posterior predictive distribution for the number of hits\nis hardly distinguishable from the observed data.\n\n\n\nAgain, the binomial model is for counts (out of some total), in this\ncase, the number of hits. But if we wanted proportions, which in this\ncase are the batting averages, we could just divide this result by the\nAB (at bats) column. Here we can see a little more nuance, especially\nthat the model shies away from the lower values more, but this would\nstill be a good fit by any standards.\n\n\n\nMixed Model\nThe lme4 model takes the glm approach as\nfar as syntax goes cbind(successes, non-successes). Very\nstraightforward, and fast, as it doesn’t actually estimate the random\neffects, but instead predicts them. The predicted random\neffects are in fact akin to empirical bayes estimates5.\n\n\nglmer_full = lme4::glmer(cbind(H, AB-H) ~ 1 + (1|playerID), \n                         data = career_eb,\n                         family = binomial)\n\n\n\n\n\n\nFiltered Data Models\nSince David’s original ‘prior’ was based only on observations for\nthose who had at least 500+ at bats (essentially a full season), the\nfollowing re-runs the previous models just for the filtered data set, to\nsee how those comparisons turn out.\n\n\nbayes_filtered = brm(H|trials(AB) ~ 1 + (1|playerID), \n                     data = career_eb %>% filter(AB >= 500),\n                     family = binomial,\n                     iter = 1000,\n                     seed = 1234,\n                     thin = 4,\n                     cores = 4)\n\nglmer_filtered = lme4::glmer(cbind(H, AB-H) ~ 1 + (1|playerID), \n                             data = career_eb %>% filter(AB >= 500),\n                             family = binomial)\n\n\n\nPrediction Comparisons\nNow we’re ready to make some comparisons. We’ll combine the fits from\nthe models to the original data set.\n\n\ncareer_other = career_eb %>% \n  mutate(\n    bayes_estimate = fitted(bayes_full)[,1] / AB,\n    glmer_estimate = fitted(glmer_full),\n  )\n\ncareer_other_filtered = career_filtered %>% \n  mutate(\n    bayes_filtered_estimate = fitted(bayes_filtered)[,1] / AB,\n    glmer_filtered_estimate = fitted(glmer_filtered),\n  ) %>% \n  select(playerID, contains('filter'))\n\ncareer_all = left_join(career_other, \n                       career_other_filtered)\n\n\n\n\n\n\nWe can see that the fully Bayesian and standard mixed models are\nessentially giving us the same values. We start to see slight\ndifferences with the EB estimates, especially for those with fewer\nat-bats. When there is less data, the EB estimates appear to pull more\nsharply toward the prior.\nTop and bottom predictions\nIf we just look at the top 10, we would not come to any different\nconclusions (only full data models shown).\n\n\ntop_10_eb = career_all %>% \n  top_n(10, eb_estimate) %>% \n  select(playerID, eb_estimate)\n\ntop_10_bayes = career_all %>% \n  top_n(10, bayes_estimate) %>% \n  select(playerID, bayes_estimate)\n\ntop_10_mixed = career_all %>% \n  top_n(10, glmer_estimate) %>% \n  select(playerID, glmer_estimate)\n\n\n\n\n\nplayerID\n\n\neb_estimate\n\n\nbayes_estimate\n\n\nglmer_estimate\n\n\ndelahed01\n\n\n0.342\n\n\n0.343\n\n\n0.342\n\n\ngehrilo01\n\n\n0.337\n\n\n0.337\n\n\n0.337\n\n\ngwynnto01\n\n\n0.336\n\n\n0.336\n\n\n0.336\n\n\nhamilbi01\n\n\n0.340\n\n\n0.341\n\n\n0.340\n\n\nheilmha01\n\n\n0.338\n\n\n0.338\n\n\n0.338\n\n\nhornsro01\n\n\n0.355\n\n\n0.355\n\n\n0.355\n\n\njacksjo01\n\n\n0.350\n\n\n0.350\n\n\n0.350\n\n\nkeelewi01\n\n\n0.338\n\n\n0.338\n\n\n0.338\n\n\nlajoina01\n\n\n0.336\n\n\n0.336\n\n\n0.336\n\n\nterrybi01\n\n\n0.337\n\n\n0.337\n\n\n0.337\n\n\nSame for the bottom 10, although we see a little more wavering on the\nfitted values, as some of these are the ones who have relatively fewer\nat bats, and would see more shrinkage as a result.\n\n\nbottom_10_eb = career_all %>% \n  top_n(-10, eb_estimate) %>% \n  select(playerID, eb_estimate)\n\nbottom_10_bayes = career_all %>% \n  top_n(-10, bayes_estimate) %>% \n  select(playerID, bayes_estimate)\n\nbottom_10_mixed = career_all %>% \n  top_n(-10, glmer_estimate) %>% \n  select(playerID, glmer_estimate)\n\n\n\n\n\nplayerID\n\n\neb_estimate\n\n\nbayes_estimate\n\n\nglmer_estimate\n\n\narmbrch01\n\n\n0.200\n\n\n0.198\n\n\n0.197\n\n\nbakerge01\n\n\n0.196\n\n\n0.195\n\n\n0.194\n\n\nbergebi01\n\n\n0.179\n\n\n0.178\n\n\n0.178\n\n\neastehe01\n\n\n0.197\n\n\n0.195\n\n\n0.195\n\n\ngladmbu01\n\n\n0.197\n\n\n0.195\n\n\n0.195\n\n\nhumphjo01\n\n\n0.196\n\n\n0.192\n\n\n0.193\n\n\noylerra01\n\n\n0.191\n\n\n0.190\n\n\n0.190\n\n\nryanmi02\n\n\n0.202\n\n\n0.201\n\n\n0.201\n\n\ntraffbi01\n\n\n0.201\n\n\n0.200\n\n\n0.199\n\n\nvukovjo01\n\n\n0.196\n\n\n0.194\n\n\n0.194\n\n\nExtreme predictions\nNow let’s look at some more extreme predictions. Those who averaged 0\nor 1 for their lifetime batting average. Note that none of these will\nhave very many plate appearances, and will show the greatest shrinkage.\nAs a reminder, the filtered models did not include any of these\nindividuals, and so are not shown.\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the Results\nThe following reproduces David’s plots. I start with his original\nimage, altered only to be consistent with my visualization choices\nthat use different color choices, add transparency, and allow size to\nreflect the number of at bats. Here is his explanation:\n\nThe horizontal dashed red line marks \\(y=\\alpha_0/\\alpha_0+\\beta_0=0.259\\) -\nthat’s what we would guess someone’s batting average was if we had no\nevidence at all. Notice that points above that line tend to move down\ntowards it, while points below it move up. The diagonal red line marks\n\\(x=y\\). Points that lie close to it\nare the ones that didn’t get shrunk at all by empirical Bayes. Notice\nthat they’re the ones with the highest number of at-bats (the brightest\nblue): they have enough evidence that we’re willing to believe the naive\nbatting average estimate.\n\n\n\n\nAgain, this is the same plot, but the size (along with color), which\nrepresents the number of at-bats, shows more clearly how observations\ndon’t exhibit as much shrinkage when there is enough information.\nHere is the same plot against the full bayes estimates. The original\nlines are kept, but I add lines representing the average of the whole\ndata, and the intercept from the Bayesian analysis (which is essentially\nthe same as with the mixed model). In this case, estimates are pulled\ntoward the estimated model mean.\n\n\n\n\nWe can also look at the density plots for more perspective. The full\ndata models for the Bayesian and mixed models are basically coming to\nthe same conclusions. The filtered data estimates center on the filtered\ndata mean batting average as expected, but the mixed and Bayesian models\nshow more variability in the estimates, as they are not based on the\nfull data. As the EB prior is based on the filtered data, the\ndistribution of values is similar to the others, but shifted to the\nfiltered data mean. Thus it is coming to slightly different conclusions\nabout the expected batting averages in general6.\n\n\n\n\n\n\n\nSummary\nHere we have enhanced the original empirical bayes story with the\naddition of full bayes estimates and those from a standard mixed model.\nIn terms of approximating Bayesian results, the empirical bayes are\nsimilar, but shifted due to the choice of prior. On the practical side,\nthe mixed model would be easier to run and appears to more closely\napproximate the full bayes approach. In your own situation, any of these\nmight be viable.\nAddendum\nBeta-binomial\nDavid mentioned the beta-binomial distribution in his post. In this\ncase, the prior for the probability of the binomial is assumed to be\nbeta distributed. The brms\nvignette shows an example of how to use this distribution, and the\nfollowing reproduces it for our data. You will likely need more\niterations and possibly other fiddling to obtain convergence.\n\n\nbeta_binomial2 <- custom_family(\n  \"beta_binomial2\", \n  dpars = c(\"mu\", \"phi\"),\n  links = c(\"logit\", \"log\"), \n  lb = c(NA, 0),\n  type = \"int\", \n  vars = \"trials[n]\"\n)\n\n\nstan_funs <- \"\n  real beta_binomial2_lpmf(int y, real mu, real phi, int T) {\n    return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi);\n  }\n  int beta_binomial2_rng(real mu, real phi, int T) {\n    return beta_binomial_rng(T, mu * phi, (1 - mu) * phi);\n  }\n\"\n\n\nstanvars <- stanvar(scode = stan_funs, block = \"functions\") +\n  stanvar(as.integer(career_eb$AB), name = \"trials\")\n\nfit2 <- brm(\n  H ~ (1|playerID), \n  data = career_eb, \n  family = beta_binomial2, \n  stanvars = stanvars,\n  seed = 1234,\n  iter = 1000,\n  cores = 4\n)\n\nsummary(fit2)\n\nexpose_functions(fit2, vectorize = TRUE)\n\npredict_beta_binomial2 <- function(i, draws, ...) {\n  mu <- draws$dpars$mu[, i]\n  phi <- draws$dpars$phi\n  N <- draws$data$trials[i]\n  beta_binomial2_rng(mu, phi, N)\n}\n\nfitted_beta_binomial2 <- function(draws) {\n  mu <- draws$dpars$mu\n  trials <- draws$data$trials\n  trials <- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE)\n  mu * trials\n}\n\npp_check(fit2)\nfitted(fit2, summary = T)[,1]\nfitted(fit2, summary = T)[,1]/career_eb$AB\nprior_summary(fit2)\n\n\n\n## Using stan for just an intercept only model to get the parameter estimates\n# takes about a minute per chain\n\nmodel_code = \"\ndata {\n  int N;\n  int H [N];\n  int AB [N];\n\n}\n\nparameters {\n  real alpha;  // setting lower = 0 provides serious estimation problems, without, just warnings\n  real beta;\n}\n\nmodel {\n  H ~ beta_binomial(AB, alpha, beta);\n}\n\ngenerated quantities {\n  vector[N] avg;\n  int pred [N];\n\n  pred = beta_binomial_rng(AB, alpha, beta);\n\n  for (n in 1:N) avg[n] = 1.0*pred[n] / AB[n];\n}\n\"\n\nlibrary(rstan)\ndata_list = list(H = career_eb$H, AB = career_eb$AB, N = nrow(career_eb))\n\nbayes_beta_binomial = stan(model_code = model_code,\n                           data  = data_list,\n                           seed  = 1234,\n                           iter  = 1000,\n                           cores = 4)\n\nprint(bayes_beta_binomial, digits = 3)\n# dr_bin = c(78.661, 224.875)           # beta estimates from DR post\n# dr_betabin = c(75, 222)               # beta binomial estimates from DR pots\n# stan_betabin = c(74.784, 223.451)     # stan estimates from above ~ .251 avg\n\n\n\nDavid is actually responsible for me\nstarting to do blog posts this year as a supplement to my more involved\ndocuments and workshops. At his talk at the RStudio conference, he laid\nout the benefits of blogging and in general doing anything to share\none’s work with the greater community, and it made sense to me to use\nthis as a less formal outlet.↩︎\nThis is possible for mixed models for\ncounts like binomial and poisson (and other distributions) where we\ndon’t estimate the residual variance, as it is determined by the nature\nof the distribution’s mean-variance relationship. In this case, it\nallows us to deal with overdispersion in this model via the random\neffect variance. See the GLMM\nFAQ.↩︎\nOne chain always struggled with the\nbrms defaults, but diagnostics were okay.↩︎\nNot sorry!↩︎\nSee Bates’\ncomment.↩︎\nI actually redid the empirical bayes\nbased on the full data. One issue was that fitting the beta required\nnudging the 0s and 1s, because the beta distribution doesn’t include 0\nand 1. In the end, the resulting estimates mostly followed the original\ndata. It had a very large and long tail for values less than .200, and\non the the other end, estimated some batting averages over .500.\n\n↩︎\n",
    "preview": "posts/2019-06-21-empirical-bayes/../../img/priorLikePosterior.png",
    "last_modified": "2022-08-07T17:57:26-04:00",
    "input_file": {},
    "preview_width": 1872,
    "preview_height": 892
  },
  {
    "path": "posts/2019-05-14-shrinkage-in-mixed-models/",
    "title": "Shrinkage in Mixed Effects Models",
    "description": "A demonstration of random effects",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2019-05-14",
    "categories": [
      "regression",
      "mixed models"
    ],
    "contents": "\nTable of Contents\nIntroduction\nAnalysisData\n\nRun the baseline model\nVisualize the baseline model\nMore subject level variance\nMore slope variance\nFewer observations per cluster\nMore observations per cluster\nUnbalanced\nSummary\nOther demos\nIntroduction\nThe following is a demonstration of shrinkage, sometimes called partial-pooling, as it occurs in mixed effects models. For some background, one can see the section of my document on mixed models here, and the document in general for an introduction to mixed models. Part of the inspiration of this document comes from some of the visuals seen here.\nIt is often the case that we have data such that observations are clustered in some way (e.g. repeated observations for units over time, students within schools, etc.). In mixed models, we obtain cluster-specific effects in addition to those for standard coefficients of our regression model. The former are called random effects, while the latter are typically referred to as fixed effects or population-average effects.\nIn other circumstances, we could ignore the clustering, and run a basic regression model. Unfortunately this assumes that all observations behave in the same way, i.e. that there are no cluster-specific effects, which would often be an untenable assumption. Another approach would be to run separate models for each cluster. However, aside from being problematic due to potentially small cluster sizes in common data settings, this ignores the fact that clusters are not isolated and potentially have some commonality.\nMixed models provide an alternative where we have cluster specific effects, but ‘borrow strength’ from the population-average effects. In general, this borrowing is more apparent for what would otherwise be more extreme clusters, and those that have less data. The following will demonstrate how shrinkage arises in different data situations.\nAnalysis\nFor the following we run a basic mixed model with a random intercept and random slopes for a single predictor variable. There are a number of ways to write such models, and the following does so for a single cluster \\(c\\) and observation \\(i\\). \\(y\\) is a function of the lone covariate \\(x\\), and otherwise we have a basic linear regression model. In this formulation, the random effects for a given cluster (\\(re_{*c}\\)) are added to each fixed effect (intercept \\(b_0\\) and the effect of \\(x\\), \\(b_1\\)). The random effects are multivariate normally distributed with some covariance. The per observation noise \\(\\sigma\\) is assumed constant across observations.\n\\[\\mu_{ic} = (b_0 + \\mathrm{re}_{0c})+ (b_1+\\mathrm{re}_{1c})*x_{ic}\\] \\[\\mathrm{re}_{0}, \\mathrm{re}_{1} \\sim \\mathcal{N}(0, \\Sigma)\\] \\[y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\]\nSuch models are highly flexible and have many extensions, but this simple model is enough for our purposes.\nData\nDefault settings for data creation are as follows:\nobs_per_cluster (observations per cluster) = 10\nn_cluster (number of clusters) = 100\nintercept (intercept) = 1\nbeta (coefficient for x) = .5\nsigma (observation level standard deviation) = 1\nsd_int (standard deviation for intercept random effect)= .5\nsd_slope (standard deviation for x random effect)= .25\ncor (correlation of random effect) = 0\nbalanced (fraction of overall sample size) = 1\nseed (for reproducibility) = 888\nIn this setting, \\(x\\) is a standardized variable with mean zero and standard deviation of 1. Unless a fraction is provided for balanced, the \\(N\\), i.e. the total sample size, is equal to n_cluster * obs_per_cluster. The following is the function that will be used to create the data, which tries to follow the model depiction above. It requires the tidyverse package to work.\n\n\ncreate_data <- function(  \n  obs_per_cluster = 10,\n  n_cluster = 100,\n  intercept = 1,\n  beta = .5,\n  sigma = 1,\n  sd_int = .5,\n  sd_slope = .25,\n  cor = 0,\n  balanced = TRUE,\n  seed = 888\n  ) {\n  \n  set.seed(seed)\n\n  cluster = rep(1:n_cluster, each = obs_per_cluster)\n  N = n_cluster * obs_per_cluster\n  x = rnorm(N)\n\n  varmat = matrix(c(sd_int^2, cor, cor, sd_slope^2), 2, 2)\n  \n  re = mvtnorm::rmvnorm(n_cluster, sigma = varmat)\n  colnames(re) = c('Intercept', 'x')\n  \n  y = (intercept + re[cluster, 'Intercept']) + (beta + re[cluster, 'x'])*x + rnorm(N, sd = sigma)\n  \n  df = tibble(\n    y,\n    x,\n    cluster\n  )\n  \n  if (balanced < 0 | balanced > 1) {\n    stop('Balanced should be a proportion to sample.')\n  } else {\n    df = sample_frac(df, balanced)\n  }\n  \n  df\n}\n\nThe plotting functions can be found on GitHub for those interested, but won’t be shown here1.\n\n\n\n\n\n\nRun the baseline model\nWe will use lme4 to run the analysis. We can see that the model recovers the parameters fairly well, even with the default of only 1000 observations.\n\n\ndf = create_data()\n\nlibrary(lme4)\nmod = lmer(y ~ x + (x|cluster), df)\nsummary(mod, cor=F) \n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (x | cluster)\n   Data: df\n\nREML criterion at convergence: 3012.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.93924 -0.63528 -0.00611  0.61562  2.87215 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n cluster  (Intercept) 0.29134  0.5398       \n          x           0.05987  0.2447   0.30\n Residual             0.99244  0.9962       \nNumber of obs: 1000, groups:  cluster, 100\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.93647    0.06281   14.91\nx            0.54405    0.04270   12.74\n\nVisualize the baseline model\nNow it is time to visualize the results. We will use gganimate to bring the shrinkage into focus. We start with the estimates that would be obtained by a fixed effects, or ‘regression-by-cluster’ approach. The movement shown will be of those (default 100) cluster-specific estimates toward the mixed model estimates. On the x axis is the estimate for the intercepts, on the y axis are the estimated slopes of the x covariate. The plots for this and other settings are zoomed in based on the random effect variance inputs for this baseline model, in an attempt to make the plots more consistent and comparable across the different settings. As such, the most extreme points often will not be shown at their starting point.\n\n\n\n\n\n\n\n\n\n\nWe see more clearly what the mixed model does. The general result is that cluster-specific effects (lighter color) are shrunk back toward the population-average effects (the ‘black hole’), as the imposed normal distribution for the random effects makes the extreme values less probable. Likewise, those more extreme cluster-specific effects, some of which are not displayed as they are so far from the population average, will generally have the most shrinkage imposed. In terms of prediction, it is akin to introducing bias for the cluster specific effects while lowering variance for prediction of new data, and allows us to make predictions on new categories we have not previously seen - we just assume an ‘average’ cluster effect, i.e. a random effect of 0.\nNow we’ll look at what happens under different data circumstances.\nMore subject level variance\nWhat happens when we add more subject level variance relative to the residual variance? The mixed model will show relatively less shrinkage, and what were previously less probable outcomes are now more probable2, and thus opting for the clusters to speak for themselves.\n\n\ndf = create_data(sd_int = 1, sd_slope = 1)\n\n\n\n\nMore slope variance\nIf we add more slope variance relative to the intercept variance, this more or less changes the orientation of the original plot. The shrinkage will be more along the x axis.\nOne point to keep in mind is that the slope variance is naturally on a very different scale than the intercept variance, usually many times smaller3. This can make the model more difficult to estimate. As such, scaling the covariate (e.g. to mean 0 and standard deviation of 1) is typically recommended, and at least in the linear model case, scaling the target variable can help as well.\n\n\ndf = create_data(sd_int = .25, sd_slope = 1)\n\n\n\n\nFewer observations per cluster\nIf we have fewer observations within each cluster, the more likely extreme values will present in the by-cluster approach, and thus more shrinkage is applied when using a mixed model. In this setting, we have relatively less knowledge about the groups, so we would prefer to lean our cluster estimates toward the population average. In the most extreme case of having only one or two observations per cluster, we could only estimate the fixed effects as the cluster-specific effects4.\n\n\ndf = create_data(obs_per_cluster = 3)\n\n\n\n\n\nMore observations per cluster\nThe opposite case is seen with more observations. We see that the estimates do not so easily fly to extreme values to begin with. With enough observations per cluster, you likely will see little shrinkage except with the more extreme cases.\n\n\ndf = create_data(obs_per_cluster = 100)\n\n\n\n\nUnbalanced\nWith unbalanced data, we see the combination of having more vs. fewer observations per group. Those clusters with more observations will generally exhibit less shrinkage, and those with fewer observations the opposite, though this is tempered by the relative variance components. In the following, the point size represents the cluster size.\n\n\ndf = create_data(balanced = .5, sd_int = .5, sd_slope = .5, sigma = .25)\n\n\n\n\nSummary\nMixed models incorporate some amount of shrinkage for cluster-specific effects. Data nuances will determine the relative amount of ‘strength borrowed’, but in general, such models provide a good way for the data to speak for itself when it should, and reflect an ‘average’ when there is little information. An additional benefit is that thinking about models in this way can be seen as a precursor to Bayesian approaches, which can allow for even more flexibility via priors, and more control over how shrinkage is added to the model.\nOther demos\nBen Bolker, author lme4 on stackexchange\nTristan Mahr, Plotting partial pooling in mixed-effects models\nDiscussion on cluster size\nThey are a bit messy, as I’m sort of tricking gganimate to do this sort of plot while supplying numerous options for myself for testing and eventual use.↩\nThink of a normal distribution with standard deviation of 1- almost all of the probability is for values falling between 3 and -3. The probability of values beyond that would be less than .001. On the other hand, if the standard deviation is 3, the probability of a value beyond \\(\\pm\\) 3 is ~ .32, i.e. very likely.↩\nFor example, if you had income in dollars as a target and education level (in years) as a covariate, the intercept would be on the scale of income, roughly somewhere between 0 - max income (i.e. a range of potentially millions), while the effect of one year of education might only have a range of 0 to a few thousand.↩\nIn order to have a random effects model you’d need at least two observations per cluster, though this would only allow you to estimate random intercepts. Note that with unbalanced data, it is fine to have singletons or only very few observations. Singletons can only contribute to the intercept estimate however.↩\n",
    "preview": "posts/2019-05-14-shrinkage-in-mixed-models/../../img/shrinkage/preview.gif",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-12-mediation-models/",
    "title": "Mediation Models",
    "description": "Various package options for conducting mediation analysis",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2019-03-12",
    "categories": [
      "SEM",
      "mediation"
    ],
    "contents": "\nTable of Contents\nIntroduction\nData\nModel\nPackagesmediation\nlavaan\npiecewiseSEM\npsych\nbrms\n\nMore complexityInteractions\nGeneralized Linear Models\nMissing data\n\nAlternativesbnlearn\nPython\nStata\n\nSummary\nUpdated August 04, 2020. Code can be downloaded here.\n\nIntroduction\nIn some situations we may consider the indirect effect of some variable on an outcome or result. As an example, poor living conditions at home in childhood may decrease learning outcomes in school, which subsequently have a negative effect on later quality of life, for example, lifetime income earnings. In another case we might consider a single variable collected at multiple time points, such that there exists an effect of the variable at time 1 on time 2, and time 2 on time 3. The basic idea is something like:\n\\[\\mathcal{A} \\rightarrow \\mathcal{B} \\rightarrow \\mathcal{C}\\]\nIn other words, \\(\\mathcal{A}\\) leads to \\(\\mathcal{B}\\), and then \\(\\mathcal{B}\\) leads to \\(\\mathcal{C}\\). With mediation models, we posit an intervening variable between the normal covariate \\(\\rightarrow\\) outcome path that we might have in the standard regression setting, and these models allow us to investigate such behaviors. In the above, the intervening variable, or mediator, is \\(\\mathcal{B}\\). It is often the case that we still might have a direct effect of \\(\\mathcal{A}\\) on \\(\\mathcal{C}\\), but as with the model in general, this would be theoretically motivated.\nMediation analysis is very popular in social science disciplines, though by no means restricted to those, and usually conducted under the guise of structural equation modeling (SEM), which itself is a specific orientation of graphical models more generally1. The graphical model of a mediation model might look like the following.\n\nConfounding and mediation are not distinguishable statistically in the standard linear model setting, only conceptually. One way to think about it is that confounding doesn’t require a causal relationship, and/or could be a common cause between the variable of interest and the outcome. See MacKinnon et al.\n\n\n\nIn this case, a and b reflect the indirect path of the effect of \\(\\mathrm{X}\\) on the outcome through the mediator, while c' is the direct effect of \\(\\mathrm{X}\\) on the outcome after the indirect path has been removed (c would be the effect before positing the indirect effect, and c - c' equals the indirect effect). The total effect of \\(\\mathrm{X}\\) is the combined indirect and direct effects.\nI should note a few things based on what I see in consulting across dozens of disciplines. To begin, it seems very few people who think they need a mediation model actually do. For example, if you cannot think of your model in temporal or physical terms, such that \\(\\mathrm{X}\\) necessarily leads to the mediator, which then necessarily leads to the outcome, you likely do not need a mediation model. If you could see the arrows going either direction, again, you probably don’t need such a model. Also, if when describing your model, everyone thinks you’re talking about an interaction (a.k.a. moderation), you might not need this. And finally, as one might suspect, if there is no strong correlation between key variables (\\(\\mathrm{X}\\)) and mediator (path a), and if there is no strong correlation between mediator and the outcome(s) (path b), you probably don’t need this. While nothing will stop you from doing mediation analysis, without such prerequisites, you will almost certainly have a weak and probably more confusing model than you otherwise would have.\nIn short, mediation works best when there are strongly implied causal connections among the variables. Even then, such a model should be compared to simpler model of no mediation2. In any case, there are a few very easy ways to investigate such models in R, and that is the goal here, just to demonstrate how you can get started.\nData\nFor demonstration of mediation models with the different packages, we will use the jobs data set that comes with the mediation package. Here is the description.\nJob Search Intervention Study (JOBS II). JOBS II is a randomized field experiment that investigates the efficacy of a job training intervention on unemployed workers. The program is designed to not only increase reemployment among the unemployed but also enhance the mental health of the job seekers. In the JOBS II field experiment, 1,801 unemployed workers received a pre-screening questionnaire and were then randomly assigned to treatment and control groups. Those in the treatment group participated in job-skills workshops. In the workshops, respondents learned job-search skills and coping strategies for dealing with setbacks in the job-search process. Those in the control condition received a booklet describing job-search tips. In follow-up interviews, the two key outcome variables were a continuous measure of depressive symptoms based on the Hopkins Symptom Checklist, and a binary variable, representing whether the respondent had become employed.\nHere is a description of the variables in this demonstration. There are others available you might also want to play around with.\necon_hard: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: Indicator variable for sex. 1 = female\nage: Age in years.\neduc: Factor with five categories for educational attainment.\njob_seek: A continuous scale measuring the level of job-search self-efficacy with values from 1 to 5. The mediator variable.\ndepress2: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: Indicator variable for whether participant was randomly selected for the JOBS II training program. 1 = assignment to participation.\n\n\ndata(jobs, package = 'mediation')\n\nModel\nGiven this data the models for the mediator and outcome are as follows:\n\\[\n\\begin{aligned}\n\\mathrm{\\color{#00b294}{job\\_seek}} &\\sim \\mathrm{\\color{#b2001d}{treatment} + econ\\_hard + sex + age} \\\\\n\\mathrm{depression} &\\sim \\mathrm{\\color{#b2001d}{treatment} + econ\\_hard + sex + age + \\color{#00b294}{job\\_seek}}\n\\end{aligned}\n\\]\nThus we expect the job skills training to have a negative effect on depression (i.e. an increase in well-being), but at least part of this would be due to a positive effect on job search.\nAs a graphical model, we might depict it succinctly as follows.\n\n\n\nPackages\nWe will look at the following packages to demonstrate how one can conduct mediation analysis in R:\nmediation\nlavaan\npsych\nbrms\nWhile these will be the focus, I’ll also note some other alternatives, including Python and Stata.\nmediation\nWe will start with the mediation package, as it basically requires no more programming ability to conduct than one possesses already from running standard regression models in R. The package provides the average causal mediation effect, defined as follows from the help file and Imai’s articles3:\n\nThe average causal mediation effect (ACME) represents the expected difference in the potential outcome when the mediator took the value that would realize under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nNote how this definition is focused on expected or predicted values conditional on the treatment value. This notion of counterfactuals, or what would the observation look like under the opposite setting, has a long history in modeling at this point. Think of it this way, if one is in the treatment group, they would have a specific value for the mediator, and, given that, they would then have a specific expected value for the outcome. However, we could posit the same observation as being in the control group as well, and assess the effect on the outcome through the mediator just the same. We can assess the potential outcomes while holding the treatment constant. Thinking of outcome changes given the value of the mediator makes no assumption about the model type. This is how the mediation package is able to incorporate different models for the mediator vs. the outcome. For example, the mediator could be binary, requiring a logistic regression model, while the outcome model might be a survival model.\n\nAs this document is a tools-based demo and not for depth, see the works of Judea Pearl for more details.\nIn our example, we will stick with standard (normal) linear models. Note also, that while our treatment is a binary variable, this generalizes to the continuous case, where we consider the result of a one unit movement on the ‘treatment’. For the mediation package to work, we simply run our respective models for the mediator and outcome, then use the mediate function to get the final result.\n\n\nlibrary(mediation)\n\nmodel_mediator <- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome  <- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximation\n?mediate\nmediation_result <- mediate(\n  model_mediator, \n  model_outcome, \n  sims = 500,\n  treat = \"treat\",\n  mediator = \"job_seek\"\n)\n\ndetach(package:mediation)\ndetach(package:MASS)\n\nThe result is based on simulations of a multivariate normal draw of the coefficients given their estimated covariance matrix. The algorithm is summarized as follows (from Imai et al. 2010)\nFit models for the observed outcome and mediator variables.\nSimulate model parameters from their sampling distribution.\nRepeat the following three steps:\nsimulate the potential values of the mediator,\nsimulate the potential outcomes given the simulated values of the mediator,\ncompute the causal mediation effects.\n\nCompute summary statistics such as point estimates and confidence intervals.\n\nWith this approach we can obtain the average difference and corresponding quantiles based on the simulated draws.\n\n\nsummary(mediation_result)\nplot(mediation_result)\n\n\n\n\n\nEstimate\n\n\n95% CI Lower\n\n\n95% CI Upper\n\n\np-value\n\n\nACME\n\n\n-0.016\n\n\n-0.038\n\n\n0.009\n\n\n0.220\n\n\nADE\n\n\n-0.045\n\n\n-0.127\n\n\n0.047\n\n\n0.292\n\n\nTotal Effect\n\n\n-0.061\n\n\n-0.149\n\n\n0.027\n\n\n0.188\n\n\nProp. Mediated\n\n\n0.226\n\n\n-3.222\n\n\n1.596\n\n\n0.344\n\n\n\nThe results above demonstrate that the ACME is not statistically distinct from zero, or no mediation. The average direct effect is negative but likewise not statistically notable, neither is the total effect (indirect + direct effect). Also provided is the soi disant ‘proportion mediated’, which is the ratio of the indirect effect to the total. However this is not a proportion, and can even be negative, and so is mostly a meaningless number.\nPros\nStandard R models and syntax\nMultiple types of models for both mediator and outcome\nProvides multiple results simultaneously\nGood documentation and associated articles are freely available\nCan do ‘moderated’ mediation\nLimitations\nUse of MASS4\nSimple random effects models\nFunctionality maybe limited with some model complexities\nNo latent variable capabilities\nlavaan\nIn the specific case where both mediation and outcome models are standard linear models with a normal distribution for the target variable, the indirect effect is equivalent to the product of the a and b paths in the previous diagram. The direct effect is the c' path. A comparison of standalone direct effect, which we might call c, vs this estimated direct effect in the mediation model c', is such that c - c' = a*b. What was mentioned earlier might now be more clear, if either a or b are nearly zero, then the indirect effect can only be nearly zero, so it is prudent to investigate such relationships beforehand.\nThis product-of-paths (or difference in coefficients) approach is the one we will take with the lavaan package, and in fact, as of this writing, that is our only way of going about it. lavaan is specifically geared toward structural equation modeling, such as factor analysis, growth models, and mediation models like we’re conducting here, and is highly recommended for such models. While it is limited to the standard linear model case to assess mediation, it is the only one of our tools that can incorporate latent variables readily5. For example, we could have our depression outcome as a latent variable underlying the individual questionnaire items. In addition, we could also incorporate multiple mediators and multiple outcomes.\nlavaan can still estimate the model with binary or ordinal variables, there just is no way to produce the proper indirect effect, at least not without a lot more effort.\nTo keep things as we have been discussing, I will label the a, b and c' paths in lavaan according to how they have been depicted previously. Otherwise lavaan is very easy to use, and in the case of observed variables, uses standard R formula notation for the models. Beyond that we define the effects of interest that we want to calculate with the := operator. We specify the model in its entirety as a simple character string, then use the sem function to do the analysis.\n\n\nlibrary(lavaan)\n\nsem_model = '\n  job_seek ~ a*treat + econ_hard + sex + age\n  depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\n \n  # direct effect\n  direct := c\n \n  # indirect effect\n  indirect := a*b\n \n  # total effect\n  total := c + (a*b)\n'\n\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)  # compare with ACME in mediation\n\nlavaan 0.6-6 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of free parameters                         11\n                                                      \n  Number of observations                           899\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws              500\n  Number of successful bootstrap draws             500\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  job_seek ~                                          \n    treat      (a)    0.066    0.049    1.332    0.183\n    econ_hard         0.053    0.024    2.242    0.025\n    sex              -0.008    0.047   -0.163    0.870\n    age               0.005    0.002    1.934    0.053\n  depress2 ~                                          \n    treat      (c)   -0.040    0.044   -0.905    0.365\n    econ_hard         0.149    0.022    6.908    0.000\n    sex               0.107    0.038    2.831    0.005\n    age               0.001    0.002    0.332    0.740\n    job_seek   (b)   -0.240    0.030   -8.079    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .job_seek          0.524    0.030   17.610    0.000\n   .depress2          0.373    0.022   17.178    0.000\n\nR-Square:\n                   Estimate\n    job_seek          0.011\n    depress2          0.120\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    direct           -0.040    0.045   -0.904    0.366\n    indirect         -0.016    0.012   -1.324    0.185\n    total            -0.056    0.046   -1.224    0.221\n\nWe see the same output before and can compare our indirect parameter to the ACME we had before, the direct effect is compared to the ADE, and the total compares to the previous total effect. The values are essentially the same.\nNote also that the output shows the \\(R^2\\) value for both models. In the case of job_seek, we can see that the reason we’re not finding much in the way of mediation is because the covariates involved do not explain any variation in the mediator to begin with. Preliminary investigation would have saved us the trouble in this case.\nPros\nCan handle multiple mediators\nCan handle multiple ‘treatments’\nCan handle multiple outcomes\nCan use latent variables\nSome multilevel support\nCan do moderated mediation and mediated moderation (though not for latent variables)\nLimitations\nRequires additional coding to estimate the indirect effect\nSingle random effects\nWhile the models could incorporate binary or ordinal variables for the mediator/outcomes, there is no straightforward way to calculate the indirect effect in the manner of the mediation package in those settings.\npiecewiseSEM\nThe piecewiseSEM package works very similar to the mediation package. The nice thing about this relative to the mediation package is that piecewiseSEM can handle additional types of models, as well as provide additional output (e.g. standardized results), additional options (e.g. multigroup, correlated residuals), and visualization of the model.\n\n\nlibrary(piecewiseSEM)\n\nmodel_mediator <- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome  <- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\nmediation_result <-  psem(model_mediator, model_outcome, data = jobs)\n\nsummary(mediation_result)\n\nStructural Equation Model of mediation_result \n\nCall:\n  job_seek ~ treat + econ_hard + sex + age\n  depress2 ~ treat + econ_hard + sex + age + job_seek\n\n    AIC      BIC\n 26.000   88.417\n\n---\nTests of directed separation:\n\n No independence claims present. Tests of directed separation not possible.\n\nGlobal goodness-of-fit:\n\n  Fisher's C = 0 with P-value = 1 and on 0 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error  DF Crit.Value P.Value Std.Estimate    \n  job_seek     treat   0.0656    0.0515 894     1.2748  0.2027       0.0425    \n  job_seek econ_hard   0.0532    0.0246 894     2.1612  0.0309       0.0720   *\n  job_seek       sex  -0.0076    0.0487 894    -0.1567  0.8755      -0.0052    \n  job_seek       age   0.0046    0.0023 894     1.9779  0.0482       0.0658   *\n  depress2     treat  -0.0403    0.0435 893    -0.9255  0.3550      -0.0291    \n  depress2 econ_hard   0.1485    0.0208 893     7.1323  0.0000       0.2248 ***\n  depress2       sex   0.1068    0.0411 893     2.5957  0.0096       0.0818  **\n  depress2       age   0.0006    0.0020 893     0.3306  0.7410       0.0104    \n  depress2  job_seek  -0.2400    0.0282 893    -8.4960  0.0000      -0.2682 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n  job_seek   none      0.01\n  depress2   none      0.12\n\nWe can use it’s plotting capabilities to create a quick visualization of the model.\n\nThe plot is using Diagrammer and graphviz under the hood, which is appealing to me as I use those anyway, so could use the generated code as a starting point.\n\n\nplot(mediation_result)\n\n\nUnfortunately, there is no automatic way to calculate the indirect effects at present, so one would have to bootstrap the results by hand.\n\nThere used to be a package to calculate indirect effects of a psem object, semEff, but it currently doesn’t work.\n\n\n\nPros\nStandard R models and syntax\nMultiple types of models for both mediator and outcome\nSome SEM-style results (e.g. fit, standardized coefficients, AIC)\nQuick plot of results\nCan handle multiple mediators, ‘treatments’, and outcomes\nLimitations\nDoesn’t automatically calculate indirect effects\nNo latent variable capabilities\npsych\nThe psych package takes advantage of the fact that in the standard linear model case, one can obtain the results via the appropriate regression models based on the covariance matrices alone. It’s very similar to lavaan, although using an ordinary least squares approach as opposed to maximum likelihood. The nice thing here is a syntax that allows you to focus only on the effect of interest, or include everything, which is nice if you were interested in the indirect effects for economic hardship, age, and sex as well.\nFor this demo we’ll use the cleaned up version using the -, instead of +, for the non-treatment effects. This just means they are included with the models, but results are not shown concerning them. The mediator is identified with (). Another bonus is a quick plot of the results, showing the difference between the unadjusted and adjusted direct effects, and the appropriate bootstrapped interval.\n\n\nlibrary(psych)\n\nmediation_psych = mediate(\n  depress2 ~ treat + (job_seek) - econ_hard - sex - age, \n  data = jobs,\n  n.iter = 500\n)\n\n\nmediation_psych\n\nMediation/Moderation Analysis \nCall: mediate(y = depress2 ~ treat + (job_seek) - econ_hard - sex - \n    age, data = jobs, n.iter = 500)\n\nThe DV (Y) was  depress2* . The IV (X) was  treat* . The mediating variable(s) =  job_seek* . Variable(s)  partialled out were econ_hard sex age\n\nTotal effect(c) of  treat*  on  depress2*  =  -0.06   S.E. =  0.05  t  =  -1.24  df=  895   with p =  0.21\nDirect effect (c') of  treat*  on  depress2*  removing  job_seek*  =  -0.04   S.E. =  0.15  t  =  14.91  df=  893   with p =  4.6e-45\nIndirect effect (ab) of  treat*  on  depress2*  through  job_seek*   =  -0.02 \nMean bootstrapped indirect effect =  -0.02  with standard error =  0.01  Lower CI =  -0.04    Upper CI =  0.01\nR = 1.07 R2 = 1.15   F = -3510.4 on 2 and 893 DF   p-value:  1 \n\n To see the longer output, specify short = FALSE in the print statement or ask for the summary\n\nsummary(mediation_psych)\n\nCall: mediate(y = depress2 ~ treat + (job_seek) - econ_hard - sex - \n    age, data = jobs, n.iter = 500)\n\nDirect effect estimates (traditional regression)    (c') \n          depress2*   se     t  df     Prob\nIntercept      2.21 0.15 14.91 893 4.60e-45\ntreat         -0.04 0.04 -0.93 893 3.55e-01\njob_seek      -0.24 0.03 -8.50 893 8.14e-17\n\nR = 1.07 R2 = 1.15   F = -3510.4 on 2 and 893 DF   p-value:  1 \n\n Total effect estimates (c) \n      depress2*   se     t  df  Prob\ntreat     -0.06 0.05 -1.24 895 0.215\n\n 'a'  effect estimates \n          job_seek   se     t  df      Prob\nIntercept     3.67 0.13 29.33 894 5.65e-133\ntreat         0.07 0.05  1.27 894  2.03e-01\n\n 'b'  effect estimates \n         depress2*   se    t  df     Prob\njob_seek     -0.24 0.03 -8.5 894 7.83e-17\n\n 'ab'  effect estimates (through mediators)\n      depress2*  boot   sd lower upper\ntreat     -0.02 -0.02 0.01 -0.04  0.01\n\nSame results, different packaging, but possibly the easiest route yet as it only required one function call. The psych package also handles multiple mediators and outcomes as a bonus.\nPros\nEasiest syntax, basically a one line model\nQuick plot of results\nCan handle multiple mediators, ‘treatments’, and outcomes\nCan do ‘moderated’ mediation\nLimitations\nLimited to standard linear model (lm)\nUse of MASS\nbrms\nFor our next demo we come to what I feel is the most powerful package, brms. The name stands for Bayesian Regression Modeling with Stan, and Stan is a powerful probabilistic programming language for Bayesian analysis. I won’t go into details about Bayesian analysis, but feel free to see my document that does.\nWe generally do as we have before, specifying the mediator model and the outcome model. brms doesn’t do anything special for mediation analysis, but its hypothesis function can allow us to test the product-of-paths approach. Furthermore, the sjstats package will essentially provide the results in the same way the mediation package does for us, and for that matter, the mediation package is basically an attempt at a Bayesian solution using frequentist methods anyway. If we did have different distributions for the outcome and mediator, we’d have an relatively easy time getting these average prediction values and their differences, as Bayesian approaches are always thinking about posterior predictive distributions. In any case, here is the code.\n\n\nlibrary(brms)\n\nmodel_mediator <- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcome  <- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\n\nmed_result = brm(\n  model_mediator + model_outcome + set_rescor(FALSE), \n  data = jobs\n)\nsave(med_result, file = 'data/mediation_brms.RData')\n\n\n\n\nload('data/mediation_brms.RData')\nsummary(med_result)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: job_seek ~ treat + econ_hard + sex + age \n         depress2 ~ treat + job_seek + econ_hard + sex + age \n   Data: jobs (Number of observations: 899) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\njobseek_Intercept      3.67      0.12     3.43     3.91 1.00     6699     3749\ndepress2_Intercept     2.21      0.15     1.92     2.50 1.00     6174     3091\njobseek_treat          0.07      0.05    -0.03     0.17 1.00     6322     2709\njobseek_econ_hard      0.05      0.02     0.00     0.10 1.00     6266     2656\njobseek_sex           -0.01      0.05    -0.10     0.09 1.00     5741     2655\njobseek_age            0.00      0.00     0.00     0.01 1.00     6539     2846\ndepress2_treat        -0.04      0.04    -0.12     0.04 1.00     5458     3102\ndepress2_job_seek     -0.24      0.03    -0.30    -0.18 1.00     5950     2938\ndepress2_econ_hard     0.15      0.02     0.11     0.19 1.00     7543     3102\ndepress2_sex           0.11      0.04     0.03     0.19 1.00     5599     2699\ndepress2_age           0.00      0.00    -0.00     0.00 1.00     4555     2887\n\nFamily Specific Parameters: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_jobseek      0.73      0.02     0.69     0.76 1.00     6639     3276\nsigma_depress2     0.61      0.01     0.59     0.64 1.00     6145     2987\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# using brms we can calculate the indirect effect as follows\n# hypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\n# sjstats provides similar printing as the mediation package\n# print(sjstats::mediation(med_result), digits=4)\nsjstats::mediation(med_result) %>% kable_df()\n\neffect\n\n\nvalue\n\n\nhdi.low\n\n\nhdi.high\n\n\ndirect\n\n\n-0.039\n\n\n-0.112\n\n\n0.031\n\n\nindirect\n\n\n-0.015\n\n\n-0.036\n\n\n0.005\n\n\nmediator\n\n\n-0.240\n\n\n-0.286\n\n\n-0.193\n\n\ntotal\n\n\n-0.055\n\n\n-0.133\n\n\n0.017\n\n\nproportion mediated\n\n\n0.277\n\n\n-0.813\n\n\n1.366\n\n\nIn the output, anything with jobseek_* is a result for the mediator model, while depress2_* is for the outcome. We have the same old story at this point, but with the Bayesian approach we have more fun things to look at. For example, we can see that we aren’t actually capturing the skewness of depression outcome well. Our predicted values vs. the observed don’t quite match up. We’re a little better for the mediator, but perhaps still a little high with some of our model-based predictions.\n\n\npp_check(med_result, resp = 'depress2') + ggtitle('Depression Outcome')\n\n\npp_check(med_result, resp = 'jobseek') + ggtitle('Mediator')\n\n\n\nThe faint lines (yrep) are posterior predictive draws.\nPros\nStraightforward syntax\nExtremely powerful- Models are mostly limited to one’s imagination\nBasically does what the mediation package approximates\nAll the perks of Bayesian inference: diagnostics, posterior predictive checks, model comparison, etc.\nLimitations\nSlower to estimate\n‘By-hand’ calculations needed for going beyond the standard linear model, but this is already a common approach from the Bayesian perspective\nSome comfort with the Bayesian approach required\nMore complexity\nSome of the packages mentioned can handle more complex models or provide additional approaches to investigate indirect effects.\nInteractions\nSome models involve interactions either for the mediation model or outcome, and unfortunately this is often referred to as mediated moderation or moderated mediation. I personally don’t see the advantage to giving ambiguous names to what otherwise might be a straightforward concept (if still not-so-straightforward model), but that ship sailed long ago. I’m not going to go into the details, but the idea is that you might have an interaction term somewhere in the model, and the interaction might involve the treatment variable, the mediator, or both.\nSuffice it to say, since we’re using standard modeling tools like lm and extensions of it, incorporating interactions is trivial for all of the above packages, but the product-of-paths type of approach doesn’t hold (a*b != c').\nGeneralized Linear Models\nIn some cases our mediator or outcome may be binary, count, or something where assuming a normal distribution might not be the best idea. Or we might want to investigate nonlinear relationships among the treatment/mediator/outcome. Or we might have data that has correlated observations like repeated measurements or similar. The mediation package prides itself on this in particular, but brms can do anything it can do and more, though you might have to do a little more work to actually calculate the result. lavaan can actually do a limited set of models for binary and ordinal variables, but getting the appropriate indirect estimate would require a very tedious by-hand approach.\nMissing data\nOften when dealing with such data, especially in the social sciences, data is often missing on any of the covariates. Sometimes we can drop these if there isn’t too many, but in other cases we will want to do something about it. The packages lavaan, psych, and brms provide one or more ways to deal with the situation (e.g. multiple imputation).\nAlternatives\nWe have been depicting the models as networks of nodes, with arcs/edges/paths connecting them. Our discussion revolves around what are called Directed Acyclic Graphs (DAG) where the arrows can only go one direction with no feedback loops. The result of any outcome variable is a function of the arrows preceding it, and conditionally independent of others. Some theoretical models may relax this, and others may have no arrows at all, i.e. are undirected, such that we are interested in just the connections (e.g. with some social networks).\nbnlearn\nThe bnlearn package allows investigation of directed, partially directed, and undirected graphs. In terms of DAGs, we can use it to essentially duplicate the mediation models we’ve been discussing. The nice thing though is that this package will efficiently test paths for inclusion rather than assume them, but we can still impose theoretical constraints as needed. Not only can we then search for the paths of interest in a principled way with bayesian networks and Pearl’s causal graph theory as a basis, we also will have tools to further avoid overfitting via cross-validation.\nFor the initial model, we’ll make sure that paths exist between treatment - mediator, treatment - outcome, and mediator - outcome (the whitelist). We will disallow nonsensical paths like having arrows to the treatment (which was randomly assigned), sex, economic hardship, and age (the blacklist). Otherwise, we’ll see what the data suggests.\n\n\nwhitelist = data.frame(\n  from = c('treat', 'treat', 'job_seek'),\n  to   = c('job_seek', 'depress2', 'depress2')\n)\n\nblacklist = expand.grid(\n  from = colnames(mediation_result$model.y$model),\n  to   = c('treat', 'sex', 'age', 'econ_hard')\n)\n\n# For simpler output we'll use treatment and sex as numeric (explained later)\nlibrary(dplyr)\n\njobs_trim = jobs %>% \n  select(depress2, treat, econ_hard, sex, age, job_seek) %>% \n  mutate(\n    treat = as.numeric(jobs$treat),\n    sex = as.numeric(jobs$sex)\n    )\n\n\n# extract path coefficients if desired\n# parameters = bn.fit(model, jobs_trim)\n# parameters$job_seek\n# parameters$econ_hard\n# parameters$depress2\n\n\n\nlibrary(bnlearn)\n\nmodel = gs(jobs_trim, whitelist = whitelist, blacklist = blacklist)\n\nplot(model)\n\n\nWe see in the plot that things have changed a bit. For example, age now only relates to job seeking self-efficacy, and sex only has an effect on depression.\nIf we restrict the paths to only be what they are in our previous examples, we’d get the same results.\n\n\nlibrary(bnlearn)\n\nwhitelist = data.frame(\n  from = c('treat', 'age', 'sex', 'econ_hard', 'treat', 'job_seek', 'age', 'sex', 'econ_hard'),\n  to   = c('job_seek', 'job_seek','job_seek','job_seek', 'depress2', 'depress2', 'depress2', 'depress2', 'depress2')\n)\n\nblacklist = expand.grid(\n  from = colnames(mediation_result$model.y$model),\n  to   = c('treat', 'sex', 'age', 'econ_hard')\n) \n\nmodel = gs(jobs_trim, whitelist = whitelist, blacklist = blacklist)\nplot(model)\n\n\nparameters = bn.fit(model, jobs_trim)\n\nparameters$depress2$coefficients\n\n  (Intercept)         treat     econ_hard           sex           age      job_seek \n 2.2076414333 -0.0402647000  0.1485433818  0.1068048699  0.0006488642 -0.2399549527 \n\nparameters$job_seek$coefficients\n\n (Intercept)        treat    econ_hard          sex          age \n 3.670584908  0.065615003  0.053162413 -0.007637336  0.004586492 \n\nThe main thing to note is that the estimated parameters equal the same thing we got with previous packages. It’s essentially equivalent to using lavaan with the default maximum likelihood estimator.\nIf we use treatment and sex as factors, bnlearn will produce conditional models that are different depending on the factor value taken. In other words, one would have a separate model for when treatment == 'treatment' and one for when treatment == control. In our case, this would be identical to allowing everything to interact with treatment, e.g. lm( job_seek ~ treat * (econ_hard + sex + age)), and likewise for the depression model. This would extend to potentially any binary variable (e.g. including sex). If the mediator is a binary variable, this is likely what we’d want to do.\nPython\nCSCAR director Kerby Shedden has given a Python workshop on mediation models, so I show the statsmodels implementation here. It follows the Imai approach and so can be seen as the Python version of the mediation package. The output is essentially the same as what you would have using treatment as a factor variable, where you get separate results for each treatment category. This is unnecessary for our demo, so you can just compare the ‘average’ results to the previous mediation package results.\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation\nimport numpy as np\nimport pandas as pd\n\n\noutcome_model = sm.OLS.from_formula(\"depress2 ~ treat + econ_hard + sex + age + job_seek\",\n                                    data = jobs)\n\nmediator_model = sm.OLS.from_formula(\"job_seek ~ treat + econ_hard + sex + age\",\n                                     data = jobs)\n\nmed = Mediation(outcome_model, mediator_model, \"treat\", \"job_seek\")\n\nmed_result = med.fit(n_rep = 500)\n\nprint(np.round(med_result.summary(), decimals = 3))\n\n                          Estimate  Lower CI bound  Upper CI bound  P-value\nACME (control)              -0.016          -0.048           0.014    0.332\nACME (treated)              -0.016          -0.048           0.014    0.332\nADE (control)               -0.043          -0.130           0.044    0.308\nADE (treated)               -0.043          -0.130           0.044    0.308\nTotal effect                -0.059          -0.144           0.029    0.208\nProp. mediated (control)     0.241          -1.710           2.254    0.364\nProp. mediated (treated)     0.241          -1.710           2.254    0.364\nACME (average)              -0.016          -0.048           0.014    0.332\nADE (average)               -0.043          -0.130           0.044    0.308\nProp. mediated (average)     0.241          -1.710           2.254    0.364\n\nStata\nFinally, I provide an option in Stata using its sem command. Stata makes it easy to get the indirect effects in this example, but it does so for every covariate, so the output is a bit verbose to say the least6. For those working with Stata, they do not need a separate SEM package to get these sorts of results.\n\n\nuse \"data\\jobs.dta\"\n\nsem (job_seek <- treat econ_hard sex age) (depress2 <- treat econ_hard sex age job_seek), cformat(%9.3f) pformat(%5.2f)\n\nestat teffects, compact cformat(%9.3f) pformat(%5.2f)\n\n\n\n. use 'data\\jobs.dta'\n\n. \n. sem (job_seek <- treat econ_hard sex age) (depress2 <- treat econ_hard sex age job_seek), \ncformat(%9.3f) pformat(%5.2f)\n\nEndogenous variables\n\nObserved:  job_seek depress2\n\nExogenous variables\n\nObserved:  treat econ_hard sex age\n\nFitting target model:\n\nIteration 0:   log likelihood = -7711.0956  \nIteration 1:   log likelihood = -7711.0956  \n\nStructural equation model                       Number of obs     =        899\nEstimation method  = ml\nLog likelihood     = -7711.0956\n\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n       treat |      0.066      0.051     1.28    0.20       -0.035       0.166\n   econ_hard |      0.053      0.025     2.17    0.03        0.005       0.101\n         sex |     -0.008      0.049    -0.16    0.88       -0.103       0.088\n         age |      0.005      0.002     1.98    0.05        0.000       0.009\n       _cons |      3.671      0.125    29.41    0.00        3.426       3.915\n  -----------+----------------------------------------------------------------\n  depress2   |\n    job_seek |     -0.240      0.028    -8.52    0.00       -0.295      -0.185\n       treat |     -0.040      0.043    -0.93    0.35       -0.125       0.045\n   econ_hard |      0.149      0.021     7.16    0.00        0.108       0.189\n         sex |      0.107      0.041     2.60    0.01        0.026       0.187\n         age |      0.001      0.002     0.33    0.74       -0.003       0.004\n       _cons |      2.208      0.148    14.96    0.00        1.918       2.497\n-------------+----------------------------------------------------------------\nvar(e.job_~k)|      0.524      0.025                         0.478       0.575\nvar(e.depr~2)|      0.373      0.018                         0.340       0.409\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0)   =      0.00, Prob > chi2 =      .\n\n. \n. estat teffects, compact cformat(%9.3f) pformat(%5.2f)\n\n\nDirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n       treat |      0.066      0.051     1.28    0.20       -0.035       0.166\n   econ_hard |      0.053      0.025     2.17    0.03        0.005       0.101\n         sex |     -0.008      0.049    -0.16    0.88       -0.103       0.088\n         age |      0.005      0.002     1.98    0.05        0.000       0.009\n  -----------+----------------------------------------------------------------\n  depress2   |\n    job_seek |     -0.240      0.028    -8.52    0.00       -0.295      -0.185\n       treat |     -0.040      0.043    -0.93    0.35       -0.125       0.045\n   econ_hard |      0.149      0.021     7.16    0.00        0.108       0.189\n         sex |      0.107      0.041     2.60    0.01        0.026       0.187\n         age |      0.001      0.002     0.33    0.74       -0.003       0.004\n------------------------------------------------------------------------------\n\n\nIndirect effects\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n  -----------+----------------------------------------------------------------\n  depress2   |\n       treat |     -0.016      0.012    -1.26    0.21       -0.040       0.009\n   econ_hard |     -0.013      0.006    -2.10    0.04       -0.025      -0.001\n         sex |      0.002      0.012     0.16    0.88       -0.021       0.025\n         age |     -0.001      0.001    -1.93    0.05       -0.002       0.000\n------------------------------------------------------------------------------\n\n\nTotal effects\n------------------------------------------------------------------------------\n             |                 OIM\n             |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  job_seek   |\n       treat |      0.066      0.051     1.28    0.20       -0.035       0.166\n   econ_hard |      0.053      0.025     2.17    0.03        0.005       0.101\n         sex |     -0.008      0.049    -0.16    0.88       -0.103       0.088\n         age |      0.005      0.002     1.98    0.05        0.000       0.009\n  -----------+----------------------------------------------------------------\n  depress2   |\n    job_seek |     -0.240      0.028    -8.52    0.00       -0.295      -0.185\n       treat |     -0.056      0.045    -1.24    0.21       -0.144       0.032\n   econ_hard |      0.136      0.022     6.31    0.00        0.094       0.178\n         sex |      0.109      0.043     2.55    0.01        0.025       0.192\n         age |     -0.000      0.002    -0.22    0.82       -0.004       0.004\n------------------------------------------------------------------------------\n  \n\nSummary\nModels with indirect effects require careful theoretical consideration to employ for data analysis. However, if the model is appropriate for your data situation, it is quite easy to get results from a variety of packages in R. Furthermore, one does not need to use a structural equation modeling package to conduct an analysis with indirect effects, and in fact, one can get far using standard R syntax. For strictly observed, i.e. no latent, variables, no SEM tool is necessary, or even recommended.\n\\[\\mathcal{Enjoy\\ your\\ model\\ exploration!}\\]\nPackage comparison summarized\nThe following table may help one decide which package to use for their needs given their theoretical considerations.\n\n\n\n\nmediation\n\n\nlavaan\n\n\npiecewiseSEM\n\n\npsych\n\n\nbrms\n\n\nAutomatic\n\n\n•\n\n\n\n\n\n\n•\n\n\n•*\n\nMultiple Treatments☺\n\n•\n\n\n•\n\n\n•\n\n\n•\n\n\n•\n\n\nMultiple Mediators\n\n\n•\n\n\n•\n\n\n•\n\n\n•\n\n\n•\n\n\nMultiple Outcomes\n\n\n\n\n•\n\n\n•\n\n\n•\n\n\n•\n\n\nBeyond SLM†\n\n•\n\n\n•\n\n\n•\n\n\n\n\n•\n\n\nRandom Effects\n\n\n•\n\n\n•\n\n\n•\n\n\n\n\n•\n\n\nMissing Values\n\n\n\n\n•\n\n\n\n\n•*\n\n\n•\n\n\nLatent Variables\n\n\n\n\n•\n\n\n\n\n\n\n•*\n\n\n* approximately, with some caveats\n\n\n☺ May require rerunning aspects of the model\n\n\n† Standard linear model, as estimated by lm\n\n\n\nI have a much more detailed document on SEM, including mediation analysis.↩︎\nFor some reason you don’t see this in practice much, and one wonders what was done to make the data amenable to such a model if it wasn’t warranted.↩︎\nImai makes his articles available at his website.↩︎\nMASS has been superseded by others for over a decade at this point, and it mostly just tends to muck up your tidyverse and other packages when it’s loaded. It’s a fine package (and was great back in the day), but if you want to use it in a package, it would be good to not load it (or other packages) in the environment just to use a function or two. I mostly just see it used for mvrnorm (multivariate normal distribution) and glm.nb, but there are other packages with that functionality that would provide additional benefits, and not mask dplyr functions, which are among the most commonly used in the R community.↩︎\nbrms is working on it.↩︎\nThe options in the code are there to suppress/minimize what can be.↩︎\n",
    "preview": "posts/2019-03-12-mediation-models/../../img/mediation_blog_img.png",
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {},
    "preview_width": 2908,
    "preview_height": 1354
  },
  {
    "path": "posts/2018-09-23-using-radix/",
    "title": "Radix",
    "description": "Using Radix/Distill for Scientific Publishing or a Website",
    "author": [
      {
        "name": "Michael Clark",
        "url": "https://m-clark.github.io"
      }
    ],
    "date": "2018-09-23",
    "categories": [],
    "contents": "\nUpdate: RStudio has decided since this post to refer to the radix package as distill.\nI first came across Radix on the Distill journal website, and if you’ve ever read any of those articles, you’ve likely appreciated the layout and feel of them. Now the RStudio group has brought it to R markdown. It’s already proven a great format for scientific publishing, but you also have the ability to create a static website or blog with it as well. Previously I was using Dean Attali’s Beautiful Jekyll for my website. It’s a great approach and easy to use in general, at least just to get a decent looking website going without much effort. However, I found it somewhat difficult to keep the changes and updates in sync with my custom css. Radix appears to have a little less overhead, can create starter posts with a single function, works within the standard R Markdown approach, and has other built-in niceties (e.g. Creative commons support, easy citations, see below). And as a fan of Tufte’s books, I love asides (margin notes). Another nice thing is that one doesn’t have to work within the Jekyll framework. Not that I had to do anything with it directly, but I’d rather use the same approach I do with bookdown and other R markdown formats, and that’s pretty much how Radix works. I assume this would make it more portable also, or at least generalizable beyond the GitHub pages approach.\n\nHi! I’m an aside!\nOnce you install the radix package, you then have the option for creating an RStudio project with Radix templates for a standard website or blog. They aren’t really different, but if you want to blog you might as well go with that option. I actually went with the standard website first and added the blog elements after. For blog posts, you’ll need to knit the files to html before they will be processed when you build your site. There is some information specific to using GitHub pages, Netlify and others if you want to use those. You may find it easier to put files (e.g. images) in the folder specific to that post, but relative paths work also if you have a general site image folder.\n\nIf you have a lot of templates available you may not see the Radix ones without scrolling down.\nSome caveats. Radix for the R-verse is brand new, so there isn’t much documentation, or bells and whistles for that matter, though the latter isn’t necessarily a bad thing. I had issues trying to do a custom footer, because all that’s available is evidently to use raw html (really?), and what worked on the site pages didn’t work on the blog posts. One thing that tripped me up is that you have to knit the posts before it will be processed for the actual site, which one might forget to update the html even if the .Rmd file has been. Note that the preview of the site in the RStudio viewer may be off a bit at times, but it should look how you want when you actually publish.\nIn the end though, I was basically able to update my site to the Radix version without much effort, with less customization required, and now I can use my usual R markdown approach for documents and building the site.\nLearn more about using Radix at https://rstudio.github.io/distill.\n\n\n",
    "preview": {},
    "last_modified": "2021-01-26T17:25:36-05:00",
    "input_file": {}
  }
]
