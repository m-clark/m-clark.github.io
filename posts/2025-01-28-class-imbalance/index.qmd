---
title: Imbalanced Outcomes
subtitle: Challenges & Solutions for This Common Situation
date: 2025-04-7
tags: ['classification', 'imbalanced data', 'thresholds', 'roc', 'pr', 'cost-sensitive learning', 'resampling', 'ensemble methods', 'smote']
author: 
    - name: Michael Clark
      email: statsdatasci@gmail.com
      url: https://m-clark.github.io
      affiliation: OneSix
      affiliation-url: https://onesixsolutions.com
    # - name: Elizabeth Li
    #   email: elizabeth.li@onesixsolutions.com
    #   affiliation: OneSix
    #   affiliation-url: https://onesixsolutions.com
bibliography: ['../../bibs/class-imbalance.bib']
draft: true
categories:
    - Machine Learning
    - Regression
    - boosting
format: 
    html:
        html-table-processing: none
        # not convinced any image settings are applicable with jupyter engine
        fig-width: 8
        fig-height: 6
        default-image-extension: svg
execute: 
  echo: false
jupyter: m-clark.github.io
share:
    permalink: https://m-clark.github.io/posts/2025-01-28-class-imbalance/
    description: 'Imbalanced Outcomes: Challenges & Solutions for This Common Situation'
    divclass: 'share-buttons'
    linkedin: true
    bsky: true
    twitter: true
    email: true
    reddit: true
    facebook: true
nocite: |
    @shwartz-ziv_simplifying_2022, @foret_sharpness-aware_2021, @lin_focal_2018, @van_den_goorbergh_harm_2022, @brownlee_how_2018, @kuhn_optimizing_2014, @thomas_problem_2020, @nixon_measuring_2020, @google_classification_2025, @harrell_damage_2017
---

```{python}
#| echo: False
#| label: imports

import pandas as pd
import numpy as np

from great_tables import GT, style, loc
from plotnine import *
from plotnine.options import get_option

okabe_ito = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7']
import os



# Change the directory 
# os.chdir("../..") 

post_dir = 'posts/2025-01-28-class-imbalance/'
data_dir = 'data/class-imbalance/' # for interactive need ../../; 
np.set_printoptions(legacy='1.25') # to avoid np.float(x) wrapped around scalars

```



```{python}
#| results: hide
# theme_void
class theme_clean(theme):
    """
    A classic-looking theme, with x & y axis lines and
    no gridlines.

    Parameters
    ----------
    base_size : int
        Base font size. All text sizes are a scaled versions of
        the base font size.
    base_family : str
        Base font family.
    """

    def __init__(self, base_size=11, base_family=None):
        base_family = base_family or get_option("base_family")
        m = get_option("base_margin")
        font_size = base_size
        font_family=base_family

        theme.__init__(
            self,
            line=element_blank(),
            rect=element_blank(),
            text=element_text(
                family=font_family,
                face='plain',
                color='#4D4D4DFF',
                size=font_size,
                # ha='center',
                # va='center',
                # angle=0.0,
                # lineheight=0.9,
                margin={'t': 0.0, 'r': 0.0, 'b': 0.0, 'l': 0.0}
            ),
            axis_text_x=element_text(size = font_size * 0.8, va='top', ha='center'), # doc says float acceptable but errors
            axis_text_y=element_text(size = font_size * 0.8, ha='right'), # not clear what left and right mean for plotnine
            axis_title_x=element_text(
                ha='left',
                va='top',
                angle=0.0,
                size=font_size,
                face='bold'
            ),
            axis_title_y=element_text(
                va=1.0, # oh now it can take a float
                ha='left', # but not here, wtf?
                angle=0.0,
                size=font_size,
                family=font_family,
                face='bold'
            ),
            axis_ticks=element_blank(),
            # axis_ticks=element_line(color='#4D4D4DFF'),
            # axis_ticks_length=0,
            aspect_ratio=get_option("aspect_ratio"),
            dpi=get_option("dpi"),
            figure_size=get_option("figure_size"),
            legend_box_margin=0,
            legend_box_spacing=m * 3,
            legend_key_spacing_x=6,
            legend_key_spacing_y=2,
            legend_key_size=base_size * 1.5,
            legend_frame=element_blank(),
            # legend_ticks_length=0.2,
            legend_margin=0,
            # legend_position="right",
            legend_spacing=10,
            legend_text=element_text(
                size=base_size * 0.9,
                margin={
                    "t": m / 1.5,
                    "b": m / 1.5,
                    "l": m / 1.5,
                    "r": m / 1.5,
                    "units": "fig",
                },
            ),
            # legend_ticks=element_line(color="#CCCCCC", size=1),
            # legend_title=element_text(
            #     margin={
            #         "t": m,
            #         "b": m / 2,
            #         "l": m * 2,
            #         "r": m * 2,
            #         "units": "fig",
            #     },
            # ),
            legend_position='bottom',
            # legend_key=element_rect(fill=None, color='#fff'),
            # legend_box=element_blank(),
            legend_background=element_rect(fill=None, color='#fff'), # only way to remove box is color = background
            legend_title=element_blank(),
            panel_spacing=m,
            plot_margin=m *3, # required, and 0 isn't viable
            plot_title=element_text(
                color='#4D4D4DFF',
                size=font_size * 1.25,
                margin={'b': font_size * 0.3, 't': font_size * 0.3},
                family=font_family,
                face='bold'
            ),
            plot_subtitle=element_text(
                color='#4D4D4DFF',
                size=font_size,
                ha=0,
                family=font_family,
                face='bold'
            ),
            plot_caption=element_text(
                color='#4D4D4DFF',
                size=font_size * 0.8,
                ha=0,
                family=font_family,
                face='bold'
            ),
            plot_background=element_rect(fill='white', color=None),
            # plot_caption=element_text(
            #     size=base_size * 0.8,
            #     ha="right",
            #     va="bottom",
            #     ma="left",
            #     margin={"t": m, "units": "fig"},
            # ),
            # plot_subtitle=element_text(
            #     size=base_size * 1,
            #     va="top",
            #     ma="left",
            #     margin={"b": m, "units": "fig"},
            # ),
            # plot_title=element_text(
            #     size=base_size * 1.2,
            #     va="top",
            #     ma="left",
            #     margin={"b": m, "units": "fig"},
            # ),
            panel_background=element_rect(fill=None, color=None),
            panel_grid_major=element_line(color='#F2F2F2FF'),
            strip_align=0,
            strip_text=element_text(
                color="#1A1A1A",
                size=base_size * 0.8,
                linespacing=1.0,
                margin={
                    "t": 1 / 3,
                    "b": 1 / 3,
                    "l": 1 / 3,
                    "r": 1 / 3,
                    "units": "lines",
                },
            ),
            strip_background=element_blank(),
            complete=True,
        )


# theme_set(theme_clean()) # will result in error down the line
```

```{python}
# the docs on this are non-existent and I'm done trying for now. maybe you have to specify every single arg or something bc last it was failing on stuff I'm not even messing with
class theme_test(theme):
    def __init__(self, base_size=12.0, base_family='Roboto', center_axis_labels=False):
        base_family = base_family or get_option("base_family")
        m = get_option("base_margin")
        font_size = base_size
        font_family=base_family

        if center_axis_labels:
            haxis_just_x = 0.5
            vaxis_just_y = 0.5
            v_rotation_x = 0.0
            v_rotation_y = 0.0
        else:
            haxis_just_x = 0.0
            vaxis_just_y = 1.0
            v_rotation_x = 0.0
            v_rotation_y = 0.0


        # Use only inherited elements and make everything blank
        theme.__init__(
            self,
            aspect_ratio=get_option("aspect_ratio"),
            dpi=get_option("dpi"),
            figure_size=get_option("figure_size"),
            text=element_text(
                family=font_family,
                face='plain',
                color='#4D4D4DFF',
                size=font_size,
                ha='center',
                va='center',
                angle=0.0,
                lineheight=0.9,
                margin={'t': 0.0, 'r': 0.0, 'b': 0.0, 'l': 0.0},
                debug=False
            ),
            axis_title_x=element_text(
                ha=haxis_just_x,
                angle=v_rotation_x,
                size=1.2 * font_size,
                face='bold'
            ),
            axis_title_y=element_text(
                va=vaxis_just_y,
                ha=0.0,
                angle=v_rotation_y,
                size=1.2 * font_size,
                family=font_family,
                face='bold'
            ),
            axis_ticks=element_line(color='#4D4D4DFF'),
            plot_title=element_text(
                color='#4D4D4DFF',
                size=font_size * 1.25,
                margin={'b': font_size * 0.3},
                family=font_family,
                face='bold'
            ),
            plot_subtitle=element_text(
                color='#4D4D4DFF',
                size=font_size,
                ha=0,
                family=font_family,
                face='bold'
            ),
            plot_caption=element_text(
                color='#4D4D4DFF',
                size=font_size * 0.8,
                ha=0,
                family=font_family,
                face='bold'
            ),
            legend_position='bottom',
            legend_key=element_rect(fill='transparent', color=None),
            legend_background=element_rect(fill='transparent', color=None),
            legend_title=element_blank(),
            panel_background=element_rect(fill='transparent', color=None),
            panel_grid_major=element_line(color='#F2F2F2FF'),
            strip_background=element_blank(),
            plot_background=element_rect(fill='transparent', color=None),
            complete=True
        )

# theme_clean()
# theme_set(theme_clean())
```

> This post began around two years ago and was initially intended for the Strong Analytics blog, but for various reasons never came to fruition. We still thought the spirit of the content was valuable, so we did what we could to get it out one way or another. Both the intended audience and focus have changed since the initial drafts, but hopefully the end result is cohesive enough to be useful to a wide audience. Much gratitude is owed to Elizabeth Li for her contributions to the initial drafts, and editorial help beyond.


## Introduction

Imagine you're working on a fraud detection system for a bank. But out of millions of transactions, only a tiny fraction are fraudulent. This is a classic example of *class imbalance*, where one class we are trying to predict (fraudulent transactions) is much rarer than the other (legitimate transactions). Depending on your modeling approach, ignoring the imbalance can lead to the  predictions missing most of the fraudulent transactions, as well as other issues we'd like to avoid. Here we'll explore various issues regarding class imbalance, as well as techniques to address it. As we'll see, the problem is nuanced and, very often, simple approaches to deal with it can work very well.


### Goals

To guide our exploration, we have the following goals:

- Explain class imbalance generally
- Review examples and consequences of class imbalance
- Cover some common techniques for addressing the problem
- Investigate the effects of classification thresholds on performance metrics
- Explore a Case Study to see these concepts in action

By the end, you should have a good understanding of the challenges posed by imbalanced data, and how to address them in your own work.


## Understanding Imbalanced Data

The basic idea underpinning class imbalance is that one class is significantly less prevalent than the other, and this can happen for a variety of reasons, including the inherent nature of the data, or something about the way it was collected. For simplicity, we focus on the binary case, as it's easier to get an intuitive understanding of what 'imbalance' would mean in that setting. However, the concepts discussed here can extend to multiclass settings also. Check out the supplemental section (@sec-supp-multiclass) for a brief overview of the multiclass case.


In a typical classification task, we would use a default threshold of 0.5 to classify predicted probabilities into binary outcomes, with anything above the threshold going to one class and the rest to the other class. This approach works well when the classes are balanced, but it can fail in imbalanced settings where the minority class is underrepresented. For example, if less than 1% of the target data belongs to the minority class, a model trained without accounting for this imbalance may struggle to identify minority class instances effectively, if at all, and fall short for the task at hand.


### Real-world Examples

Class imbalance is common in many real-world scenarios, such as:

- Fraud detection: As in our opening example, the vast majority of credit card transactions are legitimate, but a small percentage are fraudulent.
- Click-through rate: Most users on a website do not click on ads, but a small percentage do.
- Disease detection: Consider classifying medical images for a rare disease. The vast majority of images may represent normal tissue, while only a small fraction show signs of the disease.
- Spam detection: In email datasets, the majority of emails are legitimate (non-spam), while only a small fraction are spam emails.


As one can see, it's quite common and natural to have data that is imbalanced at least to some extent. 


### Is imbalance a problem?

Is imbalance a problem? Well, not exactly, at least not inherently. The real challenge often arises from its implications, such as the sample size (of the minority class), model limitations, and the metrics we use to evaluate the model.

For example, let's say you had 10 million observations and 1% of them were in the minority class. Despite the imbalanced classes, you would likely have enough to work with to discover the feature-target relationships, and obtain good estimates of a variety of metrics even after test and validation splits. But with a smaller total sample or even rarer class prevalence, the problems of imbalance can be exacerbated due to the lack of data to work with, making it difficult to learn meaningful patterns or evaluate the model reliably.

In other cases, our model may not produce good predictions for the minority class, even if we have enough data. Maybe the model is not well suited to the problem, or the features aren't strong enough to make good classifications. In this case, even if your data was balanced you'd potentially still have poor performance, and it will likely be worse with imbalance.

And finally, some metrics, even common ones such as accuracy, can be deceptive in the imbalanced setting. As an example, they may reflect the majority class performance while ignoring the minority class. 

Addressing these challenges requires a thoughtful modeling approach and use of performance metrics, which we will explore in the following sections.


## Techniques for Dealing with Imbalanced Data

There are a number of techniques one can use for handling imbalanced data to help meet performance goals. Here, we will discuss a few commonly used methods, categorized into sampling-based techniques, algorithmic adjustments, and specialized approaches. We'll see some in action later with our case study.

### Sampling-based Techniques

Sampling-based techniques involve modifying the original dataset to balance the class distribution. These techniques can be broadly grouped into oversampling, undersampling, and hybrid sampling.

#### Oversampling

Oversampling is a technique where we increase the number of observations in the minority class. Most commonly this can be done by duplicating observations in the minority class or by generating synthetic observations. 

*Random over-sampling* (ROS) is akin to a bootstrap approach where we duplicate observations by sampling with replacement from the minority class, and by doing so increase the total number of observations. This is the simplest oversampling approach, and is often enough to improve model performance. Other more complicated oversampling techniques could be used, but are rarely going to do much better. But while effective, oversampling can lead to overfitting, as the model may learn to memorize the duplicated observations rather than generalize from them.

There are also several methods for generating synthetic observations. Popular techniques include *SMOTE* (Synthetic Minority Over-sampling Technique), along with SMOTE variants such as Borderline-SMOTE and *ADASYN* (Adaptive Synthetic Sampling). SMOTE generates synthetic observations by creating interpolations of nearest neighbors of the minority class observations. It can be useful if using a 'weak learner' for the model (e.g. a simple logistic regression), but generally does not lead to improvement for modeling techniques common in machine learning.

#### Undersampling

In contrast to oversampling, undersampling takes the opposite approach, reducing the number of observations in the majority class to make the target more balanced.

*Random under-sampling* (RUS) is a technique where we decrease the number of observations in the majority class by randomly selecting observations in the majority class to remove. The goal is again to create a more balanced dataset. While it can be effective in that regard, it can also lead to loss of important information, especially if the sample size is not very large to begin with.

Other techniques look at nearest neighbors of the majority class, i.e. observations that are very similar, and remove some of them assuming they are not adding useful information. Variations include approaches, e.g., *NearMiss*, that are similar to 'matched samples' techniques long employed in statistical settings.

:::{.column-margin}
A good package for implementing these techniques is the `imbalanced-learn` package in Python. The package is well-documented and easy to use, and is a good starting point for practitioners.
:::

#### Hybrid Sampling

Hybrid sampling is a technique where we combine oversampling and undersampling to create a balanced dataset. This is often done by oversampling the minority class and undersampling the majority class, and as before this could be done in a random fashion. An alternative hybrid sampling approach could also involve removing some observations from both classes that would be nearest neighbors (*TOMEK* links), which would hopefully improve the model's ability to classify the observations among those that remain, as they are more clearly delineated among the features.



### Algorithmic Techniques

We use the term 'algorithmic techniques' to refer to those that involve adjusting some aspect of the model estimation. Unlike sampling-based techniques, we do not manipulate the data directly.

#### Weighting

In some modeling contexts, we can adjust the importance of each example to handle the imbalance. The loss function is adjusted so that losses from the minority class contribute more than losses from the majority class. This is often done in the context of *cost-sensitive learning*, where the cost of misclassifying an observation in the minority class is higher than the cost of misclassifying an observation in the majority class. 

:::{.column-margin}
For some models, like logistic regression, weighting would essentially be equivalent to what is done with simple resampling techniques, negating the need to employ the latter. For example, doubling the weight of an observation would be equivalent to having two copies of the observation in the dataset. However, for other models, like tree-based methods, weighting and resampling are not equivalent. For example, in a random forest, resampling changes the composition of the bootstrap samples used to build the trees, while weighting changes the importance of each observation in the calculation of splits within each tree. As such, the difference in how the data is used can lead to different model behaviors and outcomes.
:::



#### Thresholding

Thresholding regards changing the classification cutoff to account for the imbalance in the data. The default threshold of 0.5 is an intuitive choice (@van_den_goorbergh_harm_2022), but is often not effective for imbalanced data. Instead we can use a lower threshold to potentially increase the number of positive classifications, or raise it to ensure we are only classifying the most confident observations as positive. 

Identifying an optimal threshold can significantly improve certain metrics we may be interested in, especially when we want to avoid certain types of errors. While a simple approach, it can actually be quite effective at achieving our goals in the context of imbalanced data, and there are a number of different techniques that can be used to identify the optimal threshold for our modeling purposes. Some of these are outlined in the [supplemental section](#sec-supp-thresh).



### Other Techniques

You can find other techniques that are used to handle imbalanced data for specific model settings. For example, in uplift modeling, the goal is to predict the difference in outcomes between two groups, and the data is often imbalanced. Some will use a *class variable transformation* to predict differences in outcomes between treatment and control groups.

In addition, some approaches have been used in the context of deep learning models. For example, some focus on using different loss functions like  *focal loss* (@lin_focal_2018) and *label smoothing*. Others try different optimization approaches like *sharpness-aware minimization* (SAM, see @foret_sharpness-aware_2021), data augmentation, and more (@shwartz-ziv_simplifying_2022 provides a nice overview). At this time, it's still difficult to know in advance whether deep learning models and associated methods for imbalance will work well for a typical tabular data setting, but they may be worth trying if you have the time and resources, and especially if your data is homogenous in nature[^homodata]. In general, you may need to try a few different approaches to see what works best for your data and your model. 

[^homodata]: Homogenous data has features that are generally from the same source and/or are the same type. For example, a dataset of images would be homogenous, as would a dataset of text. In those settings we likely would only be considering deep learning models, and so the techniques mentioned would be more applicable. In contrast, tabular data is often heterogeneous, with different types of features, and so the techniques mentioned [may not be as performant](https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/).



## Performance Metrics with Imbalanced Data

When evaluating classification models, performance metrics are essential for understanding how well a model is performing and for identifying areas for improvement. However, in the context of imbalanced data, these metrics can be misleading if not interpreted carefully. This is because many metrics are sensitive to the classification threshold, which determines how predicted probabilities are converted into class labels. So it's important to consider how thresholds impact both predictions and the metrics derived from them.

### The Role of Thresholds in Balancing Trade-offs

Classification models typically output predicted probabilities of the target classes. These probabilities can be used to create classifications, and those classifications form the basis for calculating a variety of performance metrics. As noted previously, when creating a classification label, we use a *threshold*, or cutoff point, to determine which class to assign to the observation. Anything beyond the threshold is classified as the positive class, and anything below is classified as the negative class. 

The choice of threshold directly affects the resulting classifications and, consequently, the performance metrics. For example, lowering the threshold can increase the number of positive predictions, which might help capture more true positives, but it can also lead to more false positives. Conversely, raising the threshold would likely result in fewer positive predictions, which can help reduce false positives but may also miss some true positives. 

It's important to note that we don't have an inherently right or wrong way to go here. For example:

- A lower threshold may be appropriate when missing positive cases (false negatives) is costly, such as in disease detection.
- A higher threshold may be better when false positives are more problematic, such as in fraud detection.

The impact of thresholds highlights the trade-offs inherent in classification tasks, and selecting a threshold that aligns with the specific goals and constraints of your application is critical. Ultimately, the choice of threshold should reflect the priorities of your use case. Now let's take a closer look at how thresholds can affect performance metrics.

### The Confusion Matrix

Many commonly used metrics such as accuracy, precision, recall, and F1 score, are derived from the *confusion matrix*. The confusion matrix is the initial summary of the relationship between the predicted and observed classes. However, any metric derived in this way will be sensitive to the classification threshold. 

On the other hand, metrics derived from predicted probabilities, such as area under a receiver operating curve (AUROC) and Brier score, are independent of the threshold, and can provide an additional view of model performance. These metrics are particularly valuable in imbalanced settings, where the threshold-dependent metrics may fail to capture the nuances of model behavior. Let's now see how this plays out in practice.


### Example: Classification with a .5 Threshold

In the following hypothetical example, the positive rate for the binary target is low - the true average underlying probability of the positive outcome is less than 0.1. Though in practice we'd not know the true probabilities, here we can visualize what they might look like in the following density plot, where the mean probability of the positive class is marked by the line. 

![Density plot of underlying true probabilities, imbalanced setting](../../img/class-imbalance/density.png){#fig-dens-true-prob}

Now assume we have a model that predicts probabilities for the target outcome. It may even produce probabilities that correspond well to the true probabilities, as shown in the following plot.

![Predicted vs. True Probabilities](../../img/class-imbalance/density_pred_vs_true.png){#fig-dens-pred-vs-true-prob}

Now, what might a confusion matrix for such data look like with a standard threshold of 0.5? In our hypothetical dataset, we have 100 total for the sample, but there are only five positive observations. Using the default threshold leads to ineffective classification of the minority class, as we only classified two observations as positive, and only one was correct. In this case our precision is 50%, meaning that half of the positive classifications were correct, but our recall is only 20%, meaning we only classified one out of five possible positive observations correctly.

```{python}
#| echo: False
#| label: tbl-confusion-matrix-1
#| tbl-cap: 'Confusion Matrix 1: Standard Threshold'

data = {
    "": ["Predicted -", "Predicted +", "Metrics"],
    "Observed -": [94, 1, "TNR: .99"],
    "Observed +": [4, 1, "TPR: .20"],
    "Metrics": ["NPV: .96", "PPV: .50", "ACC: .95"]
}

GT(pd.DataFrame(data)).opt_row_striping(row_striping=False)

```

[Abbreviations:]{style="font-size: 0.9em"}

- [*ACC*: Accuracy]{style="font-size: 0.8em"}
- [*TPR*: True Positive Rate/Sensitivity/Recall]{style="font-size: 0.8em"}
- [*TNR*: True Negative Rate/Specificity]{style="font-size: 0.8em"}
- [*PPV*: Positive Predictive Value/Precision]{style="font-size: 0.8em"}
- [*NPV*: Negative Predictive Value]{style="font-size: 0.8em"}

Although classifying one observation out of five true positive cases is not something we'd probably be satisfied with, the accuracy score is 95% because almost all the negative observations were correctly classified (true negative rate = 99%). Without any additional context, someone might think 95% accuracy was a good result, but it's actually no better than guessing, or just labeling all observations as the most common (negative) class, which would have also resulted in 95% accuracy. The other metrics in the confusion matrix like precision (PPV) and recall (TPR) provide the additional context that we need to understand the model's performance - namely, that we are not doing so well with the positive class, which is typically not a desirable outcome.


### Example: Classification with a Lower Threshold

The next plot shows different possible threshold values we could use for the predicted probabilities. With each threshold, we'd potentially create different classifications, resulting in a new confusion matrix and different metrics based on those predicted classes. 

![Predicted vs. true probabilities with different thresholds](../../img/class-imbalance/density_threshold.png){#fig-dens-thresholds}

Now let's see what happens to the performance metrics if we try using a lower classification threshold than 0.5. In this setting, the accuracy score has dropped from 95% to 84%, and the true negative rate has also decreased from 99% to 84%. However, the true positive rate has notably increased - from 20% to 80% - there are now four out of five possible correct positive classifications, though it did come at the cost of a higher false positive rate. The positive predictive value (PPV) has also dropped from 50% to 21%, meaning that only about one in five of the positive classifications were correct. This is a common trade-off in imbalanced settings, where increasing recall often comes at the cost of precision, and vice versa. In general, this result suggests that the model is now doing a better job of capturing the positive class. But while we gain in some metrics, we've lost ground in others. Again, it's a trade-off.


```{python}
#| echo: false
#| label: tbl-confusion-matrix-2
#| tbl-cap: 'Confusion Matrix 2: Lower Threshold'


data = {
    "": ["Predicted -", "Predicted +", "Metrics"],
    "Observed -": [80, 15, "TNR: .84"],
    "Observed +": [1, 4, "TPR: .80"],
    "Metrics": ["NPV: .99", "PPV: .21", "ACC: .84"]
}

GT(pd.DataFrame(data))

```



The following table provides some additional metrics to consider for the two confusion matrices. In particular we see the *F1 score*, the *balanced accuracy* score,  the area under a receiver operating curve (*AUROC*) score. These are common alternatives used in the imbalanced setting.


```{python}
#| echo: false
#| label: tbl-add-metrics
#| tbl-cap: Additional Metrics

data = {
    'Metric': ['F1', 'Bal. Acc.', 'AUROC'],
    'Conf. Matrix 1': ['.29', '.59', '.70'],
    'Conf. Matrix 2': ['.33', '.82', '.70']
}

(
    GT(pd.DataFrame(data))
    .cols_align(align='center', columns=['Conf. Matrix 1', 'Conf. Matrix 2'])
    .tab_source_note(
        source_note=
        'Confusion Matrix 1: Standard Threshold; Confusion Matrix 2: Lower Threshold'
    )
    .tab_style(
        style=style.text(color='gray25', size='0.8em'),
        locations=loc.footer()
    )
)


```

<!-- Note to self, though we have the simulated data that produced the plots, the actual data that produced the confusion matrices has been lost. All metrics are accurate for these matrices, but the AUROC is based on the simulated probabilities and test set observed y that are much larger in number than the confusion matrix. -->

The F1 score is the harmonic mean of precision and recall (true positive rate), and balanced accuracy is the average of the true positive rate and true negative rate, or the average recall for both positive and negative classes. They both are popular choices for imbalanced data, as they are less sensitive to the class imbalance than accuracy, but they are still sensitive to the threshold used to create the classifications. In this case, both improve with a lower threshold, though the F1 score only changes slightly. 

In contrast, the AUROC score is based on the predicted probabilities, and so is independent of the threshold, which is why it is constant in @tbl-add-metrics[^rocclass].  It is a useful metric in this context to consider because it considers the relationship between the true positive rate and true negative rate at many different thresholds, not just the single threshold you happened to select (@google_classification_2025). As such, this score is often a preferred metric, even in the balanced classification setting.

We've now seen a few performance metrics, but there are many available, and we'll see even more later. Each provides a different facet with which to view your model's performance. Therefore, you should decide which aspects of your model's performance are most important to your use case, and choose your metrics accordingly.

[^rocclass]: There is a way to approximate AUROC from the confusion matrix, but it is not exact, and would change with the threshold.  

## Case Study: Wine Quality Data

```{python}
#| echo: false
#| label: import-data-and-results
thresh_method = 'fbeta'
df_wine = pd.read_csv(data_dir + 'wine_quality_imbalance.csv')
df_metrics = pd.read_csv(data_dir + f'metrics_{thresh_method}.csv')

df_pred_probs = pd.read_csv(data_dir + 'predicted_probabilities.csv')
df_calibration = pd.read_csv(data_dir + 'calibration_curves.csv')

base_rate = df_wine['quality_89'].mean()
base_total = int(df_wine['quality_89'].sum())

train_test_indices = pd.read_csv(data_dir + 'train_test_indices.csv')
test_idx = train_test_indices.query('Dataset == "Test"')['Index'].values
test_total = int(df_wine.loc[test_idx, 'quality_89'].sum())

n_features = int(np.isin(df_metrics.columns,  'quality_89', invert=True).sum())
n_train = int(train_test_indices.query('Dataset == "Train"').shape[0])
n_test = int(train_test_indices.query('Dataset == "Test"').shape[0])
```


At this point, we have covered a lot of ground regarding imbalanced data. We have an idea of the concept generally, the different techniques we might use in the imbalanced setting, as well as the metrics we might use to evaluate our models. We've also seen how the choice of classification threshold can affect our model's performance metrics. To illustrate the concepts discussed in practice, we will demonstrate the application of various techniques for approaching imbalanced data. For the full notebook that produced these, see [this repo link](https://github.com/m-clark/m-clark.github.io/blob/master/posts/2025-01-28-class-imbalance/wine_quality.ipynb) and play around with it yourself.


### Data Setup

We will be using the popular wine quality data. The raw data and more detail is available at the [UCI Repository](https://archive.ics.uci.edu/ml/datasets/wine+quality). This combines two datasets, one for red wine and one for white wine. There are `{python} int(df_wine.shape[0])` total observations and `{python} n_features` features that regard different aspects of the wine, like acidity, pH, and alcohol content. We also include a binary feature for whether the color of the wine is red.


The original target is numeric, but for our purposes we are attempting to classify wines with a rating of 8 or 9 as 'high quality'. This represents `{python} base_total` high quality wines, about `{python} float(base_rate.round(2)) * 100`% of the data. Data was randomly split into a 75% training set and a 25% test set. The test set has `{python} test_total` high quality wines.

:::{.column-margin}
This is a good example of how imbalanced data can really make your data feel even smaller. We start with a decent amount of data, but only a small fraction of it is high quality wine, and our test set used for evaluation will only have `{python} test_total` high quality wines. This is not a lot to work with, and results in noisy metrics based on classification, where just a slight change in the predictions could lead to a noticeable change in the metric. For example, if we had an accuracy of say, 80%, the lower and upper bound estimate would be 66% and 90%, respectively, which is a wide range.
:::


### Models

We use two base models, a standard logistic regression model via [sklearn]{.pack} and a boosting model via [LightGBM]{.pack}. We will compare the performance of the following models:

- Basic approach that does nothing for imbalance and uses a threshold of 0.5
- Basic approach with an 'optimal' threshold
- Weighted (using `class_weight='balanced'` for the logistic and `is_unbalance=True` for the LGBM)[^wts]
- With resampling[^nosmote]

[^wts]: For the weighted models, these default weights are inversely proportional to the class frequencies, meaning the minority class is weighted more heavily while the majority class is weighted less heavily. This is a common default, but it can be adjusted as needed.

[^nosmote]: We don't look at SMOTE and related techniques, both to keep things simple, and because they rarely outperform simple resampling, especially with models that are typically strong to begin with, like boosting approaches. See @molnar_dont_2023 or @elor_smote_2022 for example.

The optimal threshold can be determined in a variety of ways, and one can look at the [supplemental section](#sec-supplemental) for more details on choosing the optimal threshold, and in particular how we did so for these results. The tables of results will identify the method used.

In addition, LGBM models were compared both with and without tuning. The latter case makes a more straightforward comparison since the parameters are identical in each case. But realistically, we would tune this type of model, so we include models tuned for number of estimators/trees, learning rate, and max depth[^tune].

[^tune]: In our exploratory phase we saw that increasing depth appeared to help oversample/weighting approaches (decreased mean prediction, lower brier, ACE, FPR), while less depth and fewer iterations appeared better for the thresholding approach (higher ROC, fewer total positive predictions for thresholding approach).


Metrics and other exploration that follows focuses on the test set. We will also look at the calibration curves for the models, which can help us understand how well the predicted probabilities align with the observed class proportions.


### Metrics

Let's first look at the metrics for the models. Along with several metrics we've already discussed, we've added a few others including:


- *Brier Score*: A measure of the mean squared difference between predicted probabilities and the true outcome. This is equivalent to the MSE for a binary target. Lower is better [see @harrell_damage_2017 for additional perspective].
- *Adaptive Calibration Error*: A measure of the calibration of the model. Lower is better [see @nixon_measuring_2020].
- *FPR*: False Positive Rate.
- *FNR*: False Negative Rate.

These metrics help us shift focus to the calibration of the model and penalties for misclassification. For each of these we'd prefer lower values. In addition, we looked at the *F2 Score*, a variant of the F-score that gives more weight to recall than precision. This is useful when false negatives are more costly than false positives, such as in fraud detection or disease diagnosis. As with F1, higher is better. We also used maximizing the F2 score to determine the optimal threshold for the models.

### Logistic Model Results

With these in mind, let's look at the logistic model results. We'll first focus on metrics that are based on the predicted classes. For metric definitions used in all the tables, see the [supplemental section](#sec-supp-metrics). Best values are bolded/blue.

```{python}
#| echo: false
#| label: metrics-logistic-setup
df_metrics_logistic = df_metrics[df_metrics['Model'].str.contains('Logistic')].drop(columns='Tuned').reset_index(drop=True)

# Index rows for precision, recall, f1, and roc that have the max value
max_indices = df_metrics_logistic[['Precision', 'Recall', 'F1 Score', 'F2 Score', 'AUROC', 'Avg Prec']].apply(lambda x: x[x == x.max()].index.to_list())

class_based = ['Model', 'Threshold', 'Mean Pred', 'Precision', 'Recall', 'F1 Score', 'F2 Score', 'FPR', 'FNR', 'Total Pos Pred', 'Threshold Method']

prob_based = ['Model', 'Threshold', 'Mean Pred', 'AUROC', 'Avg Prec', 'Brier Score', 'ACE']

max_precision, max_recall, max_f1, max_f2, max_roc, max_pr = max_indices

min_indices = df_metrics_logistic[['FPR', 'FNR', 'Brier Score', 'ACE']].apply(lambda x: x[x == x.min()].index.to_list())

min_fpr, min_fnr, min_brier, min_calib = min_indices['FPR'], min_indices['FNR'], min_indices['Brier Score'], min_indices['ACE']

thresh_value = float(df_metrics_logistic.query('Threshold != .5')['Threshold'].round(3))

mean_pred = df_metrics_logistic['Mean Pred'].round(2)
```

```{python}
#| echo: false
#| label: tbl-class-metrics-logistic
#| tbl-cap: 'Test Set Model Classification Metrics for Logistic Regression'

(
    GT(df_metrics_logistic[class_based])
    .fmt_number(decimals=3, columns=df_metrics_logistic[class_based].select_dtypes(include=np.float64).columns.to_list())
    # .tab_style(
    #     style=[
    #         style.text(color=okabe_ito[5], weight=800),
    #     ],
    #     locations=loc.column_labels() # can only specify all labels bc ....?
    # )
    .tab_style(
        style=style.text(color=okabe_ito[4], weight=800),
        locations=loc.body(
            columns=['Precision'],
            rows=max_precision
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Recall'],
            rows=max_recall
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F1 Score'],
            rows=max_f1
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F2 Score'],
            rows=max_f1
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FPR'],
            rows=min_fpr
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FNR'],
            rows=min_fnr
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```



The basic logistic with default threshold of 0.5 does very poorly, predicting only `{python} int(df_metrics_logistic.loc[0,'Total Pos Pred'])` positive case, and it doesn't even get that right! Shifting to a lower threshold for this model  (~`{python} thresh_value`) improves most metrics, and it's better than the weighted and resampling approaches on several metrics. However, the threshold improvement is no free lunch - we now classify `{python} int(df_metrics_logistic.loc[1,'Total Pos Pred'])` cases as positive, which is probably too many. As a result, the FPR is increased. 

The sampling and weighted approaches come to the same conclusions, which is expected for logistic regression. While both would be better than doing nothing at all, both predict far too many positive cases, ~9 times what's observed in the data! The mean predicted probability is also high (~`{python} f'{mean_pred[2]}'`), which is a sign that the model is overpredicting the minority class. However, this makes sense when considering the fact that the training data was resampled to a balanced state, or the model was estimated as if it were via weighting.


Now let's look at the metrics that are based on the predicted probabilities. Again, consult the [supplemental section](#sec-supp-metrics) for brief definitions of these metrics.

```{python}
#| echo: false
#| label: tbl-prob-metrics-logistic
#| tbl-cap: 'Test Set Model Probability Metrics for Logistic Regression'
(
    GT(df_metrics_logistic[prob_based])
    .fmt_number(decimals=3, columns=df_metrics_logistic[prob_based].select_dtypes(include=np.float64).columns.to_list())
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['AUROC'],
            rows=max_roc
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Avg Prec'],
            rows=max_roc
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Brier Score'],
            rows=min_brier
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['ACE'],
            rows=min_calib
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```


While AUROC and average precision aren't too different among the models, note how the Brier score and ACE are notably lower for the basic model. This suggests better calibration. Let's see what this looks like in the [calibration curves](https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html) and the predicted probabilities.

```{python}
#| echo: false
#| label: fig-calibration-logistic
#| fig-cap: Calibration Curves for Logistic Regression
# see yaml for fig opts or lack thereof
df_calib_logistic = df_calibration[df_calibration['Model'].str.contains('Logistic')]
df_calib_logistic['Dataset'] = pd.Categorical(df_calib_logistic['Dataset'], categories=['Train', 'Test'])

(
    ggplot(df_calib_logistic, aes(x='Prob Pred', y='Prob True', color='Dataset'))
    + geom_line()
    + geom_point()
    + geom_abline(linetype='dashed')
    + facet_wrap('~Model', scales='free_x')
    + scale_color_manual(values=[okabe_ito[0], okabe_ito[1]])
    + lims(y=(0, 1))
    + labs(title='Calibration Curves', x='Predicted Probability', y='Observed\nProportion')
    + theme_clean(base_size=10)
    + theme(
        axis_text_y=element_text(size = 6), # any hjust deviation from zero just slings it to the far left
        axis_title_y=element_text(size = 7, ha='left'), # any hjust deviation from zero just slings it to the far left
        # dpi=300, # setting this results in no ability to control size of the plot
    )
)
```

We can see in @fig-calibration-logistic that the basic model predicted probabilities are well-calibrated, which is also reflected in the notably lower Brier and ACE scores. Weighting and resampling lead to over-prediction of the minority class. We can see this by also examining the predicted probabilities directly, as in @fig-predicted-probs-logistic. Their mean prediction is well beyond the observed rate[^overweighted].

[^overweighted]: The oversampling and weighting approaches are overpredicting and producing nearly identical results, so are not easily distinguished in the visualization. Again this is expected for logistic regression, as weighting and oversampling are essentially equivalent in this case.

```{python}
#| echo: false
#| label: fig-predicted-probs-logistic
#| fig-cap: 'Predicted Probabilities for Logistic Regression'

df_pred_probs_logistic = df_pred_probs[df_pred_probs['Model'].str.contains('Logistic') & (df_pred_probs['Dataset'] == 'Test')]

(
    ggplot(df_pred_probs_logistic, aes(x='Predicted Probability', fill='Dataset'))
    + geom_density(
        aes(fill = 'Model', color = 'Model'),
        alpha=0.5,
        show_legend={'color': False}
    ) # color 
    + scale_fill_manual(values=[okabe_ito[0], okabe_ito[1], okabe_ito[2]], aesthetics=['fill', 'color'])
    # + scale_color_manual(values=[okabe_ito[0], okabe_ito[1], okabe_ito[2]])
    + labs(x='Predicted Probability', y='Density', title = 'Test Set Predicted Probability Distributions')
    + theme_clean()
    # + theme(legend_text=element_text(size=6))
)
```

### LGBM Model Results

Now let's look at the LGBM model results, starting with the classification metrics.


```{python}
#| echo: false
#| label: metrics-lgbm-setup
df_metrics_lgbm = df_metrics[df_metrics['Model'].str.contains('LGBM')].reset_index(drop=True)

# Index rows for precision, recall, f1, and roc that have the max value
max_indices = df_metrics_lgbm[['Precision', 'Recall', 'F1 Score', 'F2 Score', 'AUROC', 'Avg Prec']].apply(lambda x: x[x == x.max()].index.to_list())

class_based = ['Model', 'Tuned', 'Threshold', 'Mean Pred', 'Precision', 'Recall', 'F1 Score', 'F2 Score', 'FPR', 'FNR', 'Total Pos Pred', 'Threshold Method']

prob_based = ['Model', 'Tuned', 'Threshold', 'Mean Pred', 'AUROC', 'Avg Prec', 'Brier Score', 'ACE']

max_precision, max_recall, max_f1, max_f2, max_roc, max_pr = max_indices

min_indices = df_metrics_lgbm[['FPR', 'FNR', 'Brier Score', 'ACE']].apply(lambda x: x[x == x.min()].index.to_list())

min_fpr, min_fnr, min_brier, min_calib = min_indices['FPR'], min_indices['FNR'], min_indices['Brier Score'], min_indices['ACE']

# thresh_value = float(df_metrics_lgbm.query('Threshold != .5')['Threshold'].round(3))
```

```{python}
#| echo: false
#| label: tbl-class-metrics-lgbm
#| tbl-cap: 'Test Set Model Classification Metrics for LGBM MOdel'

(
    GT(df_metrics_lgbm[class_based])
    .fmt_number(decimals=3, columns=df_metrics_lgbm[class_based].select_dtypes(include=np.float64).columns.to_list())
    .tab_style(
        style=style.text(color=okabe_ito[4], weight=800),
        locations=loc.body(
            columns=['Precision'],
            rows=max_precision
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Recall'],
            rows=max_recall
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F1 Score'],
            rows=max_f1
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F2 Score'],
            rows=max_f2
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FPR'],
            rows=min_fpr
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FNR'],
            rows=min_fnr
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```



There's more to digest here, but in general we see the following:

- Unsurprisingly, these models did better than the logistic model approaches.
- The basic model with or without thresholding/tuning did better on several metrics than the weighted and resampling approaches.
- The issues with weighting and resampling were far less pronounced in the LGBM models, but were still present. For example, the mean predictions are higher than the basic models, and higher than the base rate, except for the over-sampled with tuning model.
- We also saw that for this data, the thresholding approach still led to far more predicted positives than would be expected, and with it, a higher false positive rate.

A couple other things to note. One model has perfect precision! However it comes at the cost of the highest false negative rate/lowest recall. In addition, our optimal threshold was chosen based on the value that maximized F2, but that model did not have the highest F2 score of all models. They did have the highest recall however, which is what F2 puts more weight on.

Now let's look at the probability-based metrics.

```{python}
#| echo: false
#| label: tbl-prob-metrics-lgbm
#| tbl-cap: 'Test Set Model Probability Metrics for LGBM Model'

(
    GT(df_metrics_lgbm[prob_based])
    .fmt_number(decimals=3, columns=df_metrics_lgbm[prob_based].select_dtypes(include=np.float64).columns.to_list())
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['AUROC'],
            rows=max_roc
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Avg Prec'],
            rows=max_pr
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Brier Score'],
            rows=min_brier
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['ACE'],
            rows=min_calib
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```

Much the same story - neither weighting nor over-sampling helped much here. Again, thresholding isn't applicable for these metrics, and the basic model generally does well. 


## Discussion

In general, any of these model approaches might be viable, as the choice may depend mostly on your tolerance for certain types of classification errors. For example, if you are more concerned with false positives, i.e. you really don't want to classify a wine as good if it's not, you might \*not\* prefer the thresholding models, or maybe use a thresholding approach that emphasizes precision. On the other hand, if you are more concerned with correctly judging a good wine as good, you might prefer the thresholding LGBM models that did better on recall.

It's worth pointing out that many metrics are very correlated with, or even directly dependent, upon one another. For example, if positive predictions are increased with a lower threshold, then the true positive rate will potentially increase, but the false positive rate might increase as well. Any metrics that are based on these rates will also necessarily change. This is why it's important to consider the actual classifications and the trade-offs that come with them. Choosing a model based on one metric may be essentially equivalent to choosing one based on another metric.

Note again that several of the metrics are based on the predicted probabilities, not the classifications, so the reported value will not change with the classification threshold. For model selection, you might be better off using these, or the standard loss function for the model, rather than the classification metrics that can be easily manipulated. This would be more the case in smaller data settings or very rare positive rates, where metric values could swing wildly with only slightly different classifications.


## Summary

In this post, we explored various techniques for handling imbalanced data, including sampling-based and algorithmic methods. We demonstrated the impact of different classification thresholds on performance metrics, and discussed the importance of selecting appropriate metrics for the situation. Our case study on wine quality data highlighted the practical application of these techniques and their effect on model performance. In general, we saw that simple approaches often work well.


Key takeaways:

- Imbalanced data is common in real-world settings, and can lead to issues with various model performance metrics.
- There are a number of ways to approach imbalanced data, including sampling-based and algorithmic techniques.
- Using the default classification threshold of 0.5 may not be appropriate for classification with imbalanced data, and selecting an optimal threshold may prove beneficial.
- Depending on the setting, *you can do nothing and possibly be just fine*. But if you need to do something, simple approaches often do the job.
- With metrics that depend on classification, you need to carefully consider the trade-offs that come with them.
- Focusing on metrics that don't rely on classification threshold can potentially help you avoid the pitfalls associated with class imbalance.

In conclusion, while class imbalance may present challenges for model development and evaluation, the solutions need not be complex. The key is to understand your specific use case—what types of errors are most costly, what metrics align with your goals, and what trade-offs you're willing to make. Hopefully this exploration has provided you with practical insights and a clear framework for tackling imbalanced data in your own work.



## Supplemental {#sec-supplemental}


### Table Metrics {#sec-supp-metrics}

The first table for logistic and LGBM models shows the following metrics. An arrow (↑ or ↓) indicates whether a higher or lower value is better for the metric.

- *Threshold*: the threshold value used for classification.
- *Mean Pred*: the mean predicted probability for the positive class. Ideally this should be close to the observed proportion of positive cases.
- *Precision*: the proportion of correctly predicted positive cases among all predicted positive cases. ↑
- *Recall*: the proportion of correctly predicted positive cases among all actual positive cases. ↑
- *F1 Score*: the harmonic mean of precision and recall. ↑
- *F2 Score*: a variant of the F1 score that gives more weight to recall. ↑
- *FPR*: False Positive Rate, the proportion of incorrectly predicted positive cases among all actual negative cases. ↓
- *FNR*: False Negative Rate, the proportion of incorrectly predicted negative cases among all actual positive cases. ↓
- *Total Pos Pred*: the total number of positive predictions.
- *Threshold Method*: the method used to select the classification threshold.

The second table shows the following metrics:

- *Threshold*: the threshold/cutoff value used for classification.
- *Mean Pred*: the mean predicted probability for the positive class.
- *AUROC*: the area under the receiver operating characteristic curve. ↑
- *Avg Prec*: the average precision (area under the precision-recall curve). ↑
- *Brier Score*: a measure of the mean squared difference between predicted probabilities and the true outcome. ↓
- *ACE*: the adaptive calibration error (@nixon_measuring_2020). ↓




### Choosing a Classification Threshold {#sec-supp-thresh}

 As we have noted, the default threshold of 0.5 is often used as a classification threshold, but it may not be optimal in many cases, particularly in the imbalanced setting. Each of these and other similar methods have their own advantages and may be suitable for different scenarios. Here are some common methods for selecting a classification threshold:


1. **Metric Optimization**: Choose a threshold that maximizes or minimizes a specific metric:
   - **F-Beta Scores**: Adjust the β parameter to weight the relative importance of precision vs. recall. $F_1$ gives equal weight, $F_2$ favors recall, and $F_{.5}$ favors precision.
   - **Precision-Recall Trade-off**: Select a threshold based on the precision-recall curve to achieve desired balance between these metrics.
   - **ROC-based**: Choose the threshold that maximizes the geometric mean of sensitivity and specificity.


2. **Cost-Sensitive Thresholding**: This involves selecting a threshold that minimizes the overall cost of misclassification. This can be done by assigning different weights to false positives and false negatives and choosing the threshold that results in the lowest weighted error. This does not require an algorithmic approach though, as you can just select a threshold based on domain knowledge.

3. **Other Statistical Approaches**:
   - **Youden's J Statistic**: Youden's J is defined as sensitivity + specificity - 1. This approach maximizes the sum of sensitivity and specificity, and is equivalent to maximizing vertical distance from ROC curve to diagonal.
   - **Equal Error Rate (EER)**: Sets threshold where false positive rate equals false negative rate.

4. **Data-driven**: Use the observed class proportion (base rate) as the threshold, which may be appropriate 
when you want predictions to reflect the natural class distribution. This is simple and good for a quick baseline, but may not be optimal in all cases.

For this post, we explored using different thresholds out of curiosity, and here are the results for the LGBM (Basic) models, and only those metrics that are actually affected by the threshold. The different methods explored included optimizing AUROC (`ROC`), AUPRC (`PRC`), F.5, F1, and F2 (`F*`), and simply using the base rate/observed proportion as a cutoff (`Obs. Prop.`). In the following, the untuned model parameters are identical, and the tuned models are also identical. Within each of those groups, the only difference is the threshold method used. All metrics regard the test set.


```{python}
#| label: supp-thresh-methods-data-import
df_metrics_roc = pd.read_csv(data_dir + 'metrics_roc.csv')
df_metrics_f_5 = pd.read_csv(data_dir + 'metrics_f.5.csv')
df_metrics_f1 = pd.read_csv(data_dir + 'metrics_f1.csv')
df_metrics_f2 = pd.read_csv(data_dir + 'metrics_f2.csv')
df_metrics_pr = pd.read_csv(data_dir + 'metrics_pr.csv')
df_metrics_obsprop = pd.read_csv(data_dir + 'metrics_obsprop.csv')

df_metrics_pr.loc[df_metrics_pr['Threshold Method'].notna(), 'Threshold Method'] = 'prc'
```

```{python}
#| label: tbl-threshold-methods
#| tbl-cap: 'Comparison of Threshold Methods for the LGBM Models'
dfs = [df_metrics_roc, df_metrics_f_5, df_metrics_f1, df_metrics_f2, df_metrics_pr, df_metrics_obsprop]

df_combined = pd.concat([
    (
        df[df['Model'].str.contains('LGBM(.)*\(T\)')]
        [['Threshold', 'Precision', 'Recall', 'F.5 Score', 'F1 Score', 'F2 Score', 'FPR', 'FNR', 'Total Pos Pred', 'Tuned', 'Threshold Method']]

    )
    for df in dfs
])

df_combined.loc[df_combined['Threshold Method'] == 'fbeta', 'Threshold Method'] = 'f2'
df_combined['Threshold Method'] = df_combined['Threshold Method'].str.upper()
df_combined.loc[df_combined['Threshold Method'] == 'OBSPROP', 'Threshold Method'] = 'Obs. Prop.'

num_cols = df_combined.select_dtypes(include=np.float64).columns.to_list()
GT(df_combined.sort_values('Tuned')).fmt_number(columns=num_cols, decimals=2)
```

In our case study, thresholding that was relatively more focused on precision (F1/AUPRC) led to a high threshold, few positive predictions, and a low false positive rate. Thresholding that was relatively more focused on recall (F2) led to a notably lower threshold than .5, more positive predictions, and a higher false positive rate. The geometric mean/ROC approach led to the most positive predictions, and thus a notably high false positive rate, but also a lower false negative rate. By coincidence, the ROC and F2 approaches led to the same classification for the tuned model. 

We ultimately chose the F2 threshold approach for the reported tables because we were interested in recall, and the ROC approach led to too many positives for the untuned model. However, there's no reason for this choice other than our personal preference, and the other approaches could be just as valid depending on your own goals for wine classification 🍷.


### Brief Discussion of the Multiclass Setting {#sec-supp-multiclass}


While we've focused on binary classification, many real-world problems involve more than two classes. Multiclass settings can present additional challenges with imbalanced data, as the relationships between classes become more complex and the interpretation of performance metrics needs adjustment.

In multiclass settings, we often use the same core metrics as in binary classification (precision, recall, F1, etc.), but we need to consider how to aggregate these metrics across classes. Two common approaches are:

**Weighted Average**: This approach takes into account the number of instances in each class. Each class's metric is weighted by the proportion of instances in that class. It may be useful when you want to account for class imbalance to ensure that metrics from larger classes have a greater impact on the overall score.

**Macro Average**: This approach calculates the metric independently for each class and then takes the average of these metrics. Each class is treated equally, regardless of its size. It is useful when you want to ensure that the model performance assessment is consistent across all classes, rather than being dominated by the performance on the more frequent classes.

As in the binary case, the choice between these approaches depends on your specific goals. Use weighted averaging when you want metrics to reflect the natural class distribution, and macro averaging when you want to ensure consistent performance across all classes.
