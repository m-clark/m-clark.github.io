---
title: Imbalanced Outcomes
subtitle: Challenges & Solutions for This Common Situation
date: 2025-01-28
tags: ['classification', 'imbalanced data', 'thresholds', 'roc', 'pr', 'cost-sensitive learning', 'resampling', 'ensemble methods', 'smote']
author: 
    - name: Michael Clark
      email: statsdatasci@gmail.com
      url: https://m-clark.github.io
      affiliation: OneSix
      affiliation-url: https://onesixsolutions.com
    - name: Elizabeth Li
      email: elizabeth.li@onesixsolutions.com
      affiliation: OneSix
      affiliation-url: https://onesixsolutions.com
bibliography: ['../../bibs/class-imbalance.bib']
draft: true
categories:
    - Machine Learning
    - Regression
    - boosting
format: 
    html:
        html-table-processing: none
        # not convinced any image settings are applicable with jupyter engine
        fig-width: 8
        fig-height: 6
        default-image-extension: svg
execute: 
  echo: false
jupyter: m-clark.github.io
share:
    permalink: https://m-clark.github.io/posts/2025-01-28-class-imbalance/
    description: 'Imbalanced Outcomes: Challenges & Solutions for This Common Situation'
    divclass: 'share-buttons'
    linkedin: true
    bsky: true
    twitter: true
    email: true
    reddit: true
    facebook: true
nocite: |
    @shwartz-ziv_simplifying_2022, @foret_sharpness-aware_2021, @lin_focal_2018, @van_den_goorbergh_harm_2022, @brownlee_how_2018, @kuhn_optimizing_2014, @thomas_problem_2020, @nixon_measuring_2020, @google_classification_2025, @harrell_damage_2017
---


```{python}
#| echo: False
#| label: imports

import pandas as pd
import numpy as np

from great_tables import GT, style, loc
from plotnine import *
from plotnine.options import get_option

okabe_ito = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7']
import os



# Change the directory 
# os.chdir("../..") 

post_dir = 'posts/2025-01-28-class-imbalance/'
data_dir = 'data/class-imbalance/' # for interactive need ../../; 

```



```{python}
#| results: hide
# theme_void
class theme_clean(theme):
    """
    A classic-looking theme, with x & y axis lines and
    no gridlines.

    Parameters
    ----------
    base_size : int
        Base font size. All text sizes are a scaled versions of
        the base font size.
    base_family : str
        Base font family.
    """

    def __init__(self, base_size=11, base_family=None):
        base_family = base_family or get_option("base_family")
        m = get_option("base_margin")
        font_size = base_size
        font_family=base_family

        theme.__init__(
            self,
            line=element_blank(),
            rect=element_blank(),
            text=element_text(
                family=font_family,
                face='plain',
                color='#4D4D4DFF',
                size=font_size,
                # ha='center',
                # va='center',
                # angle=0.0,
                # lineheight=0.9,
                margin={'t': 0.0, 'r': 0.0, 'b': 0.0, 'l': 0.0}
            ),
            axis_text_x=element_text(size = font_size * 0.8, va='top', ha='center'), # doc says float acceptable but errors
            axis_text_y=element_text(size = font_size * 0.8, ha='right'), # not clear what left and right mean for plotnine
            axis_title_x=element_text(
                ha='left',
                va='top',
                angle=0.0,
                size=font_size,
                face='bold'
            ),
            axis_title_y=element_text(
                va=1.0, # oh now it can take a float
                ha='left', # but not here, wtf?
                angle=0.0,
                size=font_size,
                family=font_family,
                face='bold'
            ),
            axis_ticks=element_blank(),
            # axis_ticks=element_line(color='#4D4D4DFF'),
            # axis_ticks_length=0,
            aspect_ratio=get_option("aspect_ratio"),
            dpi=get_option("dpi"),
            figure_size=get_option("figure_size"),
            legend_box_margin=0,
            legend_box_spacing=m * 3,
            legend_key_spacing_x=6,
            legend_key_spacing_y=2,
            legend_key_size=base_size * 1.5,
            legend_frame=element_blank(),
            # legend_ticks_length=0.2,
            legend_margin=0,
            # legend_position="right",
            legend_spacing=10,
            legend_text=element_text(
                size=base_size * 0.9,
                margin={
                    "t": m / 1.5,
                    "b": m / 1.5,
                    "l": m / 1.5,
                    "r": m / 1.5,
                    "units": "fig",
                },
            ),
            # legend_ticks=element_line(color="#CCCCCC", size=1),
            # legend_title=element_text(
            #     margin={
            #         "t": m,
            #         "b": m / 2,
            #         "l": m * 2,
            #         "r": m * 2,
            #         "units": "fig",
            #     },
            # ),
            legend_position='bottom',
            # legend_key=element_rect(fill=None, color='#fff'),
            # legend_box=element_blank(),
            legend_background=element_rect(fill=None, color='#fff'), # only way to remove box is color = background
            legend_title=element_blank(),
            panel_spacing=m,
            plot_margin=m *3, # required, and 0 isn't viable
            plot_title=element_text(
                color='#4D4D4DFF',
                size=font_size * 1.25,
                margin={'b': font_size * 0.3, 't': font_size * 0.3},
                family=font_family,
                face='bold'
            ),
            plot_subtitle=element_text(
                color='#4D4D4DFF',
                size=font_size,
                ha=0,
                family=font_family,
                face='bold'
            ),
            plot_caption=element_text(
                color='#4D4D4DFF',
                size=font_size * 0.8,
                ha=0,
                family=font_family,
                face='bold'
            ),
            plot_background=element_rect(fill='white', color=None),
            # plot_caption=element_text(
            #     size=base_size * 0.8,
            #     ha="right",
            #     va="bottom",
            #     ma="left",
            #     margin={"t": m, "units": "fig"},
            # ),
            # plot_subtitle=element_text(
            #     size=base_size * 1,
            #     va="top",
            #     ma="left",
            #     margin={"b": m, "units": "fig"},
            # ),
            # plot_title=element_text(
            #     size=base_size * 1.2,
            #     va="top",
            #     ma="left",
            #     margin={"b": m, "units": "fig"},
            # ),
            panel_background=element_rect(fill=None, color=None),
            panel_grid_major=element_line(color='#F2F2F2FF'),
            strip_align=0,
            strip_text=element_text(
                color="#1A1A1A",
                size=base_size * 0.8,
                linespacing=1.0,
                margin={
                    "t": 1 / 3,
                    "b": 1 / 3,
                    "l": 1 / 3,
                    "r": 1 / 3,
                    "units": "lines",
                },
            ),
            strip_background=element_blank(),
            complete=True,
        )


# theme_set(theme_clean()) # will result in error down the line
```

```{python}
# the docs on this are non-existent and I'm done trying for now. maybe you have to specify every single arg or something bc last it was failing on stuff I'm not even messing with
class theme_test(theme):
    def __init__(self, base_size=12.0, base_family='Roboto', center_axis_labels=False):
        base_family = base_family or get_option("base_family")
        m = get_option("base_margin")
        font_size = base_size
        font_family=base_family

        if center_axis_labels:
            haxis_just_x = 0.5
            vaxis_just_y = 0.5
            v_rotation_x = 0.0
            v_rotation_y = 0.0
        else:
            haxis_just_x = 0.0
            vaxis_just_y = 1.0
            v_rotation_x = 0.0
            v_rotation_y = 0.0


        # Use only inherited elements and make everything blank
        theme.__init__(
            self,
            aspect_ratio=get_option("aspect_ratio"),
            dpi=get_option("dpi"),
            figure_size=get_option("figure_size"),
            text=element_text(
                family=font_family,
                face='plain',
                color='#4D4D4DFF',
                size=font_size,
                ha='center',
                va='center',
                angle=0.0,
                lineheight=0.9,
                margin={'t': 0.0, 'r': 0.0, 'b': 0.0, 'l': 0.0},
                debug=False
            ),
            axis_title_x=element_text(
                ha=haxis_just_x,
                angle=v_rotation_x,
                size=1.2 * font_size,
                face='bold'
            ),
            axis_title_y=element_text(
                va=vaxis_just_y,
                ha=0.0,
                angle=v_rotation_y,
                size=1.2 * font_size,
                family=font_family,
                face='bold'
            ),
            axis_ticks=element_line(color='#4D4D4DFF'),
            plot_title=element_text(
                color='#4D4D4DFF',
                size=font_size * 1.25,
                margin={'b': font_size * 0.3},
                family=font_family,
                face='bold'
            ),
            plot_subtitle=element_text(
                color='#4D4D4DFF',
                size=font_size,
                ha=0,
                family=font_family,
                face='bold'
            ),
            plot_caption=element_text(
                color='#4D4D4DFF',
                size=font_size * 0.8,
                ha=0,
                family=font_family,
                face='bold'
            ),
            legend_position='bottom',
            legend_key=element_rect(fill='transparent', color=None),
            legend_background=element_rect(fill='transparent', color=None),
            legend_title=element_blank(),
            panel_background=element_rect(fill='transparent', color=None),
            panel_grid_major=element_line(color='#F2F2F2FF'),
            strip_background=element_blank(),
            plot_background=element_rect(fill='transparent', color=None),
            complete=True
        )

# theme_clean()
# theme_set(theme_clean())
```

## Introduction

Imagine you're working on a fraud detection system for a bank. But out of millions of transactions, only a tiny fraction are fraudulent. This is a classic example of *class imbalance*, where one class (fraudulent transactions) is much rarer than the other (legitimate transactions). Ignoring this imbalance can lead to the model predictions missing most of the fraudulent transactions. In this blog post, we'll explore various issues regarding imbalance, as well as techniques to address it. As we'll see, the problem is nuanced and, very often, simple approaches work very well.




### Goals

To guide our exploration, we have set the following goals:

- Explain class imbalance generally
- Review examples and consequences of class imbalance
- Cover some common techniques
    - Sampling-based
    - Algorithmic
- Demonstrate what happens when different classification thresholds are used
- Explore the effects of classification thresholds on performance metrics



## Understanding Imbalanced Data

As an example, consider a binary target outcome with groups A and B. In the typical data setting, we might simply run the model and create a binary classification for any predicted probabilities above a threshold of .5. If you had collected data and half of the target observations came from group A and the other from group B, then your approach would be appropriate to use a 0.5 classification threshold when classifying the target as A or B.  With a good enough model, your predictions should match the underlying observed data. 

Now consider the following scenario. If you were collecting data and less than 1% of the target data came from group A and all the rest of the data came from group B, then you would likely want your model to appropriately account for the lower prevalence rate of group A in the target. However, the status quo approach of  using a 0.5 threshold will not achieve this because it assumes a balanced outcome. So what are we to do?  First, let's get a better understanding of our situation.


### Real-world Examples

It's not difficult to find examples of class imbalance in the real world. Here are a few examples:

- Fraud detection: The vast majority of credit card transactions are legitimate, but a small percentage are fraudulent.
- Disease detection: Most people do not have a rare disease, but a small percentage do. Even common diseases are rare for certain populations.
- Click-through rate prediction: Most users do not click on ads, but a small percentage do.

As one can see, it's fairly easy to come up with situations where one class is much more prevalent than the other. 


### Is imbalance a problem?

Well, not exactly, as we'll see. For one thing, we are referring to the binary case specifically, as it's easier to get an intuitive understanding of 'imbalance'. If we have more than two classes, we can still have imbalance, but it's not as easy to define unless we specifically know the expected class distribution very well. So for this post, we will focus on the binary case.

Rather than the problem being imbalance itself, the problem often resides in the sample size (of the minority class), models we use, and the metrics we use to evaluate the model. For example, if you had 10 million observations and 1% of them were in the minority class, you would likely have enough to work with to discover the feature-target relationships. You would have 100,000 observations in the minority class to work with, and this should allow for good estimates of a variety of metrics even after test and validation splits. But with a smaller total sample or even rarer class prevalence, the problems of imbalance can be exacerbated due to the lack of data to work with.

In other cases, our model may not produce good predictions for the minority class, even if we have enough data. Maybe the model is not well suited to the problem, or the features aren't strong enough to make good classifications. In this case, even if your data was balanced you'd potentially still have poor performance.

And finally, some metrics are not as useful by themselves in the presence of imbalance. We'll dive more deeply into this issue later, but for now, let's consider some common techniques for handling imbalanced data.


## Techniques for Dealing with Imbalanced Data

There are a number of techniques one can use for handling imbalanced data to help meet performance goals. Here, we will discuss a few commonly used methods.

### Sampling-based Techniques

Sampling-based techniques involve modifying the original dataset to balance the class distribution. These techniques can be broadly categorized into oversampling, undersampling, and hybrid sampling.

#### Oversampling

Oversampling is a technique where we increase the number of observations in the minority class. Most commonly this can be done by duplicating observations in the minority class or by generating synthetic observations. 

*Random over-sampling* (ROS) is akin to a bootstrap approach where we duplicate observations by sampling with replacement from the minority class to increase the total number of observations. This is the simplest oversampling approach and is often enough to improve model performance. Other more complicated sampling techniques could be used, but are rarely going to improve performance.

There are also several methods for generating synthetic observations. Popular techniques include *SMOTE* (Synthetic Minority Over-sampling Technique), along with SMOTE variants such as *ADASYN* (Adaptive Synthetic Sampling) and Borderline-SMOTE. SMOTE generates synthetic observations by creating interpolations of nearest neighbors of the minority class observations. It can be useful if using a weak learner for the model (e.g. a simple logistic regression), but generally does not lead to improvement for modeling techniques common in machine learning.

#### Undersampling

*Random under-sampling* (RUS) is a technique where we decrease the number of observations in the majority class by randomly selecting observations in the majority class to remove. These techniques look at nearest neighbors of the majority class and remove some of them. Other variations include different approaches to some sort of 'matched samples' techniques long employed in statistical settings, e.g., *NearMiss*.

:::{.column-margin}
A good package for implementing these techniques is the `imbalanced-learn` package in Python. The package is well-documented and easy to use, and is a good starting point for anyone looking to implement these techniques in their own work.
:::

#### Hybrid Sampling

Hybrid sampling is a technique where we combine oversampling and undersampling to create a balanced dataset. This is often done by oversampling the minority class and undersampling the majority class, and as before this could be done in a random fashion. An alternative hybrid sampling approach could also involve removing some observations from both classes that would be nearest neighbors (*TOMEK* links), which would hopefully improve the model's ability to classify the observations among those that remain, as they are more clearly delineated among the features.


### Algorithmic Techniques

We use the term 'algorithmic' techniques to refer to those that involve adjusting some aspect of the model estimation. Unlike resampling, we do not manipulate the data directly.

#### Weighting

In some modeling contexts, we can adjust the importance of each example to handle the imbalance. The loss function is adjusted so that losses from the minority class contribute more than losses from the majority class. This is often done in the context of *cost-sensitive learning*, where the cost of misclassifying an observation in the minority class is higher than the cost of misclassifying an observation in the majority class. 

:::{.column-margin}
For some models, like logistic regression, weighting would essentially be equivalent to what is done with simple resampling techniques, negating the need to employ the latter. For example, doubling the weight of an observation would be equivalent to having two copies of the observation in the dataset. However, for other models, like tree-based methods, weighting and resampling are not equivalent. For example, in a random forest, resampling changes the composition of the bootstrap samples used to build the trees, while weighting changes the importance of each observation in the calculation of splits within each tree. As such, the difference in how the data is used can lead to different model behaviors and outcomes.
:::



#### Thresholding

Thresholding regards changing the cutoff to account for the imbalance in the data. For example, instead of using a default threshold of 0.5, we can use a lower threshold to increase the number of positive classifications. We can use different thresholds to better match the imbalanced data, or guard against certain types of errors. While a simple approach, it can actually be quite effective at achieving our goals in the context of imbalanced data, and there are a number of different techniques that can be used to identify the optimal threshold for our modeling purposes.



#### Other

You can still find other techniques that are used to handle imbalanced data for specific model settings. For example, in uplift modeling, the goal is to predict the difference in outcomes between two groups, and the data is often imbalanced. Some have tried *class variable transformation* that involves a specific feature that is usually a binary treatment vs. control categorization. This approach changes the binary outcome to a numeric one, and then uses a regression model to predict the difference in outcomes.

In addition, some techniques have been used in the context of deep learning models, focusing on different losses like  *focal loss* (@lin_focal_2018) and *label smoothing*. Others try different optimization approaches like *sharpness-aware minimization* (SAM, see @foret_sharpness-aware_2021), data augmentation techniques, and more (@shwartz-ziv_simplifying_2022 provides a nice overview).

It's still difficult to know that these models/techniques will work in practice for a typical tabular data setting, but may be worth trying if you have the time and resources, and especially if your data is homogenous in nature. In general, you may need to try a few different techniques to see what works best for your data and your model. 



## Prediction with Imbalanced Data

As mentioned, changing the classification threshold is a common approach when dealing with imbalanced data. This approach is especially appealing because it is not only relatively simple to implement, but also because it involves training the model on the observed prevalence rates of the target classes, which is beneficial for model generalization. We can compare it to sampling based methods, which seek to modify the prevalence rates in the model data to improve performance. 

When thinking about threshold modification, although the default classification threshold of 0.5 is commonly used, nothing requires this other than it is an intuitive choice (@van_den_goorbergh_harm_2022).  And the default is appropriate if you have evenly split data. For example, let's say you were collecting survey data, and half of the binary target data came from group A and the other from group B, and you ultimately wanted your model to predict the group membership. Then you would be correct to use your model to make balanced classifications. Also, if we artificially create a balanced target, through resampling for example, then the threshold of 0.5 is appropriate, at least for training.

But the default threshold is not set in stone, and identifying an optimal threshold for an imbalanced binary target can prove useful. Using the previous example, if your survey data had less than 1% of the binary target coming from group A and all the rest coming from group B, then you likely would want your classification to somehow account for this, as some model settings may not even produce predicted probabilities that exceed the default 0.5 threshold. sEven with resampling techniques, your test or future data would still have the same imbalance, yet the model would be trained on balanced data, and this may cause problems down the line, as we will see.

To better understand what's going on, let's turn our attention to commonly used performance metrics for classification models, and how they are affected by the classification threshold.


## Performance Metrics with Imbalanced Data

Evaluating the performance of models trained on imbalanced data requires careful consideration of various metrics. These metrics help us understand how well the model is performing and where it might be falling short, and typically provide the means for preferring one modeling approach over others. Some metrics are based on the confusion matrix, which pits predicted classes against observed target classes, making these metrics sensitive to the classification threshold. Other metrics are instead derived from the predicted probabilities themselves, and so are not dependent on the classification threshold.


### The Confusion Matrix

Selecting the appropriate classification threshold for our data is not only important for producing useful predicted classes, but also for correctly assessing our model's performance based on what's most important to us. Some of the most common metrics are pulled from the confusion matrix, which pits predicted classes vs. observed outcome classes. Naturally, these predicted classes are dependent on the classification threshold used, such that certain thresholds can lead to unexpected results. For example, the model could end up with a nearly perfect accuracy score, but no predictive power as evidenced by very low recall scores. In other cases, results may not be statistically different from guessing the more dominant class. Below are two examples that demonstrate the impact that the classification threshold has on performance metrics.



#### Example: Classification with a .5 Threshold

In the following example, the positive rate for the binary target is low - the true average underlying probability of the positive outcome is less than 0.1. We can visualize the true probabilities in the following density plot, where the mean probability of the positive class is marked by the orange line. 

![Density plot of underlying true probabilities, imbalanced setting](../../img/class-imbalance/density.png){#fig-dens-true-prob}

What might a confusion matrix for such data look like with a standard threshold of 0.5? In this example, there are only five positive observations out of the total 100, and since the default threshold ineffectively classifies the observations, we only correctly classify one positive observation.

```{python}
#| echo: False
#| label: tbl-confusion-matrix-1
#| tbl-cap: 'Confusion Matrix 1: Standard Threshold'

data = {
    "": ["Predicted -", "Predicted +", "Metrics"],
    "Observed -": [94, 1, "TNR: .99"],
    "Observed +": [4, 1, "TPR: .20"],
    "Metrics": ["NPV: .96", "PPV: .50", "ACC: .95"]
}

GT(pd.DataFrame(data)).opt_row_striping(row_striping=False)

```

Abbreviations

- *ACC*: Accuracy
- *TPR*: True Positive Rate/Sensitivity/Recall
- *TNR*: True Negative Rate/Specificity
- *PPV*: Positive Predictive Value/Precision
- *NPV*: Negative Predictive Value

In terms of the underlying probabilities, even if our predicted probabilities match up well, we may not be able to classify the positive observations correctly using the standard threshold.

![Predicted vs. True Probabilities](../../img/class-imbalance/density_pred_vs_true.png){#fig-dens-pred-vs-true-prob}

Although classifying one observation out of five is not necessarily good, the overall accuracy score is 95% because almost all the negative observations were correctly classified. Without any additional context, someone might think this accuracy score was a good result, but it's actually no better than guessing, or just labeling all observations as the most common class. Similar to the accuracy score, but unlike the true positive rate, all the other metrics in the confusion matrix show that we really only get the negatives right, which is typically not a  desirable outcome.


#### Example: Classification with a Lower Threshold

The next plot shows different possible threshold values for the predicted probabilities. With each threshold, we'd potentially create different classifications, resulting in different metrics based on our predicted classification. 

![Predicted vs. true probabilities with different thresholds](../../img/class-imbalance/density_threshold.png){#fig-dens-thresholds}

Now let's see what happens to the performance metrics if we try using a lower classification threshold. The confusion matrix below results from using a lower classification threshold. The accuracy score has dropped from 95% to 84%, and the true negative rate has also decreased from 99% to 84%. However, the true positive rate has notably increased - from 20% to 80% - there are now four out of five possible correct positive classifications, and both the positive and negative predictive values have also increased. Even though accuracy and the true negative rate have dropped, this increase in the other scores suggests that the model is now doing a better job of capturing the positive class. 


```{python}
#| echo: false
#| label: tbl-confusion-matrix-2
#| tbl-cap: 'Confusion Matrix 2: Lower Threshold'


data = {
    "": ["Predicted -", "Predicted +", "Metrics"],
    "Observed -": [80, 15, "TNR: .84"],
    "Observed +": [1, 4, "TPR: .80"],
    "Metrics": ["NPV: .99", "PPV: .79", "ACC: .84"]
}

GT(pd.DataFrame(data))

```

#### Additional Metrics to Consider

In addition to the confusion matrix itself, other metrics are commonly used to assess a classification model's performance, including the *F1 score*, the *balanced accuracy* score, and the area under a receiver operating curve (*AUROC*) score, and others.  We show how these metrics are, or are not, affected by the classification threshold below. 


```{python}
#| echo: false
#| label: tbl-add-metrics
#| tbl-cap: Additional Metrics

data = {
    'Metric': ['F1', 'Bal. Acc.', 'AUROC'],
    'Conf. Matrix 1': ['.29', '.59', '.83'],
    'Conf. Matrix 2': ['.26', '.72', '.83']
}

(
    GT(pd.DataFrame(data))
    .cols_align(align='center', columns=['Conf. Matrix 1', 'Conf. Matrix 2'])
    .tab_source_note(
        source_note=
        'Confusion Matrix 1: Standard Threshold; Confusion Matrix 2: Lower Threshold'
    )
    .tab_style(
        style=style.text(color='gray25', size='0.8em'),
        locations=loc.footer()
    )
)
```

F1 and balanced accuracy are both derived from confusion matrix metrics, making them dependent on the chosen classification threshold. The F1 score calculation uses precision and recall, and balanced accuracy uses recall and the true negative rate. As such, these are liable to change if the classification threshold changes, which is seen in the comparison of the two confusion matrices above.

The AUROC score, however, is based on the predicted probabilities, and so is independent of the chosen classification threshold, which is why it is constant in @tbl-add-metrics.  It is an especially beneficial score to consider because it relies on the predicted probabilities in its calculation, and considers the relationship between the true positive rate and true negative rate at many different thresholds, not just the single threshold you happened to select (@google_classification_2025). Therefore, the AUROC score will not change if the classification threshold changes, rather it will only change if the model itself changes. As such, this score is often a preferred metric even in the balanced classification setting.

Although we have chosen to discuss only a few common performance metrics in this post, there are several different performance metrics available to you, and we'll see even more later. Each performance metric reports on a different facet of your model's performance. Therefore, you should decide which aspects of your model's performance are most important to your use case, and choose your metrics accordingly.


## Case Study: Wine Quality Data

```{python}
#| echo: false
#| label: import-data-and-results
thresh_method = 'fbeta'
df_wine = pd.read_csv(data_dir + 'wine_quality_imbalance.csv')
df_metrics = pd.read_csv(data_dir + f'metrics_{thresh_method}.csv')

df_pred_probs = pd.read_csv(data_dir + 'predicted_probabilities.csv')
df_calibration = pd.read_csv(data_dir + 'calibration_curves.csv')

base_rate = df_wine['quality_89'].mean()
base_total = int(df_wine['quality_89'].sum())

train_test_indices = pd.read_csv(data_dir + 'train_test_indices.csv')
test_idx = train_test_indices.query('Dataset == "Test"')['Index'].values
test_total = int(df_wine.loc[test_idx, 'quality_89'].sum())

n_features = int(np.isin(df_metrics.columns,  'quality_89', invert=True).sum())
n_train = int(train_test_indices.query('Dataset == "Train"').shape[0])
n_test = int(train_test_indices.query('Dataset == "Test"').shape[0])
```


To illustrate the concepts discussed, we will demonstrate the application of various techniques for approaching imbalanced data. For the full notebook that produced this post, see [this repo link](https://github.com/m-clark/m-clark.github.io/blob/master/posts/2025-01-28-class-imbalance/wine_quality.ipynb). 


### Data Setup

We will be using the popular wine quality data. The raw data and more detail is available at the [UCI Repository](https://archive.ics.uci.edu/ml/datasets/wine+quality). This combines two datasets, one for red wine and one for white wine. There are `{python} int(df_wine.shape[0])` total observations and `{python} n_features` features that regard different aspects of the wine, like acidity, pH, and alcohol content. We also include a binary feature for whether the color of the wine is red.


The original target is numeric, but for our purposes we are attempting to classify wines with a rating of 8 or 9 as high quality. This represents `{python} base_total` high quality wines, about `{python} float(base_rate.round(2)) * 100`% of the data. Data was randomly split into a 75% training set and a 25% test set. The test set has `{python} test_total` high quality wines.

:::{.column-margin}
This is a good example of how imbalanced data can really make your data feel even smaller. We start with a decent amount of data, but only a small fraction of it is high quality wine, and our test set will only have `{python} test_total` high quality wines. This is not a lot to work with, and results in noisy metrics based on classification, where just a slight change in the predictions could lead to a noticeable change in the classification metric. For example, if we had an accuracy of say, 80%, the lower and upper bound estimate would be 66% and 90%, respectively, which is a wide range.
:::


### Models

We use two base models, a standard logistic regression model via [sklearn]{.pack} and a boosting model via [LightGBM]{.pack}. We will compare the performance of the following scenarios:

- Basic approach that does nothing for imbalance and uses a threshold of 0.5
- Basic approach with an 'optimal' threshold
- Weighted (using `class_weight='balanced'` for the logistic and `is_unbalance=True` for the LGBM)[^wts]
- With resampling[^nosmote]

The optimal threshold can be determined in a variety of ways. See the [supplemental section](#sec-supplemental) for more details on choosing the optimal threshold. The results tables will identify the method used.

In addition, LGBM models were compared both with and without tuning. The latter case makes a more straightforward comparison since the parameters are identical in each case. But realistically, we would tune this type of model, so we include models tuned for number of estimators/trees, learning rate, and max depth[^tune].

[^tune]: In our exploratory phase we saw that increasing depth appeared to help oversample/weighting (decreased mean prediction, lower brier, ACE, FPR), while less depth and fewer iterations appeared better for the thresholding approach (higher ROC, fewer total pos pred for thresholding approach).


Metrics and other exploration focuses on the test set. We will also look at the calibration curves for the models, which can help us understand how well the predicted probabilities align with the observed class proportions.

[^wts]: For the weighted models, these default weights are inversely proportional to the class frequencies, meaning the minority class is weighted more heavily while the majority class is weighted less heavily. This is a common default, but it can be adjusted as needed.

[^nosmote]: We don't look at SMOTE and related techniques, both to keep things simple, and because they rarely outperform simple resampling, especially with models that are typically strong to begin with, like boosting approaches. See @molnar_dont_2023 or @elor_smote_2022 for example.


### Metrics

Let's first look at the metrics for the models. Along with several metrics we've already discussed, we've added a few others including:

- *Brier Score*: A measure of the mean squared difference between predicted probabilities and the true outcome. This is equivalent to the MSE for a binary target. Lower is better [see @harrell_damage_2017 for additional perspective].
- *Adaptive Calibration Error*: A measure of the calibration of the model. Lower is better [see @nixon_measuring_2020].
- *FPR*: False Positive Rate.
- *FNR*: False Negative Rate.

These metrics help us shift focus to the calibration of the model and penalties for misclassification. For each of these we'd prefer lower values.

### Logistic Model Results

With these in mind, let's look at the logistic model results. We'll first focus on metrics that are based on the predicted classes.

```{python}
#| echo: false
#| label: metrics-logistic-setup
df_metrics_logistic = df_metrics[df_metrics['Model'].str.contains('Logistic')].drop(columns='Tuned').reset_index(drop=True)

# Index rows for precision, recall, f1, and roc that have the max value
max_indices = df_metrics_logistic[['Precision', 'Recall', 'F1 Score', 'F2 Score', 'AUROC', 'Avg Prec']].apply(lambda x: x[x == x.max()].index.to_list())

class_based = ['Model', 'Threshold', 'Mean Pred', 'Precision', 'Recall', 'F1 Score', 'F2 Score', 'FPR', 'FNR', 'Total Pos Pred', 'Threshold Method']

prob_based = ['Model', 'Threshold', 'Mean Pred', 'AUROC', 'Avg Prec', 'Brier Score', 'ACE']

max_precision, max_recall, max_f1, max_f2, max_roc, max_pr = max_indices

min_indices = df_metrics_logistic[['FPR', 'FNR', 'Brier Score', 'ACE']].apply(lambda x: x[x == x.min()].index.to_list())

min_fpr, min_fnr, min_brier, min_calib = min_indices['FPR'], min_indices['FNR'], min_indices['Brier Score'], min_indices['ACE']

thresh_value = float(df_metrics_logistic.query('Threshold != .5')['Threshold'].round(3))

mean_pred = df_metrics_logistic['Mean Pred'].round(2)
```

```{python}
#| echo: false
#| label: tbl-class-metrics-logistic
#| tbl-cap: 'Test Set Model Classification Metrics for Logistic Regression'

(
    GT(df_metrics_logistic[class_based])
    .fmt_number(decimals=3, columns=df_metrics_logistic[class_based].select_dtypes(include=np.float64).columns.to_list())
    # .tab_style(
    #     style=[
    #         style.text(color=okabe_ito[5], weight=800),
    #     ],
    #     locations=loc.column_labels() # can only specify all labels bc ....?
    # )
    .tab_style(
        style=style.text(color=okabe_ito[4], weight=800),
        locations=loc.body(
            columns=['Precision'],
            rows=max_precision
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Recall'],
            rows=max_recall
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F1 Score'],
            rows=max_f1
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F2 Score'],
            rows=max_f1
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FPR'],
            rows=min_fpr
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FNR'],
            rows=min_fnr
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```



The basic logistic with default threshold of 0.5 does very poorly, predicting only `{python} int(df_metrics_logistic.loc[0,'Total Pos Pred'])` positive case, and it doesn't even get that right. Changing to a more appropriate threshold for this model  (~`{python} thresh_value`) improves most metrics, and it's better than the weighted and resampling approaches on several metrics. However, the threshold improvement is no free lunch - we now classify `{python} int(df_metrics_logistic.loc[1,'Total Pos Pred'])` cases as positive, which is probably too many. As a result, the FPR is increased. 

The sampling and weighted approaches come to the same conclusions, which is expected for logistic regression. While both would be better than doing nothing at all, both predict far too many positive cases, ~9 times what's observed in the data! The mean predicted probability is also high (~`{python} mean_pred[2]`), which is a sign that the model is overpredicting the minority class. However, this makes sense when considering the fact that the training data was resampled to a balanced state, or the model was estimated as if it were via weighting.


Now let's look at the metrics that are based on the predicted probabilities.

```{python}
#| echo: false
#| label: tbl-prob-metrics-logistic
#| tbl-cap: 'Test Set Model Probability Metrics for Logistic Regression'
(
    GT(df_metrics_logistic[prob_based])
    .fmt_number(decimals=3, columns=df_metrics_logistic[prob_based].select_dtypes(include=np.float64).columns.to_list())
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['AUROC'],
            rows=max_roc
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Avg Prec'],
            rows=max_roc
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Brier Score'],
            rows=min_brier
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['ACE'],
            rows=min_calib
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```


While AUROC and average precision aren't too different among the models, note how the Brier score and ACE are notably lower for the basic model. This suggests better calibration. Let's see what this looks like in the calibration curves and predicted probabilities.

```{python}
#| echo: false
#| label: fig-calibration-logistic
#| fig-cap: Calibration Curves for Logistic Regression
# see yaml for fig opts or lack thereof
df_calib_logistic = df_calibration[df_calibration['Model'].str.contains('Logistic')]
df_calib_logistic['Dataset'] = pd.Categorical(df_calib_logistic['Dataset'], categories=['Train', 'Test'])

(
    ggplot(df_calib_logistic, aes(x='Prob Pred', y='Prob True', color='Dataset'))
    + geom_line()
    + geom_point()
    + geom_abline(linetype='dashed')
    + facet_wrap('~Model', scales='free_x')
    + scale_color_manual(values=[okabe_ito[0], okabe_ito[1]])
    + lims(y=(0, 1))
    + labs(title='Calibration Curves', x='Predicted Probability', y='Observed\nProportion')
    + theme_clean(base_size=10)
    + theme(
        axis_text_y=element_text(size = 6), # any hjust deviation from zero just slings it to the far left
        axis_title_y=element_text(size = 7, ha='left'), # any hjust deviation from zero just slings it to the far left
    )
)
```

We can see in @fig-calibration-logistic that the basic model predicted probabilities are well-calibrated, which is also reflected in the notably lower Brier and ACE scores. Weighting and resampling lead to over-prediction of the minority class. We can see this by also examining the predicted probabilities themselves, as in @fig-predicted-probs-logistic. Their mean prediction is well beyond the observed rate[^overweighted].

[^overweighted]: The oversampling and weighting approaches are overpredicting are producing nearly identical results, so are not easily distinguished in the visualization.

```{python}
#| echo: false
#| label: fig-predicted-probs-logistic
#| fig-cap: 'Predicted Probabilities for Logistic Regression'

df_pred_probs_logistic = df_pred_probs[df_pred_probs['Model'].str.contains('Logistic') & (df_pred_probs['Dataset'] == 'Test')]

(
    ggplot(df_pred_probs_logistic, aes(x='Predicted Probability', fill='Dataset'))
    + geom_density(
        aes(fill = 'Model', color = 'Model'),
        alpha=0.5,
        show_legend={'color': False}
    ) # color 
    + scale_fill_manual(values=[okabe_ito[0], okabe_ito[1], okabe_ito[2]], aesthetics=['fill', 'color'])
    # + scale_color_manual(values=[okabe_ito[0], okabe_ito[1], okabe_ito[2]])
    + labs(x='Predicted Probability', y='Density', title = 'Test Set Predicted Probability Distributions')
    + theme_clean()
    # + theme(legend_text=element_text(size=6))
)
```

### LGBM Model Results

Now let's look at the LGBM model results, starting with the classification metrics.


```{python}
#| echo: false
#| label: metrics-lgbm-setup
df_metrics_lgbm = df_metrics[df_metrics['Model'].str.contains('LGBM')].reset_index(drop=True)

# Index rows for precision, recall, f1, and roc that have the max value
max_indices = df_metrics_lgbm[['Precision', 'Recall', 'F1 Score', 'F2 Score', 'AUROC', 'Avg Prec']].apply(lambda x: x[x == x.max()].index.to_list())

class_based = ['Model', 'Tuned', 'Threshold', 'Mean Pred', 'Precision', 'Recall', 'F1 Score', 'F2 Score', 'FPR', 'FNR', 'Total Pos Pred', 'Threshold Method']

prob_based = ['Model', 'Tuned', 'Threshold', 'Mean Pred', 'AUROC', 'Avg Prec', 'Brier Score', 'ACE']

max_precision, max_recall, max_f1, max_f2, max_roc, max_pr = max_indices

min_indices = df_metrics_lgbm[['FPR', 'FNR', 'Brier Score', 'ACE']].apply(lambda x: x[x == x.min()].index.to_list())

min_fpr, min_fnr, min_brier, min_calib = min_indices['FPR'], min_indices['FNR'], min_indices['Brier Score'], min_indices['ACE']

# thresh_value = float(df_metrics_lgbm.query('Threshold != .5')['Threshold'].round(3))
```

```{python}
#| echo: false
#| label: tbl-class-metrics-lgbm
#| tbl-cap: 'Test Set Model Classification Metrics for LGBM MOdel'

(
    GT(df_metrics_lgbm[class_based])
    .fmt_number(decimals=3, columns=df_metrics_lgbm[class_based].select_dtypes(include=np.float64).columns.to_list())
    .tab_style(
        style=style.text(color=okabe_ito[4], weight=800),
        locations=loc.body(
            columns=['Precision'],
            rows=max_precision
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Recall'],
            rows=max_recall
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F1 Score'],
            rows=max_f1
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['F2 Score'],
            rows=max_f1
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FPR'],
            rows=min_fpr
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['FNR'],
            rows=min_fnr
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```



There's more to digest here, but in general we see the following:

- Unsurprisingly, these models did better than the logistic model approaches.
- The basic model with or without thresholding/tuning did better on several metrics than the weighted and resampling approaches.
- The issues with weighting and resampling were far less pronounced in the LGBM models, but were still present. For example, the mean predictions are higher than the basic models, and higher than the base rate, except for the over-sampled and tuned model.
- We also saw that for this data, the thresholding approach still led to far more predicted positives than would be expected, and with it, a higher false positive rate.

Now let's look at the probability-based metrics.

```{python}
#| echo: false
#| label: tbl-prob-metrics-lgbm
#| tbl-cap: 'Test Set Model Probability Metrics for LGBM Model'

(
    GT(df_metrics_lgbm[prob_based])
    .fmt_number(decimals=3, columns=df_metrics_lgbm[prob_based].select_dtypes(include=np.float64).columns.to_list())
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['AUROC'],
            rows=max_roc
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Avg Prec'],
            rows=max_roc
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['Brier Score'],
            rows=min_brier
        )
    )
    .tab_style(
        style=[
            style.text(color=okabe_ito[4], weight=800),
        ],
        locations=loc.body(
            columns=['ACE'],
            rows=min_calib
        )
    )
    .tab_source_note(        
        '(T) means that classification is based on an optimal threshold, but the model is the same as the corresponding "Basic" model. See the supplemental section for details.'
    )
)
```

Much the same story. Again, thresholding isn't applicable for these metrics, and the basic model generally does well.


## Discussion

In general, any of these model approaches might be viable, as the choice may depend on your tolerance for certain types of classification errors. For example, if you are more concerned with false positives, i.e. you really don't want to classify a wine as good if it's not, you might \*not\* prefer the thresholding models, or maybe use a thresholding approach that emphasizes precision. On the other hand, if you are more concerned with misjudging a good wine as bad, you might prefer the thresholding models that emphasize recall.

It's worth pointing out that many metrics are very correlated with or even dependent upon one another. For example, if positive predictions are increased with a lower threshold, then the true positive rate will potentially increase, but so will the false positive rate. Any metrics that are based on these rates will also necessarily change. This is why it's important to consider the actual classifications and the trade-offs that come with them. Choosing a model based on one metric may be essentially equivalent to choosing based on another metric.

Note that the actual AUROC and average precision values are based on the predicted probabilities, not the classifications, so the reported value will not change with the classification threshold. Likewise Brier score and ACE are based on the predicted probabilities, so they will not change with the classification threshold. For model selection, you might be better off using these or the standard loss function for the model, rather than the classification metrics that can be easily manipulated. This would be more the case in smaller data settings or very rare positive rates, where metric values could swing wildly with only slightly different classifications.


## Summary

In this post, we explored various techniques for handling imbalanced data, including sampling-based and algorithmic methods. We demonstrated the impact of different classification thresholds on performance metrics, and discussed the importance of selecting appropriate metrics for the situation. Our case study on wine quality data highlighted the practical application of these techniques and their effectiveness in improving model performance. In general, we saw that simple approaches often work well.


Key takeaways:

- Imbalanced data is common in real-world settings and can lead to issues with model performance for various metrics.
- There are a number of ways to approach imbalanced data, including sampling-based and algorithmic techniques.
- Using the default classification threshold of 0.5 may not be appropriate for classification with imbalanced data, and selecting an optimal threshold may prove beneficial.
- Depending on the setting, *you can do nothing and possibly be just fine*. But if you need to do something, simple approaches often do the job.
- You really need to consider the actual classifications and trade-offs that come with them.
- Focusing on metrics that don't rely on classification threshold can potentially help you avoid the pitfalls associated with class imbalance.

In the end, the best approach will depend on your specific use case and the trade-offs you are willing to make, which reflect misclassification costs and the importance you place on different types of errors. We hope this post has provided you with a better understanding of how to approach imbalanced data and improve the interpretation of model performance.


## Supplemental {#sec-supplemental}


### Choosing a Classification Threshold {#sec-supp-thresh}

Choosing an appropriate classification threshold can be important for classification in the imbalanced data setting. The default threshold of 0.5 is often used, but it may not be optimal in many cases. Here are some common methods for selecting a classification threshold:

1. **Maximizing Common Metrics**: One approach is to choose a threshold that maximizes a specific performance metric, such as the F1 score, precision, recall, or the area under the receiver operating characteristic curve (AUROC). Common metrics to maximize include:
    - **Area Under Precision-Recall Curve/Average Precision**: The precision-recall curve shows the trade-off between precision and recall for different thresholds. You can select a threshold that achieves a desired balance between precision and recall, depending on your specific goals.
    - **F-Beta Score**: The $F_1$ score is the harmonic mean of precision and recall. It is particularly useful when you want to balance false positives and false negatives. You can calculate the $F_1$ score for different thresholds and select the one that maximizes it. By changing a $\beta$ parameter, you can adjust the balance between precision and recall. For example, $F_2$ score gives more weight to recall than precision, while $F_{.5}$ score gives more weight to precision than recall.
    - **Geometric mean/(ROC) Curve**: This approach provides a balance between sensitivity (true positive rate) and specificity (true negative rate) by taking the square root of their product. This metric is particularly helpful when you want to ensure that both sensitivity and specificity are reasonably high, avoiding the scenario where one is high at the expense of the other. While typically applied in this fashion relative to AUROC, it can also be relative to AUPRC (i.e. the Area Under the Precision-Recall Curve). For the latter case, it is very similar to F1 score.


2. **Cost-Sensitive Thresholding**: In some applications, the costs of false positives and false negatives are different. Cost-sensitive thresholding involves selecting a threshold that minimizes the overall cost of misclassification. This can be done by assigning different weights to false positives and false negatives and choosing the threshold that results in the lowest weighted error. This does not require an algorithmic approach, you can just select a threshold based on domain knowledge.

3. **Youden's J Statistic**: This method involves selecting the threshold that maximizes Youden's J statistic, which is defined as sensitivity + specificity - 1. This approach aims to find a balance between sensitivity (true positive rate) and specificity (true negative rate). It is equivalent to finding the threshold on the ROC that is farthest from the diagonal line.

4. **Equal Error Rate (EER)**: The EER is the point at which the false positive rate equals the false negative rate. This threshold can be useful in situations where you want to balance the rates of false positives and false negatives.


5. **Calibration Plots**: Calibration plots can help in selecting a threshold by showing how well the predicted probabilities align with the actual outcomes. A well-calibrated model will have predicted probabilities that match the observed frequencies. By examining the calibration plot, you can choose a threshold that aligns with your desired level of confidence in the predictions.

Each of these methods has its own advantages and may be suitable for different scenarios. The choice of method will depend on the specific requirements of your application and the trade-offs you are willing to make between different types of errors. 

For this post, we explored using different thresholds out of curiosity, and here are the results. We only show the LGBM (Basic) models, and only those metrics that are actually affected by the threshold. We also show the threshold method used for each model, which includes optimizing AUROC (`roc`), AUPRC (`prc`), F1, and F2 (`fbeta`). In the following, the untuned models are identical, and the tuned models are also identical. Within each of those groups, the only difference is the threshold method used. All metrics regard the test set.


```{python}
#| label: supp-thresh-methods-data-import
df_metrics_roc = pd.read_csv(data_dir + 'metrics_roc.csv')
df_metrics_f1 = pd.read_csv(data_dir + 'metrics_f1.csv')
df_metrics_pr = pd.read_csv(data_dir + 'metrics_pr.csv')
df_metrics_f2 = pd.read_csv(data_dir + 'metrics_fbeta.csv')

df_metrics_pr.loc[df_metrics_pr['Threshold Method'].notna(), 'Threshold Method'] = 'prc'
```

```{python}
#| label: tbl-threshold-methods
#| tbl-cap: 'Comparison of Threshold Methods for the LGBM Models'
dfs = [df_metrics_roc, df_metrics_f1, df_metrics_pr, df_metrics_f2]
metrics = ['ROC', 'F1', 'PR', 'F2']

df_combined = pd.concat([
    (
        df[df['Model'].str.contains('LGBM(.)*\(T\)')]
        [['Threshold', 'Precision', 'Recall', 'F1 Score', 'F2 Score', 'FPR', 'FNR', 'Total Pos Pred', 'Tuned', 'Threshold Method']]

    )
    for df, metric in zip(dfs, metrics)
])

GT(df_combined.sort_values('Tuned').round(3))
```

In our case study, thresholding that was relatively more focused on precision (F1/AUPRC) led to a high threshold, few positive predictions, and a low false positive rate. Thresholding that was relatively more focused on recall (F2) led to a notably lower threshold than .5, more positive predictions, and a higher false positive rate. The geometric mean/roc approach led to the most positive predictions, and thus a notably high false positive rate, but also a lower false negative rate. By coincidence, the roc and fbeta approaches led to the same classification for the tuned model.


### Brief Discussion of the Multiclass Setting

In the multiclass setting, we are often concerned with the same metrics as the binary case, but we'll have to consider how to aggregate the metrics. Typically this takes two forms. 

**Weighted Average**: This approach takes into account the number of instances in each class. Each class's metric is weighted by the proportion of instances in that class. It may be useful when you want to account for class imbalance to ensure that metrics from larger classes have a greater impact on the overall score.

**Macro Average**: This approach calculates the metric independently for each class and then takes the average of these metrics. Each class is treated equally, regardless of its size. It is useful when you want to ensure that the model performance assessment is consistent across all classes, rather than being dominated by the performance on the more frequent classes.

In addition, the standard accuracy metric is often of primary interest in a multiclass setting, as a model that is merely guessing would usually not have a typically good accuracy score, and high accuracy is often a good indicator of a good model. But it's still useful to consider other metrics like precision, recall, and F1 score, especially in the presence of a very dominant class, or other more obvious multiclass imbalance.