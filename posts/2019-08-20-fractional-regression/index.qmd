---
title: "Fractional Regression"
description: |
  A quick primer regarding data between zero and one, including zero and one
date: 2019-08-20
image: ../../img/prate-vis-1.svg
draft: false
tags: [fractional regression, beta regression, logistic regression, zero inflated, one inflated, percentage, fraction, stata, fracreg]
categories:
  - regression
  - GLM
  - mixed models
share:
    permalink: 'https://m-clark.github.io/posts/2019-08-20-fractional-regression/'
    description: 'Fractional Regression'
    divclass: 'share-buttons'
    linkedin: true
    bsky: true
    twitter: true
    email: true
    reddit: true
    facebook: true
---


```{r setup, include=FALSE}
library(tidyverse)
library(tidyext)
library(broom)
library(kableExtra)
library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}
```

> NB: This post was revisited when updating the website early 2025, and some changes were required. Attempts to keep things consistent were made, but if you feel you've found an issue, please post it at [GitHub](http://github.com/m-clark/m-clark.github.io/issues).


## Introduction

It is sometimes the case that you might have data that falls primarily between zero and one.  For example, these may be proportions, grades from 0-100 that can be transformed as such, reported percentile values, and similar. If you had the raw counts where you also knew the denominator or total value that created the proportion, you would be able to just use standard <span class="emph" style = "">logistic regression</span> with the <span class="emph" style = "">binomial distribution</span>.  Similarly, if you had a binary outcome (i.e. just zeros and ones), this is just a special case, so the same model would be applicable.  Alternatively, if all the target variable values lie between zero and one, <span class="emph" style = "">beta regression</span> is a natural choice for which to model such data.  However, if the variable you wish to model has values between zero and one, and additionally, you also have zeros or ones, what should you do?

Some suggest adding a 'fudge factor' to the zeros or ones to put all values on the (0, 1) interval, so that beta regression could still be employed. Others might implement zero/one-inflated beta regression if a larger percentage of the observations are at the boundaries.  However, as we will see, you already have more standard tools that are appropriate for this modeling situation, and this post will demonstrate some of them.

#### Related models

- Binomial logistic for binary and count/proportional data, i.e. $x$ successes out of $n$ trials (can use standard glm tools)
- Beta regression for (0, 1), i.e. only values *between* 0 and 1 (see [betareg](https://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf), [DirichletReg](https://cran.r-project.org/web/packages/DirichletReg/vignettes/DirichletReg-vig.pdf), [mgcv](https://cran.r-project.org/web/packages/mgcv/), [brms](https://cran.r-project.org/web/packages/brms/)  packages)
- Zero/One-inflated binomial or  beta regression for cases including a relatively high amount of zeros and ones ([brms](https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf), [VGAM](https://www.stat.auckland.ac.nz/~yee/VGAM/), [gamlss](https://www.gamlss.com/))

## Stata example

It might seem strange to start with an example using Stata[^ruser], but if you look this sort of thing up, you'll almost certainly come across the [Stata demonstration](https://www.stata.com/features/overview/fractional-outcome-models/) using the <span class="func" style = "">fracreg</span> command.  For comparison we'll use the data in the corresponding [documentation](https://www.stata.com/manuals/rfracreg.pdf).  The data regards the expected participation rate in 401(k) plans for a cross-section of firms[^401k]. They define participation rate (`prate`) as the fraction of eligible employees in a firm that participate in a 401(k) plan.  This is modeled by the matching rate of employee 401(k) contributions (`mrate`), the (natural) log of the total number of employees (`ltotemp`), the age of the plan (`age`), and whether the 401(k) plan is the only retirement plan offered by the employer (`sole`).  Here we do not use quadratic effects for `ltotemp` and `age` as in the Stata documentation, though we do use an additive modeling approach later that could be implemented for the same purpose instead[^gamvsquad].

```{r initial-import, echo=FALSE}
d = haven::read_dta('data/401k.dta')
```


The following shows the distribution of the target variable.  There are no zeroes in the participation rate, however the amount of ones is `r rnd(100*mean(d$prate==1), 1)`%.

<br>

```{r prate-vis, echo=FALSE, out.width='50%'}
d %>% 
  ggplot() +
  geom_density(
    aes(prate), 
    color = '#ff5500',
    fill = '#ff550080'
  ) +
  labs(y = '', title = 'Distribution of Participation Rate') +
  theme_clean() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    title = element_text(size = 12)
  )
```


The following specifies a fractional regression with logit link. Probit and heteroscedastic probit are also available.

```{stata stata, engine.path="C:/Program Files/Stata16/StataSE-64.exe", eval=file.exists("C:/Program Files/Stata16/StataSE-64.exe")}
use http://www.stata-press.com/data/r14/401k

fracreg logit prate mrate c.ltotemp c.age i.sole
```

```{r nostata, echo=FALSE, eval = !file.exists("C:/Program Files/Stata16/StataSE-64.exe")}
# in case no access to Stata on development device

# code
# use http://www.stata-press.com/data/r14/401k
# fracreg logit prate mrate c.ltotemp c.age i.sole

cat(read_lines('scripts/stata_output.txt'), sep = '\n')
```

Perhaps not surprisingly, all of the covariates are statistically notable.  With the logistic link, the coefficients can be exponentiated to provide odds ratios[^orstata].  Stata's is one of the few tools that is specifically advertised to model such outcomes, but as we're about to see, you don't need Stata's command, or even a special package in R, once you know what's going on.

## R GLM

It turns out that the underlying likelihood for fractional regression in Stata is the same as the standard binomial likelihood we would use for binary or count/proportional outcomes.  In the following, $y$ is our target variable, $X\beta$ is the linear predictor, and $g(.)$ is the link function, for example, the logit.

$$\mathcal{L} \sim y(\ln{g(X\beta)}) + (1-y)(1-\ln{g(X\beta)})$$

As such, we can just use <span class="func" style = "">glm</span> like we would for count or binary outcomes. It will warn you that the outcome isn't integer as it expects, but in this case we can just ignore the warning.

```{r glm, warning=TRUE}
d = haven::read_dta('data/401k.dta')

model_glm = glm(
  prate ~ mrate + ltotemp + age + sole,
  data = d,
  family = binomial
)

# summary(model_glm)
```

```{r glm-summary, echo=FALSE}
tidy(model_glm) %>% 
  kable_df()
```


```{r glm-se, echo=FALSE}
se_baseline_glm = summary(model_glm)$coefficients
```

So the model runs fine, and the coefficients are the same as the Stata example.  The only difference regards the standard errors, but we can fix that.


## Robust standard errors

The difference in the standard errors is that, by default, Stata reports robust standard errors. We can use the <span class="pack" style = "">sandwich</span> package to get them in R.  The <span class="pack" style = "">lmtest</span> package provides a nice summary table.

```{r robust-se}
library(lmtest)
library(sandwich)

se_glm_robust = coeftest(model_glm, vcov = vcovHC(model_glm, type="HC"))
```

```{r robust-se-print, echo=FALSE}
tidy(se_glm_robust) %>% 
  kable_df()
```

So now we have the same result via a standard R generalized linear model and Stata.  Likewise, you could just use the <span class="func" style = "">glm</span> command in Stata with the `vce(robust)` option.


## Quasibinomial

We could also use the <span class="emph" style = "">quasibinomial</span> family.  [Quasi-likelihoods](https://en.wikipedia.org/wiki/Quasi-likelihood) are similar to standard likelihood functions, but technically do not relate to any particular probability distribution[^fracregquasi]. Using this family would  provide the same result as the previous glm, but without the warning.

::: {.column-margin}
From the R help file for `?family`: The quasibinomial and quasipoisson families differ from the binomial and poisson families only in that the dispersion parameter is not fixed at one, so they can model over-dispersion.  

Also, as noted in the StackExchange link in the [references][references], while by default the variance estimate is 'robust', possibly leading to standard errors that are similar, the basic result is not the same as using the robust standard errors.
:::

```{r quasi}
model_quasi = glm(
  prate ~ mrate + ltotemp + age + sole,
  data = d,
  family = quasibinomial
)

# summary(model_quasi)
```

```{r quasi-summary, echo=FALSE}
tidy(model_quasi) %>% 
  kable_df()
```

We can get robust standard errors for the quasi-likelihood approach as well, but they were already pretty close.

```{r robust-se-quasi-print, echo=1}
se_glm_robust_quasi = coeftest(model_quasi, vcov = vcovHC(model_quasi, type="HC"))

tidy(se_glm_robust_quasi) %>% 
  kable_df()
```

## Mixed model with per-observation random effect


It turns out that we can also use a [mixed model](https://m-clark.github.io/mixed-models-with-R/) approach.  For some distributions such as binomial and poisson, the variance is directly tied to the mean function, and so does not have to be estimated.  In these scenarios, we can insert a per-observation random effect and estimate the associated variance.  This extra source of variance can account for overdispersion, similar to what the scale parameter estimate does for the quasibinomial.  

I initially attempted to do so using the popular mixed model package <span class="pack" style = "">lme4</span> and its <span class="func" style = "">glmer</span> function, with an observation level random effect.  While I've had success using this package with such models in the past, in this particular instance, all failed to converge with default optimization settings across multiple optimizers. As such, those results are not shown.

```{r lme4, eval=1:2}
d$id = 1:nrow(d)

model_glmm = lme4::glmer(
  prate ~ mrate + ltotemp + age + sole + (1 | id),
  data = d,
  family = binomial
)

summary(model_glmm, cor=F)

test_models = lme4::allFit(model_glmm)

summary(test_models)
```

<br>

We have options though.  The <span class="pack" style = "">glmmTMB</span> package was able to estimate the model.

<br>

```{r glmmTMB}
library(glmmTMB)

model_glmm = glmmTMB(
  prate ~ mrate + ltotemp + age + sole + (1 | id),
  data = d,
  family = binomial,
  REML = TRUE
)

# summary(model_glmm)
```

```{r glmm-summary, echo=FALSE}
broom.mixed::tidy(model_glmm, effects = 'fixed') %>% 
  select(-effect, -component) %>% 
  kable_df()
```

We can maybe guess why <span class="func" style = "">glmer</span> was struggling.  The extra variance is estimated by <span class="func" style = "">glmmTMB</span> to be basically zero.

```{r glmm-se, echo=FALSE}
se_glmm_std = as.data.frame(summary(model_glmm)$coefficients$cond)
```


Lately, I've been using <span class="pack" style = "">mgcv</span> to do most of my mixed models, so we can try a GAM instead.  The following is equivalent to the glm-quasibinomial approach before.

```{r mgcv}
library(mgcv)

model_gam_std = gam(
  prate ~ mrate + ltotemp + age + sole, 
  data = d, 
  family = quasibinomial
)

# summary(model_gam_std)
```

```{r mgcv-summary, echo=FALSE}
tidy(model_gam_std, parametric = TRUE) %>% 
  kable_df()
```

```{r mgcv-se, echo=FALSE}
se_gam_std = tidy(model_gam_std, parametric = TRUE)
colnames(se_gam_std)[2:3] = c('Estimate', "Std. Error")

# not sure it's handling gam correctly?
# se_gam_std = coeftest(model_gam_std, vcov = vcovHC(model_gam_std, type="HC"))
```


The following adds the per observation random effect as with the mixed model.  Unlike with <span class="pack" style = "">lme4</span> or <span class="pack" style = "">glmmTMB</span>, you can technically use the `quasi` family here as well, but I will follow Bates' thinking and avoid doing so[^Bates].  I will also calculate the robust standard errors.

```{r mgcv-re}
model_gam_re = gam(
  prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),
  data = d,
  family = binomial,
  method = 'REML'
)

# summary(model_gam_re)
```

```{r mgcv-re-summary, echo=FALSE}
tidy(model_gam_re, parametric = TRUE) %>% 
  kable_df()
```

```{r mgcv-re-se, echo=FALSE}
se_gam_re_robust = coeftest(model_gam_re, vcov = vcovHC(model_gam_re, type="HC"))
```

```{r mgcv2, echo=FALSE, eval=FALSE}
model_gam_re_quasi = gam(
  prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),
  d,
  family = quasibinomial,
  method = 'REML'
)

summary(model_gam_re_quasi)

se_gam_re_quasi = coeftest(model_gam_re_quasi, vcov = vcovHC(model_gam_re_quasi, type="HC"))
```


## Summarized results

The following tables show the results of the models.  The first table regards the estimated coefficients, the second the standard errors.  There are no differences for the coefficients.  For standard errors, some approaches are definitely working better than others.

```{r robust-se-summary, echo=FALSE, cache=FALSE}
se_stata = matrix(
  c(c(-2.391717, 1.157832, -.2072429, .0345786, .1655762),
    c(.1061292, .0749231, .0141468, .0027604, .0506445)
  ), ncol = 2
)

colnames(se_stata) = c('Estimate', 'Std. Error')

robust = mget(ls(pattern = '^se\\_'))
names(robust) = str_remove(names(robust), '^se_')


robust %>% 
  map_df(function(x) data.frame(x[,'Estimate', drop=F]), .id = 'model') %>% 
  spread2(model, value = `Estimate`) %>% 
  slice(-6) %>%   # remove se for smooth
  select(baseline_glm, stata, glm_robust, glm_robust_quasi, everything()) %>%
  kable_df(digits = 4, caption = 'Coefficients')

robust %>% 
  map_df(function(x) data.frame(x[,'Std. Error', drop=F]), .id = 'model') %>% 
  spread2(model, value = `Std..Error`) %>% 
  slice(-6) %>%   # remove se for smooth
  select(baseline_glm, stata, glm_robust, glm_robust_quasi, everything()) %>% 
  kable_df(digits = 4, caption = 'Standard Errors')
```



## Conclusion

Fractional data occurs from time to time.  While Stata and R have specific functionality for such outcomes,  more commonly used statistical tools can be used, which might provide additional means of model exploration.  In the demo above, a standard glm with robust errors would be fine, and the simplest to pull off.  With that as a basis, other complexities could be incorporated in more or less a standard fashion.





## References

[Stata demo](https://www.stata.com/features/overview/fractional-outcome-models/)

[Stata reference on fracreg command](https://www.stata.com/manuals/rfracreg.pdf)

McCullagh P. and Nelder, J. A. (1989) Generalized Linear Models. London: Chapman and Hall.

Papke & Wooldridge. (1996) Econometric Methods For Fractional Response Variables With An Application To 401 (K) Plan Participation Rates.  [link](https://onlinelibrary.wiley.com/doi/epdf/10.1002/%28SICI%291099-1255%28199611%2911%3A6%3C619%3A%3AAID-JAE418%3E3.0.CO%3B2-1)

Ramalho, E., Ramalho, J. & Coelho, L. (2016) Exponential Regression of Fractional-Response Fixed-Effects Models with an Application to Firm Capital Structure. Journal of Econometric Methods. [link](https://www.degruyter.com/view/j/jem.2018.7.issue-1/jem-2015-0019/jem-2015-0019.xml)

Ramalho, E., Ramalho, J. & Murteira, J. (2011) Alternative Estimating And Testing Empirical Strategies For Fractional Regression Models. [link](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-6419.2009.00602.x)

StackExchange has some more useful discussion, e.g. more on standard error differences between the approaches and other context [link](https://stats.stackexchange.com/questions/89999/how-to-replicate-statas-robust-binomial-glm-for-proportion-data-in-r/205040), [link2](https://stats.stackexchange.com/questions/189190/glm-with-logit-link-and-gaussian-family-to-predict-a-continuous-dv-between-0-and)





[^ruser]: Given that I'm an avid R user. But if that was not apparent, then using Stata is possibly no surprise at all! `r emo::ji('smile')`

[^401k]: I added the original data, which has the raw values and many more observations, to my [noiris package](https://github.com/m-clark/noiris).

[^orstata]: In Stata you can just add the option `, or` to the end of the model line.

[^gamvsquad]: I actually played with this a bit.  The quadratic would be okay for `age`, but log firm size has a little more going on and `mrate` should also be allowed to wiggle.  There would also be some interesting smooth interactions.  In short, a [generalized additive model](https://m-clark.github.io/generalized-additive-models/) is pretty much always a better option than trying to guess polynomials.

[^fracregquasi]: This is in fact what <span class="func" style = "">fracreg</span> in Stata is doing.

[^Bates]: From [Doug Bates](lme4.r-forge.r-project.org/slides/2011-03-16-Amsterdam/5GLMM.pdf):  In many application areas using 'pseudo' distribution families, such as quasibinomial and quasipoisson, is a popular and well-accepted technique for accommodating variability that is apparently larger than would be expected from a binomial or a Poisson distribution. This amounts to adding an extra parameter, like σ, the common scale parameter in a LMM, to the distribution of the response. It is possible to form an estimate of such a quantity during the IRLS algorithm but it is an artificial construct. There is no probability distribution with such a parameter. I find it difficult to define maximum likelihood estimates without a probability model. It is not clear how this 'distribution which is not a distribution' could be incorporated into a GLMM. This, of course, does not stop people from doing it but I don’t know what the estimates from such a model would mean. 




