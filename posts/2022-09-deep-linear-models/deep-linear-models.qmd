---
title: "Deep Linear Models"
description: |
  A demonstration using pytorch
date: '2022-10-10'
image: ../../img/nnet.png   
bibliography: ../../bibs/dl-reg.bib
draft: false
keywords: [deep learning, torch, pytorch, fastai, demo, embeddings, TabNet, XGBoost, lightgbm, gradient boosting, linear model]
categories:
  - deep learning
  - boosting
  - GLM
  - regression
  - machine learning
nocite: | 
  @howard2022linreg, @howard2022neuralnet, @raschka2022chrono, @clark2022dl4tab1, @clark2022dl4tab2,  @howard2022neuralnet2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = TRUE, 
  eval      = TRUE, 
  message   = FALSE,
  warning   = FALSE,
  comment   = NA,
  R.options = list(width = 120),
  cache         = FALSE,
  cache.rebuild = FALSE,
  cache.lazy    = FALSE,
  fig.align = 'center',
  fig.asp   = .7,
  dev       = 'png',
  dev.args  = list(bg = 'transparent')
)


library(tidyverse)

theme_clean <- function (
  font_size = 12,
  font_family = "",
  center_axis_labels = FALSE
) {
  
  if (center_axis_labels) {
    haxis_just_x <- 0.5
    vaxis_just_y <- 0.5
    v_rotation_x <- 0
    v_rotation_y <- 0
  }
  else {
    haxis_just_x <- 0
    vaxis_just_y <- 1
    v_rotation_x <- 0
    v_rotation_y <- 0
  }
  
  ggplot2::theme(
    text = ggplot2::element_text(
      family = font_family,
      face   = "plain",
      color  = "gray30",
      size   = font_size,
      hjust  = 0.5,
      vjust  = 0.5,
      angle  = 0,
      lineheight = 0.9,
      margin = ggplot2::margin(),
      debug  = FALSE
    ),
    axis.title.x = ggplot2::element_text(
      hjust = haxis_just_x,
      angle = v_rotation_x,
      size  = 0.8 * font_size
    ),
    axis.title.y = ggplot2::element_text(
      vjust = vaxis_just_y,
      hjust = 0,
      angle = v_rotation_y,
      size  = 0.8 * font_size
    ),
    axis.ticks        = ggplot2::element_line(color = "gray30"),
    title             = ggplot2::element_text(color = "gray30", size = font_size * 1.25),
    plot.subtitle     = ggplot2::element_text(color = "gray30", size = font_size * .75, hjust = 0),
    plot.caption      = ggplot2::element_text(color = "gray30", size = font_size * .5, hjust = 0),
    legend.position   = 'bottom', 
    legend.key        = ggplot2::element_rect(fill = "transparent", color = NA),
    legend.background = ggplot2::element_rect(fill = "transparent", color = NA),
    legend.title      = ggplot2::element_blank(), 
    panel.background  = ggplot2::element_blank(),
    panel.grid        = ggplot2::element_blank(),
    strip.background  = ggplot2::element_blank(),
    plot.background   = ggplot2::element_rect(fill = "transparent", color = NA),
  )
}

# set the theme as default
theme_set(theme_clean())

# set other point/line default colors; in most cases, we can use the color from
# default discrete scale for more consistency across plots.
# paletteer::palettes_d$colorblindr$OkabeIto
update_geom_defaults('vline',   list(color = 'gray25',  alpha = .25))  # vlines and hlines are typically not attention grabbers so set alpha
update_geom_defaults('hline',   list(color = 'gray25',  alpha = .25))  # usually a zero marker
update_geom_defaults('point',   list(color = '#E69F00', alpha = .5))   # alpha as usually there are many points
update_geom_defaults('smooth',  list(color = '#56B4E9', alpha = .15))
update_geom_defaults('line',    list(color = '#56B4E9', alpha = .5))
update_geom_defaults('bar',     list(color = '#E69F00', fill = '#E69F00'))  
update_geom_defaults('col',     list(color = '#E69F00', fill = '#E69F00'))
update_geom_defaults('dotplot', list(color = '#E69F00', fill = '#E69F00'))

# use colorblind safe colors for categories; if you supply a continuous value to
# color you'll get an error, but you just have to use `myplot +
# scale_color_continous()` or whatever to override this; likewise you can always
# override this scale for categorical schemes if desired also. Note that this
# will apply for both color and fill, which is usually what we want.

okabe_ito = c(
  '#E69F00',
  '#56B4E9',
  '#009E73',
  '#F0E442',
  '#0072B2',
  '#D55E00',
  '#CC79A7',
  '#999999'
)

ggplot <- function(...) ggplot2::ggplot(...) + 
  # okabe ito colorblind safe scheme
  scale_color_manual(
    values = okabe_ito,
    drop = FALSE,
    aesthetics = c('color', 'fill')
  )

gt <- function(..., decimals = 2, title = NULL, subtitle = NULL) {
  gt::gt(...) %>% 
    gt::fmt_number(
      columns = where(is.numeric),
      decimals = decimals
    ) %>% 
    gt::tab_header(title = title, subtitle = subtitle) %>% 
    gtExtras::gt_theme_nytimes()
}

gt_theme <-   
  list(
    # report median (IQR) and n (percent) as default stats in `tbl_summary()`
    "tbl_summary-str:continuous_stat" = "{mean} ({sd})",
    "tbl_summary-str:categorical_stat" = "{n} ({p})"
  )

rnd = tidyext::rnd
```

```{r pysetup, include=FALSE}
library(reticulate)
# use_python('/Users/micl/Library/r-miniconda-arm64/bin/python3.9')
use_python('/Users/micl/opt/anaconda3/envs/xgb_lgb/bin/python')
# to use, set the project python to the above, it seems not to work with use_python
```

## Introduction

This post gives a by-hand example of a linear model using [pytorch]{.pack}. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn't up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some [pytorch]{.pack} basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!

For this demo we'll use [an example by [fastai]{.pack}](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch), which is a great resource for [getting started with deep learning](https://course.fast.ai/). While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples[^1]. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions[^2].

[^1]: I won't actually use [fastai]{.pack}, since they aren't up to supporting M1/2 Macs very well. I think it was only used for the train/test data split anyway. I would rant a bit about this, but a lot of [fastai]{.pack} is geared toward non-local computing, and the fault is really with Apple and NVidia as near as I can tell.

[^2]: I'm also not going to go into broadcasting, submitting to Kaggle, and other things that I don't think are necessary for our purposes here.

## Getting Started

Let's get the primary packages loaded first.

```{python, imports}
import pandas as pd
import numpy as np
import torch
```

Next, we'll use the well-known [titanic dataset](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch/data), and to start things off, we'll need to get a sense of what we're dealing with. The basic idea is that we'd like to predict survival based on key features like sex, age, ticket class and more.

```{python, read-titanic}
# non-kaggle-requiring url here: https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/data/dl-linear-regression/titanic/train.csv
df_titanic_train = pd.read_csv('data/dl-linear-regression/titanic/train.csv')
# df_titanic_train
```

```{python, describe}
df_titanic_train.describe()
```

## Initial Data Processing

The data is not ready for modeling as is, so we'll do some additional processing to get it ready. We'll check out the missing values and replace them with modes[^3].

[^3]: Just as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here, and sometimes that's enough.

```{python show-na}
df_titanic_train.isna().sum()
```

```{python replace-na}
modes = df_titanic_train.mode().iloc[0]

df_titanic_train.fillna(modes, inplace = True)

df_titanic_train.describe(include = (np.number))
```

With features, sometimes it is worthwhile to log transform data for potentially more efficient optimization search. Since we have zeros, we add 1 before taking the log.

```{python hist-fare, fig.show='hide'}
df_titanic_train['Fare'].hist()
```

![](../../img/dl-linreg/fare-hist.png){width="50%"}

Now the transformed data looks a little more manageable. More to the point, we won't potentially have huge coefficients relative to other covariates because of the range of the data.

```{python l1p-fare, fig.show='hide'}
df_titanic_train['LogFare'] = np.log1p(df_titanic_train['Fare'])

df_titanic_train['LogFare'].hist()
```

![](../../img/dl-linreg/fare-hist-log.png){width="50%"}

The `Pclass` (passenger class) feature is actually categorical.

```{python inspect-cats}
pclasses = sorted(df_titanic_train.Pclass.unique())
pclasses
```

Here are the other categorical features.

```{python inspect-more-cats}
df_titanic_train.describe(include = [object])
```

In order to use categorical variables, they need to be changed to numbers[^4], so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use *embeddings*[^5], particularly for things that have lots of unique categories.

[^4]: Even though every modeling tool requires this, strangely very few in the Python world offer options for automatic handling of such things, but it's getting better.

[^5]: We actually aren't too far removed from this in our model coming up, the main difference is that we don't treat the categorical feature part of the model separately.

```{python dummify}
df_titanic_train = pd.get_dummies(df_titanic_train, columns = ["Sex", "Pclass", "Embarked"])
```

Let's take a look at our data now.

```{python show-data-with-dummies}
df_titanic_train.columns
```

```{python show-ready-data}
df_titanic_train.head()
```

## Getting Started with pytorch

### Setup

Now we are ready to prep things for specific use with [pytorch]{.pack}. I will not use the same terminology as in Jeremy's original post, so for us, `target` = 'dependent variable' and `X` is our feature matrix[^6]. Both of these will be [pytorch]{.pack} *tensors*, which for our purposes is just another word for an array of arbitrary size.

[^6]: I'll not perpetuate calling features/predictor variables that are clearly not independent as independent. That nomenclature really only works for randomized experiments, and that is definitely not the case here.

```{python target-tensor}
from torch import tensor

target = tensor(df_titanic_train.Survived)
```

```{python dummy-features}
dummies = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']
all_features = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummies 

X = tensor(df_titanic_train[all_features].values, dtype = torch.float)
X.shape
```

## Setting up a linear model

We have our feature matrix and target variable prepped. The next step is to map the features to the target by means of predicted values. In linear regression, we typically call the weights that produce the predictions *coefficients*, but in standard deep/machine learning terminology, they are usually called *weights*, or more generally, *parameters*. Here, we generate some random values between -.5 and .5 to get started[^7]:.

[^7]: You could use [torch.randn]{.func} to get standard normal values, and often times we'll even start with just zeros if we really are just doing a standard linear model.

```{python random-coeffs}
torch.manual_seed(442)

n_coeff = X.shape[1]
coeffs = torch.rand(n_coeff) - 0.5  # default would produce values from 0 to 1
coeffs
```

The original post did a form of min-max scaling to the features, basically putting everything on a potentially \[0, 1\] scale. Here we'll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.

```{python scale-X}
# vals,indices = X.max(dim=0)
# X = X / vals
X_means = X.mean(dim = 0, keepdim = True)
X_sds   = X.std(dim = 0)

X_sc = (X - X_means) / X_sds

# X_sc.mean(dim = 0)  # all means = 0 
# X_sc.std(dim = 0)   # all sd = 1
```

As noted in the original post and worth iterating here for our statistical modeling crowd, we don't estimate an intercept for this model and keep all the dummy coded features. The following takes our coefficients, multiplies them by their respective feature, and sums them.

```{python init-preds}
preds = (X_sc * coeffs).sum(axis = 1)
preds[:10]
```

We can calculate our *loss*, the difference in our predictions versus the target values, in many ways. Here we get the mean squared error.

```{python init-loss}
loss = torch.square(preds - target).mean()
loss
```

Now we'll create functions that do the previous steps, and finally, give it a test run! In the original [fastai]{.pack} formulation, they use mean absolute error for the loss, which actually is just the `L1loss` that is available in torch. For a change of pace, we'll keep our mean squared error, which is sometimes called *L2* loss (this will create different results from the original notebook). I create the option within the function for you to do either. Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.

```{python func-calc_predsloss-init}
def calc_preds(X, weights):
    return((X * weights).sum(axis = 1))

def calc_loss(X, weights, target, which = 'l2'):
    preds = calc_preds(X, weights)
    
    # torch.abs(calc_preds(X, coeffs)-target).mean()  # original

    if which == 'l2':
      loss = torch.nn.MSELoss()
    else: 
      loss = torch.nn.L1Loss()
      
    L = loss(preds, target.float())
      
    return(L)

calc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which = 'l1')
```

### Doing a Gradient Descent Step

We can continue our journey onward to actually estimating the weights rather than specifying them directly, since we definitely don't want to just keep guessing! This is an iterative process where we still start with an initial (random) guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps *epochs*, and getting our next guess requires calculating what's called a *gradient*. Here are some resources for more detail:

-   [How Does a Neural Net Really Work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work): great intro by Jeremy Howard
-   [Some by-hand code using gradient descent for linear regression, R](https://m-clark.github.io/models-by-example/stochastic-gradient-descent.html), [Python](https://m-clark.github.io/models-by-example/supplemental.html#python-sgd): By yours truly

In any case, this is basic functionality within [pytorch]{.pack}, and it will keep track of each step taken.

```{python require-grad}
coeffs.requires_grad_()
```

```{python redo-loss}
loss = calc_loss(X_sc, coeffs, target)
loss
```

We use [backward]{.func} to calculate the gradients and inspect them.

```{python backward-step}
loss.backward()

coeffs.grad
```

Each time backward is called, the gradients are added to the previous values. We can see here that they've now doubled.

```{python another-round}
loss = calc_loss(X_sc, coeffs, target)

loss.backward()

coeffs.grad
```

What we want instead is to set them back to zero after they are used for our estimation step. The following does this.

```{python zero-grad}
loss = calc_loss(X_sc, coeffs, target)

loss.backward()

with torch.no_grad():
    coeffs.sub_(coeffs.grad * 0.1)     # sub subtracts in place
    coeffs.grad.zero_()                # zeros out in place
    print(calc_loss(X, coeffs, target))

```

### Training the Linear Model

We typically would split our data into training and test. We can do so here, or keep this data as training and import `test.csv` for the test set. The latter is actually used for the Kaggle submission, but that's not a goal here. We'll use [scikit-learn]{.pack} for the splitting.

```{python train-test-split}
from sklearn.model_selection import train_test_split

# test size .2 in keeping with fastai RandomSplitter default
train_x, valid_x, train_y, valid_y = train_test_split(
  X_sc, 
  target.float(), 
  test_size = 0.2, 
  random_state = 808
)
  

len(train_x), len(valid_x) # might be one off of the original notebook
```

As before, we'll create functions to help automate our steps:

-   one to initialize the weights
-   a function to update weights
-   one to do a full epoch (using weights to calculate loss, updating weights)
-   one to train the entire model (run multiple times/epochs)

As mentioned, the approach here is to create functions that are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each `verbose` value epoch (e.g. `verbose = 10` means you'll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (possibly not too big a deal depending on your IDE).

```{python fun-linmod-training}
def init_weights(n_wts): 
    return (torch.rand(n_wts) - 0.5).requires_grad_()

def update_weights(weights, lr):
    weights.sub_(weights.grad * lr)
    weights.grad.zero_()

def one_epoch(X, weights, target, lr, verbose = 1, i = 1):
    loss = calc_loss(X, weights, target)
    loss.backward()
    
    with torch.no_grad(): update_weights(weights, lr)
    
    if verbose != 0:
        if i % verbose == 0:
            print(f'{loss: 3f}', end = '\n ')

def train_model(X, target, epochs = 30, lr = 1e-3, verbose = 1):
    torch.manual_seed(442)
    coeffs = init_weights(X.shape[1])
    
    for i in range(epochs): 
        one_epoch(X, coeffs, target, lr = lr, i = i, verbose = verbose)
    return coeffs

```

Try out the functions if you like (not shown).

```{python calc-loss-linmod, eval=FALSE}
calc_loss(X_sc, init_weights(X_sc.shape[1]), target).backward()
```

```{python one-epoch-linmod, eval=FALSE}
one_epoch(train_x, init_weights(train_x.shape[1]), train_y, .01)
```

Now train the model for multiple epochs. The loss drops very quickly before becoming more steady.

```{python train-linmod}
coeffs_est = train_model(train_x, train_y, epochs = 50, verbose = 5, lr = .2)
```

Let's create a function to show our estimated parameters/weights/coefficients in a pretty fashion.

```{python fun-show_coeffs}
def show_coeffs(estimates): 
  coef_dict = dict(zip(all_features, estimates.requires_grad_(False).numpy()))
  return pd.DataFrame(coef_dict, index = ['value']).T

show_coeffs(coeffs_est)
```

### Measuring Accuracy

It's one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data. The following function will convert our estimates to a binary value like our target, and compares them to the target. Depending on how you did your training setup, it might be pretty bad or at least better than guessing.

```{python init-accuracy}
def acc(X, weights, target): 
    return (target.bool() == (calc_preds(X, weights) > 0.5)).float().mean()

acc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)
```

### Using sigmoid

Nothing about the previous setup actually produces a result on the probability scale, so basing a cutoff of .5 is meaningless. you can inspect them and might see values are above 1 or below zero, which we generally don't want[^8]. However we do have a solution. The *sigmoid function*[^9] allows us to transform our predictions to values between 0 and 1, i.e. probabilities in this context, and in particular, the probability of survival. Then our [acc]{.func} function will be more appropriate, where any probability \> .5 will be given a value of 1 (or `True` technically), while others will be 0/`False`.

[^8]: Unless you are an economist, in which case you call it a *linear probability model* and ignore the ridiculous predictions because you have very fine standard errors.

[^9]: A lot of R folks seem unaware that the base R [plogis]{.func} function accomplishes this.

```{python use-sigmoid}
def calc_preds(X, weights):
    return torch.sigmoid((X*weights).sum(axis = 1))
```

We also will do more iterations, and fiddle with the learning rate (a.k.a. step size)

```{python retrain-with-sigmoid}
coeffs_est = train_model(
  train_x,
  train_y,
  epochs = 500,
  lr = 1,
  verbose = 100
)
```

Not too shabby!

```{python acc-with-sigmoid}
acc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)
```

```{python coeffs-with-sigmoid}
show_coeffs(coeffs_est)
```

In implementing the sigmoid, let's go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster)[^10]. To do this, the coefficients will need to be a column vector, so we change our [init_coeffs]{.func} function slightly[^11].

[^10]: The `@` operator is essentially the dot product, so `x@y` is `np.dot(x, y)`

[^11]: The [fastai]{.pack} demo also changes the target to a column vector, but this doesn't seem necessary.

```{python mat-mult}
def calc_preds(X, weights): 
    return torch.sigmoid(X@weights)

def init_coeffs(n_wts): 
    return (torch.rand(n_wts, 1) * 0.1).requires_grad_()

```

Now our functions are more like the mathematical notation we'd usually see for linear regression.

$$\hat{y} = X\beta$$

### Compare to Linear/Logistic Regression

Before getting too excited, let's compare our results to basic linear and logistic regression. The linear regression is more like our model before using the sigmoid transformation, while the logistic is more like when we used it. Depending on your settings, the logistic regression is probably doing better at this point.

```{python lin-prob-model}
from sklearn import linear_model
from sklearn.metrics import accuracy_score


reg = linear_model.LinearRegression()
reg.fit(train_x, train_y)

acc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)
```

```{python acc-torch-vs-logreg}
reg = linear_model.LogisticRegression()
reg.fit(train_x, train_y)

accuracy_score(valid_y, reg.predict(valid_x)).round(4)
```

It looks like our coefficient estimates are similar to the logistic regression ones.

```{python logreg-coefs}
show_coeffs(coeffs_est).assign(logreg = np.round(reg.coef_.T, 4))
```

## A Neural Network

<img src="../../img/nnet.png" style="display:block; margin: 0 auto; width:33%"/>

At this point we've basically reproduced a general linear model. A *neural network*, on the other hand, has from one to many *hidden layers* of varying types in between input and output. Let's say we have a single layer with two nodes. For a *fully connected* or *dense* network, we'd need weights to map our features to each node of the hidden layer (`n_wts` \* `n_hidden` parameters total), and then another set of weights to map the hidden nodes to our next layer, which is our output, basically creating the predicted values. You can think of it as a second hidden layer with a single output node. With additional hidden nodes we add more complexity, but also flexibility, to the model. But this may come at a price, e.g. more difficulty with training due to the additional parameters that have to be estimated.

So basically we need matrices of weights, and the following function allows us to create those. We also add a *bias/intercept/constant* for the hidden-to-output processing. In the first layer, we divide the weights by `n_hidden` to create sums at the next layer that are of similar magnitude as the inputs. In general though, there are many ways to [initialize weights](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/).

```{python fun-init_weights_nn}
def init_weights(n_wts, n_hidden = 20):
    layer1 = (torch.rand(n_wts, n_hidden) - 0.5) / n_hidden # n_wts x n_hidden matrix of weights
    layer2 = torch.rand(n_hidden, 1) - 0.3                  # n_hidden weights
    const  = torch.rand(1)[0]                               # constant
    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()
```

Now we revise our [calc_preds]{.func} function to incorporate all the weights. Initially, we extract the different sets of weights that are estimated by the model. For the original inputs, we multiply them by the layer 1 weights and sum. Then we apply a transformation to them to induce nonlinearity. Typical approaches are the sigmoid function we used before, hyperbolic tangent, and, probably the most common, the [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)). The original notebook used relu, while I use a more recent one called *Mish*, which is a variant of relu. The hidden layer nodes then get multiplied by their respective weights and summed with the constant added. We then use our sigmoid function to get the probability scale as before.

```{python fun-calc_preds-nn}
import torch.nn.functional as F

def calc_preds(X, weights):
    l1, l2, const = weights
    res = F.mish(X@l1)
    res = res@l2 + const
    return torch.sigmoid(res).flatten()
```

With additional sets of weights, we use an update loop.

```{python fun-update_weights-nn}
def update_weights(weights, lr):
    for layer in weights:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()
```

```{python coeffs-est-nn}
coeffs_est = train_model(train_x, train_y, epochs = 50, lr = 1, verbose = 10)
```

At this point we're doing a little bit better in general, and even better than standard logistic regression on the test set!

```{python acc-compare-nn}
acc(train_x, coeffs_est, train_y), \
acc(valid_x, coeffs_est, valid_y), \
accuracy_score(valid_y, reg.predict(valid_x)).round(4)

```

## Deep Learning

We previously used a single hidden layer, but we want to go deeper! That's the whole point of deep learning right? The following modifies our previous functions to allow for an arbitrary number of layers. You'll note there are some hacks to get the weights in a good way for each layer[^12], but you normally wouldn't have to do that on your own, since most tools will provide sensible modifications.

[^12]: And they probably aren't as good for the changes I've made.

```{python funs-dl}
def one_epoch(X, weights, target, lr, verbose = 1, i = 1):
    loss = calc_loss(X, weights, target)
    loss.backward()
    
    with torch.no_grad(): update_weights(weights, lr)
    
    if verbose != 0:
        if i % verbose == 0:
            print(f'{loss: 3f}', end = '\n ')

# change loss to binary
def calc_loss(X, weights, target, which = 'l2'):
    preds = calc_preds(X, weights)

    loss = torch.nn.BCELoss()

    L = loss(preds, target.float())

    return(L)


def init_weights(n_wts, hiddens):  
    sizes = [n_wts] + hiddens + [1]
    n = len(sizes)
    layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]
    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]
    for l in layers+consts: l.requires_grad_()
    return layers, consts


def calc_preds(X, weights):
    layers, consts = weights
    n = len(layers)
    res = X
    
    for i, l in enumerate(layers):
        res = res@l + consts[i]
    
    if i != n-1: 
      res = F.mish(res)
      
    
    return torch.sigmoid(res).flatten()

def update_weights(weights, lr):
    layers, consts = weights
    for layer in layers + consts:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()

def train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):
    torch.manual_seed(442)
    coeffs = init_weights(X.shape[1], hiddens)
    
    for i in range(epochs): 
        if verbose != 0:
            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)
    
    return coeffs

```

With everything set up, let's do some deep linear modeling! You can play around with the number of hidden layers, number of nodes and other settings. Feel free to explore!

```{python estimate-dl}
coeffs_est = train_model(
  train_x,
  train_y,
  hiddens = [500, 250, 100],
  epochs  = 500,
  lr      = 1e-4,
  verbose = 10
)
```

Hooray! Our best model yet (at least tied).

```{python compare-dl}
pd.DataFrame({
    'acc_train': acc(train_x, coeffs_est, train_y).flatten(), 
    'acc_test': acc(valid_x, coeffs_est, valid_y).flatten(), 
    'acc_test_glm': accuracy_score(valid_y, (reg.predict(valid_x) > .5).astype(int)).round(6)
}, index = ['value'])
```

## The Elephant in the Room

As noted in my previous posts \[[1](https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/), [2](https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/)\], probably your biggest challenge in implementing a deep learning model for tabular data, one with mixed data types and other complexities, is beating an off the shelf boosting model. Here is a quick demo with [lightgbm]{.pack}.

```{python lgbm}
from lightgbm import LGBMClassifier

model = LGBMClassifier(
  # n_estimators = 500,  # the sorts of parameters you can play with (many more!)
  # max_depth    = 4,
  # reg_alpha    = .1
)

model.fit(train_x, train_y)

model.score(valid_x, valid_y)


# sklearn example
# from sklearn.ensemble import HistGradientBoostingClassifier
# 
# res = HistGradientBoostingClassifier().fit(train_x.numpy(), train_y.numpy())
# 
# res.score(valid_x.numpy(), valid_y.numpy())
```

No tuning at all, and we're already doing significantly better. Granted, if you use a packaged DL model for tabular data like the one in [fastai]{.pack}, you should be doing better than our little demo. Even then though, you may still find the boosting results tough to beat.

```{python compare-lgbm}
df_accs = pd.DataFrame({ 
    'acc_test_dl':   acc(valid_x, coeffs_est, valid_y).flatten(), 
    'acc_test_glm':  accuracy_score(valid_y, (reg.predict(valid_x) > .5).astype(int)).round(6),
    'acc_test_lgbm': model.score(valid_x, valid_y).round(6)
}, index = ['value'])

df_accs
```

```{python compare-lgbm-perc}
df_perc_improvement = 100 * (df_accs / df_accs.iloc[0,1] - 1)  # % improvement
df_perc_improvement
```

```{python kaggle-sub, eval=FALSE, echo=FALSE}
df_kaggle = pd.read_csv('data/dl-linear-regression/titanic/test.csv')
df_kaggle = pd.get_dummies(df_kaggle, columns = ["Sex", "Pclass", "Embarked"])
df_kaggle['LogFare'] = np.log1p(df_kaggle['Fare'])

X_kag = (tensor(df_kaggle[all_features].values, dtype = torch.float) - X_means) / X_sds

```

## Summary

This was a lot of work to do slightly better than a logistic regression! However, there is a lot going on with a typical DL model that would likely prove even better. But it also serves as a reminder to have a suitable baseline, and as we saw with the lightgbm model, it can take little effort to do very well without deep learning. Hopefully though, the peek behind the scenes to do some 'deep' linear modeling can make it more accessible for you.

```{python, playing-around, eval=FALSE, echo=FALSE}
def one_epoch(X, weights, target, lr, verbose = 1, i = 1):
    loss = calc_loss(X, weights, target)
    loss.backward()
    
    with torch.no_grad(): update_weights(weights, lr)
    
    if verbose != 0:
        if i % verbose == 0:
            print(f'{loss: 3f}', end = '\n ')

# change loss to binary
def calc_loss(X, weights, target, which = 'l2'):
    preds = calc_preds(X, weights)

    loss = torch.nn.BCELoss()

    L = loss(preds, target.float())

    return(L)


def init_weights(n_wts, hiddens):  
    sizes = [n_wts] + hiddens + [1]
    n = len(sizes)
    # layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]
    layers = [torch.nn.init.xavier_normal_(torch.zeros(sizes[i], sizes[i + 1])) for i in range(n - 1)]
    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]
    for l in layers+consts: l.requires_grad_()
    return layers, consts


def calc_preds(X, weights):
    layers, consts = weights
    n = len(layers)
    res = X
    
    for i, l in enumerate(layers):
        res = res@l + consts[i]
    
    if i != n-1: 
      res = F.gelu(res)
      res = F.dropout(res, p=.2)
    
    return torch.sigmoid(res).flatten()


def update_weights(weights, lr):
    layers, consts = weights
    for layer in layers + consts:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()


def train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):
    torch.manual_seed(442)
    coeffs = init_weights(X.shape[1], hiddens)
    
    for i in range(epochs): 
        if verbose != 0:
            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)
    
    return coeffs
  

coeffs_est = train_model(
  train_x,
  train_y,
  hiddens = [50, 25, 10],
  epochs  = 500,
  lr      = 1e-3,
  verbose = 100
)

acc(valid_x, coeffs_est, valid_y).flatten()
```
