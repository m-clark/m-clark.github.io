<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Michael Clark</title>
<link>https://m-clark.github.io/</link>
<atom:link href="https://m-clark.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>&quot;Statistics, Data, Science&quot;
</description>
<generator>quarto-1.6.30</generator>
<lastBuildDate>Wed, 01 Jan 2025 05:00:00 GMT</lastBuildDate>
<item>
  <title>Some News for the New Year</title>
  <dc:creator>Michael Clark</dc:creator>
  <link>https://m-clark.github.io/posts/2025-news/</link>
  <description><![CDATA[ 





<section id="update" class="level2">
<h2 class="anchored" data-anchor-id="update">Update</h2>
<p>Among many things that have happened recently, I’ve had a daughter, and <a href="https://m-clark.github.io/book-of-models">written a book</a> to come out this year. Fun stuff!</p>
<p>Not so fun - I’m also attempting to migrate my distill site to quarto and using VS Code, which has been, at best, very difficult. After a lot of effort, it looks like I finally got quarto to use the appropriate python environment (and I’ve given up on trying to get ‘post-specific’ environments to work). Then came the general publishing problems…</p>
<p>In the end I may lose a lot of the previous code content, since quarto doesn’t appear to respect the old web cached objects I had associated with prior posts (which included now defunct or notably modified packages). It also has to use a different directory output, which means I have update links along with rerunning old posts.</p>
<p>I love using Quarto, and highly recommend it for most things (including if you never created a website before), but this particular aspect has been not at all straightforward. So, while my website will ultimately look slightly better, and hopefully be easier to maintain, the old content will be lacking for a while while I try to redo them. Stay tuned.</p>
</section>
<section id="migration-issues" class="level2">
<h2 class="anchored" data-anchor-id="migration-issues">Migration Issues</h2>
<p>Issues I came across in case it’s useful to others:</p>
<ul>
<li>https://github.com/quarto-dev/quarto-cli/issues/10276</li>
<li>https://github.com/quarto-dev/quarto-cli/issues/5220</li>
<li>Deployment error (had to ‘rerun all’ from github itself)</li>
<li>Default radian pointing to wrong python environment which would then automatically load that environment and ignore any other env setting.</li>
<li>https://github.com/quarto-dev/quarto-cli/issues/9929 (I think this was because I was in the gh-pages branch and not the main branch)</li>
</ul>
<p>What my ultimate solution was:</p>
<p>For Python:</p>
<ul>
<li>The only env I could get things to recognize was a conda env in a default location for conda envs. My preference for uv created env, and secondarily, standard py env would not be recognized.
<ul>
<li>Would not recognize any env in project directories</li>
</ul></li>
<li>In .Rprofile (not <code>.Renviron</code>, not <code>_environment</code>, which were not resepected) put <code>Sys.setenv(RETICULATE_PYTHON = "~/anaconda3/envs/m-clark-github-io/bin/python")</code> followed by <code>library(reticulate)</code>.</li>
</ul>
<p>For publishing:</p>
<ul>
<li>The biggest issue was the inability to use the top-level directory as the output_dir as I had before.</li>
<li>I also now have to change every post file from its previous name to ‘index.qmd’ within the date-named directory in order for previous links to work. I could add an <a href="https://quarto.org/docs/websites/website-navigation.html#redirects">alias</a> to every one of the files and let them redirect, but I prefer the cleaner address, and it’s easier to rename the files collectively than to add aliases to every post.</li>
<li>I had to discover that you can’t be in the gh-pages branch (which I’m still fuzzy as to the need of). It’s mentioned in the doc, but not stressed or highlighted at all.</li>
</ul>
<p>Once I was able to get quarto to render the pages in the first place, it published pretty easily via <code>quarto publish gh-pages</code>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{clark2025,
  author = {Clark, Michael},
  title = {Some {News} for the {New} {Year}},
  date = {2025-01-01},
  url = {https://m-clark.github.io/posts/2025-news/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-clark2025" class="csl-entry quarto-appendix-citeas">
Clark, Michael. 2025. <span>“Some News for the New Year.”</span> January
1, 2025. <a href="https://m-clark.github.io/posts/2025-news/">https://m-clark.github.io/posts/2025-news/</a>.
</div></div></section></div> ]]></description>
  <guid>https://m-clark.github.io/posts/2025-news/</guid>
  <pubDate>Wed, 01 Jan 2025 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Long time no see…</title>
  <dc:creator>Michael Clark</dc:creator>
  <link>https://m-clark.github.io/posts/2024-05-20/</link>
  <description><![CDATA[ 





<section id="book-in-progess" class="level2">
<h2 class="anchored" data-anchor-id="book-in-progess">Book in progess</h2>
<p>TLDR: <a href="https://m-clark.github.io/book-of-models">https://m-clark.github.io/book-of-models</a></p>
<p>Been a long time since I posted. Part of this was due to the fact that I had almost completely transferred my site to quarto, but then never got around to finishing it. That will happen eventually, and I will hopefully start posting again at that point.</p>
<p>But the real news is that I am working on a new book. It is a book on exploring models in data science, currently titled <em>Models Demystified</em>. In it, Seth Berry and I attempt to cover a wide range of models, from the simple to the complex, in a way that is accessible to those who are not experts in statistics or machine learning, or might be coming from one area and would like the basics in the other. It covers quite a bit of ground, but tries to stick to the core what’s necessary to get started doing <em>good enough</em> modeling. We’re excited about it, and hope to have it out by the end of the year on CRC press in print, but you can <a href="https://m-clark.github.io/book-of-models">check it out now while it’s in progress</a>. Hope you enjoy it!</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{clark2024,
  author = {Clark, Michael},
  title = {Long Time No See...},
  date = {2024-05-20},
  url = {https://m-clark.github.io/posts/2024-05-20/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-clark2024" class="csl-entry quarto-appendix-citeas">
Clark, Michael. 2024. <span>“Long Time No See...”</span> May 20, 2024.
<a href="https://m-clark.github.io/posts/2024-05-20/">https://m-clark.github.io/posts/2024-05-20/</a>.
</div></div></section></div> ]]></description>
  <category>miscellaneous</category>
  <guid>https://m-clark.github.io/posts/2024-05-20/</guid>
  <pubDate>Mon, 20 May 2024 04:00:00 GMT</pubDate>
  <media:content url="https://m-clark.github.io/img/book_gp.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>Stuff Going On</title>
  <dc:creator>Michael Clark</dc:creator>
  <link>https://m-clark.github.io/posts/2023-03-misc/</link>
  <description><![CDATA[ 





<p>It’s been a bit so thought I’d force myself to post a couple things I’ve played around with, or that aren’t ready yet for a full post, or won’t be one.</p>
<section id="football-players-still-dont-know-penalty-kick-basics" class="level2">
<h2 class="anchored" data-anchor-id="football-players-still-dont-know-penalty-kick-basics">Football players still don’t know penalty kick basics</h2>
<p>Did a quick and dirty Bayesian analysis to get posterior probabilities for location, controlling for various factors. As a side note, I won the office world cup challenge with a fancy model of which I will never reveal the details, but may or may not have included lots of guessing and luck.</p>
<p><img src="https://m-clark.github.io/img/world_cup_penalty_bayes.jpg" class="img-fluid"></p>
</section>
<section id="tabular-data-post" class="level2">
<h2 class="anchored" data-anchor-id="tabular-data-post">Tabular data post</h2>
<p>I finally did my first post at the <a href="https://www.strong.io/blog/">Strong blog</a>! It’s a <a href="https://www.strong.io/blog/deep-learning-for-tabular-data-an-overview">high-level overview of tabular data and deep learning</a> that summarizes some of my previous posts <a href="https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/">here</a> and <a href="https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/">here</a>.</p>
</section>
<section id="class-imbalance" class="level2">
<h2 class="anchored" data-anchor-id="class-imbalance">Class Imbalance</h2>
<p>For my next post at the Strong blog, Elizabeth Monroe and I are working on a similarly high-level overview of issues with class imbalance we’ve been coming across. I will probably provide even more details and simulation results in a post on this site eventually, but here is a preview plot showing (mis)calibration plots at varying degrees of imbalance and different sample sizes.</p>
<p><img src="https://m-clark.github.io/img/calibration_plot_default_avg.png" class="img-fluid"></p>
</section>
<section id="two-years-at-strong" class="level2">
<h2 class="anchored" data-anchor-id="two-years-at-strong">Two years at Strong</h2>
<p>Hard to believe for me anyway, but I’ve been out of academia for two years now, after previously spending my professional lifetime there. Aside from some of the obvious differences, one of the more satisfying changes for me has been that the skills I’ve acquired are utilized on a daily basis, and something I need to continuously develop for the job. At Strong our clients want good results in a timely fashion, and though the results might be notably complex, they still need to be sufficiently interpretable as well as reproducible/production-ready. I also have come across more desire for causal explanations from clients, which might be surprising to what is typically assumed for academia vs.&nbsp;industry. Clients obviously require buy-in for what we do, but they ultimately defer to us for the expertise we provide.</p>
<p><a href="https://www.strong.io/">Strong Analytics</a> was a great move for me, because they clearly value the strong academic background of its employees, but are practically minded, and focus on skills that allow one to be nimble enough to get the clients what they need. Just like I was in academia, I am surrounded by a diverse group of smart folks I respect a lot, and am happy to solve some tough problems with. I feel I’ve learned how to get things done in a more efficient manner, and do a better job of explaining what I’ve done to wider audience.</p>
<p>Among some things I miss with academia, one was working with faculty and grad students who were just starting with an idea, and continuing a relationship with them until ultimately getting to publication or a successful dissertation defense after a very long journey. Another was giving workshops regularly where you could help people with their initial baby steps into the large world of data science. In general, it was easy to feel personally invested in the individuals you were working with, and their successes felt like your own.</p>
<p>However, in academia it was often a struggle to get buy-in for more complicated methods or new techniques, because the stakes were typically lower and people knew the minimum required to get them published, defended or whatever, and mostly just wanted help getting to that point. There’s nothing wrong with that necessarily, that’s just the practical reality, and a reflection of what’s valued in academia. Despite that, I can say I definitely had some good partnerships with people involved in challenging research that was very rewarding, and those projects made it generally very satisfying to work in academia.</p>
<p>Ultimately though, I’m happy to have made the jump. It’s a bit weird to me how much drama there is on this topic on twitter and elsewhere. It’s really not that big of a deal which route you go, and in the grand scheme of things, almost no one will care if you work in academia or industry but you. There are pros and cons to both, and people should just pick what will make them happier.</p>
</section>
<section id="coming-up" class="level2">
<h2 class="anchored" data-anchor-id="coming-up">Coming up</h2>
<p>Whenever I can get around to it, I’ll try and post on those class imbalance simulations mentioned above, conformal prediction, and some other fun stuff. Stay tuned!</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{clark2023,
  author = {Clark, Michael},
  title = {Stuff {Going} {On}},
  date = {2023-03-10},
  url = {https://m-clark.github.io/posts/2023-03-misc/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-clark2023" class="csl-entry quarto-appendix-citeas">
Clark, Michael. 2023. <span>“Stuff Going On.”</span> March 10, 2023. <a href="https://m-clark.github.io/posts/2023-03-misc/">https://m-clark.github.io/posts/2023-03-misc/</a>.
</div></div></section></div> ]]></description>
  <category>miscellaneous</category>
  <guid>https://m-clark.github.io/posts/2023-03-misc/</guid>
  <pubDate>Fri, 10 Mar 2023 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Deep Linear Models</title>
  <dc:creator>Michael Clark</dc:creator>
  <link>https://m-clark.github.io/posts/2022-09-deep-linear-models/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<blockquote class="blockquote">
<p>NB: I attempted to update my website in 2025 which required rerunning these in a new env. Hopefully the output still makes sense.</p>
</blockquote>
<p>This post gives a by-hand example of a linear model using <span class="pack">pytorch</span>. A good question to ask right off the bat would be- why would anyone do this? We use deep learning typically because linear regression isn’t up to the task! Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like linear regression, but want to get into deep learning in a conceptual manner. Another is to just see some <span class="pack">pytorch</span> basics in a simple setting. And one last reason is that maybe you want to incorporate a more standard statistical modeling approach into some other deep learning endeavor. Everyone can join the party!</p>
<p>For this demo we’ll use <a href="https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch">an example by <span class="pack">fastai</span></a>, which is a great resource for <a href="https://course.fast.ai/">getting started with deep learning</a>. While their example serves as a basis, I will generalize the functionality so that you can play around with the settings and try other data examples<sup>1</sup>. In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions<sup>2</sup>.</p>
</section>
<section id="getting-started" class="level2">
<h2 class="anchored" data-anchor-id="getting-started">Getting Started</h2>
<p>Let’s get the primary packages loaded first.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> lgb</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span></code></pre></div>
</div>
<p>Next, we’ll use the well-known <a href="https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch/data">titanic dataset</a>, and to start things off, we’ll need to get a sense of what we’re dealing with. The basic idea is that we’d like to predict survival based on key features like sex, age, ticket class and more.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># non-kaggle-requiring url here: https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/data/dl-linear-regression/titanic/train.csv</span></span>
<span id="cb2-2">df_titanic_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../../data/dl-linear-regression/titanic/train.csv'</span>)</span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># df_titanic_train</span></span></code></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df_titanic_train.describe()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare
count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000
mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208
std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429
min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000
25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400
50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200
75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000
max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200

[8 rows x 7 columns]</code></pre>
</div>
</div>
</section>
<section id="initial-data-processing" class="level2">
<h2 class="anchored" data-anchor-id="initial-data-processing">Initial Data Processing</h2>
<p>The data is not ready for modeling as is, so we’ll do some additional processing to get it ready. We’ll check out the missing values and replace them with modes<sup>3</sup>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df_titanic_train.isna().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">modes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_titanic_train.mode().iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb7-2"></span>
<span id="cb7-3">df_titanic_train.fillna(modes, inplace <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-4"></span>
<span id="cb7-5">df_titanic_train.describe(include <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (np.number))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare
count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000
mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208
std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429
min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000
25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400
50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200
75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000
max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200

[8 rows x 7 columns]</code></pre>
</div>
</div>
<p>With features, sometimes it is worthwhile to log transform data for potentially more efficient optimization search. Since we have zeros, we add 1 before taking the log.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">df_titanic_train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fare'</span>].hist()</span></code></pre></div>
</div>
<p><img src="https://m-clark.github.io/img/dl-linreg/fare-hist.png" class="img-fluid" style="width:50.0%"></p>
<p>Now the transformed data looks a little more manageable. More to the point, we won’t potentially have huge coefficients relative to other covariates because of the range of the data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">df_titanic_train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LogFare'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df_titanic_train[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fare'</span>])</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># df_titanic_train['LogFare'].hist()</span></span></code></pre></div>
</div>
<p><img src="https://m-clark.github.io/img/dl-linreg/fare-hist-log.png" class="img-fluid" style="width:50.0%"></p>
<p>The <code>Pclass</code> (passenger class) feature is actually categorical.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">pclasses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(df_titanic_train.Pclass.unique())</span>
<span id="cb11-2">pclasses</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[np.int64(1), np.int64(2), np.int64(3)]</code></pre>
</div>
</div>
<p>Here are the other categorical features.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">df_titanic_train.describe(include <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                           Name   Sex  Ticket    Cabin Embarked
count                       891   891     891      891      891
unique                      891     2     681      147        3
top     Braund, Mr. Owen Harris  male  347082  B96 B98        S
freq                          1   577       7      691      646</code></pre>
</div>
</div>
<p>In order to use categorical variables, they need to be changed to numbers<sup>4</sup>, so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use <em>embeddings</em><sup>5</sup>, particularly for things that have lots of unique categories.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">df_titanic_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.get_dummies(df_titanic_train, columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sex"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Pclass"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Embarked"</span>])</span></code></pre></div>
</div>
<p>Let’s take a look at our data now.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">df_titanic_train.columns</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Index(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',
       'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male', 'Pclass_1',
       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],
      dtype='object')</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">df_titanic_train.head()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   PassengerId  Survived  ... Embarked_Q  Embarked_S
0            1         0  ...      False        True
1            2         1  ...      False       False
2            3         1  ...      False        True
3            4         1  ...      False        True
4            5         0  ...      False        True

[5 rows x 18 columns]</code></pre>
</div>
</div>
</section>
<section id="getting-started-with-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-with-pytorch">Getting Started with pytorch</h2>
<section id="setup" class="level3">
<h3 class="anchored" data-anchor-id="setup">Setup</h3>
<p>Now we are ready to prep things for specific use with <span class="pack">pytorch</span>. I will not use the same terminology as in Jeremy’s original post, so for us, <code>target</code> = ‘dependent variable’ and <code>X</code> is our feature matrix<sup>6</sup>. Both of these will be <span class="pack">pytorch</span> <em>tensors</em>, which for our purposes is just another word for an array of arbitrary size.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tensor</span>
<span id="cb20-2">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.device(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cpu'</span>)</span>
<span id="cb20-3">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor(df_titanic_train.Survived)</span></code></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">dummies <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Sex_male'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Sex_female'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pclass_1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pclass_2'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pclass_3'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Embarked_C'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Embarked_Q'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Embarked_S'</span>]</span>
<span id="cb21-2">all_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Age'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SibSp'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Parch'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LogFare'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> dummies </span>
<span id="cb21-3"></span>
<span id="cb21-4">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_titanic_train[all_features].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(pd.to_numeric).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>)</span>
<span id="cb21-5">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tensor(X.values, dtype <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>)</span>
<span id="cb21-6"></span>
<span id="cb21-7">X.shape</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([891, 12])</code></pre>
</div>
</div>
</section>
</section>
<section id="setting-up-a-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-a-linear-model">Setting up a linear model</h2>
<p>We have our feature matrix and target variable prepped. The next step is to map the features to the target by means of predicted values. In linear regression, we typically call the weights that produce the predictions <em>coefficients</em>, but in standard deep/machine learning terminology, they are usually called <em>weights</em>, or more generally, <em>parameters</em>. Here, we generate some random values between -.5 and .5 to get started<sup>7</sup>:.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">torch.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">442</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;torch._C.Generator object at 0x156153330&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">n_coeff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb25-2">coeffs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.rand(n_coeff) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># default would produce values from 0 to 1</span></span>
<span id="cb25-3">coeffs</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,
         0.2799, -0.4392,  0.2103,  0.3625])</code></pre>
</div>
</div>
<p>The original post did a form of min-max scaling to the features, basically putting everything on a potentially [0, 1] scale. Here we’ll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># vals,indices = X.max(dim=0)</span></span>
<span id="cb27-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># X = X / vals</span></span>
<span id="cb27-3">X_means <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.mean(dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, keepdim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb27-4">X_sds   <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.std(dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb27-5"></span>
<span id="cb27-6">X_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> X_means) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> X_sds</span>
<span id="cb27-7"></span>
<span id="cb27-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># X_sc.mean(dim = 0)  # all means = 0 </span></span>
<span id="cb27-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># X_sc.std(dim = 0)   # all sd = 1</span></span></code></pre></div>
</div>
<p>As noted in the original post and worth iterating here for our statistical modeling crowd, we don’t estimate an intercept for this model and keep all the dummy coded features. The following takes our coefficients, multiplies them by their respective feature, and sums them.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (X_sc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> coeffs).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb28-2">preds[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([ 0.6000, -1.9341,  0.2080,  0.1723, -0.0032,  0.3088, -0.5066,  1.6219,
         0.6990, -1.2584])</code></pre>
</div>
</div>
<p>We can calculate our <em>loss</em>, the difference in our predictions versus the target values, in many ways. Here we get the mean squared error.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.square(preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> target).mean()</span>
<span id="cb30-2">loss</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(1.3960)</code></pre>
</div>
</div>
<p>Now we’ll create functions that do the previous steps, and finally, give it a test run! In the original <span class="pack">fastai</span> formulation, they use mean absolute error for the loss, which actually is just the <code>L1loss</code> that is available in torch. For a change of pace, we’ll keep our mean squared error, which is sometimes called <em>L2</em> loss (this will create different results from the original notebook). I create the option within the function for you to do either. Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calc_preds(X, weights):</span>
<span id="cb32-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span>((X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> weights).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb32-3"></span>
<span id="cb32-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calc_loss(X, weights, target, which <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l2'</span>):</span>
<span id="cb32-5">    preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calc_preds(X, weights)</span>
<span id="cb32-6">    </span>
<span id="cb32-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># torch.abs(calc_preds(X, coeffs)-target).mean()  # original</span></span>
<span id="cb32-8"></span>
<span id="cb32-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> which <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l2'</span>:</span>
<span id="cb32-10">      loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.MSELoss()</span>
<span id="cb32-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>: </span>
<span id="cb32-12">      loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.L1Loss()</span>
<span id="cb32-13">      </span>
<span id="cb32-14">    L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loss(preds, target.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>())</span>
<span id="cb32-15">      </span>
<span id="cb32-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span>(L)</span>
<span id="cb32-17"></span>
<span id="cb32-18">calc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l1'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor(1.3960), tensor(0.8891))</code></pre>
</div>
</div>
<section id="doing-a-gradient-descent-step" class="level3">
<h3 class="anchored" data-anchor-id="doing-a-gradient-descent-step">Doing a Gradient Descent Step</h3>
<p>We can continue our journey onward to actually estimating the weights rather than specifying them directly, since we definitely don’t want to just keep guessing! This is an iterative process where we still start with an initial (random) guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps <em>epochs</em>, and getting our next guess requires calculating what’s called a <em>gradient</em>. Here are some resources for more detail:</p>
<ul>
<li><a href="https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work">How Does a Neural Net Really Work?</a>: great intro by Jeremy Howard</li>
<li><a href="https://m-clark.github.io/models-by-example/stochastic-gradient-descent.html">Some by-hand code using gradient descent for linear regression, R</a>, <a href="https://m-clark.github.io/models-by-example/supplemental.html#python-sgd">Python</a>: By yours truly</li>
</ul>
<p>In any case, this is basic functionality within <span class="pack">pytorch</span>, and it will keep track of each step taken.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">coeffs.requires_grad_()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,
         0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calc_loss(X_sc, coeffs, target)</span>
<span id="cb36-2">loss</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(1.3960, grad_fn=&lt;MseLossBackward0&gt;)</code></pre>
</div>
</div>
<p>We use <span class="func">backward</span> to calculate the gradients and inspect them.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">loss.backward()</span>
<span id="cb38-2"></span>
<span id="cb38-3">coeffs.grad</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-0.9311,  0.6245,  0.4957, -0.7423,  0.6008, -0.6008, -0.9158,  0.0938,
         0.7127, -1.7183,  0.1715,  1.3974])</code></pre>
</div>
</div>
<p>Each time backward is called, the gradients are added to the previous values. We can see here that they’ve now doubled.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calc_loss(X_sc, coeffs, target)</span>
<span id="cb40-2"></span>
<span id="cb40-3">loss.backward()</span>
<span id="cb40-4"></span>
<span id="cb40-5">coeffs.grad</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-1.8621,  1.2491,  0.9914, -1.4847,  1.2015, -1.2015, -1.8317,  0.1877,
         1.4254, -3.4366,  0.3431,  2.7947])</code></pre>
</div>
</div>
<p>What we want instead is to set them back to zero after they are used for our estimation step. The following does this.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calc_loss(X_sc, coeffs, target)</span>
<span id="cb42-2"></span>
<span id="cb42-3">loss.backward()</span>
<span id="cb42-4"></span>
<span id="cb42-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb42-6">    coeffs.sub_(coeffs.grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)     <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sub subtracts in place</span></span>
<span id="cb42-7">    coeffs.grad.zero_()                <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># zeros out in place</span></span>
<span id="cb42-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(calc_loss(X, coeffs, target))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-0.1836, -0.0488,  0.0922, -0.0035, -0.4435, -0.1345,  0.7624,  0.2854,
         0.0661,  0.0763,  0.1588, -0.0567], requires_grad=True)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
tensor(37.9424)</code></pre>
</div>
</div>
</section>
<section id="training-the-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-linear-model">Training the Linear Model</h3>
<p>We typically would split our data into training and test. We can do so here, or keep this data as training and import <code>test.csv</code> for the test set. The latter is actually used for the Kaggle submission, but that’s not a goal here. We’ll use <span class="pack">scikit-learn</span> for the splitting.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb44-2"></span>
<span id="cb44-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># test size .2 in keeping with fastai RandomSplitter default</span></span>
<span id="cb44-4">train_x, valid_x, train_y, valid_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(</span>
<span id="cb44-5">  X_sc, </span>
<span id="cb44-6">  target.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(), </span>
<span id="cb44-7">  test_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, </span>
<span id="cb44-8">  random_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">808</span></span>
<span id="cb44-9">)</span>
<span id="cb44-10">  </span>
<span id="cb44-11"></span>
<span id="cb44-12"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(train_x), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(valid_x) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># might be one off of the original notebook</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(712, 179)</code></pre>
</div>
</div>
<p>As before, we’ll create functions to help automate our steps:</p>
<ul>
<li>one to initialize the weights</li>
<li>a function to update weights</li>
<li>one to do a full epoch (using weights to calculate loss, updating weights)</li>
<li>one to train the entire model (run multiple times/epochs)</li>
</ul>
<p>As mentioned, the approach here is to create functions that are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each <code>verbose</code> value epoch (e.g.&nbsp;<code>verbose = 10</code> means you’ll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (possibly not too big a deal depending on your IDE).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> init_weights(n_wts): </span>
<span id="cb46-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (torch.rand(n_wts) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>).requires_grad_()</span>
<span id="cb46-3"></span>
<span id="cb46-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_weights(weights, lr):</span>
<span id="cb46-5">    weights.sub_(weights.grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> lr)</span>
<span id="cb46-6">    weights.grad.zero_()</span>
<span id="cb46-7"></span>
<span id="cb46-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> one_epoch(X, weights, target, lr, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb46-9">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calc_loss(X, weights, target)</span>
<span id="cb46-10">    loss.backward()</span>
<span id="cb46-11">    </span>
<span id="cb46-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(): update_weights(weights, lr)</span>
<span id="cb46-13">    </span>
<span id="cb46-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb46-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb46-16">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: 3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span>
<span id="cb46-17"></span>
<span id="cb46-18"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> train_model(X, target, epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span>, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb46-19">    torch.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">442</span>)</span>
<span id="cb46-20">    coeffs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> init_weights(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb46-21">    </span>
<span id="cb46-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(epochs): </span>
<span id="cb46-23">        one_epoch(X, coeffs, target, lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr, i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> verbose)</span>
<span id="cb46-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> coeffs</span></code></pre></div>
</div>
<p>Try out the functions if you like (not shown).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">calc_loss(X_sc, init_weights(X_sc.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), target).backward()</span></code></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">one_epoch(train_x, init_weights(train_x.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), train_y, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">.01</span>)</span></code></pre></div>
</div>
<p>Now train the model for multiple epochs. The loss drops very quickly before becoming more steady.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">coeffs_est <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_model(train_x, train_y, epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">.2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1.375618
  0.296216
  0.284019
  0.281221
  0.280271
  0.279923
  0.279794
  0.279746
  0.279728
  0.279721
 </code></pre>
</div>
</div>
<p>Let’s create a function to show our estimated parameters/weights/coefficients in a pretty fashion.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> show_coeffs(estimates): </span>
<span id="cb51-2">  coef_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(all_features, estimates.requires_grad_(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).numpy()))</span>
<span id="cb51-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame(coef_dict, index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>]).T</span>
<span id="cb51-4"></span>
<span id="cb51-5">show_coeffs(coeffs_est)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               value
Age        -0.090825
SibSp      -0.054449
Parch      -0.016111
LogFare     0.046320
Sex_male   -0.406538
Sex_female -0.171426
Pclass_1    0.408707
Pclass_2    0.335766
Pclass_3    0.329800
Embarked_C  0.057091
Embarked_Q  0.032813
Embarked_S  0.039464</code></pre>
</div>
</div>
</section>
<section id="measuring-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="measuring-accuracy">Measuring Accuracy</h3>
<p>It’s one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data. The following function will convert our estimates to a binary value like our target, and compares them to the target. Depending on how you did your training setup, it might be pretty bad or at least better than guessing.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> acc(X, weights, target): </span>
<span id="cb53-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (target.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> (calc_preds(X, weights) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().mean()</span>
<span id="cb53-3"></span>
<span id="cb53-4">acc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor(0.7051), tensor(0.6425))</code></pre>
</div>
</div>
</section>
<section id="using-sigmoid" class="level3">
<h3 class="anchored" data-anchor-id="using-sigmoid">Using sigmoid</h3>
<p>Nothing about the previous setup actually produces a result on the probability scale, so basing a cutoff of .5 is meaningless. you can inspect them and might see values are above 1 or below zero, which we generally don’t want<sup>8</sup>. However we do have a solution. The <em>sigmoid function</em><sup>9</sup> allows us to transform our predictions to values between 0 and 1, i.e.&nbsp;probabilities in this context, and in particular, the probability of survival. Then our <span class="func">acc</span> function will be more appropriate, where any probability &gt; .5 will be given a value of 1 (or <code>True</code> technically), while others will be 0/<code>False</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calc_preds(X, weights):</span>
<span id="cb55-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.sigmoid((X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>weights).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span></code></pre></div>
</div>
<p>We also will do more iterations, and fiddle with the learning rate (a.k.a. step size)</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">coeffs_est <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_model(</span>
<span id="cb56-2">  train_x,</span>
<span id="cb56-3">  train_y,</span>
<span id="cb56-4">  epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>,</span>
<span id="cb56-5">  lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb56-6">  verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb56-7">)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.314158
  0.154329
  0.154237
  0.154232
  0.154232
 </code></pre>
</div>
</div>
<p>Not too shabby!</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">acc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor(0.7823), tensor(0.7989))</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">show_coeffs(coeffs_est)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               value
Age        -0.516476
SibSp      -0.423656
Parch      -0.179623
LogFare     0.396468
Sex_male   -0.927410
Sex_female  0.349448
Pclass_1    0.713895
Pclass_2    0.320935
Pclass_3    0.078919
Embarked_C  0.107378
Embarked_Q  0.082943
Embarked_S -0.036137</code></pre>
</div>
</div>
<p>In implementing the sigmoid, let’s go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster)<sup>10</sup>. To do this, the coefficients will need to be a column vector, so we change our <span class="func">init_coeffs</span> function slightly<sup>11</sup>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calc_preds(X, weights): </span>
<span id="cb62-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.sigmoid(X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>weights)</span>
<span id="cb62-3"></span>
<span id="cb62-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> init_coeffs(n_wts): </span>
<span id="cb62-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (torch.rand(n_wts, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>).requires_grad_()</span></code></pre></div>
</div>
<p>Now our functions are more like the mathematical notation we’d usually see for linear regression.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D%20=%20X%5Cbeta"></p>
</section>
<section id="compare-to-linearlogistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="compare-to-linearlogistic-regression">Compare to Linear/Logistic Regression</h3>
<p>Before getting too excited, let’s compare our results to basic linear and logistic regression. The linear regression is more like our model before using the sigmoid transformation, while the logistic is more like when we used it. Depending on your settings, the logistic regression is probably doing better at this point.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> linear_model</span>
<span id="cb63-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy_score</span>
<span id="cb63-3"></span>
<span id="cb63-4"></span>
<span id="cb63-5">reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linear_model.LinearRegression()</span>
<span id="cb63-6">reg.fit(train_x, train_y)</span></code></pre></div>
<div class="cell-output-display">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LinearRegression</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>LinearRegression()</pre></div> </div></div></div></div>
</div>
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">acc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor(0.7989), tensor(0.7821))</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1">reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linear_model.LogisticRegression()</span>
<span id="cb66-2">reg.fit(train_x, train_y)</span></code></pre></div>
<div class="cell-output-display">
<style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-2 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked=""><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LogisticRegression</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html">?<span>Documentation for LogisticRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>LogisticRegression()</pre></div> </div></div></div></div>
</div>
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">accuracy_score(valid_y.numpy(), reg.predict(valid_x))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.7821229050279329</code></pre>
</div>
</div>
<p>It looks like our coefficient estimates are similar to the logistic regression ones.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">show_coeffs(coeffs_est).assign(logreg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(reg.coef_.T, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               value  logreg
Age        -0.516476 -0.4799
SibSp      -0.423656 -0.4191
Parch      -0.179623 -0.1265
LogFare     0.396468  0.3441
Sex_male   -0.927410 -0.6262
Sex_female  0.349448  0.6262
Pclass_1    0.713895  0.3941
Pclass_2    0.320935  0.0675
Pclass_3    0.078919 -0.3945
Embarked_C  0.107378  0.0546
Embarked_Q  0.082943  0.0655
Embarked_S -0.036137 -0.0890</code></pre>
</div>
</div>
</section>
</section>
<section id="a-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="a-neural-network">A Neural Network</h2>
<p><img src="https://m-clark.github.io/img/nnet.png" style="display:block; margin: 0 auto; width:33%"></p>
<p>At this point we’ve basically reproduced a general linear model. A <em>neural network</em>, on the other hand, has from one to many <em>hidden layers</em> of varying types in between input and output. Let’s say we have a single layer with two nodes. For a <em>fully connected</em> or <em>dense</em> network, we’d need weights to map our features to each node of the hidden layer (<code>n_wts</code> * <code>n_hidden</code> parameters total), and then another set of weights to map the hidden nodes to our next layer, which is our output, basically creating the predicted values. You can think of it as a second hidden layer with a single output node. With additional hidden nodes we add more complexity, but also flexibility, to the model. But this may come at a price, e.g.&nbsp;more difficulty with training due to the additional parameters that have to be estimated.</p>
<p>So basically we need matrices of weights, and the following function allows us to create those. We also add a <em>bias/intercept/constant</em> for the hidden-to-output processing. In the first layer, we divide the weights by <code>n_hidden</code> to create sums at the next layer that are of similar magnitude as the inputs. In general though, there are many ways to <a href="https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/">initialize weights</a>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> init_weights(n_wts, n_hidden <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>):</span>
<span id="cb71-2">    layer1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (torch.rand(n_wts, n_hidden) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_hidden <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># n_wts x n_hidden matrix of weights</span></span>
<span id="cb71-3">    layer2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.rand(n_hidden, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>                  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># n_hidden weights</span></span>
<span id="cb71-4">    const  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.rand(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]                               <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># constant</span></span>
<span id="cb71-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()</span></code></pre></div>
</div>
<p>Now we revise our <span class="func">calc_preds</span> function to incorporate all the weights. Initially, we extract the different sets of weights that are estimated by the model. For the original inputs, we multiply them by the layer 1 weights and sum. Then we apply a transformation to them to induce nonlinearity. Typical approaches are the sigmoid function we used before, hyperbolic tangent, and, probably the most common, the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">relu</a>. The original notebook used relu, while I use a more recent one called <em>Mish</em>, which is a variant of relu. The hidden layer nodes then get multiplied by their respective weights and summed with the constant added. We then use our sigmoid function to get the probability scale as before.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn.functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> F</span>
<span id="cb72-2"></span>
<span id="cb72-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calc_preds(X, weights):</span>
<span id="cb72-4">    l1, l2, const <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb72-5">    res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.mish(X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>l1)</span>
<span id="cb72-6">    res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> res<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>l2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> const</span>
<span id="cb72-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.sigmoid(res).flatten()</span></code></pre></div>
</div>
<p>With additional sets of weights, we use an update loop.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_weights(weights, lr):</span>
<span id="cb73-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> weights:</span>
<span id="cb73-3">        layer.sub_(layer.grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> lr)</span>
<span id="cb73-4">        layer.grad.zero_()</span></code></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">coeffs_est <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_model(train_x, train_y, epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.325837
  0.155810
  0.141485
  0.137652
  0.136034
 </code></pre>
</div>
</div>
<p>At this point we’re doing a little bit better in general, and even better than standard logistic regression on the test set!</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1">acc(train_x, coeffs_est, train_y), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb76-2">acc(valid_x, coeffs_est, valid_y), <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb76-3">accuracy_score(valid_y.numpy(), reg.predict(valid_x))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(tensor(0.8160), tensor(0.8045), 0.7821229050279329)</code></pre>
</div>
</div>
</section>
<section id="deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning">Deep Learning</h2>
<p>We previously used a single hidden layer, but we want to go deeper! That’s the whole point of deep learning right? The following modifies our previous functions to allow for an arbitrary number of layers. You’ll note there are some hacks to get the weights in a good way for each layer<sup>12</sup>, but you normally wouldn’t have to do that on your own, since most tools will provide sensible modifications.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb78" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> one_epoch(X, weights, target, lr, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb78-2">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calc_loss(X, weights, target)</span>
<span id="cb78-3">    loss.backward()</span>
<span id="cb78-4">    </span>
<span id="cb78-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad(): update_weights(weights, lr)</span>
<span id="cb78-6">    </span>
<span id="cb78-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb78-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb78-9">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: 3f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> '</span>)</span>
<span id="cb78-10"></span>
<span id="cb78-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># change loss to binary</span></span>
<span id="cb78-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calc_loss(X, weights, target, which <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'l2'</span>):</span>
<span id="cb78-13">    preds <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> calc_preds(X, weights)</span>
<span id="cb78-14"></span>
<span id="cb78-15">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.nn.BCELoss()</span>
<span id="cb78-16"></span>
<span id="cb78-17">    L <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> loss(preds, target.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>())</span>
<span id="cb78-18"></span>
<span id="cb78-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span>(L)</span>
<span id="cb78-20"></span>
<span id="cb78-21"></span>
<span id="cb78-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> init_weights(n_wts, hiddens):  </span>
<span id="cb78-23">    sizes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [n_wts] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> hiddens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb78-24">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(sizes)</span>
<span id="cb78-25">    layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(torch.rand(sizes[i], sizes[i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>sizes[i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span>
<span id="cb78-26">    consts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [(torch.rand(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)]</span>
<span id="cb78-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> l <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> layers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>consts: l.requires_grad_()</span>
<span id="cb78-28">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> layers, consts</span>
<span id="cb78-29"></span>
<span id="cb78-30"></span>
<span id="cb78-31"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> calc_preds(X, weights):</span>
<span id="cb78-32">    layers, consts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb78-33">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(layers)</span>
<span id="cb78-34">    res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X</span>
<span id="cb78-35">    </span>
<span id="cb78-36">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, l <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(layers):</span>
<span id="cb78-37">        res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> res<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>l <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> consts[i]</span>
<span id="cb78-38">    </span>
<span id="cb78-39">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>: </span>
<span id="cb78-40">      res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.mish(res)</span>
<span id="cb78-41">      </span>
<span id="cb78-42">    </span>
<span id="cb78-43">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.sigmoid(res).flatten()</span>
<span id="cb78-44"></span>
<span id="cb78-45"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_weights(weights, lr):</span>
<span id="cb78-46">    layers, consts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb78-47">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> layer <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> consts:</span>
<span id="cb78-48">        layer.sub_(layer.grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> lr)</span>
<span id="cb78-49">        layer.grad.zero_()</span>
<span id="cb78-50"></span>
<span id="cb78-51"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> train_model(X, target, hiddens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>], epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-3</span>, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb78-52">    torch.manual_seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">442</span>)</span>
<span id="cb78-53">    coeffs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> init_weights(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], hiddens)</span>
<span id="cb78-54">    </span>
<span id="cb78-55">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(epochs): </span>
<span id="cb78-56">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb78-57">            one_epoch(X, coeffs, target, lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr, verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> verbose, i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i)</span>
<span id="cb78-58">    </span>
<span id="cb78-59">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> coeffs</span></code></pre></div>
</div>
<p>With everything set up, let’s do some deep linear modeling! You can play around with the number of hidden layers, number of nodes and other settings. Feel free to explore!</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1">coeffs_est <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_model(</span>
<span id="cb79-2">  train_x,</span>
<span id="cb79-3">  train_y,</span>
<span id="cb79-4">  hiddens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">250</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>],</span>
<span id="cb79-5">  epochs  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>,</span>
<span id="cb79-6">  lr      <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>,</span>
<span id="cb79-7">  verbose <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb79-8">)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 5.123790
  0.666971
  0.653124
  0.640325
  0.628476
  0.617496
  0.607313
  0.597861
  0.589081
  0.580918
  0.573322
  0.566249
  0.559658
  0.553510
  0.547772
  0.542413
  0.537403
  0.532715
  0.528326
  0.524212
  0.520354
  0.516733
  0.513330
  0.510130
  0.507118
  0.504281
  0.501605
  0.499080
  0.496695
  0.494439
  0.492305
  0.490283
  0.488366
  0.486547
  0.484820
  0.483178
  0.481616
  0.480129
  0.478712
  0.477361
  0.476072
  0.474840
  0.473663
  0.472538
  0.471461
  0.470429
  0.469440
  0.468493
  0.467583
  0.466710
 </code></pre>
</div>
</div>
<p>Hooray! Our best model yet (at least tied).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb81" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1">pd.DataFrame({</span>
<span id="cb81-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc_train'</span>: acc(train_x, coeffs_est, train_y).flatten(), </span>
<span id="cb81-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc_test'</span>: acc(valid_x, coeffs_est, valid_y).flatten(), </span>
<span id="cb81-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc_test_glm'</span>: accuracy_score(valid_y.numpy(), (reg.predict(valid_x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">.5</span>).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>))</span>
<span id="cb81-5">}, index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       acc_train  acc_test  acc_test_glm
value    0.77809  0.804469      0.782123</code></pre>
</div>
</div>
</section>
<section id="the-elephant-in-the-room" class="level2">
<h2 class="anchored" data-anchor-id="the-elephant-in-the-room">The Elephant in the Room</h2>
<p>As noted in my previous posts [<a href="https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/">1</a>, <a href="https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/">2</a>], probably your biggest challenge in implementing a deep learning model for tabular data, one with mixed data types and other complexities, is beating an off the shelf boosting model. Here is a quick demo with <span class="pack">lightgbm</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LGBMClassifier</span>
<span id="cb83-2"></span>
<span id="cb83-3">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LGBMClassifier(</span>
<span id="cb83-4">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># n_estimators = 500,  # the sorts of parameters you can play with (many more!)</span></span>
<span id="cb83-5">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># max_depth    = 4,</span></span>
<span id="cb83-6">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># reg_alpha    = .1</span></span>
<span id="cb83-7">)</span>
<span id="cb83-8"></span>
<span id="cb83-9">model.fit(train_x, train_y)</span></code></pre></div>
<div class="cell-output-display">
<style>#sk-container-id-3 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-3 {
  color: var(--sklearn-color-text);
}

#sk-container-id-3 pre {
  padding: 0;
}

#sk-container-id-3 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-3 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-3 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-3 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-3 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-3 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-3 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-3 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-3 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-3 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-3 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-3 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-3 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-3 div.sk-label label.sk-toggleable__label,
#sk-container-id-3 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-3 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-3 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-3 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-3 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-3 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-3 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-3 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-3 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LGBMClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked=""><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LGBMClassifier</div></div><div><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>LGBMClassifier()</pre></div> </div></div></div></div>
</div>
<div class="sourceCode cell-code" id="cb84" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1">model.score(valid_x, valid_y.numpy())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.8491620111731844</code></pre>
</div>
<div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"></span>
<span id="cb86-2"></span>
<span id="cb86-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># sklearn example</span></span>
<span id="cb86-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># from sklearn.ensemble import HistGradientBoostingClassifier</span></span>
<span id="cb86-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># </span></span>
<span id="cb86-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># res = HistGradientBoostingClassifier().fit(train_x.numpy(), train_y.numpy())</span></span>
<span id="cb86-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># </span></span>
<span id="cb86-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># res.score(valid_x.numpy(), valid_y.numpy())</span></span></code></pre></div>
</div>
<p>No tuning at all, and we’re already doing significantly better. Granted, if you use a packaged DL model for tabular data like the one in <span class="pack">fastai</span>, you should be doing better than our little demo. Even then though, you may still find the boosting results tough to beat.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb87" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1">df_accs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({ </span>
<span id="cb87-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc_test_dl'</span>:   acc(valid_x, coeffs_est, valid_y).flatten(), </span>
<span id="cb87-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc_test_glm'</span>:  accuracy_score(valid_y.numpy(), (reg.predict(valid_x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">.5</span>).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)),</span>
<span id="cb87-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'acc_test_lgbm'</span>: model.score(valid_x, valid_y.numpy())</span>
<span id="cb87-5">}, index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>]).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb87-6"></span>
<span id="cb87-7">df_accs</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       acc_test_dl  acc_test_glm  acc_test_lgbm
value       0.8045        0.7821         0.8492</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb89" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1">df_perc_improvement <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (df_accs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> df_accs.iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># % improvement</span></span>
<span id="cb89-2">df_perc_improvement</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       acc_test_dl  acc_test_glm  acc_test_lgbm
value     2.864075           0.0       8.579466</code></pre>
</div>
</div>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This was a lot of work to do slightly better than a logistic regression! However, there is a lot going on with a typical DL model that would likely prove even better. But it also serves as a reminder to have a suitable baseline, and as we saw with the lightgbm model, it can take little effort to do very well without deep learning. Hopefully though, the peek behind the scenes to do some ‘deep’ linear modeling can make it more accessible for you.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-clark2022dl4tab2" class="csl-entry">
Clark, Michael. 2022a. <span>“Deep Learning for Tabular Data.”</span> <a href="https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/">https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/</a>.
</div>
<div id="ref-clark2022dl4tab1" class="csl-entry">
———. 2022b. <span>“This Is Definitely Not All You Need.”</span> <a href="https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/">https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/</a>.
</div>
<div id="ref-howard2022neuralnet" class="csl-entry">
Howard, Jeremy. 2022a. <span>“How Does a Neural Net Really Work?”</span> <em>Kaggle</em>. <a href="https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work">https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work</a>.
</div>
<div id="ref-howard2022linreg" class="csl-entry">
———. 2022b. <span>“Linear Model and Neural Net from Scratch.”</span> <em>Kaggle</em>. <a href="https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch">https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch</a>.
</div>
<div id="ref-howard2022neuralnet2" class="csl-entry">
———. 2022c. <span>“What Is Torch.nn Really?”</span> <em>Kaggle</em>. <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">https://pytorch.org/tutorials/beginner/nn_tutorial.html</a>.
</div>
<div id="ref-raschka2022chrono" class="csl-entry">
Raschka, Sebastian. 2022. <span>“A Short Chronology of Deep Learning for Tabular Data.”</span> <a href="https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html">https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I won’t actually use <span class="pack">fastai</span>, since they aren’t up to supporting M1/2 Macs very well. I think it was only used for the train/test data split anyway. I would rant a bit about this, but a lot of <span class="pack">fastai</span> is geared toward non-local computing, and the fault is really with Apple and NVidia as near as I can tell.↩︎</p></li>
<li id="fn2"><p>I’m also not going to go into broadcasting, submitting to Kaggle, and other things that I don’t think are necessary for our purposes here.↩︎</p></li>
<li id="fn3"><p>Just as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here, and sometimes that’s enough.↩︎</p></li>
<li id="fn4"><p>Even though every modeling tool requires this, strangely very few in the Python world offer options for automatic handling of such things, but it’s getting better.↩︎</p></li>
<li id="fn5"><p>We actually aren’t too far removed from this in our model coming up, the main difference is that we don’t treat the categorical feature part of the model separately.↩︎</p></li>
<li id="fn6"><p>I’ll not perpetuate calling features/predictor variables that are clearly not independent as independent. That nomenclature really only works for randomized experiments, and that is definitely not the case here.↩︎</p></li>
<li id="fn7"><p>You could use <span class="func">torch.randn</span> to get standard normal values, and often times we’ll even start with just zeros if we really are just doing a standard linear model.↩︎</p></li>
<li id="fn8"><p>Unless you are an economist, in which case you call it a <em>linear probability model</em> and ignore the ridiculous predictions because you have very fine standard errors.↩︎</p></li>
<li id="fn9"><p>A lot of R folks seem unaware that the base R <span class="func">plogis</span> function accomplishes this.↩︎</p></li>
<li id="fn10"><p>The <code>@</code> operator is essentially the dot product, so <code>x@y</code> is <code>np.dot(x, y)</code>↩︎</p></li>
<li id="fn11"><p>The <span class="pack">fastai</span> demo also changes the target to a column vector, but this doesn’t seem necessary.↩︎</p></li>
<li id="fn12"><p>And they probably aren’t as good for the changes I’ve made.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{clark2022,
  author = {Clark, Michael},
  title = {Deep {Linear} {Models}},
  date = {2022-10-10},
  url = {https://m-clark.github.io/posts/2022-09-deep-linear-models/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-clark2022" class="csl-entry quarto-appendix-citeas">
Clark, Michael. 2022. <span>“Deep Linear Models.”</span> October 10,
2022. <a href="https://m-clark.github.io/posts/2022-09-deep-linear-models/">https://m-clark.github.io/posts/2022-09-deep-linear-models/</a>.
</div></div></section></div> ]]></description>
  <category>deep learning</category>
  <category>boosting</category>
  <category>GLM</category>
  <category>regression</category>
  <category>machine learning</category>
  <guid>https://m-clark.github.io/posts/2022-09-deep-linear-models/</guid>
  <pubDate>Mon, 10 Oct 2022 04:00:00 GMT</pubDate>
  <media:content url="https://m-clark.github.io/img/nnet.png" medium="image" type="image/png" height="120" width="144"/>
</item>
<item>
  <title>Deep Learning for Tabular Data</title>
  <dc:creator>Michael Clark</dc:creator>
  <link>https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In a previous post, I offered <a href="../2021-07-15-dl-for-tabular/">a summary of several articles</a> that came out over the summer of 2021 regarding the application of deep learning (DL) methods to tabular data. DL has shown astounding success in the natural language processing, computer vision, and other fields, but when it comes to the sorts of data common in other situations, especially where data is usually smaller and of mixed source and type (e.g.&nbsp;demographic, social science, biological data), results were mostly unimpressive for complex DL architectures. In particular, it did not appear that DL methods could consistently compete with, much less consistently beat, common machine learning (ML) approaches such as gradient boosting (e.g.&nbsp;XGBoost). Here I provide a bit of an update, as another few articles have come along continuing the fight.</p>
</section>
<section id="tldr-the-meta-analysis" class="level2">
<h2 class="anchored" data-anchor-id="tldr-the-meta-analysis">TLDR: the meta-analysis</h2>
<p>I collected most of the results from the summarized articles here and those covered in the previous post to see if we come to any general conclusions about which methods are best or work best in certain settings. In the following tables, I excluded those I knew to be image data, as well as datasets where I thought results were indistinguishable across all models tested (e.g.&nbsp;less than 1% difference in accuracy). This left comparisons for 92 datasets across six articles. However, it’s important to note that these were not independent datasets or studies. For example, Gorishniy et al.&nbsp;are the source of two papers and essentially the same testing situations, and other datasets were common across papers (e.g.&nbsp;Higgs Boson). In the rare situations there was a tie, I gave the nod to boosting methods as a. the whole point is to do better than those, b. they are the easier model to implement, and c.&nbsp;they are not always given the same advantages in these studies (e.g.&nbsp;pre-processing).</p>
<section id="feature-type" class="level5">
<h5 class="anchored" data-anchor-id="feature-type">Feature Type</h5>
<p>The following shows results by feature type.</p>
<ul>
<li><em>Heterogeneous</em>: at least 10% of categorical or numeric data with the rest of the other</li>
<li><em>Minimal combo</em>: means any feature inclusion of a different type. In the second table I collapse to ‘any heterogeneous’.</li>
<li><em>Boost</em>: Any boosting method (most of the time it’s XGBoost but could include lightGBM or other variant)</li>
<li><em>MLP</em>: multilayer perceptron or some variant</li>
<li><em>DL_complex</em>: A DL method more complex than MLP and which is typically the focus of the paper</li>
</ul>
<p>The results suggest that current DL approaches’ strength is mostly with purely numeric data, and for heterogeneous data, simpler MLP or Boosting will generally prevail. I initially thought that boosting would do even better with heterogeneous data, and I still suspect that with more heterogeneous data and on more equal footing, results would tilt even more.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-feature-type" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-feature-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Feature Type
</figcaption>
<div aria-describedby="tbl-feature-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="wyjwtoybok" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
  
  
<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false" data-bgcolor="#FFFFFF">
<thead style="border-style: none;">
<tr class="gt_col_headings header" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: none; border-bottom-width: 1px; border-bottom-color: #334422; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3;">
<th id="winner_model_type" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" style="text-align: left; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">winner_model_type</th>
<th id="All Cat" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">All Cat</th>
<th id="All Num" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">All Num</th>
<th id="Heterogeneous" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">Heterogeneous</th>
<th id="Min. Combo" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">Min. Combo</th>
</tr>
</thead>
<tbody class="gt_table_body" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #FFFFFF;">
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">Boost</td>
<td class="gt_row gt_right" headers="All Cat" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">2</td>
<td class="gt_row gt_right" headers="All Num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">10</td>
<td class="gt_row gt_right" headers="Heterogeneous" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">14</td>
<td class="gt_row gt_right" headers="Min. Combo" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">6</td>
</tr>
<tr class="even" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">MLP</td>
<td class="gt_row gt_right" headers="All Cat" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">2</td>
<td class="gt_row gt_right" headers="All Num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">4</td>
<td class="gt_row gt_right" headers="Heterogeneous" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">9</td>
<td class="gt_row gt_right" headers="Min. Combo" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">11</td>
</tr>
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">DL_complex</td>
<td class="gt_row gt_right" headers="All Cat" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">0</td>
<td class="gt_row gt_right" headers="All Num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">22</td>
<td class="gt_row gt_right" headers="Heterogeneous" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">7</td>
<td class="gt_row gt_right" headers="Min. Combo" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">5</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output-display">
<div id="sonptfdbgd" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
  
  
<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false" data-bgcolor="#FFFFFF">
<thead style="border-style: none;">
<tr class="gt_col_headings header" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: none; border-bottom-width: 1px; border-bottom-color: #334422; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3;">
<th id="winner_model_type" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" style="text-align: left; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">winner_model_type</th>
<th id="All Cat" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">All Cat</th>
<th id="All Num" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">All Num</th>
<th id="Any Combo" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">Any Combo</th>
</tr>
</thead>
<tbody class="gt_table_body" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #FFFFFF;">
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">Boost</td>
<td class="gt_row gt_right" headers="All Cat" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">2</td>
<td class="gt_row gt_right" headers="All Num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">10</td>
<td class="gt_row gt_right" headers="Any Combo" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">20</td>
</tr>
<tr class="even" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">MLP</td>
<td class="gt_row gt_right" headers="All Cat" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">2</td>
<td class="gt_row gt_right" headers="All Num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">4</td>
<td class="gt_row gt_right" headers="Any Combo" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">20</td>
</tr>
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">DL_complex</td>
<td class="gt_row gt_right" headers="All Cat" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">0</td>
<td class="gt_row gt_right" headers="All Num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">22</td>
<td class="gt_row gt_right" headers="Any Combo" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">12</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="samplefeature-set-size" class="level5">
<h5 class="anchored" data-anchor-id="samplefeature-set-size">Sample/Feature Set Size</h5>
<p>The following suggests that complex DL methods are going to require a lot of data to perform better. This isn’t that surprising but the difference here is quite dramatic. Interestingly, MLP methods worked well for fewer features. N total in this case means total size reported (not just training).</p>
<div class="cell" data-layout-align="center">
<div id="tbl-feature-size" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-feature-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Sample Size
</figcaption>
<div aria-describedby="tbl-feature-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="kfoppiraiy" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
  
  
<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false" data-bgcolor="#FFFFFF">
<thead style="border-style: none;">
<tr class="gt_col_headings header" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: none; border-bottom-width: 1px; border-bottom-color: #334422; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3;">
<th id="winner_model_type" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" style="text-align: left; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">winner_model_type</th>
<th id="N features" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">N features</th>
<th id="N total" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">N total</th>
</tr>
</thead>
<tbody class="gt_table_body" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #FFFFFF;">
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">Boost</td>
<td class="gt_row gt_right" headers="N features" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">209</td>
<td class="gt_row gt_right" headers="N total" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">133,309</td>
</tr>
<tr class="even" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">DL_complex</td>
<td class="gt_row gt_right" headers="N features" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">207</td>
<td class="gt_row gt_right" headers="N total" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">530,976</td>
</tr>
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">MLP</td>
<td class="gt_row gt_right" headers="N features" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">114</td>
<td class="gt_row gt_right" headers="N total" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">114,164</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="target-type" class="level5">
<h5 class="anchored" data-anchor-id="target-type">Target Type</h5>
<p>In the following we compare binary (bin), multiclass (mc), and numeric (num) target results<sup>1</sup>, but there’s no strong conclusion for this. The main thing to glean from this is that these papers do not test numeric targets nearly enough. Across dozens of disciplines and countless datasets that I’ve come across in various settings, if anything, this ratio should be reversed.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-target" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-target-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Target Type
</figcaption>
<div aria-describedby="tbl-target-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="owdiwpomyb" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
  
  
<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false" data-bgcolor="#FFFFFF">
<thead style="border-style: none;">
<tr class="gt_col_headings header" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: none; border-bottom-width: 1px; border-bottom-color: #334422; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3;">
<th id="winner_model_type" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" style="text-align: left; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">winner_model_type</th>
<th id="bin" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">bin</th>
<th id="mc" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">mc</th>
<th id="num" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">num</th>
</tr>
</thead>
<tbody class="gt_table_body" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #FFFFFF;">
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">Boost</td>
<td class="gt_row gt_right" headers="bin" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">17</td>
<td class="gt_row gt_right" headers="mc" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">10</td>
<td class="gt_row gt_right" headers="num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">5</td>
</tr>
<tr class="even" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">DL_complex</td>
<td class="gt_row gt_right" headers="bin" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">17</td>
<td class="gt_row gt_right" headers="mc" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">11</td>
<td class="gt_row gt_right" headers="num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">6</td>
</tr>
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">MLP</td>
<td class="gt_row gt_right" headers="bin" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">10</td>
<td class="gt_row gt_right" headers="mc" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">14</td>
<td class="gt_row gt_right" headers="num" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="combinations" class="level5">
<h5 class="anchored" data-anchor-id="combinations">Combinations</h5>
<p>In the following I look at any heterogeneous, smaller data (N &lt; 200,000). A complex DL model will likely not do great in this setting.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-combo" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-combo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Combinations
</figcaption>
<div aria-describedby="tbl-combo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="ygctjnvvxs" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
  
  
<table class="gt_table do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false" data-bgcolor="#FFFFFF">
<thead style="border-style: none;">
<tr class="gt_col_headings header" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: none; border-bottom-width: 1px; border-bottom-color: #334422; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3;">
<th id="winner_model_type" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" style="text-align: left; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">winner_model_type</th>
<th id="n" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="text-align: right; border-style: none; background-color: #FFFFFF; font-size: 12px; font-weight: normal; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; font-variant-numeric: tabular-nums; color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase;" scope="col" data-bgcolor="#FFFFFF" data-valign="bottom">n</th>
</tr>
</thead>
<tbody class="gt_table_body" style="border-style: none; border-top-style: none; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #FFFFFF;">
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">Boost</td>
<td class="gt_row gt_right" headers="n" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">19</td>
</tr>
<tr class="even" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">DL_complex</td>
<td class="gt_row gt_right" headers="n" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">8</td>
</tr>
<tr class="odd" style="border-style: none;">
<td class="gt_row gt_left" headers="winner_model_type" style="text-align: left; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">MLP</td>
<td class="gt_row gt_right" headers="n" style="text-align: right; border-style: none; padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; font-variant-numeric: tabular-nums; font-family: 'Source Sans Pro'; font-weight: 400;" data-valign="middle">19</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>Now, on to the details of some of the recent results that were included.</p>
</section>
</section>
<section id="on-embeddings-for-numerical-features-in-tabular-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="on-embeddings-for-numerical-features-in-tabular-deep-learning">On Embeddings for Numerical Features in Tabular Deep Learning</h2>
<ul>
<li><em>Authors</em>: Gorishniy, Rubachev, &amp; Babenko</li>
<li><em>Year</em>: 2022</li>
<li><a href="https://arxiv.org/abs/2203.05556">Arxiv Link</a></li>
</ul>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p><span class="citation" data-cites="gorishniy2022embeddings">Yura Gorishniy, Rubachev, and Babenko (2022)</span> pit several architectures against one another, such as standard multilayer perceptron (MLP), ResNet, and their own transformer approach (see <span class="citation" data-cites="gorishniy2021tabular">Yuri Gorishniy et al. (2021)</span>). Their previous work, which was summarized in my earlier post, was focused on the architecture, while here they focus on <em>embedding</em> approaches. The primary idea is to take the value of some feature and expand it to some embedding space, then use the embedding in lieu of the raw feature. It can essentially be seen as a pre-processing task.</p>
<p>One approach they use is <em>piecewise linear encoding</em> (PLE), which they at one point describe as ‘a continuous alternative to the one-hot encoding’<sup>2</sup>. Another embedding they use is basically a fourier transform.</p>
</section>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<ul>
<li>12 public datasets mostly from previous works on tabular DL and Kaggle competitions.</li>
<li>Sizes were from ~10K to &gt;1M.</li>
<li>Target variables were binary, multiclass, or numeric.</li>
<li>The number of features ranged from 8 to 200.<br>
</li>
<li>9 of 12 data sets had only numeric features, two had a single categorical feature, and unfortunately, only one of these might be called truly <em>heterogeneous</em>, i.e., with a notable mix of categorical and numeric features<sup>3</sup>.</li>
</ul>
</section>
<section id="models-explored" class="level3">
<h3 class="anchored" data-anchor-id="models-explored">Models Explored</h3>
<ul>
<li><em>CatBoost</em></li>
<li><em>XGBoost</em></li>
<li><em>MLP</em>, <em>MLP*</em></li>
<li><em>ResNet</em>, <em>ResNet*</em></li>
<li><em>Transformer*</em></li>
</ul>
<p>* Using proposed embeddings</p>
</section>
<section id="quick-summary" class="level3">
<h3 class="anchored" data-anchor-id="quick-summary">Quick Summary</h3>
<ul>
<li>A mix of results with no clear/obvious winners (results are less distinguishable if one keeps to the actual precision of the performance metrics, and even less so if talking about statistical differences in performance).
<ul>
<li>Several datasets showed no practical difference across any model (e.g.&nbsp;all accuracy results within ~.01 of each other).</li>
</ul></li>
<li>Embedding-based approaches generally tend to improve over their non-embedding counter parts (e.g.&nbsp;MLP + embedding &gt; MLP), this was possibly the clearest result of the paper.</li>
<li>I’m not sure we could say the same for ResNet, where results were similar with or without embedding</li>
<li>XGBoost was best on the one truly heterogeneous dataset.</li>
</ul>
<!-- see code/dl_for_tabular/tbl_processing.rmd -->
<!-- ![Results Table](../../img/dl-for-tab/primary_results.png)  -->
<p>In general this was an interesting paper, and I liked the simple embedding approaches used. It was nice to see that they may be useful in some contexts. The fourier transform is something that analysts (including our team at <a href="https://strong.io">Strong</a>) have used in boosting, so I’m a bit curious why they don’t do Boosting + embeddings for comparison for that or both embedding types. These embeddings can be seen as a pre-processing step, so nothing would keep someone from using them for any model.</p>
<p>Another interesting aspect was how little difference there was in model performance. It seemed half the datasets showed extremely small differences between any model type.</p>
</section>
</section>
<section id="saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training" class="level2">
<h2 class="anchored" data-anchor-id="saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training">SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training</h2>
<ul>
<li><em>Authors</em>: Somepalli, Goldblum, Schwarzschild, Bayan-Bruss, &amp; Goldstein</li>
<li><em>Year</em>: 2021</li>
<li><a href="https://arxiv.org/abs/2106.01342">Arxiv Link</a></li>
</ul>
<section id="overview-1" class="level3">
<h3 class="anchored" data-anchor-id="overview-1">Overview</h3>
<p>This paper applies BERT-style attention over rows and columns, along with embedding/data augmentation. They distinguish the standard attention over features, with intersample attention of rows. In addition, they use <em>CutMix</em> for data augmentation (originally devised for images), which basically combines pairs of observations to create a new observation<sup>4</sup>. Their model is called <em>SAINT</em>, the Self-Attention and Intersample Attention Transformer.</p>
</section>
<section id="data-1" class="level3">
<h3 class="anchored" data-anchor-id="data-1">Data</h3>
<ul>
<li>16 data sets</li>
<li>All classification, 2 multiclass</li>
<li>6 are heterogeneous, 2 notably so</li>
<li>Sizes 200 to almost 500K</li>
</ul>
</section>
<section id="models-explored-1" class="level3">
<h3 class="anchored" data-anchor-id="models-explored-1">Models Explored</h3>
<ul>
<li>Logistic Regression (!)</li>
<li>Random Forest</li>
<li>Boosting
<ul>
<li>CatBoost</li>
<li>XGBoost</li>
<li>LightGBM</li>
</ul></li>
<li>MLP</li>
<li>TabNet</li>
<li>VIME</li>
<li>TabTransformer</li>
<li>SAINT</li>
</ul>
</section>
<section id="quick-summary-1" class="level3">
<h3 class="anchored" data-anchor-id="quick-summary-1">Quick Summary</h3>
<ul>
<li><p>It seems the SAINT does quite well on some of the data, and average AUROC across all datasets is higher than XGB.</p></li>
<li><p>Main table shows only 9 datasets though, which they call ‘representative’ but it’s not clear what that means when you only have 16 to start. One dataset showed near perfect classification for all models so will not be considered. Of the 15 total remaining:</p>
<ul>
<li>SAINT wins 10 (including 3 heterogeneous)</li>
<li>Boosting wins 5 (including 2 heterogeneous)</li>
</ul></li>
<li><p>SAINT benefits from <em>data augmentation</em>. This could have been applied to any of the other models, but doesn’t appear to have been done.</p></li>
<li><p>At least they also used some form of logistic regression as a baseline, though I couldn’t find details on its implementation (e.g.&nbsp;regularization, including interactions). I don’t think this sort of simple baseline is utilized enough.</p></li>
</ul>
<p>This is an interesting result, but somewhat dampened by lack of including numeric targets and more heterogeneous data. The authors include small data settings which is great, and are careful to not generalize despite some good results, which I can appreciate.</p>
<p>I really like the fact they also compare a simple logistic regression to these models, because if you’re not able to perform notably better relative to the simplest model one could do, then why would we care? The fact that logistic regression is at times competitive and even beats boosting/SAINT methods occasionally gives me pause though. Perhaps some of these data are not sufficiently complex to be useful in distinguishing these methods? It is realistic though. While it’s best not to assume as such, sometimes a linear model is appropriate given the features and target at hand.</p>
</section>
</section>
<section id="self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning">Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning</h2>
<ul>
<li><em>Authors</em>: Kossen, Band, Lyle, Gomez, Rainforth, &amp; Gal</li>
<li><em>Year</em>: 2021</li>
<li><a href="https://arxiv.org/abs/2106.02584">Arxiv Link</a></li>
</ul>
<section id="overview-2" class="level3">
<h3 class="anchored" data-anchor-id="overview-2">Overview</h3>
<p>This paper introduces <em>Non-Parametric Transformers</em>, which focus on holistic processing of multiple inputs, and attempts to consider an entire dataset as input as opposed to a single row. Their model attempts to learn relations between data points to aid prediction. They use a mask to identify prediction points from the non-masked data, i.e.&nbsp;the entire <img src="https://latex.codecogs.com/png.latex?X_%7B%5Ctextrm%7Bnot%20masked%7D%7D%5Ctext%7B%20%7D"> data used to predict <img src="https://latex.codecogs.com/png.latex?X_%7B%5Ctextrm%7Bmasked%7D%7D%5Ctext%7B%20%7D">. The X matrix actually includes the target (also masked vs.&nbsp;not). At prediction, the model is able to make use of the correlations of inputs of training to ultimately make a prediction.</p>
</section>
<section id="data-2" class="level3">
<h3 class="anchored" data-anchor-id="data-2">Data</h3>
<ul>
<li>10 datasets from UCI, 2 are image (CIFAR MNIST)</li>
<li>4 binary, 2 multiclass, 4 numeric targets</li>
</ul>
</section>
<section id="models-explored-2" class="level3">
<h3 class="anchored" data-anchor-id="models-explored-2">Models Explored</h3>
<ul>
<li>NPT</li>
<li>Boosting
<ul>
<li>GB</li>
<li>XGB</li>
<li>CatBoost</li>
<li>LightGBM</li>
</ul></li>
<li>Random Forest</li>
<li>TabNet</li>
<li>Knn</li>
</ul>
</section>
<section id="quick-summary-2" class="level3">
<h3 class="anchored" data-anchor-id="quick-summary-2">Quick Summary</h3>
<ul>
<li>Good performance of these models, but not too different from best boosting model for any type of data.
<ul>
<li>NPT best on binary classification, but similar to CatBoost</li>
<li>Same as XGB and similar to MLP on multiclass</li>
<li>Boosting slightly better on numeric targets, but NPT similar</li>
</ul></li>
<li>As seen several times now, TabNet continues to underperform</li>
<li>k-nn regression worst (not surprising)</li>
</ul>
<p>When I first read the abstract where they say “We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input.”, I immediately was like ‘What about this, that, and those?’. The key phrase was ‘deep learning’, because the authors note later that this has a very long history in the statistical modeling realm. I was glad to see in their background of the research that they explicitly noted the models that came to my mind, like gaussian processes, kernel regression, etc. Beyond that, many are familiar with techniques like knn-regression and predictive mean matching, so it’s definitely not new to consider more than a single data point for prediction. I thought it was good of them to add k-nn regression to the model mix, even though it was not going to do well compared to the other approaches.</p>
<p>Though the author’s acknowledge a clear thread/history here, I’m not sure this result is the fundamental shift they claim, versus a further extension/expansion into the DL domain. Even techniques that may work on a single input at a time may ultimately be taking advantage of correlations among the inputs (e.g.&nbsp;spatial correlations in images). Also, automatic learning of feature interactions is standard even in basic regularized regression settings, but here their focus is on observation interactions (but see k-nn regression).</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In the two reviews on DL for tabular data that I’ve done, it appears there is more work in store for DL methods applied to tabular data. While it’d be nice to have any technique that would substantially improve prediction for such settings, I do have a suspicion results are likely rosier than they are, since that is just about the case for any newly touted technique, and at least in some cases, I don’t think we’re even making apple to apple comparisons.</p>
<p>That said, I do feel like some ground has been made for DL applications for tabular data, in that architectures can now more consistently performing as well as boosting methods in certain settings, especially if we include MLP. In the end though, results don’t appear strong enough to warrant a switch from boosting for truly heterogeneous data, or even tabular data in general. I feel like someday we’ll maybe have a breakthrough, but in the meantime, we can just agree that messy data is hard stuff to model, and the best tool is whichever one works for your specific situation.</p>
</section>
<section id="guidelines-for-future-research" class="level2">
<h2 class="anchored" data-anchor-id="guidelines-for-future-research">Guidelines for future research</h2>
<p>I was thinking about what would be a convincing result, the type of setting and setup where if a DL technique was consistently performing statistically better than boosting methods, I’d be impressed. So I’ve made a list of things I’d like to see more of, and which would make for a better story if the DL method were to beat out other techniques.</p>
<ul>
<li><p>Always use heterogeneous data. For giggles let’s say 20%+ of the minority feature type.</p></li>
<li><p>Features should at least be minimally correlated, if not notably so.</p></li>
<li><p>Image data results are not interesting (why would we use boosting on this in practice?).</p></li>
<li><p>Numeric targets should at least be as much of focus as categorical targets.</p></li>
<li><p>Include ‘small’ datasets.</p></li>
<li><p>Include very structured data (e.g.&nbsp;clustered with repeated observations, geographical points, time series).</p></li>
<li><p>Use a flexible generalized additive or similar penalized regression with interactions as a baseline statistical model.</p></li>
<li><p>Maybe add survival targets to the mix.</p></li>
<li><p>If using a pre-processing step that is done outside of modeling, this likely should be applied to non-DL methods for better comparison, especially, if we’re only considering predictive accuracy and don’t care too much about interpretation.</p></li>
<li><p>Note your model variants <strong>before</strong> analyzing any data. Tweaking/torturing model architecture after results don’t pan out is akin to p-hacking in the statistical realm, and likewise wastes both researcher and reader’s time.</p></li>
<li><p>Regarding results…</p>
<ul>
<li>Don’t claim differences that you don’t have precision to do so, or at least back them up with an actual statistical test.</li>
<li>If margin of error in the metrics is overlapping, while statistically they could be different, practically they probably aren’t to most readers. Don’t make a big deal about it.</li>
<li>It is unlikely anyone will be interested in three decimal place differences for rmse/acc type metrics, and statistically, results often don’t even support two decimal precision.</li>
<li>Report how you are obtaining uncertainty in any error estimates.</li>
<li>If straightforward, try to give an estimate of total tuning/run times.</li>
</ul></li>
<li><p>With the datasets</p>
<ul>
<li>Name datasets exactly how they are named at the source you obtained them from, provide direct links</li>
<li>Provide a breakdown for both feature and target types</li>
<li>Provide clear delineation of total/training/validation/test sizes</li>
</ul></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-gorishniy2022embeddings" class="csl-entry">
Gorishniy, Yura, Ivan Rubachev, and Artem Babenko. 2022. <span>“On Embeddings for Numerical Features in Tabular Deep Learning.”</span> <em>arXiv Preprint arXiv:2203.05556</em>.
</div>
<div id="ref-gorishniy2021tabular" class="csl-entry">
Gorishniy, Yuri, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021. <span>“Revisiting Deep Learning Models for Tabular Data.”</span> <em>arXiv Preprint arXiv:2106.11959</em>.
</div>
<div id="ref-kadra2021tabular" class="csl-entry">
Kadra, Arlind, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021. <span>“Regularization Is All You Need: Simple Neural Nets Can Excel on Tabular Data.”</span> <em>arXiv Preprint arXiv:2106.11189</em>.
</div>
<div id="ref-shwartz2021tabular" class="csl-entry">
Shwartz-Ziv, Ravid, and Amitai Armon. 2021. <span>“Tabular Data: Deep Learning Is Not All You Need.”</span> <em>arXiv Preprint arXiv:2106.03253</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I don’t refer to numeric targets as ‘regression’ because that’s silly for so many reasons. 😄↩︎</p></li>
<li id="fn2"><p>A quick look suggests it’s not too dissimilar from a <a href="https://en.wikipedia.org/wiki/B-spline#Definition">b-spline</a>.↩︎</p></li>
<li id="fn3"><p>I’ll let you go ahead and make your own prediction about which method was best on that data set.↩︎</p></li>
<li id="fn4"><p>It’s not clear to me how well this CutUp approach would actually preserve feature correlations. My gut tells me the feature correlations of this approach would be reduced relative to the observed, since the variability of the new observations is likely reduced. This ultimately may not matter for predictive purposes or their ultimate use in embeddings. However, I wonder if something like SMOTE, random (bootstrap) sampling, other DL methods like autoencoders, or similar approaches might do the same or better.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{clark2022,
  author = {Clark, Michael},
  title = {Deep {Learning} for {Tabular} {Data}},
  date = {2022-05-01},
  url = {https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-clark2022" class="csl-entry quarto-appendix-citeas">
Clark, Michael. 2022. <span>“Deep Learning for Tabular Data.”</span> May
1, 2022. <a href="https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/">https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/</a>.
</div></div></section></div> ]]></description>
  <category>deep learning</category>
  <category>machine learning</category>
  <guid>https://m-clark.github.io/posts/2022-04-01-more-dl-for-tabular/</guid>
  <pubDate>Sun, 01 May 2022 04:00:00 GMT</pubDate>
  <media:content url="https://m-clark.github.io/img/nnet.png" medium="image" type="image/png" height="120" width="144"/>
</item>
<item>
  <title>Double Descent</title>
  <dc:creator>Michael Clark</dc:creator>
  <link>https://m-clark.github.io/posts/2021-10-30-double-descent/</link>
  <description><![CDATA[ 





<p>A co-worker passed along a recent article <span class="citation" data-cites="dar2021farewell">(Dar, Muthukumar, and Baraniuk 2021)</span> on the topic of <em>double descent</em> in machine learning. I figured I’d summarize some key points I came across while perusing it and some referenced articles. In addition, I’ll provide an accessible example demonstrating the phenomenon.</p>
<section id="what-is-double-descent" class="level2">
<h2 class="anchored" data-anchor-id="what-is-double-descent">What is double descent?</h2>
<section id="bias-variance-trade-off" class="level3">
<h3 class="anchored" data-anchor-id="bias-variance-trade-off">Bias-variance trade-off</h3>
<p>To understand double descent you have to revisit the concept of the <em>bias-variance trade-off</em>. Without going into too much detail, the main idea with it is that having an overly complex model leads to <em>overfitting</em> the training data, which results in worse prediction on new data, at least relative to what simpler models would have done. The classic figure looks like the following, where blue is the <em>training error</em> and the red is the <em>test error</em>. Thin lines represent one path of complexity (e.g.&nbsp;across a random sample of the data), while the thicker lines are the average at a particular point of model complexity.</p>
<p><img src="https://m-clark.github.io/img/double-descent/biasvar2.svg" class="img-fluid"></p>
<p>If we don’t have a sufficiently complex model, both training and test error will be poor, the case of <em>underfitting</em>. Our model is a poor approximation of the true underlying function, and predicts poorly on data both seen and unseen. When we have too much model complexity relative to the size of our data (e.g.&nbsp;more covariates, nonlinear effects, interactions, etc.), we pass into the overfit situation. Essentially, while our model function would result in a decrease in error with the data it’s trained on (lower bias as it better approximates the true underlying function), with too much complexity, you’d also eventually have notable changes in prediction (high variance) with any slight deviation in the underlying training data. We can even get to the point where we fit the training data perfectly, but it will be overly susceptible to the noise in the data, and not do well with unseen observations.</p>
<p>To combat this, we usually attempt to find a balance between overly simple and overly complex models. This would be the point where test error is among its lowest point for a desirable level of complexity (e.g.&nbsp;around 20-25 df in the figure above), before it begins to rise again. This may be accomplished more explicitly, for example, picking a model through cross-validation, or more implicitly, for example, through regularization (<span class="citation" data-cites="belkin2019reconciling">Belkin et al. (2019)</span>). For more detail on the bias-variance trade-off, you can look at the exposition in the main article noted above, <a href="https://m-clark.github.io/introduction-to-machine-learning/concepts.html#bias-variance-tradeoff">my document here</a>, or any number of places, as it is an extremely well-known idea in machine learning.</p>
</section>
<section id="double-descent" class="level3">
<h3 class="anchored" data-anchor-id="double-descent">Double Descent</h3>
<p>The funny thing is, it turns out that the above actually only applies to a specific scenario, one which we will call <em>underparameterized</em> models. We can simplify this notion by just thinking of the case where the number of our parameters to estimate is less than or equal to the number of observations we have to work with. Nowadays though, it’s not uncommon to have what we’d call <em>overparameterized</em> models, such as random forests and neural networks, sometimes with even billions of parameters, far exceeding the data size. In this scenario, when we revisit the trade-off, something unusual happens!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://m-clark.github.io/img/double-descent/double_descent_concept.png" class="img-fluid figure-img"></p>
<figcaption>Figure from <span class="citation" data-cites="dar2021farewell">Dar, Muthukumar, and Baraniuk (2021)</span></figcaption>
</figure>
</div>
<p>Such models may have near zero training error, yet do well on unseen data. As we increase complexity, we see something like a second bias-variance trade-off beyond the point where the data is perfectly fit (interpolated). This point is where model complexity (e.g.&nbsp;in terms of number of parameters) <em>p</em> equals the number of observations <em>N</em>, and this is where the realm of the overparameterized models begins. Now test error begins to drop again with increasing complexity.</p>
</section>
</section>
<section id="an-example" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="an-example">An example</h2>
<p>I thought it would be amusing to try this with the classic <code>mtcars</code> data set available in base R. With this data, our goal will be to predict fuel consumption in miles per gallon (<code>mpg</code>). First we will split the data into training and test components. We create a model where our number of parameters (<code>p</code>), in this case standard regression coefficients, will equal the number of observations (<code>N</code>). Some of the more technically savvy will know that if the number of features and/or parameters to estimate <code>p</code> equals the number of observations <code>N</code>, a standard linear regression model will fit the data perfectly<sup>1</sup>, demonstrated below.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>If not familiar, the <code>mtcars</code> object is a data frame that comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).</p>
</div></div><div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">nc <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ncol</span>(mtcars) </span>
<span id="cb1-2">nr <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> nc</span>
<span id="cb1-3">fit_perfect <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lm</span>(mpg <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> ., mtcars[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>nr, ])</span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># summary(fit_perfect) # not shown, all inferential estimates are NaN</span></span></code></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://m-clark.github.io/posts/2021-10-30-double-descent/index_files/figure-html/vis-fit-perfect-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Now let’s look at the test error, our prediction on the unseen data we didn’t use in fitting the model. When we do, we see the usual bias-variance trade-off. Our generalizability capabilities have plummeted, as we have overfit the training data and were unable to accommodate unseen observations. We are even predicting negative mpg in some cases!</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://m-clark.github.io/posts/2021-10-30-double-descent/index_files/figure-html/vis-fit-perfect-test-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="p-n" class="level3">
<h3 class="anchored" data-anchor-id="p-n">p ≤ N</h3>
<p>Let’s extend the demonstration more fully. We now create models of increasing complexity, starting with an intercept only model (i.e.&nbsp;just using the mean for prediction), to one where all other columns (10) in the data are predictors. Here I repeatedly sampled <code>mtcars</code> of size <img src="https://latex.codecogs.com/png.latex?N%20=%2010"> for training, the remainder for test, and also shuffled the columns each time, doing so for a total of 250 times<sup>2</sup>. Here is the result- the classic bias variance trade-off curve. The larger dot shows the test error minimum, at about 3 covariates (plus intercept). The vertical line denotes our point of interpolation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://m-clark.github.io/img/double-descent/dd_mtcars_under.svg" class="img-fluid figure-img"></p>
<figcaption>Double Descent in the underparameterized setting.</figcaption>
</figure>
</div>
</section>
<section id="p-n-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="p-n-1">p &gt; N</h3>
<p>So with one of the simpler data sets around we were able to demonstrate the bias-variance trade-off clearly. But now let’s try overparameterized models! We don’t need anything fancy or complicated to do this, so for our purposes, I’m just going to add cubic spline basis expansions for the <code>wt</code>, <code>disp</code>, and <code>hp</code> features<sup>3</sup>. This will definitely be enough to put us in a situation where we have more parameters than data, i.e.&nbsp;p &gt; N, but doesn’t make things too abstract<sup>4</sup>.</p>
<p>The basic linear model approach we might typically use fails to estimate the additional parameters in this situation, so we need a different estimator. Some are familiar with penalized regression techniques such as <em>lasso</em> and <em>ridge</em> regression, and we could use those here. However, I’ll use <em>ridgeless regression</em>, as depicted in <span class="citation" data-cites="hastie2019surprises">Hastie et al. (2019)</span>, and which, like ridge regression, is a straightforward variant of the usual least squares regression<sup>5</sup>. I estimate the coefficients/weights on the training data, and make predictions for the training and test set, calculating their respective errors. Here is an example of the primary function used.</p>
<div class="cell" data-layout-align="center" data-code_folding="true">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">fit_ridgeless <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(X_train, y, X_test, y_test){</span>
<span id="cb2-2">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get the coefficient estimates</span></span>
<span id="cb2-3">  b <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pseudo_inv</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">crossprod</span>(X_train)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">crossprod</span>(X_train, y)</span>
<span id="cb2-4">  </span>
<span id="cb2-5">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get training/test predictions</span></span>
<span id="cb2-6">  predictions_train <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> X_train <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> b</span>
<span id="cb2-7">  predictions_test  <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> X_test <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%*%</span> b</span>
<span id="cb2-8">  </span>
<span id="cb2-9">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get training/test error</span></span>
<span id="cb2-10">  rmse_train <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>((y <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> predictions_train[,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb2-11">  rmse_test  <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mean</span>((y_test <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> predictions_test[,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb2-12">  </span>
<span id="cb2-13">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># return result</span></span>
<span id="cb2-14">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb2-15">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">b =</span> b,</span>
<span id="cb2-16">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">predictions_train =</span> predictions_train,</span>
<span id="cb2-17">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">predictions_test  =</span> predictions_test,</span>
<span id="cb2-18">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rmse_train =</span> rmse_train,</span>
<span id="cb2-19">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">rmse_test  =</span> rmse_test</span>
<span id="cb2-20">  )</span>
<span id="cb2-21">}</span></code></pre></div>
</div>
<p>We can test the function as follows with as little as 10 observations, where p (all predictor coefficients plus intercept = 11 parameters) is greater than N (10). This demonstrates that the ridgeless approach can provide an estimate for all the parameters (unlike the standard <code>lm</code> function), and we also see very low training error, but relatively high test error (in terms of the root mean square error.)</p>
<div class="cell" data-layout-align="center" data-code_folding="true">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">n <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb3-2"></span>
<span id="cb3-3">X <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cbind</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, mtcars[, <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb3-4">y <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> mtcars<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>mpg <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># mpg is the first column</span></span>
<span id="cb3-5"></span>
<span id="cb3-6">X_train <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> X[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>n, ]</span>
<span id="cb3-7">y_train <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> mtcars<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>mpg[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>n]</span>
<span id="cb3-8">X_test  <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> X[<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>n),]</span>
<span id="cb3-9">y_test  <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> y[<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span>n)]</span>
<span id="cb3-10"></span>
<span id="cb3-11">result <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fit_ridgeless</span>(X_train, y_train, X_test, y_test)</span></code></pre></div>
</div>
<div id="tbl-test-fit-ridgeless" class="cell anchored" data-layout-align="center">
<div class="cell-output-display">
<div id="ikdmqttnwq" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>@import url("https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap");
@import url("https://fonts.googleapis.com/css2?family=Libre+Franklin:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap");
@import url("https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap");
#ikdmqttnwq table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#ikdmqttnwq thead, #ikdmqttnwq tbody, #ikdmqttnwq tfoot, #ikdmqttnwq tr, #ikdmqttnwq td, #ikdmqttnwq th {
  border-style: none;
}

#ikdmqttnwq p {
  margin: 0;
  padding: 0;
}

#ikdmqttnwq .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: none;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ikdmqttnwq .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#ikdmqttnwq .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ikdmqttnwq .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ikdmqttnwq .gt_heading {
  background-color: #FFFFFF;
  text-align: left;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ikdmqttnwq .gt_bottom_border {
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ikdmqttnwq .gt_col_headings {
  border-top-style: none;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: none;
  border-bottom-width: 1px;
  border-bottom-color: #334422;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ikdmqttnwq .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 12px;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ikdmqttnwq .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 12px;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ikdmqttnwq .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ikdmqttnwq .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ikdmqttnwq .gt_column_spanner {
  border-bottom-style: none;
  border-bottom-width: 1px;
  border-bottom-color: #334422;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ikdmqttnwq .gt_spanner_row {
  border-bottom-style: hidden;
}

#ikdmqttnwq .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#ikdmqttnwq .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ikdmqttnwq .gt_from_md > :first-child {
  margin-top: 0;
}

#ikdmqttnwq .gt_from_md > :last-child {
  margin-bottom: 0;
}

#ikdmqttnwq .gt_row {
  padding-top: 7px;
  padding-bottom: 7px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ikdmqttnwq .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#ikdmqttnwq .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#ikdmqttnwq .gt_row_group_first td {
  border-top-width: 2px;
}

#ikdmqttnwq .gt_row_group_first th {
  border-top-width: 2px;
}

#ikdmqttnwq .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ikdmqttnwq .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#ikdmqttnwq .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#ikdmqttnwq .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ikdmqttnwq .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ikdmqttnwq .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ikdmqttnwq .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#ikdmqttnwq .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ikdmqttnwq .gt_table_body {
  border-top-style: none;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #FFFFFF;
}

#ikdmqttnwq .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ikdmqttnwq .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ikdmqttnwq .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ikdmqttnwq .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ikdmqttnwq .gt_left {
  text-align: left;
}

#ikdmqttnwq .gt_center {
  text-align: center;
}

#ikdmqttnwq .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ikdmqttnwq .gt_font_normal {
  font-weight: normal;
}

#ikdmqttnwq .gt_font_bold {
  font-weight: bold;
}

#ikdmqttnwq .gt_font_italic {
  font-style: italic;
}

#ikdmqttnwq .gt_super {
  font-size: 65%;
}

#ikdmqttnwq .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#ikdmqttnwq .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#ikdmqttnwq .gt_indent_1 {
  text-indent: 5px;
}

#ikdmqttnwq .gt_indent_2 {
  text-indent: 10px;
}

#ikdmqttnwq .gt_indent_3 {
  text-indent: 15px;
}

#ikdmqttnwq .gt_indent_4 {
  text-indent: 20px;
}

#ikdmqttnwq .gt_indent_5 {
  text-indent: 25px;
}
</style>

<table class="gt_table caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead>
<tr class="gt_col_headings header">
<th id="b" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase" scope="col">b</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">0.84</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">−1.69</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">0.08</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">−0.08</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">2.76</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">−1.29</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">0.24</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">2.32</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">3.26</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">2.26</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="b" style="font-family: 'Source Sans Pro'; font-weight: 400">0.66</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output-display">
<div id="ltoophxgfc" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>@import url("https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap");
@import url("https://fonts.googleapis.com/css2?family=Libre+Franklin:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap");
@import url("https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap");
#ltoophxgfc table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#ltoophxgfc thead, #ltoophxgfc tbody, #ltoophxgfc tfoot, #ltoophxgfc tr, #ltoophxgfc td, #ltoophxgfc th {
  border-style: none;
}

#ltoophxgfc p {
  margin: 0;
  padding: 0;
}

#ltoophxgfc .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: none;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ltoophxgfc .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#ltoophxgfc .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ltoophxgfc .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ltoophxgfc .gt_heading {
  background-color: #FFFFFF;
  text-align: left;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ltoophxgfc .gt_bottom_border {
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ltoophxgfc .gt_col_headings {
  border-top-style: none;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: none;
  border-bottom-width: 1px;
  border-bottom-color: #334422;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ltoophxgfc .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 12px;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ltoophxgfc .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 12px;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ltoophxgfc .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ltoophxgfc .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ltoophxgfc .gt_column_spanner {
  border-bottom-style: none;
  border-bottom-width: 1px;
  border-bottom-color: #334422;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ltoophxgfc .gt_spanner_row {
  border-bottom-style: hidden;
}

#ltoophxgfc .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#ltoophxgfc .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ltoophxgfc .gt_from_md > :first-child {
  margin-top: 0;
}

#ltoophxgfc .gt_from_md > :last-child {
  margin-bottom: 0;
}

#ltoophxgfc .gt_row {
  padding-top: 7px;
  padding-bottom: 7px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ltoophxgfc .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#ltoophxgfc .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#ltoophxgfc .gt_row_group_first td {
  border-top-width: 2px;
}

#ltoophxgfc .gt_row_group_first th {
  border-top-width: 2px;
}

#ltoophxgfc .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ltoophxgfc .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#ltoophxgfc .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#ltoophxgfc .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ltoophxgfc .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ltoophxgfc .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ltoophxgfc .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#ltoophxgfc .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ltoophxgfc .gt_table_body {
  border-top-style: none;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #FFFFFF;
}

#ltoophxgfc .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ltoophxgfc .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ltoophxgfc .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ltoophxgfc .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ltoophxgfc .gt_left {
  text-align: left;
}

#ltoophxgfc .gt_center {
  text-align: center;
}

#ltoophxgfc .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ltoophxgfc .gt_font_normal {
  font-weight: normal;
}

#ltoophxgfc .gt_font_bold {
  font-weight: bold;
}

#ltoophxgfc .gt_font_italic {
  font-style: italic;
}

#ltoophxgfc .gt_super {
  font-size: 65%;
}

#ltoophxgfc .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#ltoophxgfc .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#ltoophxgfc .gt_indent_1 {
  text-indent: 5px;
}

#ltoophxgfc .gt_indent_2 {
  text-indent: 10px;
}

#ltoophxgfc .gt_indent_3 {
  text-indent: 15px;
}

#ltoophxgfc .gt_indent_4 {
  text-indent: 20px;
}

#ltoophxgfc .gt_indent_5 {
  text-indent: 25px;
}
</style>

<table class="gt_table caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead>
<tr class="gt_col_headings header">
<th id="rmse_train" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase" scope="col">rmse_train</th>
<th id="rmse_test" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="color: #A9A9A9; font-family: 'Source Sans Pro'; text-transform: uppercase" scope="col">rmse_test</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_right" headers="rmse_train" style="font-family: 'Source Sans Pro'; font-weight: 400">0.05</td>
<td class="gt_row gt_right" headers="rmse_test" style="font-family: 'Source Sans Pro'; font-weight: 400">5.79</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>If we do this for more complex models (max linear features, plus each additional set of features associated with a cubic spline basis expansions), we obtain the following. Now we see the second descent in test error takes form!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://m-clark.github.io/img/double-descent/dd_mtcars_over.svg" class="img-fluid figure-img"></p>
<figcaption>Double Descent in the overparameterized setting.</figcaption>
</figure>
</div>
<p>Putting our results together gives us the double descent curve.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://m-clark.github.io/img/double-descent/dd_mtcars.svg" class="img-fluid figure-img"></p>
<figcaption>Double Descent in the overparameterized setting.</figcaption>
</figure>
</div>
<aside>
Note that this all holds for the most part with classification problems, including multiclass (or multivariate/class targets).
</aside>
<p>We not only see the double descent pattern, but we can also note that the global test error minimum occurs with the model with the most parameters. The gray dot is the lowest test error with the underparameterized settings, while the dark red is the global test error minimum.</p>
</section>
</section>
<section id="why-does-this-happen" class="level2">
<h2 class="anchored" data-anchor-id="why-does-this-happen">Why does this happen?</h2>
<p>Understanding the double descent phenomenon is an area of active research, and there are some technical issues we won’t cover here. However, we can note a couple things more broadly. When we’re in the underparameterized situation, we ultimately begin to force features that have no association with the target to fit the data anyway. Once you move beyond the point of where these features are useful, test error begins to rise again, until the point of interpolation where test error is even worse than guessing (or just guessing in the classification case).</p>
<p>Beyond the interpolation point, all models we potentially employ using this estimation technique will have the capacity to fit the training data perfectly, i.e.&nbsp;zero bias. This allows us to fit the remaining noise in the data with the additional features employed by the more complex models. There is no guarantee that among the models you fit that the lowest test error will be found relative to the underparameterized setting. However, the lowest test error to be found is ‘out there’ somewhere<sup>6</sup>. So adding complexity will potentially allow you to find improved test error.</p>
<p>Another way to put it is that we have a single class of models to consider, and under and overparameterized are special cases of that more general class. Any one of these might result in the lowest test error. The overparameterized models, which may contain complex nonlinearities and interactions, are likely to be more compatible with the data than the simpler models<sup>7</sup>. So odds are good that at least one of them will have a smaller test error as well. In any case, restricting ourselves to the underparameterized setting is definitely no guarantee that we will find the most performant model.</p>
<p>One caveat is that the model we used is an example of ‘implicit’ regularization, one in which there is no hyper-parameter to set (or discover through cross-validation), like with ridge and lasso. With other techniques (e.g.&nbsp;optimally chosen ridge regression estimator) we may still be able to achieve optimal test error without complete interpolation, and show a reduced peak.</p>
<p><span class="citation" data-cites="dar2021farewell">Dar, Muthukumar, and Baraniuk (2021)</span> note that in the overparameterized setting, we can distinguish the signal part of the error term that reduces as a function of N/p, where the noise part of the error term is a function of p/N. In addition, there is a portion of test error related to model misspecification, which will always decrease with overparameterization. In addition, one must consider both feature correlations as well as correlations among observations. Having more complex covariance structure doesn’t negate the double descent phenomenon, but they suggest that, for example, cases where there is low effective dimension within these additional features will more readily display the double descent.</p>
<p>Another issue is that in any given situation it is difficult to know where in the realm of available models we exist presently. So additional complexity, or even additional data, may in fact hurt performance <span class="citation" data-cites="nakkiran2019deep">(Nakkiran et al. 2019)</span>.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The double descent phenomenon is a quite surprising scenario, especially for those who have only heard of the classical bias-variance trade off. There is still much to learn regarding it, but such research is off and running. For practical purposes, it is worth keeping it in mind to aid us in model selection and thinking about our modeling strategies in general.</p>
<!-- [^ridgeless]:  -->



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-belkin2019reconciling" class="csl-entry">
Belkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019. <span>“Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off.”</span> <em>Proceedings of the National Academy of Sciences</em> 116 (32): 15849–54.
</div>
<div id="ref-dar2021farewell" class="csl-entry">
Dar, Yehuda, Vidya Muthukumar, and Richard G Baraniuk. 2021. <span>“A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning.”</span> <em>arXiv Preprint arXiv:2109.02355</em>.
</div>
<div id="ref-hastie2019surprises" class="csl-entry">
Hastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. 2019. <span>“Surprises in High-Dimensional Ridgeless Least Squares Interpolation.”</span> <em>arXiv Preprint arXiv:1903.08560</em>.
</div>
<div id="ref-nakkiran2019deep" class="csl-entry">
Nakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2019. <span>“Deep Double Descent: Where Bigger Models and More Data Hurt.”</span> <em>arXiv Preprint arXiv:1912.02292</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>R<sup>2</sup> = 1 in the standard linear model setting.↩︎</p></li>
<li id="fn2"><p>Note that the intercept term is added after data shuffling so when p = 1 it is the intercept only model, i.e.&nbsp;guessing the mean.↩︎</p></li>
<li id="fn3"><p>I used <span class="pack" style="">mgcv</span> to so this, then added them in whole for each term to the previously shuffled model matrix. These columns are not shuffled. By default these will add 10 columns each to the model matrix.↩︎</p></li>
<li id="fn4"><p>For more on generalized additive models, see <a href="https://m-clark.github.io/generalized-additive-models/">my document</a>.↩︎</p></li>
<li id="fn5"><p>Ridgeless regression has the same form as the ‘normal’ equations for least squares, but instead of <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Csim%20(X%5ETX)%5E%7B-1%7D%20%5Ccdot%20X%5ETy">, we have <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Csim%20(X%5ETX)%5E%7B+%7D%20%5Ccdot%20X%5ETy"> where the first part is the pseudo-inverse of <img src="https://latex.codecogs.com/png.latex?X">. It is similar to equations for ridge regression (see my demo <a href="https://m-clark.github.io/models-by-example/penalized-maximum-likelihood.html#l2-ridge-regularization">here</a>) and can be seen as an approximation to it as the ridge penalty tends toward zero.↩︎</p></li>
<li id="fn6"><p>Fox Mulder told me so.↩︎</p></li>
<li id="fn7"><p>Because nature is just funny that way.↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{clark2021,
  author = {Clark, Michael},
  title = {Double {Descent}},
  date = {2021-11-13},
  url = {https://m-clark.github.io/posts/2021-10-30-double-descent/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-clark2021" class="csl-entry quarto-appendix-citeas">
Clark, Michael. 2021. <span>“Double Descent.”</span> November 13, 2021.
<a href="https://m-clark.github.io/posts/2021-10-30-double-descent/">https://m-clark.github.io/posts/2021-10-30-double-descent/</a>.
</div></div></section></div> ]]></description>
  <category>deep learning</category>
  <category>machine learning</category>
  <guid>https://m-clark.github.io/posts/2021-10-30-double-descent/</guid>
  <pubDate>Sat, 13 Nov 2021 05:00:00 GMT</pubDate>
  <media:content url="https://m-clark.github.io/img/double-descent/dd_mtcars.png" medium="image" type="image/png" height="86" width="144"/>
</item>
<item>
  <title>This is definitely not all you need</title>
  <dc:creator>Michael Clark</dc:creator>
  <link>https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>I’ve been a little perplexed at the lack of attention of deep learning (DL) toward what I consider to be ‘default’ data in my world, often referred to as <em>tabular data</em>, where typically we have a two dimensional input of observations (rows) and features (columns) and inputs are of varying type, scale and source. Despite the ubiquity of such data in data science generally, and despite momentous advances in areas like computer vision and natural language processing, at this time, it’s not very clear what the status of DL for tabular data is.</p>
<p>There have been developments in the area recently though, with some modeling approaches, such as TabNet, gaining traction. In June of 2021, I actually came across three papers on <a href="arxiv.org">Arxiv</a> that were making related claims about the efficacy of DL for tabular data. As in many academic and practical (and life) pursuits, results of these studies are nuanced, so I thought I’d help myself and others by summarizing here.</p>
</section>
<section id="goal" class="level2">
<h2 class="anchored" data-anchor-id="goal">Goal</h2>
<p>I want to know if, e.g.&nbsp;time and/or resources are limited, whether it will be worth diving into a DL model if I have a satisfactory simpler/easier one ready to implement that does pretty well. Perhaps this answer is already, ‘if it ain’t broke, don’t fix it’, but given the advancements in other data domains, it would be good to assess what the current state of DL with tabular data is.</p>
</section>
<section id="caveats" class="level2">
<h2 class="anchored" data-anchor-id="caveats">Caveats</h2>
<ul>
<li>I’m not going to do more than give a cursory summary of the articles, and provide no in-depth explanation of the models. For more detail, see the corresponding articles and references for the models therein. You are not going to learn how to use TabNet, NODE, transformers, etc., for tabular data.</li>
<li>There are other decent articles on the topic not covered here. Some are referenced in these more recent offerings, so feel free to peruse.</li>
</ul>
</section>
<section id="quick-take" class="level2">
<h2 class="anchored" data-anchor-id="quick-take">Quick Take</h2>
<p>In case you don’t want any detail, here’s a quick summary based on my impressions from these articles. Right now, if you want to use DL on tabular data, don’t make a fuss of it. A simple architecture, even a standard multi-layer perceptron, will likely do as well as more complicated ones. In general though, the amount of effort put into prep/tuning may not be worth it for many typical tabular data settings, for example, relative to a suitably flexible statistical model (e.g.&nbsp;GAMM) or a default fast boosting implementation like XGBoost. However, DL models are already thinking ‘big data’, so for very large data situations, a DL model might make a great choice, as others may not be computationally very viable. It also will not be surprising at all that in the near future some big hurdle may be overcome as we saw with DL applications in other fields, in which case some form of DL may be ‘all you need’.</p>
<p>Now, on to the rest!</p>
</section>
<section id="tabular-data-deep-learning-is-not-all-you-need" class="level2">
<h2 class="anchored" data-anchor-id="tabular-data-deep-learning-is-not-all-you-need">Tabular Data: Deep Learning is Not All You Need</h2>
<section id="paper-info" class="level3">
<h3 class="anchored" data-anchor-id="paper-info">Paper Info</h3>
<ul>
<li><em>Who</em>: Shwartz-Ziv &amp; Armon</li>
<li><em>Where</em>: Intel</li>
<li><em>When</em>: 2021-06-21 V1</li>
<li><a href="https://arxiv.org/pdf/2106.03253v1.pdf">Arxiv Link</a></li>
</ul>
</section>
<section id="from-the-abstract" class="level3">
<h3 class="anchored" data-anchor-id="from-the-abstract">From the Abstract</h3>
<blockquote class="blockquote">
<p>We analyze the deep models proposed in four recent papers across eleven datasets, nine of which were used in these papers, to answer these questions. We show that in most cases, each model performs best on the datasets used in its respective paper but significantly worse on other datasets. Moreover, our study shows that XGBoost (Chen and Guestrin, 2016) usually outperforms the deep models on these datasets. Furthermore, we demonstrate that the hyperparameter search process was much shorter for XGBoost.</p>
</blockquote>
</section>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>For each model they used the data that was implemented in the original model papers by the authors (e.g.&nbsp;the dataset used in the TabNet article), and also used their suggested parameter settings. They tested all the models against their own data, plus the other papers’ data, plus two additional data sets that were not used in any of the original papers.</p>
</section>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<p>They use eleven total datasets. Nine datasets are those used in the original papers on TabNet, DNF-Net, and NODE, drawing three datasets from each paper. Additionally, Shwartz-Ziv &amp; Armon use two Kaggle datasets not used in any of those papers. Sample sizes ranged from 7k to 1M, 10-2000 features, with two being numeric targets, while the other target variables ranged from 2-7 classes. Datasets are described in detail in the paper along with links to the source (all publicly available).</p>
</section>
<section id="models-explored" class="level3">
<h3 class="anchored" data-anchor-id="models-explored">Models Explored</h3>
<p>Brief summaries of the DL models are found in the paper.</p>
<ul>
<li><em>XGBoost</em></li>
<li><em>TabNet</em></li>
<li><em>Neural Oblivious Decision Ensembles</em> (NODE)</li>
<li><em>DNF-Net</em></li>
<li><em>1D-CNN</em></li>
</ul>
</section>
<section id="quick-summary" class="level3">
<h3 class="anchored" data-anchor-id="quick-summary">Quick Summary</h3>
<section id="not-counting-the-ensemble-methods" class="level5">
<h5 class="anchored" data-anchor-id="not-counting-the-ensemble-methods">Not counting the ensemble methods…</h5>
<ul>
<li>TabNet did best on all of its own data sets, but was not the best model on any other.</li>
<li>NODE each did best on 2 of its own 3 data sets, but not on any other.</li>
<li>DNF-Net best on one of its own 3 data sets, but not on any other.</li>
<li>XGBoost was best on the remaining 5 datasets.</li>
</ul>
</section>
<section id="counting-the-ensemble-methods" class="level5">
<h5 class="anchored" data-anchor-id="counting-the-ensemble-methods">Counting the ensemble methods…</h5>
<ul>
<li>TabNet did best on 2 of its own 3 data sets, but was not the best model on any other.</li>
<li>DNF-Net and NODE each did best on one of its own 3 data sets, but not on any other.</li>
<li>XGBoost was best on one dataset.</li>
</ul>
<p>Of those, XGB was notably better on ‘unseen’ data, and comparable to the best performing ensemble. A simple ensemble was also very performant. From the paper:</p>
<blockquote class="blockquote">
<p>The ensemble of all the models was the best model with 2.32% average relative increase, XGBoost was the second best with 3.4%, 1D-CNN had 7.5%, TabNet had 10.5%, DNF-Net had 11.8% and NODE had 14.2% (see Tables 2 and 3 in the appendix for full results).</p>
</blockquote>
<p>As a side note, XGBoost + DL was best, but that defeats the purpose in my opinion. Presumably any notably more complicated setting will be potentially better with enough complexity, but unless there is an obvious way on how to add such complexity, it’s mostly an academic exercise. However, as the authors note, if search is automated, maybe the complexity of combining the models is less of an issue.</p>
</section>
</section>
<section id="other-stuff" class="level3">
<h3 class="anchored" data-anchor-id="other-stuff">Other stuff</h3>
<section id="kudos" class="level5">
<h5 class="anchored" data-anchor-id="kudos">Kudos</h5>
<p>The authors cite the <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">No Free Lunch theorem</a> in the second paragraph, something that appears to be lost on many (most?) of these types of papers touting small increases in performance for some given modeling approach.</p>
</section>
<section id="issues" class="level5">
<h5 class="anchored" data-anchor-id="issues">Issues</h5>
<p>There are always things like training process/settings that are difficult to fully replicate. By the time authors publish any paper, unless exact records are kept, the iterations (including discussions that rule out various paths) are largely lost to time. This isn’t a knock on this paper, just something to keep in mind.</p>
</section>
<section id="opinion" class="level5">
<h5 class="anchored" data-anchor-id="opinion">Opinion</h5>
<p>I liked this one in general. They start by giving the competing models their best chance with their own settings and data, which was processed and trained in the same way. Even then, those models still either didn’t perform best, and/or performed relatively poorly on any other dataset.</p>
</section>
</section>
</section>
<section id="regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data" class="level2">
<h2 class="anchored" data-anchor-id="regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data">Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data</h2>
<section id="paper-info-1" class="level3">
<h3 class="anchored" data-anchor-id="paper-info-1">Paper Info</h3>
<ul>
<li><em>Who</em>: Kadra et al.</li>
<li><em>Where</em>: U of Freiburg, Leibniz U (Germany)</li>
<li><em>When</em>: 2021-06-06 V1</li>
<li><a href="https://arxiv.org/pdf/2106.11189.pdf4">Arxiv Link</a></li>
</ul>
</section>
<section id="from-the-abstract-1" class="level3">
<h3 class="anchored" data-anchor-id="from-the-abstract-1">From the Abstract</h3>
<blockquote class="blockquote">
<p>Tabular datasets are the last “unconquered castle” for deep learning… In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters.</p>
</blockquote>
<blockquote class="blockquote">
<p>We empirically assess the impact of these regularization cocktails for MLPs on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.</p>
</blockquote>
<blockquote class="blockquote">
<p>We emphasize that some of these publications claim to outperform Gradient Boosted Decision Trees (GDBT) [1, 37], and other papers explicitly stress that their neural networks do not outperform GBDT on tabular datasets [38, 22]. In contrast, we do not propose a new kind of neural architecture, but a novel paradigm for learning a combination of regularization methods.**</p>
</blockquote>
</section>
<section id="overview-1" class="level3">
<h3 class="anchored" data-anchor-id="overview-1">Overview</h3>
<p>This data is more about exploring regularization techniques (e.g.&nbsp;data augmentation, model averaging via dropout) rather than suggesting any particular model is superior. Even in the second paragraph they state their results do not suggest a performance gain over boosting methods. Their focus is on potentially improving DL for tabular data through regularization with two hypotheses:</p>
<ul>
<li>Regularization cocktails outperform state-of-the-art deep learning architectures on tabular datasets.</li>
<li>Regularization cocktails outperform Gradient-Boosted Decision Trees, as the most commonly used traditional ML method for tabular data.</li>
</ul>
</section>
<section id="data-1" class="level3">
<h3 class="anchored" data-anchor-id="data-1">Data</h3>
<p>Forty total datasets ranging from as little as ~400 observations to over 400k, and between 4 and 2000 features. All were categorical targets, with about half binary. All available at openml.org with target ID provided.</p>
</section>
<section id="models-explored-1" class="level3">
<h3 class="anchored" data-anchor-id="models-explored-1">Models Explored</h3>
<p>Comparison models:</p>
<ul>
<li><em>TabNet</em>: (with author’s proposed defaults)</li>
<li><em>NODE</em>: (with author’s proposed defaults)</li>
<li><em>Autogluon</em>: Tabular: can use other techniques but restricted to ensembles of neural nets for this demo</li>
<li><em>ASK-GBDT</em>: GB via Auto-sklearn (Note this tool comes from one of the authors )</li>
<li><em>XGBoost</em>: Original implementation</li>
<li><em>MLP</em>: Multilayer Perceptron - 9 layers with 512 hidden units each.</li>
<li><em>MLP+D</em>: MLP with Dropout</li>
<li><em>MLP+C</em>: MLP with regularization cocktail</li>
</ul>
</section>
<section id="quick-summary-1" class="level3">
<h3 class="anchored" data-anchor-id="quick-summary-1">Quick Summary</h3>
<ul>
<li>To begin, their regularization cocktail approach is the clear winner on these datasets, having one outright on over 40% of them (based on table 2).</li>
<li>Standard XGB performed best (or tied for best) 8 of the 40 data sets, while it or ASK-GBDT were best for 12 datasets combined.</li>
<li>Simple MLP was best once, while MLP with dropout was best 5 times, while the cocktail method was best in general, across 19 datasets.</li>
<li>The ‘fancy’ DL models were the worst performers across the board. TabNet never performed best, and NODE only did once, but the latter also repeatedly failed due to memory issues or run-time limitations (this memory issue was mentioned in the previous paper also).</li>
<li>Head-to-head, the cocktail beat the standard XGB 26 out of 38 times with three ties. So it wins 65% of the time against XGB, 70% against ASK-GBDT, but 60% against either (i.e.&nbsp;some XGB approach).</li>
</ul>
</section>
<section id="other-stuff-1" class="level3">
<h3 class="anchored" data-anchor-id="other-stuff-1">Other Stuff</h3>
<section id="kudos-1" class="level5">
<h5 class="anchored" data-anchor-id="kudos-1">Kudos</h5>
<ul>
<li>Recognize that tabular data is understudied in mainstream DL literature</li>
<li>They used a lot of datasets</li>
<li>They look at the simplest DL models for comparison</li>
</ul>
</section>
<section id="issues-1" class="level5">
<h5 class="anchored" data-anchor-id="issues-1">Issues</h5>
<ul>
<li><p>I wonder why there was not a single numeric outcome among so many datasets. Furthermore, some of the data are image classification (e.g.&nbsp;Fashion-MNIST), so I’m not sure why they’re included. I wouldn’t use a ‘tabular’ technique when standard computer vision approaches already work so well.</p></li>
<li><p>I’m not familiar with the augmentation techniques they mention, which were devised for image classification, but there have been some used for tabular data for a couple decades at this point that were not mentioned, including simple upsampling, or imputation methods (e.g.&nbsp;SMOTE). That’s not a beef with the article at all, I’ve long wondered why people haven’t been using data augmentation for tabular data given it’s success elsewhere (including for tabular data!).</p></li>
<li><p>They use a standard t-tests of ranks, but if we’re going to use this sort of approach, we’d maybe want to adjust for all the tests done, and probably for all pairwise comparisons (they show such a table for the regularization methods). Depending on the approach and cutoff, the XGB vs.&nbsp;Cocktail difference may not be significant.</p></li>
<li><p>Also, I couldn’t duplicate these p-values with R’s default settings for Wilcoxon signed rank tests, and there does in fact seem to be inconsistency between the detailed results and Wilcoxon summaries. For example, in the regularization tests of Table 9, <code>Cocktail</code> vs.&nbsp;<code>WD</code> and <code>DO</code> shows two ties in the first four data sets, yet only 1 tie is reported in the comparison chart for both (Figure 4). For the models, Table 2 show 3 ties of <code>XGB</code> &amp; the <code>Cocktail</code>, with 1 for <code>ASK-G</code> and <code>Cocktail</code>, but 2 and 0 are reported for their Wilcoxon tests. It’s not clear what they did for NODE with all the NAs. I do not believe these discrepancies, nor adjusting for multiple comparisons, will change the results (I re-did those myself).</p></li>
</ul>
</section>
<section id="opinion-1" class="level5">
<h5 class="anchored" data-anchor-id="opinion-1">Opinion</h5>
<p>If we ignore the regularization focus and just look at the model comparisons, I’m not overly convinced we have a straightforward victory for cocktail vs.&nbsp;GB as implied in the conclusion. Results appear to be in favor of their proposed method, but not enough to be a near-guarantee in a particular setting, so we’re back to square one of just using the easier/faster/better tool. I’m also not sure who was questioning the use of regularization for neural networks or modeling in general, so the comparison to any model without some form of regularization isn’t as interesting to me. What is interesting to me is that we have another round of evidence that the fancier DL models like TabNet do not perform that well relative to GB or simpler DL architectures.</p>
</section>
</section>
</section>
<section id="revisiting-deep-learning-models-for-tabular-data" class="level2">
<h2 class="anchored" data-anchor-id="revisiting-deep-learning-models-for-tabular-data">Revisiting Deep Learning Models for Tabular Data</h2>
<section id="paper-info-2" class="level3">
<h3 class="anchored" data-anchor-id="paper-info-2">Paper Info</h3>
<ul>
<li><em>Who</em>: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko</li>
<li><em>Where</em>: Yandex (Russia)</li>
<li><em>When</em>: 2021-06-22</li>
<li><a href="https://arxiv.org/abs/2106.11959">Arxiv Link</a></li>
<li><a href="https://github.com/yandex-research/rtdl">Source code</a></li>
</ul>
</section>
<section id="from-the-abstract-2" class="level3">
<h3 class="anchored" data-anchor-id="from-the-abstract-2">From the Abstract</h3>
<blockquote class="blockquote">
<p>The necessity of deep learning for tabular data is still an unanswered question addressed by a large number of research efforts. The recent literature on tabular DL proposes several deep architectures reported to be superior to traditional “shallow” models like Gradient Boosted Decision Trees. However, since existing works often use different benchmarks and tuning protocols, it is unclear if the proposed models universally outperform GBDT. Moreover, the models are often not compared to each other, therefore, it is challenging to identify the best deep model for practitioners.</p>
</blockquote>
<blockquote class="blockquote">
<p>In this work, we start from a thorough review of the main families of DL models recently developed for tabular data. We carefully tune and evaluate them on a wide range of datasets and reveal two significant findings. First, we show that the choice between GBDT and DL models highly depends on data and there is still no universally superior solution. Second, we demonstrate that a simple ResNet-like architecture is a surprisingly effective baseline, which outperforms most of the sophisticated models from the DL literature. Finally, we design a simple adaptation of the Transformer architecture for tabular data that becomes a new strong DL baseline and reduces the gap between GBDT and DL models on datasets where GBDT dominates.</p>
</blockquote>
</section>
<section id="overview-2" class="level3">
<h3 class="anchored" data-anchor-id="overview-2">Overview</h3>
<p>This paper compares different models on a variety of datasets. They are interested in the GB vs.&nbsp;DL debate, but like the previous paper, also interested in how well a simpler DL architecture might perform, and what steps might help the more complicated ones do better.</p>
</section>
<section id="data-2" class="level3">
<h3 class="anchored" data-anchor-id="data-2">Data</h3>
<p>They have 11 datasets with a mix of binary, multiclass and numeric targets. Sizes range from 20K to 1M+. There appears to be some overlap with the first paper (e.g.&nbsp;Higgs, Cover type).</p>
</section>
<section id="models-explored-2" class="level3">
<h3 class="anchored" data-anchor-id="models-explored-2">Models Explored</h3>
<section id="baselines" class="level5">
<h5 class="anchored" data-anchor-id="baselines">‘Baselines’</h5>
<ul>
<li><em>XGBoost</em></li>
<li><em>CatBoost</em></li>
<li><em>MLP</em></li>
<li><em>ResNet</em></li>
</ul>
</section>
<section id="dl-comparisons" class="level5">
<h5 class="anchored" data-anchor-id="dl-comparisons">DL Comparisons</h5>
<ul>
<li><em>SNN</em></li>
<li><em>NODE</em></li>
<li><em>TabNet</em></li>
<li><em>GrowNet</em></li>
<li><em>DCN V2</em></li>
<li><em>AutoInt</em></li>
</ul>
<p>In addition, they look at ensembles of these models, but this is not of interest to me for this post.</p>
</section>
</section>
<section id="quick-summary-2" class="level3">
<h3 class="anchored" data-anchor-id="quick-summary-2">Quick Summary</h3>
<p>Note that these refer to the ‘single model’ results, not the results for ensembles.</p>
<ul>
<li><p>Some form of boosting performed best on 4 of the 11 datasets.</p></li>
<li><p>ResNet was best on four classification tasks, but not once for numeric targets.</p></li>
<li><p>At this point you won’t be surprised at what doesn’t perform as well- TabNet, NODE, and similar. TabNet, DCN, and GrowNet were never the best performer, and the other three were best one time a piece.</p></li>
<li><p>MLP did not perform best on any data, however the authors note that it ‘is often on par or even better than some of the recently proposed DL models’.</p></li>
<li><p>They also looked at models with a ‘simple’ transformer architecture. Their results suggest better performance than the other DL models, and similar performance to ResNet.</p></li>
</ul>
</section>
<section id="other-stuff-2" class="level3">
<h3 class="anchored" data-anchor-id="other-stuff-2">Other Stuff</h3>
<section id="kudos-2" class="level5">
<h5 class="anchored" data-anchor-id="kudos-2">Kudos</h5>
<ul>
<li><p>Sharing the source code!</p></li>
<li><p>Recognizing that results at this point are complex at best given the lack of standard datasets</p></li>
</ul>
</section>
<section id="issues-2" class="level5">
<h5 class="anchored" data-anchor-id="issues-2">Issues</h5>
<ul>
<li>They note a distinction between <em>heterogeneous</em> vs.&nbsp;other types of data. They call data heterogeneous if the predictors are of mixed data types (e.g.&nbsp;categorical, numeric, count), while something like pixel data would be <em>homogeneous</em> because all the columns are essentially the same type. The latter isn’t as interesting to me for this sort enterprise, and I think the former is what most are thinking about for ‘tabular’ data, otherwise we’d just call it what it is (e.g.&nbsp;image or text data), and modeling/estimation is generally quite a bit easier when all the data is the same type. I do think it’s important that they point out that GB is better with heterogeneous data, and I think if you only look at such data, you’d likely see GB methods still outperforming or at worst on par with the best DL methods.</li>
</ul>
</section>
<section id="opinion-2" class="level5">
<h5 class="anchored" data-anchor-id="opinion-2">Opinion</h5>
<p>These results seem consistent with others at this point. Complex DL isn’t helping, and simpler architectures, even standard MLP show good performance. In the end, we still don’t have any clear winner over GB methods.</p>
</section>
</section>
</section>
<section id="overall-assessment" class="level2">
<h2 class="anchored" data-anchor-id="overall-assessment">Overall Assessment</h2>
<p>These papers put together are helpful in painting a picture of where we are at present with deep learning for tabular data, especially with mixed data types. In this setting, it seems that more complicated DL models do not seem to have any obvious gain over simpler architectures, which themselves do not consistently beat boosting methods. It may also be the case that for data of mixed data types/sources, boosting is still the standard to beat.</p>
<p>Even though these articles are geared toward comparisons to GB/XGBoost, in several settings I’ve applied them, I typically do not necessarily have appreciably greater success compared to a default setting random forest (e.g.&nbsp;from the <span class="pack" style="">ranger</span> package in R), or sufficiently flexible statistical model. Unfortunately this comparison is lacking from the papers, and would have been nice to have, especially for smaller data settings where such models are still very viable. I think a viable fast model, preferably one without any tuning required (or which simply is taken off the shelf) should be the baseline.</p>
<p>In that light, for tabular data I think one should maybe start with a baseline of a penalized regression with appropriate interactions (e.g.&nbsp;ridge/lasso), or a more flexible penalized approach (GAMM) as a baseline, the latter especially, as it can at least automatically incorporate nonlinear relationships, and tools like <span class="pack" style="">mgcv</span> or <span class="pack" style="">gpboost</span> in R can do so with very large data (1 million +) in a matter of seconds. In settings of relatively higher dimensions, interactions and nonlinearities should be prevalent enough such that basis function, tree, and DL models should be superior. Whether they are practically so is the key concern even in those settings. With smaller, noisier data of less dimension, I suspect the tuning/time effort with present day DL models for tabular data will likely not be worth it. This may change very soon however, so such an assumption should be regularly checked.</p>
<p><br> <br></p>
<p>last updated: 2024-12-27</p>
<p>Neural Net image source from <a href="https://uc-r.github.io/2018/04/09/feedforward-deep-models/">UC Business Analytics R Programming Guide</a></p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-gorishniy2021tabular" class="csl-entry">
Gorishniy, Yuri, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021. <span>“Revisiting Deep Learning Models for Tabular Data.”</span> <em>arXiv Preprint arXiv:2106.11959</em>.
</div>
<div id="ref-kadra2021tabular" class="csl-entry">
Kadra, Arlind, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021. <span>“Regularization Is All You Need: Simple Neural Nets Can Excel on Tabular Data.”</span> <em>arXiv Preprint arXiv:2106.11189</em>.
</div>
<div id="ref-shwartz2021tabular" class="csl-entry">
Shwartz-Ziv, Ravid, and Amitai Armon. 2021. <span>“Tabular Data: Deep Learning Is Not All You Need.”</span> <em>arXiv Preprint arXiv:2106.03253</em>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{clark2021,
  author = {Clark, Michael},
  title = {This Is Definitely Not All You Need},
  date = {2021-07-19},
  url = {https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-clark2021" class="csl-entry quarto-appendix-citeas">
Clark, Michael. 2021. <span>“This Is Definitely Not All You
Need.”</span> July 19, 2021. <a href="https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/">https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/</a>.
</div></div></section></div> ]]></description>
  <category>deep learning</category>
  <category>machine learning</category>
  <guid>https://m-clark.github.io/posts/2021-07-15-dl-for-tabular/</guid>
  <pubDate>Mon, 19 Jul 2021 04:00:00 GMT</pubDate>
  <media:content url="https://m-clark.github.io/img/dl-for-tab/deep_nn.png" medium="image" type="image/png" height="104" width="144"/>
</item>
</channel>
</rss>
