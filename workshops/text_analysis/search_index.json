[
["index.html", "In the beginning was the word ...", " In the beginning was the word ... An Introduction to Text Processing and Analysis with R http://m-clark.github.io/workshops/ 2017-07-25 "],
["introduction.html", "Introduction Overview Initial Steps", " Introduction Overview Dealing with text is typically not even considered in the applied statistical training of most disciplines. This is in direct conflict with how often it has to be dealt with prior to analysis, or how interesting it might be to have text be the focus of analysis. This document and corresponding workshop will aim to provide a sense of the things one can do with text, and the sorts of analyses that might be useful. It must be stressed that this is only a starting point. Goals The goal of this workshop is primarily to provide a sense of common tasks related to dealing with text as part of the data or the focus of analysis, and provide some relatively easy to use tools. Additionally, we’ll have exercises to practice, but those comfortable enough to do so should follow along with the in-text examples. Note that there is more content here than will be covered in a 2 hour workshop. Prerequisites The document is for the most part very applied in nature, and doesn’t assume much beyond familiarity with the R statistical computing environment. Note the following color coding used in this document: emphasis package function object/class link Initial Steps Download the zip file at . Be mindful of where you put it. Unzip it. Be mindful of where you put the resulting folder. Open RStudio. File/Open Project and click on the blue icon in the folder you just created. File/Open Click on the ReadMe file and do what it says. "],
["string-theory.html", "String Theory Basic data types Basic Text Functionality Regular Expressions Examples Exercises", " String Theory Basic data types R has several core data structures: Vectors Factors Lists Matrices/arrays Data frames Vectors form the basis of R data structures. There are two main types- atomic and lists. All elements of an atomic vector are the same type. Examples include: character numeric (double) integer logical Character strings When dealing with text, objects of class character are what you’d typically be dealing with. x = c(&#39;... Of Your Fake Dimension&#39;, &#39;Ephemeron&#39;, &#39;Dryswch&#39;, &#39;Isotasy&#39;, &#39;Memory&#39;) x Not much to it, but be aware there is no real limit to what is represented as a character vector. For example, in a data frame, you could have a column where each entry is one of the works of Shakespeare. Factors Although not exactly precise, one can think of factors as integers with labels. So the underlying representation of a variable for sex is 1:2 with labels ‘Male’ and ‘Female’. They are a special class with attributes, or metadata, that contains the information about the levels. x = factor(rep(letters[1:3], e=10)) attributes(x) $levels [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; $class [1] &quot;factor&quot; While the underlying representation is numeric, it is important to remember that factors are categorical. They can’t be used as numbers would be, as the following demonstrates. as.numeric(x) [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 sum(x) Error in Summary.factor(structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, : &#39;sum&#39; not meaningful for factors Because of the integer+metadata representation, factors are actually smaller than character strings, often notably so. x = sample(state.name, 10000, replace=T) format(object.size(x), units=&#39;Kb&#39;) [1] &quot;80.8 Kb&quot; format(object.size(factor(x)), units=&#39;Kb&#39;) [1] &quot;42.4 Kb&quot; format(object.size(as.integer(factor(x))), units=&#39;Kb&#39;) [1] &quot;39.1 Kb&quot; However, if memory is really a concern, it’s probably not that using factors will help, but rather better hardware. Analysis It is important to know that raw text cannot be analyzed quantitatively. There is no magic that takes a categorical variable with text labels and estimates correlations among words and other words or numeric data. Everything that can be analyzed must have some numeric representation first, and this is where factors come in. For example, here is a data frame with two categorical predictors (factor*), a numeric predictor (x), and a numeric target (y). What follows is what it looks like if you wanted to run a regression model in that setting. df = crossing(factor_1 = c(&#39;A&#39;, &#39;B&#39;), factor_2 = c(&#39;Q&#39;, &#39;X&#39;, &#39;J&#39;)) %&gt;% mutate(x=rnorm(6), y=rnorm(6)) df # A tibble: 6 x 4 factor_1 factor_2 x y &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 A J -1.9837203 -2.34251047 2 A Q -0.3020751 0.65894051 3 A X -1.6112889 0.32392924 4 B J -2.0169883 0.55759618 5 B Q 0.3149217 0.80047834 6 B X 0.3248241 0.06146516 ## model.matrix(lm(y ~ x + factor_1 + factor_2, data=df)) (Intercept) x factor_1B factor_2Q factor_2X 1 -1.984 0 0 0 1 -0.3021 0 1 0 1 -1.611 0 0 1 1 -2.017 1 0 0 1 0.3149 1 1 0 1 0.3248 1 0 1 The model.matrix function exposes the underlying matrix that is actually used in the regression analysis. You’d get a coefficient for each column of that matrix. As such, even the intercept must be represented in some fashion. For categorical data, the default coding scheme is dummy coding. A reference category is arbitrarily chosen (it doesn’t matter which, and you can always change it), while the other categories are represented by indicator variables, where a 1 represents the corresponding label and everything else is zero. For details on this coding scheme or others, consult any basic statistical modeling book. Characters vs. Factors The main thing to note is that factors are generally a statistical phenomenon, and are required to do statistical things with data that would otherwise be a simple character string. If you know the relatively few levels the data can take, you’ll generally want to use factors, or at least know that statistical packages and methods will require them. In addition, factors allow you to easily overcome the sometimes silly default alphabetical ordering of category levels in some very popular visualization packages. For other things, such as text analysis, you’ll almost certainly want character strings instead, and in many cases it will be required. It’s also worth noting that a lot of base R and other behavior will coerce strings to factors. This made a lot more sense in the early days of R, but is not really necessary these days. For more on this stuff see the following: http://adv-r.had.co.nz/Data-structures.html http://forcats.tidyverse.org/ http://r4ds.had.co.nz/factors.html https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/ http://notstatschat.tumblr.com/post/124987394001/stringsasfactors-sigh Basic Text Functionality Base R A lot of folks new to R are not aware of just how much basic text processing R comes with out of the box. Here are examples of note. paste: glue text/numeric values together substr: extract or replace substrings in a character vector grep family: use regular expressions to deal with patterns of text strsplit: split strings nchar: how many characters in a string as.numeric: convert a string to numeric if it can be strtoi: convert a string to integer if it can be (faster than as.integer) adist: string distances I probably use paste/paste0 more than most things when dealing with text, as string concatenation comes up so often. paste(c(&#39;a&#39;, &#39;b&#39;, &#39;cd&#39;), collapse=&#39;|&#39;) [1] &quot;a|b|cd&quot; paste(c(&#39;a&#39;, &#39;b&#39;, &#39;cd&#39;), collapse=&#39;&#39;) [1] &quot;abcd&quot; paste0(&#39;a&#39;, &#39;b&#39;, &#39;cd&#39;) # shortcut to collapse=&#39;&#39; [1] &quot;abcd&quot; paste0(&#39;x&#39;, 1:3) [1] &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; Beyond that, use of regular expression and functionality included in the grep family is a major way to save a lot of time during data processing. I leave that to its own section later. Packages A couple packages will probably take care of the vast majority of your standard text processing needs. Note that even if they aren’t adding anything to the functionality of the base R functions, they typically will have been optimized in some fashion stringr/stringi: more or less the same stuff you’ll find with substr, grep etc. except easier to use or faster. Also add useful functionality not in base R (e.g. str_to_title) tidyr: has functions such as unite, separate, replace_na that can often come in handy when working with data frames glue: a newer package that can be seen as a fancier paste. Most likely will be useful when creating functions or shiny apps in which variable text output is desired One issue I have with both packages and base R is that often they return a list object, when it should be simplifying to the vector format it was initially fed. This sometimes requires an additional step or two of further processing that shouldn’t be necessary, so be prepared for it1. Other In this section I’ll add some things that come to mind that might come into play when you’re dealing with text. Dates Dates are not character strings. Though they may start that way, if you actually want to treat them as dates you’ll need to convert the string to the appropriate date class. The lubridate package makes dealing with dates much easier. It comes with conversion, extraction and other functionality that will be sure to save you some time. library(lubridate) today() [1] &quot;2017-07-24&quot; today() + 1 [1] &quot;2017-07-25&quot; today() + dyears(1) [1] &quot;2018-07-24&quot; leap_year(2016) [1] TRUE span = interval(ymd(&quot;2017-07-01&quot;), ymd(&quot;2017-07-04&quot;)) span [1] 2017-07-01 UTC--2017-07-04 UTC as.duration(span) [1] &quot;259200s (~3 days)&quot; This package makes dates so much easier, you should always use it when dealing with them. Categorical Time In regression modeling with few time points, one often has to decide on whether to treat the year as categorical (factor) or numeric (continuous). This greatly depends on how you want to tell your data story or other practical concerns. For example, if you have five years in your data, treating year as categorical means you are interested in accounting for unspecified things that go on in a given year. If you treat it as numeric, you are more interested in trends. Either is fine. Encoding Encoding can be a sizable PITA sometimes, and will often come up when dealing with webscraping and other languages. The rvest and stringr packages may be able to get you past some issues at least. See their respective functions repair_encoding and str_conv as starting points on this issue. Summary of basic text functionality Being familiar with commonly used string functionality in base R and packages like stringr can save a ridiculous amount of time in your data processing. The more familiar you are with them the easier time you’ll have with text. Regular Expressions A regular expression, regex for short, is a sequence of characters that can be used as a search pattern for a string. Common operations are to merely detect, extract, or replace the matching string. There are actually many different flavors of regex for different programming languages, which are all flavors that originate with the Perl approach, or can enable the Perl approach to be used. However, knowing one means you pretty much know the others with only minor modifications if any. To be clear, not only is regex another language, it’s nigh on indecipherable. You will not learn much regex, but what you do learn will save a potentially enormous amount of time you’d otherwise spend trying to do things in a more haphazard fashion. Furthermore, practically every situation that will come up has already been asked and answered on Stack Overflow, so you’ll almost always be able to search for what you need. Here is an example: ^r.*shiny[0-9]$ What is that you may ask? Well here is an example of strings it would and wouldn’t match. grepl(c(&#39;r is the shiny&#39;, &#39;r is the shiny1&#39;, &#39;r shines brightly&#39;), pattern=&#39;^r.*shiny[0-9]$&#39;) [1] FALSE TRUE FALSE What the regex esoterically is attempting to match is any string that starts with ‘r’ and ends with ‘shiny_’ where _ is some single digit. Specifically it breaks down as follows: ^ : starts with, so ^r means starts with r . : any character * : match the preceding zero or more times shiny : match ‘shiny’ [0-9] : any digit 0-9 (note that we are still talking about strings, not actual numbered values) $ : ends with preceding Typical Uses None of it makes sense, so don’t attempt to do so. Just try to remember a couple key approaches, and search the web for the rest. Along with ^ . * [0-9] $, a couple more common ones are: [a-z] : letters a-z [A-Z] : capital letters + : match the preceding one or more times () : groupings | : logical or e.g. [a-z]|[0-9] (a lower case letter or a number) ? : preceding item is optional, and will be matched at most once. Typically used for ‘look ahead’ and ‘look behind’ \\ : escape a character, like if you actually wanted to search for a period, you’d use \\., though in R you need \\\\, i.e. double slashes, for escape. In addition, there are certain predefined characters that can be called: [:punct:] : punctuation [:blank:] : spaces and tabs [:alnum:] : alphanumeric characters Those are just a few. The key functions can be found by looking at the help file for the grep function (?grep). However, the stringr package has the same functionality with perhaps a slightly faster processing (though that’s due to the underlying stringi package). See if you can guess which of the following will turn up TRUE. grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;a&#39;) grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;^a&#39;) grepl(c(&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;), pattern=&#39;^a|a$&#39;) Scraping the web, munging data, just finding things in your scripts … you can potentially use this all the time, and not only with text analysis, as we’ll now see. dplyr helper functions The dplyr package comes with some poorly documented2 but quite useful helper functions that essentially serve as human-readable regex, which is a very good thing. These functions allow you to select variables3 based on their names. They are just calling grep in the end. starts_with: starts with a prefix (same as regex ‘^blah’) ends_with: ends with a prefix (same as regex ‘blah$’) contains: contains a literal string (same as regex ‘blah’) matches: matches a regular expression (put your regex here) num_range: a numerical range like x01, x02, x03. (same as regex ‘x[0-9][0-9]’) one_of: variables in character vector. (if you need to quote variable names, e.g. within a function) everything: all variables. (a good way to spend time doing something only to accomplish what you would have by doing nothing, or a way to reorder variables) Examples Example 1 Let’s say you’re dealing with some data that has been handled typically, that is to say, poorly. For example, you have a variable in your data representing whether something is from the north or south region. It might seem okay until… ## table(df$region) South north North north North south South 76 62 63 73 84 73 69 Even if you spotted the casing issue, there is still a white space problem4. Let’s say you want this to be capitalized ‘North’ and ‘South’. How might you do it? It’s actually quite easy with the stringr tools. library(stringr) df %&gt;% mutate(region = str_trim(region), region = str_to_title(region)) The str_trim function trims white space from either side, while str_to_title converts everything to first letter capitalized. table(df_corrected$region) North South 282 218 Example 2 Suppose you import a data frame, and the data was originally in wide format, where each column represented a year of data collection for the individual. Since it is bad form for data columns to have numbers for names, when you import it, the result looks like the following. So the problem now is to change the names to be Year_1, Year_2, etc. You might think you might have to use colnames and manually create a string of names to replace the current ones. colnames(df)[-1] = c(&#39;Year_1&#39;, &#39;Year_2&#39;, &#39;Year_3&#39;, &#39;Year_4&#39;, &#39;Year_5&#39;) Or perhaps you’re thinking of the paste0 function, which works fine and saves some typing. colnames(df)[-1] = paste0(&#39;Year_&#39;, 1:5) However, data sets may be hundreds of columns, and the columns of data may have the same pattern but not be next to one another. For example, the first few dozen columns are all data that belongs to the first wave, etc. It is tedious to figure out which columns you don’t want, but even then you’re resulting to using magic numbers with the above approach, and one column change to data will mean that redoing the name change will fail. However, the following accomplishes what we want, and is reproducible regardless of where the columns are in the data set. df %&gt;% rename_at(vars(num_range(&#39;X&#39;, 1:5)), str_replace, pattern=&#39;X&#39;, replacement=&#39;Year_&#39;) %&gt;% head id Year_1 Year_2 Year_3 Year_4 Year_5 1 1 0.37 -0.18 -0.51 0.00 -0.86 2 2 -0.05 -0.62 0.26 -1.30 0.49 3 3 -0.33 0.73 1.23 0.08 0.77 4 4 -2.28 0.89 -0.29 2.35 -0.48 5 5 1.22 -1.82 0.48 1.13 0.62 6 6 -0.18 0.22 -0.22 -0.45 -0.16 We just have to use the num_range helper function within the function that tells rename_at what it should be renaming, and let str_replace do the rest. Exercises I also don’t think it necessary to have separate functions for str_* functions in stringr depending on whether, e.g. I want ‘all’ matches (practically every situation) or just the first (very rarely). It could have just been an additional argument with default all=TRUE.↩ At least they’re exposed now.↩ And maybe some day, the rows. For now you’ll have to use a grepl/str_detect approach.↩ This is a very common issue among Excel users, and just one of the many reasons not to use it.↩ "],
["sentiment-analysis-is-sick-yo.html", "Sentiment Analysis is Sick Yo Basic idea Issues Sentiment Analysis Example Sentiment Analysis Exercise Sentiment Analysis Summary", " Sentiment Analysis is Sick Yo Basic idea A common and intuitive approach to text is sentiment analysis. In a grand sense we are interested in the emotional content of some text, e.g. posts on Facebook, tweets, or movie reviews. Most of the time, this is obvious when one reads it, but if you have hundreds of thousands or millions of strings to analyze, you’d like to be able to do so efficiently. We will use the tidytext package for our demonstration. It comes with a lexicon of positive and negative words that is actually a combination of multiple sources, one of which provides numeric ratings, while the others suggest different classes of sentiment. library(tidytext) sentiments %&gt;% slice(sample(1:nrow(sentiments))) # A tibble: 27,314 x 4 word sentiment lexicon score &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 decomposition negative nrc NA 2 imaculate positive bing NA 3 greatness positive bing NA 4 impatient negative bing NA 5 contradicting negative loughran NA 6 irrecoverableness negative bing NA 7 advisable trust nrc NA 8 humiliation disgust nrc NA 9 obscures negative bing NA 10 affliction negative bing NA # ... with 27,304 more rows The gist is that we are dealing with a specific, pre-defined vocabulary. Of course, any analysis will only be as good as the lexicon. The goal is usually to assign a sentiment score to a text, possibly an overall score, or a generally positive or negative grade. Given that, other analyses may be implemented to predict sentiment via standard regression tools or machine learning approaches. Issues Context, sarcasm, etc. Now consider the following. sentiments %&gt;% filter(word==&#39;sick&#39;) # A tibble: 5 x 4 word sentiment lexicon score &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 sick disgust nrc NA 2 sick negative nrc NA 3 sick sadness nrc NA 4 sick negative bing NA 5 sick &lt;NA&gt; AFINN -2 Despite the above assigned sentiments, the word sick has been used at least since 1960s surfing culture as slang for positive. A basic approach to sentiment analysis as described here will not be able to detect slang or other context like sarcasm. However, with lots of training data for a particular context may allow one to correctly predict such sentiment. In addition, there are, for example, slang lexicons, or one can simply add their own complements to any available lexicon. Lexicons In addition, the lexicons are going to maybe be applicable to general usage of English in the western world. Some might wonder where exactly these came from or who decided that the word abacus should be affiliated with ‘trust’. You may start your path by typing ?sentiments at the console if you have the tidytext package loaded. Sentiment Analysis Example The first thing the baby did wrong We demonstrate sentiment analysis with the text The first thing the baby did wrong, which is a very popular brief guide to parenting written by world renown psychologist Donald Barthelme who, in his spare time, also wrote postmodern literature. This particular text talks about an issue with the baby, whose name is Born Dancin’, who likes to tear pages out of books. Attempts are made by her parents to rectify the situation, without much success, but things are finally resolved at the end. The ultimate goal will be to see how sentiment in the text evolves over time. How do we start? Let’s look at the sentiments data set in the tidytext package. library(tidytext) sentiments %&gt;% slice(sample(1:nrow(sentiments))) # A tibble: 27,314 x 4 word sentiment lexicon score &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 blunder sadness nrc NA 2 solidity positive nrc NA 3 mortuary fear nrc NA 4 absorbed positive nrc NA 5 successful joy nrc NA 6 virus negative nrc NA 7 exorbitantly negative bing NA 8 discombobulate negative bing NA 9 wail negative nrc NA 10 intimidatingly negative bing NA # ... with 27,304 more rows The bing lexicon is just positive or negative. The AFINN is numerical, with ratings -5:5 that are in the score column. The others get more imaginative, but also more problematic. Why assimilate is superfluous is beyond me. It clearly should be negative given the Borg connotations. sentiments %&gt;% filter(sentiment==&#39;superfluous&#39;) # A tibble: 56 x 4 word sentiment lexicon score &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 aegis superfluous loughran NA 2 amorphous superfluous loughran NA 3 anticipatory superfluous loughran NA 4 appertaining superfluous loughran NA 5 assimilate superfluous loughran NA 6 assimilating superfluous loughran NA 7 assimilation superfluous loughran NA 8 bifurcated superfluous loughran NA 9 bifurcation superfluous loughran NA 10 cessions superfluous loughran NA # ... with 46 more rows But I digress. We start with the raw text, reading it in line by line. In what follows we read in all the texts (three) in a given directory, such that each element of ‘text’ is the work itself, i.e. text is a list column5. The unnest function will more or less unravel the work to paragraph form. library(tidytext) barth0 = data_frame(file = dir(&#39;data/texts_raw/barthelme/&#39;, full.names = TRUE)) %&gt;% mutate(text = map(file, read_lines)) %&gt;% transmute(work = basename(file), text) %&gt;% unnest(text) One of the things stressed in this document is the iterative nature of text analysis. You will consistently take two steps forward, and then one or two back as you find issues that need to be addressed. For example in a subsequent step I found there were encoding issues6, so the following attempts to fix them. In addition we want to tokenize the documents such that our tokens are sentences (e.g. as opposed to words or paragraphs). The reason for this is that I will be summarizing the sentiment at sentence level. # Fix encoding, convert to sentences barth = barth0 %&gt;% mutate(text = sapply(text, stringi::stri_enc_toutf8, is_unknown_8bit=TRUE, validate=T)) %&gt;% unnest_tokens(sentence, text, token=&#39;sentences&#39;) The next step is to drill down to just the document we want, and tokenize to the word level. However, I create a sentence id so that we can group on it later. # get baby doc, convert to words baby = barth %&gt;% filter(work==&#39;baby.txt&#39;) %&gt;% mutate(sentence_id = 1:n()) %&gt;% unnest_tokens(word, sentence, drop=F) %&gt;% ungroup Now that the data has been prepped, getting the sentiments is ridiculously easy. But that is how it is with text analysis. All the hard work is spent with the data processing. Here all we need is an inner join of our words with a sentiment lexicon of choice. This process will only retain words that are also in the lexicon. I use the numeric-based lexicon here. At that point we get a sum score of sentiment by sentence. # get sentiment via inner join baby_sentiment = baby %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;% group_by(sentence_id, sentence) %&gt;% summarise(score=sum(score)) %&gt;% ungroup The following plots sentiment over sentence (note that not every sentence will receive a sentiment score). You can read the sentence by hovering over the dot. In general the sentiment starts out negative as the problem is explained. It bounces back and forth a bit but ends on a positive note. You’ll see that some sentences’ context are not captured. For example, sentence 16 is ‘But it didn’t do any good’. However good is going to be marked as a positive sentiment in any lexicon by default. In addition, the token length will matter. Longer sentences are more likely to have some sentiment, for example. Sentiment Analysis Exercise Romeo &amp; Juliet For this exercise I’ll invite you to more or less follow along, as there is notable pre-processing that must be done. We’ll look at sentiment in Shakespeare’s Romeo and Juliet. I have a cleaner version in the raw texts folder, but we can take the opportunity to use the gutenbergr package to download it directly from Project Gutenberg, a storehouse for works that have entered the public domain. library(gutenbergr) gw0 = gutenberg_works(title == &quot;Romeo and Juliet&quot;) # look for something with this title # A tibble: 1 x 4 gutenberg_id title author gutenberg_author_id &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 1513 Romeo and Juliet Shakespeare, William 65 rnj = gutenberg_download(gw0$gutenberg_id) We’ve got the text now, but there is still work to be done. The following is a quick and dirty approach, but see the Shakespeare section to see a more deliberate one. We first slice off the initial parts we don’t want like title, author etc. Then we get rid of other tidbits that would interfere, using a little regex as well to aid the process. rnj_filtered = rnj %&gt;% slice(-(1:49)) %&gt;% filter(!text==str_to_upper(text), # will remove THE PROLOGUE etc. !text==str_to_title(text), # will remove names/single word lines !str_detect(text, pattern=&#39;^(Scene|SCENE)|^(Act|ACT)|^\\\\[&#39;)) %&gt;% select(-gutenberg_id) %&gt;% unnest_tokens(sentence, input=text, token=&#39;sentences&#39;) %&gt;% mutate(sentenceID = 1:n()) The following unnests the data to word tokens. In addition, you can remove stopwords like a, an, the etc. However, some of the stopwords have sentiments, so you would get a bit of a different result if you retain them. As Black Sheep once said, the choice is yours, and you can deal with this, or you can deal with that. # show some of the matches stop_words$word[which(stop_words$word %in% sentiments$word)] %&gt;% head(20) [1] &quot;able&quot; &quot;against&quot; &quot;allow&quot; &quot;almost&quot; &quot;alone&quot; &quot;appear&quot; &quot;appreciate&quot; &quot;appropriate&quot; &quot;available&quot; &quot;awfully&quot; &quot;believe&quot; &quot;best&quot; &quot;better&quot; &quot;certain&quot; &quot;clearly&quot; [16] &quot;could&quot; &quot;despite&quot; &quot;downwards&quot; &quot;enough&quot; &quot;furthermore&quot; # remember to call output &#39;word&#39; or antijoin won&#39;t work without a &#39;by&#39; argument rnj_filtered = rnj_filtered %&gt;% unnest_tokens(output=word, input=sentence, token=&#39;words&#39;) %&gt;% anti_join(stop_words) Now we add the sentiments via the inner_join function. Here I use ‘bing’, but you can use another, and you might get a different result. rnj_filtered %&gt;% count(word) %&gt;% arrange(desc(n)) # A tibble: 3,287 x 2 word n &lt;chr&gt; &lt;int&gt; 1 thou 276 2 thy 165 3 love 140 4 thee 139 5 romeo 110 6 night 83 7 death 71 8 hath 64 9 sir 58 10 art 55 # ... with 3,277 more rows rnj_sentiment = rnj_filtered %&gt;% inner_join(sentiments) rnj_sentiment # A tibble: 12,670 x 5 sentenceID word sentiment lexicon score &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 445 boundless positive bing NA 2 664 aching negative nrc NA 3 664 aching sadness nrc NA 4 664 aching negative bing NA 5 664 aching &lt;NA&gt; AFINN -2 6 129 question positive nrc NA 7 129 question negative loughran NA 8 1392 question positive nrc NA 9 1392 question negative loughran NA 10 100 sick disgust nrc NA # ... with 12,660 more rows rnj_sentiment_bing = rnj_sentiment %&gt;% filter(lexicon==&#39;bing&#39;) table(rnj_sentiment_bing$sentiment) negative positive 1244 833 Looks like this one is going to be a downer. The following visualizes (via plotly) the positive and negative sentiment scores as one progresses sentence by sentence through the work. rnj_sentiment_bing %&gt;% arrange(sentenceID) %&gt;% mutate(positivity = cumsum(sentiment==&#39;positive&#39;), negativity = cumsum(sentiment==&#39;negative&#39;)) %&gt;% plot_ly() %&gt;% add_lines(x=~sentenceID, y=~positivity, name=&#39;positive&#39;) %&gt;% add_lines(x=~sentenceID, y=~negativity, name=&#39;negative&#39;) %&gt;% layout(yaxis = list(title=&#39;sentiment&#39;)) %&gt;% theme_plotly() In general it’s a close game until perhaps the midway point, when negativity takes over and despair sets in with the story. By the end [[:SPOILER ALERT:]] Sean Bean is beheaded, Darth Vader reveals himself to be Luke’s father, and Verbal is Keyser Söze. Here is the same information expressed as a difference. Sentiment Analysis Summary In general, sentiment analysis can be a useful exploration of data, but it is highly dependent on the context and tools used. Note also that ‘sentiment’ can be anything, it doesn’t have to be positive vs. negative. Any vocabulary may be applied, and so it has more utility than the usual implementation. Don’t name your column ‘text’ in practice. It is a base function in R, and the tidyverse will have problems with distinguishing the function from the column name. I only do so for pedagogical reasons.↩ There are almost always encoding issues in my experience.↩ "],
["pos-taggin.html", "POS taggin’ Basic idea POS Examples POS Exercise", " POS taggin’ As an initial review of parts of speech, if you need a refresher the following Schoolhouse Rocks videos should get you squared away: A noun is a person, place, or thing. Interjections Pronouns Verbs Unpack your adjectives Lolly Lolly Lolly Get Your Adverbs Here Conjunction Junction (personal fave) Aside from those, you can also learn how bills get passed, about being a victim of gravity, a comparison of the decimal to other numeric systems used by alien species, and a host of other useful things. Basic idea With part-of-speech tagging, we classify a word with its corresponding part of speech. The following provides an example. JJ JJ NNS VBP RB Colorless green ideas sleep furiously. We have two adjectives (JJ), a plural noun (NNS), a verb (VBP), and an adverb (RB). Common analysis may then be used to predict POS given the current state of the text, comparing the grammar of different texts, human-computer interaction, or translation from one language to another. POS Examples The following approach to POS-tagging is very similar to what we did for sentiment analysis as depicted previously. We have a POS dictionary, and can use an inner join to attach the words to their POS. Unfortunately this approach is unrealistically simplistic, as additional steps would need to be taken to ensure words are correctly classified. For example, without more information, we are unable to tell if some words are being used as nouns or verbs (human being vs. being a problematic part of speech). However, this example can serve as a starting point. Barthelme &amp; Carver In the following we’ll compare three texts from Donald Barthelme: The Balloon The First Thing The Baby Did Wrong Some Of Us Had Been Threatening Our Friend Colby As another comparison, I’ve included Raymond Carver’s What we talk about when we talk about love, the unedited version. First we’ll load an unnested object from the sentiment analysis, the barth object. Then for each work we create a sentence id, unnest the data to words, join the POS data, then create counts/proportions for each POS. load(&#39;data/barth_sentences.RData&#39;) barthelme_pos = barth %&gt;% mutate(work = str_replace(work, &#39;.txt&#39;, &#39;&#39;)) %&gt;% # remove file extension group_by(work) %&gt;% mutate(sentence_id = 1:n()) %&gt;% # create a sentence id unnest_tokens(word, sentence, drop=F) %&gt;% # get words inner_join(parts_of_speech) %&gt;% # join POS count(pos) %&gt;% # count mutate(prop=n/sum(n)) Next we read in and process the Carver text in the same manner. carver_pos = data_frame(file = dir(&#39;data/texts_raw/carver/&#39;, full.names = TRUE)) %&gt;% mutate(text = map(file, read_lines)) %&gt;% transmute(work = basename(file), text) %&gt;% unnest(text) %&gt;% unnest_tokens(word, text, token=&#39;words&#39;) %&gt;% inner_join(parts_of_speech) %&gt;% count(pos) %&gt;% mutate(work=&#39;love&#39;, prop=n/sum(n)) This visualization depicts the proportion of occurrence for each part of speech across the works. It would appear Barthelme is fairly consistent, and also that relative to the Barthelme texts, Carver preferred nouns and pronouns. More taggin’ More sophisticated POS tagging would require the context of the sentence structure. Luckily there are tools to help with that here, in particular via the openNLP package. In addition, it will require a certain language model to be installed (English is only one of many available). I don’t recommend doing so unless you are really interested in this (the openNLPmodels.en package is fairly large). We’ll reexamine the Barthelme texts above with this more involved approach. Initially we’ll need to get the English-based tagger we need and load the libraries. # install.packages(&quot;openNLPmodels.en&quot;, repos = &quot;http://datacube.wu.ac.at/&quot;, type = &quot;source&quot;) library(NLP) library(tm) # make sure to load this prior to openNLP library(openNLP) library(openNLPmodels.en) Next comes the processing. This more or less follows the help file example for ?Maxent_POS_Tag_Annotator. Given the several steps involved I show only the processing for one text for clarity. Ideally you’d write a function, and use a group_by approach, to process each of the texts of interest. load(&#39;data/barthelme_start.RData&#39;) baby_string0 = barth0 %&gt;% filter(id==&#39;baby.txt&#39;) baby_string = unlist(baby_string0$text) %&gt;% paste(collapse=&#39; &#39;) %&gt;% as.String init_s_w = annotate(baby_string, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator())) pos_res = annotate(baby_string, Maxent_POS_Tag_Annotator(), init_s_w) word_subset = subset(pos_res, type==&#39;word&#39;) tags = sapply(word_subset$features , &#39;[[&#39;, &quot;POS&quot;) baby_pos = data_frame(word=baby_string[word_subset], pos=tags) %&gt;% filter(!str_detect(pos, pattern=&#39;[[:punct:]]&#39;)) Let’s take a look. I’ve also done the other Barthelme texts as well for comparison. word pos text The DT baby first JJ baby thing NN baby the DT baby baby NN baby did VBD baby wrong JJ baby was VBD baby to TO baby tear VB baby pages NNS baby out IN baby of IN baby her PRP$ baby books NNS baby As we can see, we have quite a few more POS to deal with here. They come from the Penn Treebank. The following table notes what the acronyms stand for. I don’t pretend to know all the facets to this. Plotting the differences, we now see a little more distinction between The Balloon and the other two texts. It is more likely to use the determiners, adjectives, singular nouns, and less likely to use personal pronouns and base verbs and verbs in past tense. For more information, consult the following: Penn Treebank Maxent function Note that this ‘maximum entropy’ approach is just one way to go about things. Other models include hidden markov models, conditional random fields, and more recently, deep learning techniques. POS Exercise "],
["topic-modeling.html", "Topic modeling Basic idea Steps Topic Model Example Extensions Exercise", " Topic modeling Basic idea Topic modeling as typically conducted is a tool for much more than text. The primary technique of Latent Dirichlet Allocation (LDA) should be as much a part of your toolbox as principal components and factor analysis. It can be seen merely as a dimension reduction approach, but it can also be used for its rich interpretative quality as well. The basic idea is that we’ll take a whole lot of features and boil them down to a few ‘topics’. In this sense LDA is akin to discrete PCA. Another way to think about this is more from the perspective of factor analysis, where we are keenly interested in interpretation of the result, and want to know both what terms are associated with which topics, and what documents are more likely to present which topics. In the standard setting, to be able to conduct such an analysis from text one needs a document-term matrix, where rows represent documents, and columns terms. Each cell is a count of how many times the term occurs in the document. Terms are typically words, but could be any n-gram of interest. Outside of text analysis terms could represent bacterial composition, genetic information, or whatever the researcher is interested in. Likewise, documents can be people, geographic regions, etc. The gist is, despite the common text-based application, that what constitutes a document or term is dependent upon the research question, and LDA can be applied in a variety of research settings. Steps When it comes to text analysis, most of the time in topic modeling is spent on processing the text itself. Importing/scraping it, dealing with capitalization, punctuation, removing stopwords, dealing with encoding issues, removing other miscellaneous common words. It is a highly iterative process such that once you get to the document-term matrix, you’re just going to find the stuff that was missed before and repeat the process with new ‘cleaning parameters’ in place. So getting to the analysis stage is the hard part. See the Shakespeare section, which comprises 5 acts, of which the first four and some additional scenes represent all the processing needed to get to the final scene of topic modeling. In what follows we’ll start at the end of that journey. Topic Model Example Shakespeare In this example, we’ll look at Shakespeare’s plays and poems, using a topic model with 10 topics. For our needs we’ll use the topicModels package for the analysis, and mostly others for post-processing. Due to the large number of terms, this could take a while to run depending on your machine (maybe a minute or two). We can also see how things compare with the academic classifications for the texts. load(&#39;Data/shakes_dtm_stemmed.RData&#39;) library(topicmodels) shakes_10 = LDA(convert(shakes_dtm, to = &quot;topicmodels&quot;), k = 10) Examine Terms within Topics One of the first things to do is attempt to interpret the topics, and we can start by seeing which terms are most probable for each topic. get_terms(shakes_10, 20) We can see there is a lot of overlap in these topics for top terms. Just looking at the top 10, love occurs in all of them, god and heart as well, but we could have guessed this just looking at how often they occur in general. Other measures can be used to assess term importance, such as those that seek to balance the term’s probability of occurrence within a document, and term exclusivity, or how likely a term is to occur in only one particular topic. See the Shakespeare section for some examples of those. Examine Document-Topic Expression Next we can look at which documents are more likely to express each topic. t(topics(shakes_10, 2)) For example, Hamlet is most likely to be associated with Topic 8. That topic is associated with the (stemmed words) love, prai, night, dai, ey, hear, mad, heaven, friend, soul, fair, god, life, hand, jew, word, sweet, husband, heart, leav. Hamlet is also one of the few documents that is actually a decent mix, with its second topic expressed being Topic 1, with common terms love, hand, dai, heart, death, god, friend, nobl, ey, hear, world, blood, fear, night, word, heaven, stand, peac, live, bear. They both have love, night, dai, ey, hear, heaven, friend, god, hand, word, heart among their top 20 terms. Sounds about right for Hamlet. The following visualization shows a heatmap for the topic probabilities of each document. Darker values mean higher probability for a document expressing that topic. I’ve also added a cluster analysis based on the cosine distance matrix, and the resulting dendrogram. The colored bar on the right represents the given classification of a work as history, tragedy, comedy, or poem. A couple things stand out. To begin with, most works are associated with one topic7. In terms of the discovered topics, traditional classification really probably only works for the historical works, as they cluster together as expected (except for Henry the VIII, possibly due to it being a collaborative work). Furthermore, tragedies and comedies might hit on the same topics, albeit from different perspectives. In addition, at least some works are very poetical, or at least have topics in common with the poems (love, beauty). If we take four clusters from the cluster analysis, the result boils down to Phoenix (on its own), standard poems, a mixed bag of more love-oriented works and the remaining poems, then everything else. Alternatively, one could merely classify the works based on their probable topics, which would make more sense if clustering of the works is in fact the goal. The following visualization attempts to order them based on their most probable topic. The order is based on the most likely topics across all documents. So we can see that topic modeling can be use to classify the documents themselves into groups of documents most likely to express the same sorts of topics. Extensions There are extensions of LDA used in topic modeling that will allow your analysis to go even further. Correlated Topic Models: the standard LDA does not estimate the topic correlation as part of the process. Supervised LDA: In this scenario, topics can be used for prediction, e.g. the classification of tragedy, comedy etc. (similar to PC regression) Structured Topic Models: Here we want to find the relevant covariates that can explain the topics (e.g. year written, author sex, etc.) Other: There are still other ways to examine topics. Exercise https://ldavis.cpsievert.me/reviews/reviews.html There isn’t a lot to work with in the realm of choosing an ‘optimal’ number of topics, but I investigated it via a measure called perplexity. It bottomed out at around 50 topics. Usually such an approach is done through cross-validation. However, the solution chosen has no guarantee to produce human interpretable topics.↩ "],
["summary.html", "Summary", " Summary It should be clear at this point that text can be seen as amenable to analysis as anything else in statistics. Depending on the goals, the exploration of text can take on one of many forms. In most situations, at least some preprocessing may be required, and often it will be quite an undertaking to make the text amenable to analysis. However, this is often rewarded by interesting insights and a better understanding of the data at hand, and makes possible what otherwise would not be if only human-powered analysis were applied. For more natural language processing tools in R, one should consult the corresponding task view. However, one should be aware that it doesn’t take much to strain one’s computing resources with R’s tools and standard approach. As an example, the Shakespeare corpus is very small by any standard, and even then it will take some time for certain statistics or topic modeling to be conducted. As such, one should be prepared to also spend time learning ways to make computing more efficient. Luckily, many aspects of the process may be easily distributed/parallelized. Much natural language processing is actually done with deep learning techniques, which generally requires a lot of data, notable computing resources, copious amounts of fine tuning, and often involves optimization towards a specific task. Most of the cutting edge work there is done in Python, and as a starting point for more common text-analytic approaches, you can check out the Natural Language Toolkit. Dealing with text is not always easy, but it’s definitely easier than it ever has been. The amount of tools at your disposal is vast, and more are being added all the time. One of the main take home messages is that text analysis can be a lot of fun, so enjoy the process! Best of luck with your research! \\(\\qquad\\sim\\mathbb{M}\\) "],
["shakespeare-start-to-finish.html", "Shakespeare Start to Finish ACT I. Scrape Moby and Gutenberg Shakespeare ACT II. Preliminary Cleaning ACT III. Stop words ACT IV. Other fixes ACT V. Fun stuff", " Shakespeare Start to Finish The following attempts to demonstrate the usual difficulties one encounters dealing with text by procuring and processing the works of Shakespeare. The source is MIT which has made the ‘complete’ works available on the web since 1993, plus one other from Gutenberg. The initial issue is simply getting the works from the web. Subsequently there is metadata, character names, stopwords etc. to be removed. At that point we can stem and count the words in each work, which, when complete, puts us at the point we are ready for analysis. The primary packages used are tidytext, stringr, and when things are ready for analysis, quanteda. ACT I. Scrape Moby and Gutenberg Shakespeare Scene I. Scrape main works Initially we must scrape the web to get the documents we need. The rvest package will be used as follows. Start with the url of the site Get the links off that page to serve as base urls for the works Scrape the document for each url Deal with the collection of Sonnets separately Write out results library(rvest); library(tidyverse); library(stringr) page0 = read_html(&#39;http://shakespeare.mit.edu/&#39;) works_urls0 = page0 %&gt;% html_nodes(&#39;a&#39;) %&gt;% html_attr(&#39;href&#39;) main = works_urls0 %&gt;% grep(pattern=&#39;index&#39;, value=T) %&gt;% str_replace_all(pattern=&#39;index&#39;, replacement=&#39;full&#39;) other = works_urls0[!grepl(works_urls0, pattern=&#39;index|edu|org|news&#39;)] works_urls = c(main, other) works_urls[1:3] Now we just paste the main site url to the work urls and download them. Here is where we come across our first snag. The html_text function has what I would call a bug but what the author feels is a feature. Basically it ignores line breaks of the form in certain situations. This means it will smash text together that shouldn’t be, thereby making any analysis of it fairly useless8. Luckily, @rentrop provided a solution, which is in r/fix_read_html.R. works0 = lapply(works_urls, function(x) read_html(paste0(&#39;http://shakespeare.mit.edu/&#39;, x))) source(&#39;r/fix_read_html.R&#39;) html_text_collapse(works0[[1]]) #works works = lapply(works0, html_text_collapse) names(works) = c(&quot;All&#39;s Well That Ends Well&quot; &quot;As You Like It&quot; &quot;Comedy of Errors&quot; &quot;Cymbeline&quot; &quot;Love&#39;s Labour&#39;s Lost&quot; &quot;Measure for Measure&quot; &quot;The Merry Wives of Windsor&quot; &quot;The Merchant of Venice&quot; &quot;A Midsummer Night&#39;s Dream&quot; &quot;Much Ado about Nothing&quot; &quot;Pericles Prince of Tyre&quot; &quot;The Taming of the Shrew&quot; &quot;The Tempest&quot; &quot;Troilus and Cressida&quot; &quot;Twelfth Night&quot; &quot;The Two Gentlemen of Verona&quot; &quot;The Winter&#39;s Tale&quot; &quot;King Henry IV Part 1&quot; &quot;King Henry IV Part 2&quot; &quot;Henry V&quot; &quot;Henry VI Part 1&quot; &quot;Henry VI Part 2&quot; &quot;Henry VI Part 3&quot; &quot;Henry VIII&quot; &quot;King John&quot; &quot;Richard II&quot; &quot;Richard III&quot; &quot;Antony and Cleopatra&quot; &quot;Coriolanus&quot; &quot;Hamlet&quot; &quot;Julius Caesar&quot; &quot;King Lear&quot; &quot;Macbeth&quot; &quot;Othello&quot; &quot;Romeo and Juliet&quot; &quot;Timon of Athens&quot; &quot;Titus Andronicus&quot; &quot;Sonnets&quot; &quot;A Lover&#39;s Complaint&quot; &quot;The Rape of Lucrece&quot; &quot;Venus and Adonis&quot; &quot;Elegy&quot;) Scene II. Sonnets We hit a slight nuisance with the Sonnets. The Sonnets have a bit of a different structure than the plays. All links are in a single page, with a different form for the url, and each sonnet has its own page. sonnet_urls = paste0(&#39;http://shakespeare.mit.edu/&#39;, grep(works_urls0, pattern=&#39;sonnet&#39;, value=T)) %&gt;% read_html() %&gt;% html_nodes(&#39;a&#39;) %&gt;% html_attr(&#39;href&#39;) sonnet_urls = grep(sonnet_urls, pattern = &#39;sonnet&#39;, value=T) # remove amazon link # read the texts sonnet0 = purrr::map(sonnet_urls, function(x) read_html(paste0(&#39;http://shakespeare.mit.edu/Poetry/&#39;, x))) # collapse to one &#39;Sonnets&#39; work sonnet = sapply(sonnet0, html_text_collapse) works$Sonnets = sonnet Scene III. Save and write out Now we can save our results so we won’t have to repeat any of the previous scraping. We want to save the main text object as an RData file, and write out the texts to their own file. When dealing with text, you’ll regularly want to save stages so you can avoid repeating what you don’t have to, as often you will need to go back after discovering new issues further down the line. save(works, file=&#39;data/texts_raw/shakes/moby_from_web.RData&#39;) # This will spit the text to the console unfortunately purrr::map2(works, paste0(&#39;data/texts_raw/shakes/moby/&#39;, str_replace_all(names(works), &quot; |&#39;&quot;, &#39;_&#39;), &#39;.txt&#39;), function(x, nam) write_lines(x, path=nam)) Scene IV. Read text from files After the above is done, it’s not required to redo, so we can always get what we need. I’ll start with the raw text as files, as that is one of the more common ways one deals with documents. When text is nice and clean, this can be fairly straightforward. The function at the end comes from the tidyr package. Up to that line, each element in the text column is the entire text, while the column itself is thus a ‘list-column’. In other words we have a 42 x 2 matrix. But to do what we need, we’ll want to have access to each line, and the unnest function unpacks each line within the title. The first few lines of the result is shown after. library(tidyverse); library(stringr) shakes0 = data_frame(file = dir(&#39;data/texts_raw/shakes/moby/&#39;, full.names = TRUE)) %&gt;% transmute(id = basename(file), text) %&gt;% unnest(text) save(shakes0, file=&#39;data/initial_shakes_dt.RData&#39;) # Alternate that provides for more options # library(readtext) # shakes0 = # data_frame(file = dir(&#39;data/texts_raw/shakes/moby/&#39;, full.names = TRUE)) %&gt;% # mutate(text = map(file, readtext, encoding=&#39;UTF8&#39;)) %&gt;% # unnest(text) Scene V. Add additional works It is typical to be gathering texts from multiple sources. In this case, we’ll get The Phoenix and the Turtle from the Project Gutenberg website. There is an R package that will allow us to work directly with the site, making the process straightforward9. I also considered two other works, but I refrained from “The Two Noble Kinsmen” because like many other of Shakespeare’s versions on Gutenberg, it’s basically written in a different language. I also refrained from The Passionate Pilgrim because it’s mostly not Shakespeare. When first doing this project, I actually started with Gutenberg, but it became a notable PITA. The texts were inconsistent in source, and sometimes reproduced printing errors purposely, which would have compounded typical problems. I thought it could have been solved by using the Complete Works of Shakespeare but the download only came with that title, meaning one would have to hunt for and delineate each separate work. This might not have been too big of an issue, except that there is no table of contents, nor consistent naming of titles across different printings. The MIT approach, on the other hand, was a few lines of code. This represents a common issue in text analysis when dealing with sources, a different option may save a lot of time in the end. The following code could be more succinct to deal with one text, but I initially was dealing with multiple works, so I’ve left it in that mode. In the end we’ll have a tibble with an id column for the file/work name, and another column that contains the lines of text. library(gutenbergr) works_not_included = c(&quot;The Phoenix and the Turtle&quot;) # add others if desired gute0 = gutenberg_works(title %in% works_not_included) gute = lapply(gute0$gutenberg_id, gutenberg_download) gute = mapply(function(x, y) mutate(x, id=y) %&gt;% select(-gutenberg_id), gute, works_not_included, SIMPLIFY=F) shakes = shakes0 %&gt;% bind_rows(gute) %&gt;% mutate(id = str_replace_all(id, &quot; |&#39;&quot;, &#39;_&#39;)) %&gt;% mutate(id = str_replace(id, &#39;.txt&#39;, &#39;&#39;)) %&gt;% arrange(id) # shakes %&gt;% split(.$id) # inspect save(shakes, file=&#39;data/texts_raw/shakes/shakes_df.RData&#39;) ACT II. Preliminary Cleaning If you think we’re even remotely getting close to being ready for analysis, I say Ha! to you. Our journey has only just begun (cue the Carpenters). Now we can start thinking about prepping the data for eventual analysis. One of the nice things about having the data in a tidy format is that we can use string functionality over the column of text in a simple fashion. Scene I. Remove initial text/metadata First on our to-do list is to get rid of all the preliminary text of titles, authorship etc. This is fairly easy when you think that every text will start with ACT I, or in the case of the Sonnets, the word ‘Sonnet’. We want to drop all text up to those points. I’ve created a function that will do that, and then just apply it to each works tibble10. For the poems and A Funeral Elegy for Master William Peter, we look instead for the line where his name or initials start the line. source(&#39;r/detect_first_act.R&#39;) shakes_trim = shakes %&gt;% split(.$id) %&gt;% lapply(detect_first_act) %&gt;% bind_rows shakes %&gt;% filter(id==&#39;Romeo_and_Juliet&#39;) %&gt;% head # A tibble: 6 x 2 id text &lt;chr&gt; &lt;chr&gt; 1 Romeo_and_Juliet Romeo and Juliet: Entire Play 2 Romeo_and_Juliet 3 Romeo_and_Juliet 4 Romeo_and_Juliet 5 Romeo_and_Juliet 6 Romeo_and_Juliet Romeo and Juliet shakes_trim %&gt;% filter(id==&#39;Romeo_and_Juliet&#39;) %&gt;% head # A tibble: 6 x 2 id text &lt;chr&gt; &lt;chr&gt; 1 Romeo_and_Juliet 2 Romeo_and_Juliet 3 Romeo_and_Juliet PROLOGUE 4 Romeo_and_Juliet 5 Romeo_and_Juliet 6 Romeo_and_Juliet Scene II. Miscellaneous removal Next, we’ll want to remove empty rows, any remaining titles, lines that denote the act or scene, and other stuff. I’m going to remove the word prologue and epilogue as a stopword later. While some texts have a line that just says that (PROLOGUE), others have text that describes the scene (Prologue. Blah blah) and which I’ve decided to keep. As such, we just need the word itself gone. titles = c(&quot;A Lover&#39;s Complaint&quot;, &quot;All&#39;s Well That Ends Well&quot;, &quot;As You Like It&quot;, &quot;The Comedy of Errors&quot;, &quot;Cymbeline&quot;, &quot;Love&#39;s Labour&#39;s Lost&quot;, &quot;Measure for Measure&quot;, &quot;The Merry Wives of Windsor&quot;, &quot;The Merchant of Venice&quot;, &quot;A Midsummer Night&#39;s Dream&quot;, &quot;Much Ado about Nothing&quot;, &quot;Pericles Prince of Tyre&quot;, &quot;The Taming of the Shrew&quot;, &quot;The Tempest&quot;, &quot;Troilus and Cressida&quot;, &quot;Twelfth Night&quot;, &quot;The Two Gentlemen of Verona&quot;, &quot;The Winter&#39;s Tale&quot;, &quot;King Henry IV, Part 1&quot;, &quot;King Henry IV, Part 2&quot;, &quot;Henry V&quot;, &quot;Henry VI, Part 1&quot;, &quot;Henry VI, Part 2&quot;, &quot;Henry VI, Part 3&quot;, &quot;Henry VIII&quot;, &quot;King John&quot;, &quot;Richard II&quot;, &quot;Richard III&quot;, &quot;Antony and Cleopatra&quot;, &quot;Coriolanus&quot;, &quot;Hamlet&quot;, &quot;Julius Caesar&quot;, &quot;King Lear&quot;, &quot;Macbeth&quot;, &quot;Othello&quot;, &quot;Romeo and Juliet&quot;, &quot;Timon of Athens&quot;, &quot;Titus Andronicus&quot;, &quot;Sonnets&quot;, &quot;The Rape of Lucrece&quot;, &quot;Venus and Adonis&quot;, &quot;A Funeral Elegy&quot;, &quot;The Phoenix and the Turtle&quot;) shakes_trim = shakes_trim %&gt;% filter(text != &#39;&#39;, # empties !text %in% titles, # titles !str_detect(text, &#39;^ACT|^SCENE|^Enter|^Exit|^Exeunt|^Sonnet&#39;) # acts etc. ) shakes_trim %&gt;% filter(id==&#39;Romeo_and_Juliet&#39;) # we&#39;ll get prologue later # A tibble: 3,992 x 2 id text &lt;chr&gt; &lt;chr&gt; 1 Romeo_and_Juliet PROLOGUE 2 Romeo_and_Juliet Two households, both alike in dignity, 3 Romeo_and_Juliet In fair Verona, where we lay our scene, 4 Romeo_and_Juliet From ancient grudge break to new mutiny, 5 Romeo_and_Juliet Where civil blood makes civil hands unclean. 6 Romeo_and_Juliet From forth the fatal loins of these two foes 7 Romeo_and_Juliet A pair of star-cross&#39;d lovers take their life; 8 Romeo_and_Juliet Whose misadventured piteous overthrows 9 Romeo_and_Juliet Do with their death bury their parents&#39; strife. 10 Romeo_and_Juliet The fearful passage of their death-mark&#39;d love, # ... with 3,982 more rows Scene III. Classification of works While we’re at it, we can save the classical (sometimes arbitrary) classifications of Shakespeare’s works for later comparison to what we’ll get in our analyses. We’ll save them to call as needed. shakes_types = data_frame(title=unique(shakes_trim$id)) %&gt;% mutate(class = &#39;Comedy&#39;, class = if_else(grepl(title, pattern=&#39;Adonis|Lucrece|Complaint|Turtle|Pilgrim|Sonnet|Elegy&#39;), &#39;Poem&#39;, class), class = if_else(grepl(title, pattern=&#39;Henry|Richard|John&#39;), &#39;History&#39;, class), class = if_else(grepl(title, pattern=&#39;Troilus|Coriolanus|Titus|Romeo|Timon|Julius|Macbeth|Hamlet|Othello|Antony|Cymbeline|Lear&#39;), &#39;Tragedy&#39;, class), problem = if_else(grepl(title, pattern=&#39;Measure|Merchant|^All|Troilus|Timon|Passion&#39;), &#39;Problem&#39;, &#39;Not&#39;), late_romance = if_else(grepl(title, pattern=&#39;Cymbeline|Kinsmen|Pericles|Winter|Tempest&#39;), &#39;Late&#39;, &#39;Other&#39;)) save(shakes_types, file=&#39;data/shakespeare_classification.RData&#39;) # save for later ACT III. Stop words As we’ve noted before, we’ll want to get rid of stop words, things like articles, possessive pronouns, and other very common words. In this case, we also want to include character names. However, the big wrinkle here is that this is not everyday English, so we need to get ye, thee, thine etc. In addition there are things that need to be replaced like o’er to over, which may then also be removed. In short, this is not so straightforward. Scene I. Character names We’ll get the list of character names from opensourceshakespeare.org via rvest, but I added some from the poems and others that came through, e.g. abbreviated names. shakes_char_url = &#39;https://www.opensourceshakespeare.org/views/plays/characters/chardisplay.php&#39; page0 = read_html(shakes_char_url) tabs = page0 %&gt;% html_table() shakes_char = tabs[[2]][-(1:2), c(1,3,5)] # remove header and phantom columns colnames(shakes_char) = c(&#39;Nspeeches&#39;, &#39;Character&#39;, &#39;Play&#39;) shakes_char = shakes_char %&gt;% distinct(Character,.keep_all=T) save(shakes_char, file=&#39;data/shakespeare_characters.RData&#39;) A new snag is that some characters with multiple names may be represented (typically) by the first or last or in the case of three, the middle, e.g. Sir Toby Belch. Others are still difficultly named e.g. RICHARD PLANTAGENET (DUKE OF GLOUCESTER). The following should capture everything by splitting the names on spaces, removing parentheses, and keeping unique terms. # remove paren and split chars = shakes_char$Character chars = str_replace_all(chars, &#39;\\\\(|\\\\)&#39;, &#39;&#39;) chars = str_split(chars, &#39; &#39;) %&gt;% unlist # these were found after intial processsing chars_other = c(&#39;enobarbus&#39;, &#39;marcius&#39;, &#39;katharina&#39;, &#39;clarence&#39;,&#39;pyramus&#39;, &#39;andrew&#39;, &#39;arcite&#39;, &#39;perithous&#39;, &#39;hippolita&#39;, &#39;schoolmaster&#39;, &#39;cressid&#39;, &#39;diomed&#39;, &#39;kate&#39;, &#39;titinius&#39;, &#39;Palamon&#39;, &#39;Tarquin&#39;, &#39;lucrece&#39;, &#39;isidore&#39;, &#39;tom&#39;, &#39;thisbe&#39;, &#39;paul&#39;, &#39;aemelia&#39;, &#39;sycorax&#39;, &#39;montague&#39;, &#39;capulet&#39;, &#39;collatinus&#39;) chars = unique(c(chars, chars_other)) chars = chars[chars != &#39;&#39;] sample(chars)[1:3] [1] &quot;Children&quot; &quot;Dionyza&quot; &quot;Aaron&quot; Scene II. Old, Middle, &amp; Modern English While Shakespeare is considered Early Modern English, some text may be more historical, so I include Middle and Old English stopwords, as they were readily available from the cltk Python module (link) and also found. I also added some things to the ME list like “thou’ldst” that I found lingering after initial passes. In my initial use of Gutenberg texts, the Old English might have had some utility, but with these texts it only removes ‘wit’, so I refrain from using it. # old and me from python cltk module; # em from http://earlymodernconversions.com/wp-content/uploads/2013/12/stopwords.txt; # I also added some to me old_stops0 = read_lines(&#39;data/old_english_stop_words.txt&#39;) # sort(old_stops0) old_stops = data_frame(word=str_conv(old_stops0, &#39;UTF8&#39;), lexicon = &#39;cltk&#39;) me_stops0 = read_lines(&#39;data/middle_english_stop_words&#39;) # sort(me_stops0) me_stops = data_frame(word=str_conv(me_stops0, &#39;UTF8&#39;), lexicon = &#39;cltk&#39;) em_stops0 = read_lines(&#39;data/early_modern_english_stop_words.txt&#39;) # sort(em_stops0) em_stops = data_frame(word=str_conv(em_stops0, &#39;UTF8&#39;), lexicon = &#39;emc&#39;) Scene III. Remove stopwords We’re now ready to start removing words. However, right now, we have lines not words. We can use the tidytext function unnest_tokens, which is like unnest from tidyr, but works on different tokens, e.g. words, sentences, or paragraphs. Note that by default, the function will make all words lower case. library(tidytext) shakes_words = shakes_trim %&gt;% unnest_tokens(word, text, token=&#39;words&#39;) We also will be doing a little stemming here. I’m getting rid of suffixes that end with the suffix after an apostrophe. Many of the remaining words will either be stopwords or need to be further stemmed later. I also created a middle/modern English stemmer for words that are not caught otherwise (me_st_stem). Again this is the sort of thing you discover after initial passes (e.g. ‘criedst’). We can do that, then the anti_join of all the stopwords. source(&#39;r/st_stem.R&#39;) shakes_words = shakes_words %&gt;% mutate(word = str_trim(word), # remove possible whitespace word = str_replace(word, &quot;&#39;er$|&#39;d$|&#39;t$|&#39;ld$|&#39;rt$|&#39;st$|&#39;dst$&quot;, &#39;&#39;), # remove me style endings word = str_replace_all(word, &quot;[0-9]&quot;, &#39;&#39;), # remove sonnet numbers word = vapply(word, me_st_stem, &#39;a&#39;)) %&gt;% anti_join(em_stops) %&gt;% anti_join(me_stops) %&gt;% anti_join(data_frame(word=str_to_lower(c(chars, &#39;prologue&#39;, &#39;epilogue&#39;)))) %&gt;% anti_join(data_frame(word=str_to_lower(paste0(chars, &quot;&#39;s&quot;)))) %&gt;% # remove possessive names anti_join(stop_words) As before, you should do a couple spot checks. any(shakes_words$word == &#39;romeo&#39;) any(shakes_words$word == &#39;prologue&#39;) any(shakes_words$word == &#39;mayst&#39;) [1] FALSE [1] FALSE [1] FALSE ACT IV. Other fixes Now we’re ready to finally do the word counts. Just kidding! There is still work to do the for the remainder, and you’ll continue to spot things after runs. A big issue is the words that end in ‘st’ and ‘est’, and others that are not consistently spelled or otherwise need to be dealt with. For example, ‘crost’ will not be stemmed to ‘cross’, as ‘crossed’ would be. Finally, I limit the result to any words that have more than two characters, as my inspection suggested these be left-over suffixes or otherwise would be considered stopwords anyway. # porter should catch remaining &#39;est&#39; add_a = c(&#39;mongst&#39;, &#39;gainst&#39;) # words to add a to shakes_words = shakes_words %&gt;% mutate(word = if_else(word==&#39;honour&#39;, &#39;honor&#39;, word), word = if_else(word==&#39;durst&#39;, &#39;dare&#39;, word), word = if_else(word==&#39;wast&#39;, &#39;was&#39;, word), word = if_else(word==&#39;dust&#39;, &#39;does&#39;, word), word = if_else(word==&#39;curst&#39;, &#39;cursed&#39;, word), word = if_else(word==&#39;blest&#39;, &#39;blessed&#39;, word), word = if_else(word==&#39;crost&#39;, &#39;crossed&#39;, word), word = if_else(word==&#39;accurst&#39;, &#39;accursed&#39;, word), word = if_else(word %in% add_a, paste0(&#39;a&#39;, word), word), word = str_replace(word, &quot;&#39;s$&quot;, &#39;&#39;), # strip remaining possessives word = if_else(str_detect(word, pattern=&quot;o&#39;er&quot;), # change o&#39;er over str_replace(word, &quot;&#39;&quot;, &#39;v&#39;), word)) %&gt;% filter(!(id==&#39;Antony_and_Cleopatra&#39; &amp; word == &#39;mark&#39;)) %&gt;% # mark here is almost exclusively the character name filter(str_count(word)&gt;2) At this point we could still maybe add things to this list of additional fixes, but I think it’s time to actually start playing with the data. ACT V. Fun stuff We are finally ready to get to the fun stuff. Finally! And now things get easy. Scene I. Count the terms We can get term counts with standard dplyr approaches, and tidytext will take that and do the other things we might want. Specifically, we can use the latter to create the document term matrix which will be used in other analysis. The function cast_dfm will create a dfm class object, or ‘document-feature’ matrix, which is the same thing but recognizes this sort of stuff is not specific to words. With word counts in hand, now would be a good save point since they’ll serve as the basis for other things. term_counts = shakes_words %&gt;% group_by(id, word) %&gt;% count term_counts %&gt;% arrange(desc(n)) library(quanteda) shakes_dtm = term_counts %&gt;% cast_dfm(document=id, term=word, value=n) ## save(shakes_words, term_counts, shakes_dtm, file=&#39;data/shakes_words_df.RData&#39;) # A tibble: 115,978 x 3 # Groups: id, word [115,978] id word n &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 Sonnets love 195 2 The_Two_Gentlemen_of_Verona love 171 3 Romeo_and_Juliet love 150 4 As_You_Like_It love 118 5 Love_s_Labour_s_Lost love 118 6 A_Midsummer_Night_s_Dream love 114 7 Richard_III god 111 8 Titus_Andronicus rome 103 9 Much_Ado_about_Nothing love 92 10 Coriolanus rome 90 # ... with 115,968 more rows Now things are looking like Shakespeare, with love for everyone11. You’ll notice I’ve kept place names such as Rome, but this might be something you’d prefer to remove. Other candidates would be madam, woman, man, majesty (as in ‘his/her’) etc. This sort of thing is up to the researcher. Scene II. Stemming Now we’ll the words. While this is actually a pre-processing step, one that we’d do along with (typically after) stopword removal. I do it here to mostly demonstrate how to use quanteda to do it, as it can also be used to remove stopwords and do many of the other things we did with tidytext. Stemming will make words like eye and eyes just ey, or convert war, wars and warring to war. In other words it will reduce variations of a word to a common root form, or ‘word stem’. We could have done this in a step prior to counting the terms, but then you only have the stemmed result to work with for the document term matrix from then on. Depending on your situation, you may or may not want to stem, or maybe you’d want to compare results. The quanteda package will actually stem with the DTM and collapse the word counts accordingly. I note the difference in words before and after. shakes_dtm ncol(shakes_dtm) shakes_dtm = shakes_dtm %&gt;% dfm_wordstem() shakes_dtm ncol(shakes_dtm) Document-feature matrix of: 43 documents, 22,059 features (87.8% sparse). [1] 22059 Document-feature matrix of: 43 documents, 13,714 features (84% sparse). [1] 13714 The result is notably fewer columns, which will speed up any analysis, as well as a slightly more dense matrix. Scene III. Exploration Top features Let’s start looking more in depth. The following shows the 10 most common words and their respective counts. This is also an easy way to find candidates to add to the stopword list. Note that dai and prai are stems for day and pray. Love occurs 2.15 times as much as the most frequent word! top10 = topfeatures(shakes_dtm, 10) top10 love heart god ey dai hand hear live death night 2917 1359 1280 1275 1229 1226 1043 1015 1010 977 The following is a word cloud. They are among the most useless visual displays imaginable. Just because you can, doesn’t mean you should. If you want to display relative frequency do so. Similarity The quanteda package has some built in similarity measures such as cosine similarity, which you can treat similarly to the standard correlation (also available). I display it visually to better get a sense of things. ## textstat_simil(shakes_dtm, margin = &quot;documents&quot;, method = &quot;cosine&quot;) We can already begin to see the clusters of documents. For example, the more historical are the clump in the upper left. The oddball is The Phoenix and the Turtle, though Lover’s Complaint and the Elegy are also less similar than standard Shakespeare. The Phoenix and the Turtle is about the death of ideal love, represented by the Phoenix and Turtledove, for which there is a funeral. It actually is considered by scholars to be in stark contrast to his other output. Elegy itself is actually written for a funeral. A Lover’s Complaint is considered an inferior work by the Bard, so perhaps what we’re seeing is a reflection of that lack of quality. In general, we’re seeing things that we might expect. Readability We can examine readability scores for the texts, but for this we’ll need them in raw form. We already had them from before, I just added Phoenix from the Gutenberg download. raw_texts # A tibble: 43 x 2 id text &lt;chr&gt; &lt;list&gt; 1 A_Lover_s_Complaint.txt &lt;chr [813]&gt; 2 A_Midsummer_Night_s_Dream.txt &lt;chr [6,630]&gt; 3 All_s_Well_That_Ends_Well.txt &lt;chr [10,993]&gt; 4 Antony_and_Cleopatra.txt &lt;chr [14,064]&gt; 5 As_You_Like_It.txt &lt;chr [9,706]&gt; 6 Coriolanus.txt &lt;chr [13,440]&gt; 7 Cymbeline.txt &lt;chr [11,388]&gt; 8 Elegy.txt &lt;chr [1,316]&gt; 9 Hamlet.txt &lt;chr [13,950]&gt; 10 Henry_V.txt &lt;chr [9,777]&gt; # ... with 33 more rows With raw texts, we need to make them an official corpus object to proceed more easily. The corpus function from quanteda won’t read directly from a list column or a list at all, so we’ll convert it via the tm package, which more or less defeats the purpose of using the quanteda package except that the textstat_readability function gives us what we want, but I digress. Unfortunately the concept of readability is ill-defined, and as such, there are dozens of measures available dating back nearly 75 years. The following is based on the Coleman-Liau grade score (higher grade = more difficult). The conclusion here is first, Shakespeare isn’t exactly a difficult read, and two, the poems may be more so relative to the other works. library(tm) raw_text_corpus = corpus(VCorpus(VectorSource(raw_texts$text))) shakes_read = textstat_readability(raw_text_corpus) Lexical diversity There are also metrics of lexical diversity. As with readability, there is no one way to measure ‘diversity’. Here we’ll go back to using the standard DTM, as the focus is on the terms, whereas readability is more at the sentence level. Most standard measures of lexical diversity are variants on what is called the type-token ratio, which in our setting is the number of unique terms (types) relative to the total terms (tokens). We can use textstat_lexdiv for our purposes here, which will provide several measures of diversity by default. ld = textstat_lexdiv(shakes_dtm) This visual is based on the (absolute) scaled values of those several metrics, and might suggest that the poems are relatively more diverse. This certainly might be the case for Phoenix, but it could also be a reflection of the limitation of several of the measures, such that longer works are seen as less diverse, as tokens are added more so than types the longer the text goes. As a comparison, the following shows the results of the ‘Measure of Textual Diversity’ calculated using the koRpus package12. It is notably less affected by text length, though the conclusions are largely the same. There is notable correlation between the MTLD and readability as well13. In general, Shakespeare tends to be more expressive in poems, and less so with comedies. Scene IV. Topic model I’d say we’re now ready for topic model. That didn’t take too much did it? Running the model and exploring the topics We’ll run run one with 10 topics. We can also see how things compare with the usual classifications for the texts. Also, this will take a while to run depending on your machine (maybe a minute or two). library(topicmodels) shakes_10 = LDA(convert(shakes_dtm, to = &quot;topicmodels&quot;), k = 10, control=list(seed=1234)) One of the first things to do is to interpret the topics, and we can start by seeing which terms are most probable for each topic. get_terms(shakes_10, 20) We can see there is a lot of overlap in these topics for top terms. Just looking at the top 10, love occurs in all of them, god and heart as well, but we could have guessed this just looking at how often they occur in general. Other measures can be used to assess term importance, such as those that seek to balance the term’s probability of occurrence within a document, and term exclusivity, or how likely a term is to occur in only one particular topic. See the stm package and corresponding labelTopics function as a way to get several alternatives. As an example I show the results of their version of the following14: FREX: FRequency and EXclusivity, it is a weighted harmonic mean of a term’s rank within a topic in terms of frequency and exclusivity. lift: Ratio of the term’s probability within a topic to its probability of occurrence across all documents. Overly sensitive to rare words. score: Another approach that will give more weight to more exclusive terms. prob: This is just the raw probability of the term within a given topic. As another approach, consider the saliency and relevance of term via the LDAvis package. While you can play with it here, it’s probably easier to open it separately. Your browser does not support iframes. Given all these measures, one can now see how well they match what the topics the documents would be most associated with. t(topics(shakes_10, 3)) For example, base just on term frequency, Hamlet is most likely to be associated with Topic 8. That topic is associated with the (stemmed words) love, prai, night, dai, ey, hear, mad, heaven, friend, soul, fair, god, life, hand, jew, word, sweet, husband, heart, leav. Hamlet is also one that is actually a decent mix, with its second topic expressed being Topic 1, with common terms love, hand, dai, heart, death, god, friend, nobl, ey, hear, world, blood, fear, night, word, heaven, stand, peac, live, bear. They both have love, night, dai, ey, hear, heaven, friend, god, hand, word, heart among their top 20 terms. Sounds about right for Hamlet. The other measures pick up on things like Dane and Denmark. The following visualization shows a heatmap for the topic probabilities of each document. Darker values mean higher probability for a document expressing that topic. I’ve also added a cluster analysis based on the cosine distance matrix, and the resulting dendrogram15. The colored bar on the right represents the given classification of a work as history, tragedy, comedy, or poem. A couple things stand out. To begin with, most works are associated with one topic16. In terms of the discovered topics, traditional classification really probably only works for the historical works, as they cluster together as expected (except for Henry the VIII, possibly due to it being a collaborative work). Furthermore, tragedies and comedies might hit on the same topics, albeit from different perspectives. In addition, at least some works are very poetical, or at least have topics in common with the poems (love, beauty). If we take four clusters from the cluster analysis, the result boils down to Phoenix (on its own), standard poems, a mixed bag of more love-oriented works and the remaining poems, then everything else. Alternatively, one could merely classify the works based on their probable topics, which would make more sense if clustering of the works is in fact the goal. The following visualization attempts to order them based on their most probable topic. The order is based on the most likely topics across all documents. The following shows the average topic probability for each of the traditional classes. Topics are represented by their first five most probable terms. Aside from the poems, the classes are a good mix of topics, and appear to have some overlap. Tragedies are perhaps most diverse. Summary of Topic Models And I grow weary… FIN If you can think of a use case where x&lt;br&gt;y&lt;br&gt;z leading to xyz would be both expected as default behavior and desired please let me know.↩ If this surprises you, let me remind you that there are over 10k packages on CRAN alone.↩ I found it easier to work with the entire data frame for the function, hence splitting it on id and recombining. Some attempt was made to work within the tidyverse, but there were numerous issues to what should have been a fairly easy task.↩ Love might as well be a stopword for Shakespeare.↩ I don’t show this as I actually did it in parallel due to longer works taking a notable time to calculate MTLD.↩ The Pearson correlation between MTLD and the Coleman Liau grade readability depicted previously was .87.↩ These descriptions are from Sievert and Shirley 2014.↩ If you are actually interested in clustering the documents (or anything for that matter in my opinion), this would not be the way to do so. For one, the documents are already clustered based on most probable topic. Second, cosine distance isn’t actually a proper distance. Third, as shocking as it may seem, newer methods have been developed since the hierarchical clustering approach, which basically has a dozen arbitrary choices to be made at each step. However, as a simple means to a visualization, the method is valuable if it helps with understanding the data.↩ There isn’t a lot to work with in the realm of choosing an ‘optimal’ number of topics, but I investigated it via a measure called perplexity. It bottomed out at around 50 topics. Usually such an approach is done through cross-validation. However, the solution chosen has no guarantee to produce human interpretable topics.↩ "],
["appendix.html", "Appendix Texts R Python", " Appendix Texts Donald Barthelme “I have to admit we are mired in the most exquisite mysterious muck. This muck heaves and palpitates. It is multi-directional and has a mayor.” “You may not be interested in absurdity, but absurdity is interested in you.” The First Thing the Baby Did Wrong This short story is essentially a how-to on parenting. link The Balloon This story is about a balloon that can represent whatever you want it to. link Some of Us Had Been Threatening Our Friend Colby A brief work about etiquette and how to act in society. link Raymond Carver “It ought to make us feel ashamed when we talk like we know what we’re talking about when we talk about love.” “That’s all we have, finally, the words, and they had better be the right ones.” “Get in, get out. Don’t linger. Go on.” “There is no answer. It’s okay. But even if it wasn’t okay, what am I supposed to do?” “You’ve got to work with your mistakes until they look intended. Understand?” What We Talk About When We Talk About Love The text we use is actually Beginners, or the unedited version. A drink is required in order to read it with the proper context. Probably several. No. Definitely several. link Billy Dee Shakespeare “It works every time.” These old works have pretty much no relevance today, and are mostly forgotten by everyone except humanities faculty. The analysis of them depicted in this document is pretty much definitive, and leaves little else to say regarding them, so don’t bother reading them if you haven’t already. R Up until even a couple years ago, R was terrible at text. NLP task view monkeylearn? Python nltk gensim spacy "]
]
