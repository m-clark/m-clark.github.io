[
["index.html", "My God! It’s full of STARs! Using Astrology to Enhance Your Results!&lt;/span", " My God! It’s full of STARs! Using Astrology to Enhance Your Results!&lt;/span Michael Clark https://m-clark.github.io/workshops/stars/ 2017-04-08 "],
["preface.html", "Preface Audience Prerequisites Outline Workshop Setup", " Preface This talk/workshop provides a brief introduction to generalized additive models (GAMs), or even more generally, or structured additive regression models (STARs). They offer a natural extension to common generalized linear models, while having the flexibility to incorporate a wide range of modeling situations. Audience Specifically, I have in mind applied researchers with a burgeoning interest in using models they probably weren’t exposed to in their applied statistical training. More generally, anyone that finds it useful. Prerequisites Prerequisites include a firm understanding of linear regression and exposure to generalized linear models. Some experience with using R for statistical modeling is required to follow along with the code and do the exercises. The workshop will potentially see a wide range of skill sets and modeling experience among attendees, but aims to be accessible to all. As such, the practical is favored over the technical, but some details are provided as an appendix, with additional references for those interested in going deeper down the rabbit hole. Outline Why demo where lm fails traditional ways of dealing with problems that arise Transition: standard polynomial where it works where it isn’t applicable/fails Introducing GAMs basic info getting started Note Issues things to get used to Extensions &amp; Ties to other Additional choices for smooth terms Random effects Spatial GP Boosting/RF Technicals by-hand example penalized regression Workshop Setup The goal of the following is to create an RStudio project (i.e. a folder that represents the working directory moving forward) with a folder inside called ‘data’ that has the data for the demonstrations and exercises. For those not used to RStudio projects, read on, and use them for everything you do from now on. RStudio Click File/New Project/New Directory and give it a name, e.g. GAM_Workshop. BE MINDFUL ABOUT WHERE YOU ARE CREATING IT. You don’t have to do anything else with RStudio at this point, but leave it open. Data The data used in the document demonstrations and exercises is available at https://m-clark.github.io/workshops/stars/data.7z. It is a zipped archive containing several data files and .RData objects. You must: Download it. BE INTENTIONAL ABOUT WHERE YOU DOWNLOAD IT. Unzip it. Note that this is a different step than number 1, and failure to do this will render the stuff inside inaccessible. BE PURPOSEFUL ABOUT WHERE YOU UNZIP IT. Copy/Move it. This now usable folder needs to go into your project folder that you just created. This can actually be done as part of step 2. When you are finished, you will now have an R project folder with a data folder inside it which contains the example data sets. To demonstrate this, in RStudio you should be able to open a script and type load('data/movies_yearly.RData'). "],
["why-not-just-use-standard-methods.html", "Why not just use standard methods? Heteroscedasticity, non-normality etc.", " Why not just use standard methods? The standard linear model is ubiquitous in statistical training and application, and for good reason. It is simple to do and easy to understand. Let’s go ahead and do one to get things started. mod = lm(y ~ x1 + x2, data=dat) summary(mod) term estimate std.error statistic p.value (Intercept) 7.27 0.33 21.83 0 x1 6.36 0.44 14.62 0 x2 -5.12 0.45 -11.31 0 r.squared 0.46 Everything is nice and tidy. We have straightforward information, positive effect of x1, negative for x2, and familiar output. Depending on your context, the R2 may or may not be something exciting. Let’s look at some diagnostics1. Some issues might be present, as we might be getting a little more variance with some, especially higher, fitted values. We’re also a little loose in the tails of the distribution of the residuals. Let’s compare our predictions to the data. With a strong model we might see a cigar shaped cloud converging to a line with slope 1 as the fit gets better. We seem to be having some issues here, as the residual plot noted above. Now let’s go back and visualize the data. The follow plots both predictors against the target variable. Yikes. We certainly have a positive effect for x1, but it looks rather curvy. The other predictor doesn’t appear to have a relationship that could be classified as easily. Heteroscedasticity, non-normality etc. In many cases as above, people have some standby methods for dealing with the problem. For example, they might see the qqplot for the residuals and think some of those cases are ‘outliers’, perhaps even dropping them from analysis. Others might try a transformation of the target variable, for example, in cases of heteroscedasticity (not because of non-normality!) some might take the log. modlog = lm(log(y) ~ x1 + x2, dat) summary(modlog) term estimate std.error statistic p.value (Intercept) 1.83 0.05 33.84 0 x1 0.94 0.07 13.29 0 x2 -0.69 0.07 -9.37 0 r.squared 0.4 Well, our fit in terms of R2 has actually gone down. Let’s check the diagnostics. The transformation may have helped in some ways, but made other things worse. We continue to see some poor fitting cases and now our fit is flattening even more than it was. This is a fairly typical result. Transformations often exacerbate data issues or fail to help. What’s more, some of them lead to more difficult interpretation, or aren’t even applicable (e.g. categorical, ordinal targets). Outliers, if there was actually a standard for deeming something as such, are just indications that your model doesn’t capture the data generating process in some fashion. Cutting data out of the modeling process for that reason hasn’t been acceptable for a long time (if it ever was). Data abounds where a standard linear model performs poorly or doesn’t do a good enough job capturing the nuances of the data. There may be nonlinear relationships as above, dependency in the observations, known non-Gaussian data etc. One should be prepared to use models better suited to the situation, rather than torturing the data to fit a simplified modeling scheme. In case you are wondering, yes, these diagnostic plots are in fact base R graphics, and they still can look good, at least when I’m doing them.↩ "],
["polynomial-regression.html", "Polynomial Regression Using polynomials A more complex relationship", " Polynomial Regression Using polynomials A common application in regression to deal with nonlinear relationships involves polynomial regression. For the predictor in question, \\(x\\), we add terms e.g. quadratic (\\(x^2\\)), cubic (\\(x^3\\)) etc. to get a better fit. Consider the following data situation. Let’s fit a quadratic term and note the result. mod_poly = lm(y ~ poly(x, 2)) The R2 for this is 0.78, that’s great! But look closely and you might notice that we aren’t capturing the tails of this data at all. Here’s what the data and fit would look like if we extend the data based on the underlying true function, and things only get worse. Part of the reason is that, outside of deterministic relationships due known physical or other causes, you are unlikely to discover a quadratic relationship between variables among found data. Even when it appears to fit, without a lot of data it is almost certainly overfit due to this reason. Fitting a polynomial is more akin to enforcing our vision of how the data should be, rather than letting the data speak for itself. Sure, it might be a good approximation some of the time, just as assuming a linear relationship is, but often it’s just wishful thinking. Compare the previous result to the following fit from a generalized additive model. GAMs are susceptible to extrapolation, as is every statistical model ever created. However, the original fit (in red) is much better. Notice how it was better able to follow the straightened out data points at the high end, rather than continuing the bend the quadratic enforced. A more complex relationship Perhaps you would have been satisfied with the initial quadratic fit above or perhaps a cubic fit2. We may come across a situation where the target of interest \\(y\\) is a function of some covariate \\(x\\), whose effect is not straightforward at all. In a standard regression we can generally write the model as follows: \\[y = f(x) + e\\] If we are talking a linear relationship between y and x, f(x) might be a simple sum of the covariates. \\[y = b_0 + b_1*x + e\\] In that case we could do our standard regression model. Now consider the following functional form for x: \\[f(x) = sin(2(4x-2)) + 2e^{-(16^2)(x-.5)^2} + \\epsilon\\] \\[\\epsilon \\sim N(0,.3^2)\\] Let’s generate some data and take a look at it visually. set.seed(123) x = runif(500) mu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2)) y = rnorm(500, mu, .3) d = data.frame(x,y) Polynomial regression is problematic As above, we could try and use polynomial regression here, e.g. fitting a quadratic or cubic function within the standard regression framework. However, this is unrealistic at best and at worst isn’t useful for complex relationships. In the following even with a polynomial of degree 15 the fit is fairly poor in many areas. The same would hold true for other approaches that require the functional form to be specified (e.g. so-called logistic growth curve models). It’s maybe also obvious that a target transformation (e.g. log) isn’t going to help us in this case either. In general, we’ll need tools better suited to more complex relationships, or simply ones that don’t require us to overfit/simplify the relationship we see, or guess about the form randomly until finding a decent fit. The data was actually generated with a cubic polynomial.↩ "],
["building-up-to-gams.html", "Building up to GAMs Piecewise polynomial Introducing GAMs Polynomial spline", " Building up to GAMs Piecewise polynomial So how might we solve the problem? One way would be to divide the data into chunks at various points (knots), and fit a linear regression or polynomial model within that subset of data. While this is notably better fit than e.g., a cubic polynomial, again it is unsatisfactory. The separate fits are unconnected, leading to sometimes notably different predictions for values close together. Being fit to small amounts of data means each fit overfits that area of data, leading to a fairly ‘wiggly’ result most of the time. Introducing GAMs In essence, a GAM is a GLM. What distinguishes it from the ones you know is that, unlike a standard GLM, it is composed of a sum of smooth functions of covariates instead of or in addition to the standard covariates. Consider the standard (g)lm3: \\[y = x + \\epsilon\\] In the above, \\(y\\) is our target, \\(x\\) the predictor variable, and \\(\\epsilon\\) the error. For the GAM, we can specify it as follows: \\[y = f(x) + \\epsilon\\] Now we are dealing with some specific function of \\(x\\). It involves choosing a basis, which in technical terms means choosing a space of functions for which \\(f\\) is some element of it. On the practical side, it is a means to capture nonlinear relationships. As we’ll see later, an example would be choosing a cubic spline for the basis. Choosing a basis also means we’re selecting basis functions, which will actually go into the analysis. We can add more detail as follows: \\[y = f(x) + \\epsilon = \\sum_{j=1}^{d}B_j(x)\\gamma_j + \\epsilon\\] Above, each \\(B_j\\) is a basis function that is the transformed \\(x\\) depending on the type of basis considered, and the \\(\\gamma\\) are the corresponding regression coefficients. This might sound complicated, until you realise you’ve done this before. Let’s go back to the quadratic polynomial, which uses the polynomial basis. \\[f(x) = \\gamma_0 + \\gamma_1\\cdot x \\ldots +\\gamma_d\\cdot x^d\\] In that case \\(d=2\\) and we have our standard regression with a quadratic term, but in fact, we can use this approach to produce the bases for any polynomial. As far as mechanics go, these basis functions become extra columns in the data, just like your \\(x^2\\) etc. from the polynomial approach, and then you just run a GLM! However, the additional point is that it uses penalized estimation, something that is quite common in some modeling contexts, and not common enough in applied research. Again consider a standard GLM that we usually estimate with maximum likelihood, \\(l(\\beta)\\), where \\(\\beta\\) are the associated regression coefficients. Conceptually we can write the penalized likelihood as follows: \\[l_p(\\beta)= l(\\beta) - \\color{darkred}{\\mathcal{penalty}}\\] If you prefer least squares as the loss function, we can put it as: \\[\\mathcal{Loss} = \\sum (y-X\\beta)^2 + \\color{darkred}{\\mathcal{penalty}}\\] The penalty regards the complexity of the model, and specifically the size of the coefficients for the smooth terms. The practical side is that it will help to keep us from overfitting the data, where our smooth function might get too wiggly4. For a little more detail visit the technical section. In summary, you’re doing a GLM, but a slightly modified one. You could have always been using penalized GLM (e.g. lasso or ridge regression), and you’d have slightly better predictive capability if you had. We can use different loss functions, different penalties etc. but the concepts are the main thing to note here. Polynomial spline Let’s get things started by demonstrating the results from a GAM that uses a polynomial spline for the basis. The brave may refer to the technical details section. Conceptually it is useful to continue to think of the piece-wise approach we talked about before, however, we’ll end up with a smoother and connected result when all is said and done. This is much better. We now have a connected result that isn’t overly wiggly, and more importantly, actually fits the data quite well. I am leaving out subscripts where I don’t think it helps, and as these are conceptual depictions, they usually don’t.↩ Wiggly is a highly technical term I will use throughout the presentation.↩ "],
["practical-gam.html", "Practical GAM Getting started Fitting the model", " Practical GAM One of the more powerful modeling packages in R comes with its installation5. The mixed gam computational vehicle, or mgcv package, is an extremely flexible modeling tool, and one we’ll use for our foray into generalized additive models. If you’re familiar with standard regression models, the syntax is hardly different, but adds a great many possibilities to your modeling efforts. Getting started So GAMs can be seen as a special type of GLM, that extends the model to incorporate nonlinear and other relationships, but while keeping things within a familiar framework. Behind the scenes, extra columns are added to our model matrix to do this, but their associated coefficients are penalized to help avoid overfitting. As we will see, the main syntactical difference between using gam and using glm is that you’ll use the s function to incorporate smooth terms, or classes that include basis functions as we saw before, along with a penalty. Beyond what we did previously, mgcv also incorporates a penalized regression approach. For those familiar with lasso, ridge or other penalized approaches you’ll find the same idea here. For those new to this, it’s important to add the concept and technique to your statistical modeling toolbox. Let’s look at some more interesting data. For the following example we’ll use the iris data set. Just kidding! We’ll look at some data regarding the Internet Movie Database (IMDB) from the ggplot2movies package. If you want to look at the original, you’ll need to install the package, and the data object within is called movies. For for information, type ?movies after loading the package. However, our example will concern yearly averages based on that data set. The primary data set is movies_yearly, with a film budget variable that has been adjusted to 2016 dollars. In addition, I’ve grouped them by whether they are an action movie or not in a separate data set movies_yearly_action. And finally, the data has been filtered to contain only those years where there a decent number of movies to go into the calculations (10 or more for yearly, 5 or more for yearly_action), and feature films (length in minutes &gt; 60). library(ggplot2movies) load(file=&#39;data/movies_yearly.RData&#39;) Let’s examine the the time trend for ratings and budget. Size of the points represents the number of movies that year. For ratings there seems to be a definite curvilinear trend, where initially ratings for movies started high and decreased, but were on the upswing at later years. Adjusted for inflation, movie budgets don’t increase linearly over time as one might expect, and in fact were decreasing towards the end. Note that we only have adjusted budget data for 1948 on. Fitting the model Let’s fit a GAM to the yearly trend for ratings. I will also put in budget (in millions), movie length (in hours), and number of votes (in thousands) as standard effects. We’ll use a cubic regression spline for our basis, bs='cr', but aside from specifying the smooth term, the syntax is the same as one would use for the glm function. library(mgcv) gam_model = gam(rating ~ s(year, bs=&#39;cr&#39;) + budget_2016 + votes + length, data=movies_yearly) summary(gam_model) Family: gaussian Link function: identity Formula: rating ~ s(year, bs = &quot;cr&quot;) + budget_2016 + votes + length Parametric coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.413e+00 9.674e-01 4.562 3.65e-05 *** budget_2016 5.762e-05 3.613e-03 0.016 0.987 votes 3.921e-02 1.210e-01 0.324 0.747 length 8.192e-01 6.071e-01 1.349 0.184 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Approximate significance of smooth terms: edf Ref.df F p-value s(year) 7.064 8.063 28.01 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R-sq.(adj) = 0.879 Deviance explained = 90.1% GCV = 0.011817 Scale est. = 0.0095631 n = 58 The first part of our output contains standard linear regression results. It is important to note that there is nothing going on here that you haven’t seen in any other GLM. For example, in this case there is no relationship between budget and rating, whereas adding an hour to the film might bump up the rating around near a point6. Parametric coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.413e+00 9.674e-01 4.562 3.65e-05 *** budget_2016 5.762e-05 3.613e-03 0.016 0.987 votes 3.921e-02 1.210e-01 0.324 0.747 length 8.192e-01 6.071e-01 1.349 0.184 The next part contains our information regarding the smooth terms, of which there is only one. Approximate significance of smooth terms: edf Ref.df F p-value s(year) 7.064 8.063 28.01 &lt;2e-16 *** While this suggests a statistically significant effect, it’s important to note there is not a precisely defined p-value in this setting- it clearly states ‘approximate significance’. For some more details see the technical section or my document, but for now, we’ll start with the effective degrees of freedom, or edf. In typical OLS regression the model degrees of freedom is equivalent to the number of predictors/terms in the model. This is not so straightforward with a GAM due to the smoothing process and the penalized regression estimation procedure. In this example there are actually 9 terms associated with this smooth, but they are each penalized to some extent and thus the effective degrees of freedom does not equal 9. For hypothesis testing an alternate edf is actually used, which is the other one provided there in the summary result (Ref.df). The thing to note is that the edf would equal 1 if the model penalized the smooth term to a simple linear relationship7, and so the effective degrees of freedom falls somewhere between 1 and k-1, where k is chosen based on the basis. You can think of it as akin to the number of knots. The default here was 10. Basically if it’s 1, just leave it as a linear component, or if it’s close to k, maybe bump up the default to allow for more wiggliness. Visualizing the effects While the statistical result is helpful to some extent, the best way to interpret smooth terms is visually. The ability to do so is easy, you can just use the plot method on the model object. However, the default plot may be difficult to grasp at first. It is a component plot, which plots the original variable against the linear combination of its basis functions, i.e. the sum of each basis function multiplied by its respective coefficient. For example, if we were using a quadratic polynomial, it would be the plot of \\(x\\) against \\(b_1\\cdot x + b_2\\cdot x^2\\). For GAMs, \\(y\\) is also centered, and what you end up with is something like the following. plot(gam_model) So this tells us the contribution of the year to the model fit8. For an alternative, consider the visreg package, which focuses on standard predicted values holding other variables constant, and it just looks slightly nicer by default while allowing for more control. library(visreg) visreg(gam_model, xvar=&#39;year&#39;, partial=F) You of course can look at predictions at any relevant values for the covariates, as you can with any other model. Consider the following effect of time while keeping budget and length constant at their means, and votes at 10009. movies_yearly %&gt;% na.omit() %&gt;% transmute(budget_2016 = mean(budget_2016), length = mean(length), votes = 1, year=year) %&gt;% add_predictions(gam_model, var=&#39;Predicted_Rating&#39;) %&gt;% plot_ly() %&gt;% add_lines(~year, ~Predicted_Rating) In the end, controlling for budget, length, and popularity, movie quality decreased notably until the 70s, and remained at that level until the late 90s, when quality increased even more dramatically. For reasons I’ve not understood, other packages still depend on or extend the gam package, which doesn’t possess anywhere near the functionality (though the authors literally wrote the book on GAM), or even the splines package, which comes with base R but has even less. If you’re going to use additive models, I suggest starting with mgcv until you know it cannot do what you want, then consider the extensions to it that are out there.↩ Presumably we have to justify the time spent watching longer movies. Note also I personally consider the length effect significant, despite the p-value.↩ You can actually penalize the covariate right out of the model if desired, i.e. edf=0.↩ This is the same as what the base R function termplot would provide for a standard (g)lm. You can request the plots be applied to the non-smooth terms in the model as well. And the residuals function has an argument, type='partial' that can be applied to produce the partial residuals, which are the residuals + term. See ?termplot for more info.↩ If we had put votes at its mean, you would basically duplicate the previous plot.↩ "],
["issues.html", "Issues", " Issues There are some complexities associated with GAMs that will take some getting used to. Choice of distribution: With mgcv you have more options than the standard GLM, so put some thought into what you think might best represent the data generating process. Choice of smooth: Some might be particularly appropriate to a given situation (e.g. spatial). In many situations you’ll have a choice. They should not lead to fundamentally different conclusions though. Choosing whether to keep an effect linear: Sometimes it may look like the effect isn’t nonlinear or much more more than linear, and a choice will have to be made whether to go the simpler route. If k=1, this is straightforward. If not exactly, you will have to make a choice. Diagnostics: There are some additional diagnostics to consider, such as the k parameter. Concurvity, which is collinearity for the GAM setting, also comes into play. Model comparison: In general one can use AIC as with standard GLMs, and in addition one has the GCV statistic. However, there are special considerations for an ANOVA style approach, so see ?anova.gam for details. Fuzzy p-values: For folks that are used to complex models, e.g. penalized regression, mixed models, machine learning techniques, this is not new. For those new to such models, you must unlearn what you have learned. Bayesian approaches: The brms package can take your GAM to the Bayesian setting. Just like with any new statistical modeling technique, diving in with something you’re already familiar with is a good way to start getting comfortable. One way to do so would be to revisit an analysis you’ve done in the past and re-analyze it with a GAM. And don’t forget, you also have an entire text devoted to the package for as much detail as you want, and the help files are quite detailed as well. "],
["extensions.html", "Extensions Extensions Connections &amp; Generalizations", " Extensions Extensions More bases There are many options available in mgcv beyond cubic splines, including thin plate, duchon, p-splines and more, as well as ones for cyclical effects, geographical, and beyond. See ?smooth.terms for more information. Interactions We can implement interactions of multiple smooth terms. The following shows a model in which we could combine the effects of year and budget via the te function, along with the corresponding predictive plots. Note that the 3d is interactive, so twist it however you like to see what’s going on. gam_model_2d = gam(rating ~ te(year, budget_2016), data=movies_yearly) While we see the general year effect where the earlier and more recent movies are associated with higher ratings, it would appear that for more recent times, lower ratings are associated with bigger budget movies. There is also a way to do this interaction in a manner similar to ANOVA decomposition with both main effects and interaction. See the ti function. Grouped Effects We can apply a smooth for a continuous covariate across levels of some grouping factor. In the following, we’ll let year have a different effect for Action movies vs. other movies. gam_model_by = gam(rating ~ s(year, by=Action) + Action + budget_2016 + votes + length, data=movies_yearly_action) In this case Action movies are generally rated worse, and suffered a second and sharper dip starting in the 80s. Also, their recent rise is not quite as sharp. Another alternative specifies a specific type of smooth that would be similar to random slopes in a mixed model. We now turn to the mixed model approach with GAMs. Mixed Models Consider the matrix \\(Z\\) that includes the basis functions, with associated coefficients \\(\\gamma\\), we could then write our model in matrix form as follows: \\[ y = X\\beta + Z\\gamma + \\epsilon\\] If you’ve spent much time with mixed models, that probably looks familiar to you. This penalized regression approach not only looks like the mixed model, we can exploit this to produce the same results. The mgcv package has many tools for mixed models. For this example I will use the sleep study data that comes with the lme4 package. It has reaction times for some tests where 18 subjects were restricted to 3 hours of sleep each night for 10 days. We fit a model with (uncorrelated) random intercepts and random slopes for Days. library(lme4) mod_lme4 = lmer(Reaction ~ Days + (1|Subject) + (0 + Days|Subject), sleepstudy) mod_gamm = gam(Reaction ~ Days + s(Subject, bs=&#39;re&#39;) + s(Days, Subject, bs=&#39;re&#39;), data=sleepstudy, method=&#39;REML&#39;) # summary(mod_lme4) # summary(mod_gamm) VarCorr(mod_lme4) Groups Name Std.Dev. Subject (Intercept) 25.0513 Subject.1 Days 5.9882 Residual 25.5653 gam.vcomp(mod_gamm) Standard deviations and 0.95 confidence intervals: std.dev lower upper s(Subject) 25.051370 16.085365 39.015038 s(Days,Subject) 5.988153 4.025248 8.908263 scale 25.565254 22.791745 28.676269 Rank: 3/3 The results are essentially identical. The mgcv package can also use lme4 and nlme under the hood via other functions (gamm4 package and gamm function respectively). Extending GAMs into the mixed model world opens the door to some very powerful modeling possibilities. Spatial models Consider a data set with latitude and longitude coordinates among other covariates used to model some target variable. A spatial regression analysis might incorporate an approach to incorporate spatial covariance among the observation points (e.g. kriging). Such an approach is a special case of Gaussian process. Know what else happens to be a special case of a Gaussian process? That’s right, a GAM! As such we can add spatial models to the sorts of models covered by GAMs too. The following gives a sense of the syntax. The mgcv package even has a 'gp' smooth, because why wouldn’t it? gam(y ~ s(lat, lon, bs=&#39;gp&#39;)) # Matern What about the discrete case, where the spatial random effect is based on geographical regions? This involves a penalty that is based on the adjacency matrix of the regions, where if there are \\(g\\) regions the adjacency matrix is a \\(g \\times g\\) indicator matrix where 1 indicates region i is connected to region j, and 0 otherwise. In addition an approach similar to that for a random effect is used to incorporate observations belonging to specific regions. You’ll be shocked to know that the mgcv package has a smooth option for that, bs='mrf', where mrf stands for Markov random field, which is an undirected graph. Plotting the smooth term is akin to displaying the spatial random effect. The following shows example code. See the help for mrf for more detail. gam(crime ~ s(region, bs = &quot;mrf&quot;, xt = polygonListObject), data = df, method = &quot;REML&quot;) My God! It’s full of STARs! The incorporation of additive, mixed, and spatial regression models gets us to STARs or, structured additive regression models. This is an extremely general and powerful modeling tool that can take you very, very far10. Connections &amp; Generalizations Adaptive Basis Function Models GAMs can be seen as belonging to a family of what are called adaptive basis function models. These include random forests, neural nets, and boosting approaches. GAMs might be seen as a midpoint lying between highly interpretable standard linear models and the more black box methods just mentioned. Beyond GLM family distributions The mgcv package provides a plethora of distributions to use in modeling that might useful including: tweedie negative binomial ordered categorical beta t zero-inflated poisson (ZIP) cox proportional hazards gaulss Gaussian model for both mean (location) and standard deviation (scale) gevlss same for generalized extreme value ziplss for a hurdle approach to zero-inflation multivariate normal multinomial logistic Extending Random Effects In addition to distribution extensions that could also be incorporated with random effects, providing modeling capabilities not generally offered in mixed model packages, mgcv also allows for correlated residuals structure as with nlme, but for the generalized case. This is especially of interest in the longitudinal setting, where one expects residuals for observations closer in time to be more correlated. In addition, the distributions just noted can also be used for in the random effects setting, e.g. a mixed model with beta disrtibuted response. Gaussian Processes Where the Gaussian distribution is over vectors and defined by a mean vector and covariance matrix, a Gaussian Process is a distribution over functions. A function \\(f\\) is distributed as a Gaussian Process defined by a mean function \\(m\\) and covariance function \\(k\\). They have a close tie to RKHS methods, and generalize commonly used models in spatial modeling (kriging). In addition, a GP can be interpreted as a standard multilayer perceptron neural net with a single hidden layer consisting of an infinite number of nodes. \\[f\\sim \\mathcal{GP}(m,k)\\] It turns out that GAMs with a tensor product or cubic spline smooth are maximum a posteriori (MAP) estimates of Gaussian processes with specific covariance functions and a zero mean function. In that sense one might segue nicely to Gaussian processes if familiar with additive models. The mgcv package, … wait for it…, also allows one to use a spline form of Gaussian process bs='gp'. In case you aren’t familiar with the phrase, it comes from the book 2001: A Space Odyssey: see link. The film adaptation is considered one of the greatest films of all time, because it is.↩ "],
["summary.html", "Summary", " Summary The purpose of this workshop was to provide awareness of a powerful modeling technique, that is as easy to implement as a standard GLM. The additional complexity will take some getting used to just like any new tool would, but the payoff is worth it in my opinion. Hope you have some fun adding some wiggle to your model! "],
["exercises.html", "Exercises Exercise 1 Exercise 2 Exercise 3 Exercise 4", " Exercises Exercise 1 Let’s start off with a single covariate. We’ll use the motorcycle data set, which is a data frame giving a series of measurements of head acceleration in a simulated motorcycle accident, used to test crash helmets. times: in milliseconds after impact accel: acceleration in g data(&#39;mcycle&#39;, package=&#39;MASS&#39;) Run a gam predicting head acceleration by time with time as a smooth term. You can plot the result with visreg as follows. If you haven’t installed it, you’ll need to do so. Otherwise just use the plot function. library(visreg) visreg(my_gam_model, partial=F) Exercise 2 Next we’ll use a the yearly movie data consisting of the following variables. year: Year of release. budget_2016: Average budget in millions 2016 US dollars length: Average length in minutes. rating: Average IMDB user rating. votes: Average number of IMDB users who rated movies that year, in thousands. N: The number of movies that went into the averaging. 2a Run an lm/glm with budget_2016 as the target and your choice of covariates. Here is some starter code. load(&#39;data/movies_yearly.RData&#39;) mod_glm = glm(budget_2016 ~ x + z, data=movies_yearly) 2b Run an gam with the same model but no smooth terms (i.e. just change the function to gam) 2c Now run a gam with any smooth terms you care to. What is your interpretation of the results? Exercise 3 Use the movies_yearly_action data. This data is the same as the yearly data, but broken out into whether the movie was categorized as being and Action flick (possibly in combination of another genre) or Other. Run a GAM where the outcome is budget in millions of 2016 dollars. # If you haven&#39;t already, load the data as in exercise 2 gam_model_byAction = gam(budget_2016 ~ s(year, by=Action) + Action + votes + length, Plot the result with visreg as follows. visreg::visreg(gam_model_byAction, xvar=&#39;year&#39;, by=&#39;Action&#39;, overlay=T, partial=F) What trends do you see? Is the yearly trend the same for action movies vs other movies? Do you think both trends should be allowed to ‘wiggle’? Exercise 4 Keep it local with the following Great Lakes data set11. The following model uses a Gaussian process smooth for year and a p-spline for the monthly effect, both by the corresponding lake. Try increasing k for the year effect, and with bs='cp' (a cyclic p-spline) for the month effect. Do you think using the cyclic option, where the endpoints are equal, was viable approach? load(&#39;data/greatLakes.RData&#39;) gam_monthly_cycle = gam(meters*10 ~ lake + s(year, by=lake, bs=&#39;gp&#39;, k=10) + s(month_num, bs=&#39;ps&#39;, by=lake), data=lakes) # summary(gam_monthly_cycle) monthpd = visreg(gam_monthly_cycle, xvar=&#39;month_num&#39;, by=&#39;lake&#39;, overlay=T, plot=F)$fit yearpd = visreg(gam_monthly_cycle, xvar=&#39;year&#39;, by=&#39;lake&#39;, overlay=T, plot=F)$fit monthpd %&gt;% ggplot() + geom_line(aes(x=month_num, y=visregFit)) + scale_x_continuous(breaks=1:12, labels=month.abb) + facet_wrap(~lake, scales=&#39;free_y&#39;) + theme(axis.text.x = element_text(angle=-45, hjust = 0)) yearpd %&gt;% ggplot() + geom_line(aes(x=year, y=visregFit)) + facet_wrap(~lake, scales=&#39;free_y&#39;) Lake data from http://www.lre.usace.army.mil/Missions/Great-Lakes-Information/Great-Lakes-Water-Levels/Historical-Data/. Note that in the exercise we model the water level in decimeters.↩ "],
["technical-details.html", "Technical details GAM Introduction A detailed example The number of knots and where to put them. Interpreting output for smooth terms", " Technical details Workshops see all types of folks at different levels. This is for some of the more technically inclined, though you’ll find more context and detail in my doc and Wood (2006), from which a good chunk of this section is taken more or less directly from. GAM Introduction A GAM is a GLM whose linear predictor includes a sum of smooth functions of covariates. With link function \\(g(.)\\), model matrix \\(X\\) of \\(n\\) rows and \\(p\\) covariates (plus a column for the intercept), a vector of \\(p\\) coefficients \\(\\beta\\), and observations \\(i\\), we can write a GLM as follows: \\[\\mathrm{GLM:}\\quad g(\\mu) = X\\beta\\] For the GAM, it could look something like: \\[\\mathrm{GAM:}\\quad g(\\mu) = X\\beta + f(x_1) + f(x_2) + f(x_3, x_4) + \\ldots\\] or as we did when discussing mixed models: \\[\\mathrm{GAM:}\\quad g(\\mu) = X\\beta + Z\\gamma\\] Where \\(Z\\) represents the basis functions of some subset of \\(X\\) covariates. Thus we can have any number of smooth terms, possibly of different bases, and even combinations of them. Finally we can depict structured additive regression, or STAR, model as a GAM + random effects (\\(\\Upsilon\\)) + spatial effects (\\(\\Xi\\)) + other fun stuff. \\[\\mathrm{STAR:}\\quad g(\\mu) = X\\beta + Z\\gamma + \\Upsilon\\varpi + \\Xi\\varrho + \\ldots \\mathbf{?}\\vartheta \\] Penalized regression Consider a standard GLM that we usually estimate with maximum likelihood, \\(l(\\beta)\\), where \\(\\beta\\) are the associated regression coefficients. We can write the penalized likelihood as follows: \\[l_p(\\beta)= l(\\beta) - \\color{darkred}{\\lambda B&#39;SB}\\] Where \\(S\\) is a penalty matrix of known coefficients12. If you prefer least squares as the loss function, we can put it as: \\[\\mathcal{Loss} = \\sum (y-X\\beta)^2 + \\color{darkred}{\\lambda B&#39;SB}\\] So if we’re maximizing the likelihood, the penalty will make it lower than it otherwise would have been, and vice versa in terms of a loss function. What it means for a GAM is that we’ll add a penalty for the coefficients associated with the basis functions. The practical side is that it will help to keep us from overfitting the data, where our smooth function might get too wiggly. As \\(\\lambda \\rightarrow \\infty\\), the result is a linear fit because any wiggliness will add too much to the loss function. As \\(\\lambda \\rightarrow 0\\), we have the opposite effect, where any wiggliness is incorporated into the model. In mgcv, by default the estimated parameters are chosen via a generalized cross validation, or GCV, approach, and that statistic is reported in the summary. It modifies the loss function depicted above to approximate leave-one-out cross-validation selection. A detailed example The following will demonstrate a polynomial spline by hand. The visual display is based on a depiction in Fahrmeier et al. (2013). The approach is defined as follows, with \\(\\kappa\\) knots on the interval \\([a,b]\\) as \\(a=\\kappa_1 &lt; ... &lt; \\kappa_m =b\\): \\[y_i = \\gamma_1 + \\gamma_2X_i + ... + \\gamma_{l+1}(X_i)_+^l + \\gamma_{l+2}(X_i - \\kappa_2)_+^l ... + \\gamma_{l+m-1}(X_i - \\kappa_{m-1})_+^l + e_i\\] \\[(X_i - \\kappa_j)^l_+= \\begin{cases} (X_i-\\kappa_j)^l &amp; X_i \\geq k_j \\\\ 0 &amp; \\textrm{otherwise} \\end{cases}\\] It might look complicated, but note that there is nothing particularly special about the model itself. It is just a standard linear regression model when everything is said and done. More generally and cleanly: \\[y = f(x) + e = \\sum_{j=1}^{d}B_j(x)\\gamma_j + e\\] Above, each \\(B_j\\) is a basis function that is the transformed \\(x\\) depending on the type of basis considered, and the \\(\\gamma\\) are the corresponding regression coefficients. It looks complicated, but you’ve done this before. Let’s go back to the quadratic polynomial, there the basis function includes an intercept, x and x2. \\[f(x) = \\gamma_0 + \\gamma_1x + \\gamma_1x^l\\] In that case \\(l=2\\) and we have our standard polynomial regression. In fact, we can use this approach to produce the bases for any polynomial. The basic idea is shown more explicitly in the following: \\[B_1(x)=1, B_2(x)=x, ..., B_{l+1}(x)=x^l, B_{l+2}(x)=(x-\\kappa_2)_+^l...B_{d}(x)=(x-\\kappa_{m-1})_+^l\\] But before getting too far let’s see it in action. Here our polynomial spline will be done with degree \\(l\\) equal to 1, which means that we are just fitting a linear regression between knots. knots = seq(0, 1, by=.1) knots = knots[-length(knots)] # don&#39;t need the last value l = 1 bs = sapply(1:length(knots), function(k) ifelse(x &gt;= knots[k], (x-knots[k])^l, 0)) head(bs) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 0.2875775 0.1875775 0.08757752 0.0000000 0.000000000 0.0000000 0.0000000 0.00000000 0.0000000 0.00000000 [2,] 0.7883051 0.6883051 0.58830514 0.4883051 0.388305135 0.2883051 0.1883051 0.08830514 0.0000000 0.00000000 [3,] 0.4089769 0.3089769 0.20897692 0.1089769 0.008976922 0.0000000 0.0000000 0.00000000 0.0000000 0.00000000 [4,] 0.8830174 0.7830174 0.68301740 0.5830174 0.483017404 0.3830174 0.2830174 0.18301740 0.0830174 0.00000000 [5,] 0.9404673 0.8404673 0.74046728 0.6404673 0.540467284 0.4404673 0.3404673 0.24046728 0.1404673 0.04046728 [6,] 0.0455565 0.0000000 0.00000000 0.0000000 0.000000000 0.0000000 0.0000000 0.00000000 0.0000000 0.00000000 If we plot this against our target variable \\(y\\), it doesn’t look like much. If we multiply each basis by it’s corresponding regression coefficient we can start to interpret the result. bscoefs = coef(lmMod) bsScaled = sweep(bs, 2, bscoefs,`*`) colnames(bsScaled) = c(&#39;int&#39;, paste0(&#39;X&#39;, 1:10)) round(bscoefs, 3) int X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 0.783 -6.637 -1.388 3.839 7.366 25.769 -41.462 15.167 -7.144 -1.738 -3.394 In the plot above, the initial dot represents the global constant (\\(\\gamma_1\\), i.e. our intercept). We have a decreasing function starting from that point onward (line). Between .1 and .2 (line), the coefficient is negative again, furthering the already decreasing slope (i.e. steeper downward). The fourth coefficient is positive, which means that between .2 and .3 our decreasing trend is lessening (line). Thus our coefficients \\(j\\) tell us the change in slope for the data from the previous section of data defined by the knots. The length’s of the lines reflect the size of the coefficient, i.e. how dramatic the change is. If this gets you to thinking about interactions in more common model settings, you’re on the right track (e.g. adding a quadratic term is just letting x have an interaction with itself; same thing is going on here). Finally if we plot the sum of the basis functions, which is the same as taking our fitted values from a regression on the basis expansion of X, we get the following fit to the data. And we can see the trend our previous plot suggested. One of the most common approaches uses a cubic spline fit. So we’ll change our polynomial degree from 1 to 3. Now we’re getting somewhere. Let’s compare it to the gam function from the mgcv package. We won’t usually specify the knots directly, and even as we have set things up similar to the mgcv approach, the gam function is still doing some things our by-hand approach is not (penalized regression). We still get pretty close agreement however. We can see that we’re on the right track by using the constructor function within mgcv and a custom function for truncated power series like what we’re using above. See the example in the help file for smooth.construct for the underlying truncated power series function. I only show a few of the columns, but our by-hand construction and that used by gam are identical. xs = scale(x, scale=F) bs = sapply(1:length(knots), function(k) ifelse(x &gt;= knots[k], (x-knots[k])^l, 0)) sm = smoothCon(s(x, bs=&#39;tr&#39;, k=14), data=d, knots=list(x=knots))[[1]] head(sm$X[,1:6]) [,1] [,2] [,3] [,4] [,5] [,6] [1,] 1 -0.20770617 0.043141852 -0.0089608288 2.378290e-02 0.006599976 [2,] 1 0.29302145 0.085861568 0.0251592810 4.898725e-01 0.326094166 [3,] 1 -0.08630677 0.007448858 -0.0006428868 6.840635e-02 0.029497019 [4,] 1 0.38773372 0.150337434 0.0582908920 6.885061e-01 0.480080698 [5,] 1 0.44518360 0.198188434 0.0882302397 8.318233e-01 0.593693698 [6,] 1 -0.44972719 0.202254545 -0.0909593678 9.454771e-05 0.000000000 modelMatrix = cbind(1, xs, xs^2, xs^3, bs) all.equal(sm$X, modelMatrix) [1] TRUE Preview of other bases As an example of other types of smooth terms we might use, here are the basis functions for b-splines. They work notably differently, e.g. over intervals of \\(l+2\\) knots. The number of knots and where to put them. A natural question may arise as to how many knots to use. More knots mean more ‘wiggliness’, as demonstrated here. Note these are number of knots, not powers in a polynomial regression as shown in the main section of this document. However, as we’ll come to learn, we don’t really have to worry about this except in the conceptual sense, i.e. being able to control the wiggliness. The odds of you knowing beforehand the number of knots and where to put them is somewhere between slim and none, so this is a good thing. Interpreting output for smooth terms Effective degrees of freedom In interpreting the output from mgcv, we’ll start with the effective degrees of freedom, or edf. In typical OLS regression the model degrees of freedom is equivalent to the number of predictors/terms in the model. This is not so straightforward with a GAM due to the smoothing process and the penalized regression estimation procedure. In this example there are actually 9 terms associated with this smooth, but their corresponding parameters are each penalized to some extent and thus the effective degrees of freedom does not equal 9. For hypothesis testing an alternate edf is actually used, which is the other one provided there in the summary result (Ref.df). For more on this see my document, ?summary.gam and ?anova.gam. At this point you might be thinking these p-values are a bit fuzzy, and you’d be right. As is the case with mixed models, machine learning approaches, etc., p-values are not straightforward. The gist is, they aren’t to be used for harsh cutoffs, say, at an arbitrary .05 level, but then standard p-values shouldn’t be used that way either. If they are pretty low you can feel comfortable claiming statistical significance, but if you want a tight p-value you’ll need to go back to using a GLM. The edf would equal 1 if the model penalized the smooth term to a simple linear relationship13, and so the effective degrees of freedom falls somewhere between 1 and k-1 (or k), where k is chosen based on the basis. You can think of it as akin to the number of knots. There is functionality to choose the k value, but note the following from Wood in the help file for ?choose.k: So, exact choice of k is not generally critical: it should be chosen to be large enough that you are reasonably sure of having enough degrees of freedom to represent the underlying ‘truth’ reasonably well, but small enough to maintain reasonable computational efficiency. Clearly ‘large’ and ‘small’ are dependent on the particular problem being addressed. And the following: One scenario that can cause confusion is this: a model is fitted with k=10 for a smooth term, and the EDF for the term is estimated as 7.6, some way below the maximum of 9. The model is then refitted with k=20 and the EDF increases to 8.7 - what is happening - how come the EDF was not 8.7 the first time around? The explanation is that the function space with k=20 contains a larger subspace of functions with EDF 8.7 than did the function space with k=10: one of the functions in this larger subspace fits the data a little better than did any function in the smaller subspace. These subtleties seldom have much impact on the statistical conclusions to be drawn from a model fit, however. If you want a more statistically oriented approach, see ?gam.check. Deviance explained R-sq.(adj) = 0.51 Deviance explained = 51.8% GCV = 0.44245 Scale est. = 0.43351 n = 265 For the standard Gaussian setting, we can use our R2. Also provided is ‘deviance explained’, which in this setting is identical to the unadjusted R2, but for non-Gaussian families would be preferred. As noted above, the GCV, or generalized cross validation score, can be taken as an estimate of the mean square prediction error based on a leave-one-out cross validation estimation process. Steps can be taken to choose model parameters specifically based on this (it’s actually the default, as opposed to, e.g. maximum likelihood). Visual depiction The following reproduces the plots produced by mgcv and visreg. I’ll even use base R plotting to ‘keep it real’. First we’ll start with the basic GAM plot. First we get the term for year, i.e. the linear combination of the basis functions for year. The mgcv package provides this for you via the predict function with type='terms'. For non-smooth components this is just the original covariate times its corresponding coefficient. Next we need the partial residuals, which are just the basic residuals plus the term of interest added. With the original predictor variable, we’re ready to proceed. year_term = predict(gam_model, type=&#39;terms&#39;)[,&#39;s(year)&#39;] res_partial = residuals(gam_model) + year_term year = 1948:2005 par(mfrow=c(1,2)) plot(year, res_partial, ylim=c(-.8,.8), col=&#39;black&#39;, ylab=&#39;s(year, 7.06)&#39;, pch=19, main=&#39;ours&#39;) lines(year, year_term) plot(gam_model, se=F, residuals=T, pch=19, rug=F, ylim=c(-.8,.8), main=&#39;mgcv&#39;) Now for visreg. It uses the underlying predict function also, but gets a standard prediction while holding the other variables at key values (which you can manipulate). By default these key values are at the median for numeric variables, and most common category for categorical variables. pred_data = movies_yearly %&gt;% select(budget_2016, votes, length) %&gt;% na.omit() %&gt;% summarise_all(median) %&gt;% data.frame(year=1948:2005) preds = predict(gam_model, newdata=pred_data) res_partial = gam_model$residuals + preds par(mfrow=c(1,2)) plot(year, preds, col=&#39;dodgerblue&#39;, ylab=&#39;f(year)&#39;, lwd=3, ylim=c(5,7), type=&#39;l&#39;, main=&#39;ours&#39;) points(year, res_partial, col=&#39;gray50&#39;, pch=19) visreg(gam_model, xvar=&#39;year&#39;, points.par=list(cex=1), band=F, ylim=c(5,7), main=&#39;visreg&#39;) For example, it would be an identity matrix if we were dealing with random effects, or a neighborhood matrix when dealing with a spatial context.↩ You can actually penalize the covariate right out of the model if desired, i.e. edf=0.↩ "],
["references.html", "References", " References My doc on GAMs. Wood, S. N. 2006. Generalized Additive Models: An Introduction with R. Fahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian Marx. 2013. Regression: Models, Methods and Applications. (electronic access through UM library) "]
]
