---
title: "Tidy Speed"
description: |
  Comparison of new and updated tools
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: <i class="fas fa-percent fa-5x" style = 'color:#ff5500'></i>
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: ../../styles.css
draft: true
tags: [tidyverse, dplyr, data.table, data wrangling]
categories:
  - data wrangling
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE, 
  comment = NA,
  R.options = list(width = 120),
  cache.rebuild = FALSE,
  cache = TRUE, 
  fig.align='center', 
  dev = 'svg', 
  dev.args=list(bg = 'transparent')
)

library(tidyverse)
library(tidyext)
library(broom)
library(kableExtra)
library(visibly)

kable_df <- function(..., digits = 3) {
  kableExtra::kable(..., digits = digits) %>%
    kableExtra::kable_styling(full_width = F)
}
```


## Introduction

Here I take a look at some timings for data processing tasks.  My reason for doing so is that dtplyr has recently arisen from the dead, and tidyfast has come on the scene, so I wanted a quick reference for myself and others to see how things stack up against data.table.

So we have the following:

Base R:  Just kidding. If you're using base R approaches you will always be slower.  Functions like aggregate, tapply and similar could be used in these demos, but I leave that as an exercise to the reader.  I've done them, and it isn't pretty.
dplyr: Standard data wrangling workhorse package
tidyr: has some specific functionality not included in dplyr
data.table: another commonly used data processing package that purports to be faster and more memory efficient (usually but not always)
tidyfast: can only do a few things, but does them quickly.

## Standard grouped operation

The following demonstrates some timings from [here](http://stackoverflow.com/questions/3505701/r-grouping-functions-sapply-vs-lapply-vs-apply-vs-tapply-vs-by-vs-aggrega/34167477#34167477).  I reproduced it on my own machine based on 50 million observations. The grouped operations that are applied are just a sum and length on a vector. As this takes several seconds to do even once, I only do it one time.

```{r dttimings_big, eval=FALSE, echo=TRUE}
library(dplyr)
library(dtplyr)
library(tidyfast)
library(data.table)


set.seed(123)
n = 5e7
k = 5e5
x = runif(n)
grp = sample(k, n, TRUE)

timing_group_by_big = list()


# dplyr
timing_group_by_big[["dplyr"]] = system.time({
    df = tibble(x, grp)
    r.dplyr = summarise(group_by(df, grp), sum(x), n())
})

# dtplyr
timing_group_by_big[["dtplyr"]] = system.time({
    df = lazy_dt(tibble(x, grp))
    r.dtplyr = df %>% group_by(grp) %>% summarise(sum(x), n()) %>% collect()
})

# tidyfast
timing_group_by_big[["tidyfast"]] = system.time({
    dt = setnames(setDT(list(x, grp)), c("x","grp"))
    r.tidyfast = dt_count(dt, grp)
})

# data.table
timing_group_by_big[["data.table"]] = system.time({
    dt = setnames(setDT(list(x, grp)), c("x","grp"))
    r.data.table = dt[, .(sum(x), .N), grp]
})

timing_group_by_big = timing_group_by_big %>% 
  do.call(rbind, .) %>% 
  data.frame() %>% 
  rownames_to_column('package')
  

save(timing_group_by_big, file = 'data/timing_group_by_big.RData')
```


```{r dttimings, eval=TRUE, echo=FALSE}
load('data/timing_group_by_big.RData')

timing_group_by_big %>% 
  select(package, elapsed) %>% 
  arrange(desc(elapsed)) %>% 
  kable_df()
```

We can see that all options are notable improvements on dplyr.  Tidyfast is a little optimistic, as it can count but not do a summary operation like means or 

## Count

To make things more evenly matched, we'll just do a simple grouped count.


```{r dtplyr, echo=FALSE, eval=TRUE}
data(flights, package = 'nycflights13')
head(flights)

flights_dtp = lazy_dt(flights)

flights_dt = data.table(flights)

bm_count_flights = bench::mark(
  dplyr      = count(flights, arr_time),
  dplyr2     = group_size(group_by(flights, arr_time)),
  dtplyr     = as_tibble(count(flights_dtp, arr_time)),
  tidyfast   = dt_count(flights_dt, arr_time),
  data.table = flights_dt[, .(n = .N), by = arr_time],
  iterations = 100,
  check = FALSE
)

autoplot(bm_count_flights, alpha = .05) + 
  stat_summary(aes(group = NULL)) + 
  guides(color = 'none') + 
  visibly::theme_clean()

save(bm_count_flights, file = 'data/timing_count_flights.RData')
```


```{r count-timings, echo=FALSE, eval=TRUE}
load('data/timing_count_flights.RData')

test_table %>%
  arrange(desc(timing)) %>%
  kable_df() %>% 
  kableExtra::add_footnote('Median time in milliseconds to do a count of arr_time on nycflights::flights')

ggplot(test, aes(expr, time, color = expr, fill = expr)) +
  geom_violin(alpha = .6)  +
  # ggbeeswarm::geom_beeswarm(dodge.width = 5, alpha = .6)  +
  visibly::theme_clean()
```

Just for giggles I did the same in Python with a <span class="pack">pandas</span> <span class="objclass">DataFrame</span>, and it was notably slower than all of these options (almost 10x slower than standard dplyr).  A lot of folks that use Python primarily think R is slow, but that is mostly because they don't know how to effectively program with R for data science.

```{python timing, engine = '~/anaconda3/bin/python', eval = FALSE, echo = FALSE}
import pandas as pd

flights = r.flights

flights.set_index(["arr_time", 'year']).count(level="arr_time")

def test():
  flights.set_index(["arr_time", 'year']).count(level="arr_time")

test()
import timeit

timeit.Timer.timeit() # see documentation

test_result = timeit.timeit(stmt="test()", setup="from __main__ import test", number = 100)

# default result is in seconds for the total number of 100 runs
test_result/100*1000  # per run in milliseconds
```

## Fill in missing values

Fill in missing values by group.  I thought the original data for demonstrating this was a bit odd, so I've created a more realistic example.


```{r fill-timings}
set.seed(1234)

N = 1e6
Ng = 50000

create_missing <- function(x) {
  x[sample(1:length(x), 5)] = NA
  x
}

df_missing = data.frame(grp = rep(1:Ng, e = N/Ng)) %>% 
  arrange(grp) %>% 
  group_by(grp) %>% 
  mutate(
    x = 1:n(),
    y = rpois(n(), 5),
    z = rnorm(n(), 5)
  ) %>% 
  mutate_at(vars(x:z), create_missing)

df_missing %>% print(n = 50)

dt_missing = as.data.table(df_missing)


bm_fill <-
  bench::mark(
    tidyr = fill(group_by(df_missing, grp), x:z),
    tidyfast = dt_fill(dt_missing, x, y, z, id = grp),
    data.table = dt_missing[, c('x', 'y', 'z') := list(nafill(x, type = 'locf'), nafill(y, type = 'locf'), nafill(z, type = 'locf')), by = grp],
    check = FALSE,
    min_time = Inf,
    iterations = 5
  )
```

