---
title: "Exploring Time with R"
description: |
  Multiple avenues to time-series analysis
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/198R.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: ../../styles.css
    df_print: kable_df
    
bibliography:
  # - ../../bibs/mixed.bib  # old issue of multiple biblios with relative paths never fixed in rmd
  - ../../bibs/ts.bib
draft: true
tags: [time series, generalized additive models, mixed models, torch, deep learning, RNN, seq2seq, prophet, fable, pytorch]
categories:
  - [mixed models, time series, deep learning]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = T, 
  message   = F, 
  warning   = F, 
  comment   = NA,
  R.options = list(width = 120),
  cache.rebuild = F,
  cache = T,
  fig.align = 'center',
  fig.asp = .7,
  dev = 'svg',
  dev.args = list(bg = 'transparent')
)

library(tidyverse)
library(broom)
library(kableExtra)
# library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}

rnd = tidyext::rnd
theme_set(visibly::theme_clean())
```

## Intro

It is extremely common to have data that exists over a period of time. For example, we might have yearly sports statistics, daily manufacturing records, server logs that might be occurring many times per second, and similar.  There are many approaches we could use to model the data in these scenarios.  When there are few time points and they are clustered within other units, like repeated observations of exercise data for many individuals, we often use tools like [mixed models](https://m-clark.github.io/mixed-models-with-R/) for example, and even with many observations in a series, we can still use tools like that. But sometimes there may be no natural clustering, or we might want to use other approaches to handle additional complexity.

This post is inspired by a co-worker's efforts in using PyTorch to analyze  Chicago Transit data. [Cody Dirks](https://twitter.com/codydirks) was writing [a post](https://www.strong.io/blog) where he used a Python module developed by our group at [Strong Analytics](https://strong.io) to analyze the ridership across all the '[L](https://en.wikipedia.org/wiki/Chicago_%22L%22)'.  This post can be seen as a demonstration of some simpler models which might also be viable for a given situation such as this, allowing for quick dives, or even as ends in themselves.



## Outline

The models we'll go through are the following:

- Error models and random effects
- GAM
- More elaborate time series with seasonal and other effects
- Deep learning

In what follows I will show some more detailed code in the beginning, but won't show it later for conciseness, focusing mostly just on the basic model code. You can always find the code for these posts on my [GitHub](https:://github.com/m-clark/m-clark.github.io).



## Data Description

As will be seen with Cody's post[^codypost], over 750,000 people use the Chicago Transit Authority's 'L' system to get around the city. There are 8 interconnected rail lines named after colors- the Red, Blue, Green, Brown, Pink, Orange, Purple, and Yellow, 145 entry/exit stations, and over 2,300 combined trips by its railcars every day[^express].

[^codypost]: As of publishing this, Cody's post had not yet been pushed to the Strong website.  I will update with a link when it's available.
[^express]: There is also the Purple express line, which is very irregular compared to the others.

The city of Chicago provides ridership data that can be accessed publicly.

- [ridership](https://data.cityofchicago.org/Transportation/CTA-Ridership-L-Station-Entries-Daily-Totals/5neh-572f)
- [station info](https://data.cityofchicago.org/Transportation/CTA-System-Information-List-of-L-Stops/8pix-ypme)

In Cody's exploration, he added pertinent information regarding weather, sporting events, and more.  You can access the [processed data](https://github.com/m-clark/m-clark.github.io/tree/master/data/time-series/processed_df.csv).

`TODO: NEED LINK`

For our demonstrations we have daily ridership from 2012-2018, and we will use a variety of methods to model this.  We will use a normalized ride count (mean of 0, standard deviation of 1) as our target variable.


### Import & Setup

To get things started we'll use the tidyverse for some additional data processing, and lubridate for any date processing, for example, converting to weekdays.

```{r pkg-load-misc, cache=FALSE}
# Data Processing

library(tidyverse)
library(lubridate)


# Misc

START_DT = '2008-06-01'
END_DT   = '2018-12-31'
SPLIT_DT = '2017-06-01'
```


##### Main data 

I start with data having already been processed, but as mentioned the source is publicly available.  I use <span class="pack" style = "">data.table</span> to read it in more quickly, but it's default date class can cause issues with other packages, so I deal with that.  I also extract the year, month, weekday, etc.


```{r data-processed}
df = data.table::fread('data/time-series/processed_df.csv')

df_start = df %>% 
  as_tibble() %>% 
  select(-contains('_attributes'), -(tsun:wt22)) %>% 
  mutate(
    date      = as_date(date), # remove IDATE class
    rides_log = log(rides),
    year      = year(date),
    year_fac  = factor(year),
    month     = month(date, label = TRUE),
    day       = factor(wday(date, label = TRUE), ordered = FALSE),
    year_day  = lubridate::yday(date),
    line = factor(line),
    snow_scaled = scale(snow)[, 1],
    colors = as.character(line),
    colors = ifelse(colors == 'purple_express', 'purple4', colors),
    red_line_modernization = 
      ifelse(
        between(date, as_date('2013-05-19'), as_date('2013-10-20')), 
        1, 
        0
      )
  ) %>% 
  arrange(date, line)
```


### Training and Validation

We split our data into training and validation sets, such that everything before `r SPLIT_DT` is used for training, while everything after will be used for testing model performance.

```{r train-test-split}
df_train = df_start %>% 
  filter(date < SPLIT_DT, !is.na(rides))

df_validate = df_start %>% 
  filter(date >= SPLIT_DT, !is.na(rides))

red_line_train = df_train %>% 
  filter(line == 'red')

red_line_validate = df_validate %>% 
  filter(line == 'red')
```


### Other

Holidays are available via the <span class="pack" style = "">prophet</span> package, which we'll be demonstrating a model with later.  The data we're using already has a 'holiday vs. not' variable for simplicity, though it comes from a different source.  The <span class="pack" style = "">prophet</span> version has both the actual date and the observed date counted as a holiday, and I prefer to use both.

```{r misc-processing}
holidays = prophet::generated_holidays %>% 
  filter(country == 'US') %>% 
  mutate(ds = as.numeric(as_date(ds))) %>%
  droplevels()
```

We'll take a quick look at the red line similar to Cody's post, so we can feel we have the data processed as we should.

```{r dupe-plot, echo=FALSE}
red_line_train %>%
  filter(line == 'red', date >= '2012-01-01', date <= '2016-01-01') %>% 
  ggplot(aes(date, rides)) +
  geom_rect(
    aes(
      xmin = as_date('2013-05-19'),
      xmax = as_date('2013-10-20'),
      ymin = 0,
      ymax = Inf
    ), 
    fill = 'gray90'
  ) +
  geom_line(color = 'darkred', alpha = .5) +
  geom_vline(
    aes(xintercept = date),
    data  = . %>% 
      filter(date == floor_date(date, unit = 'year')) %>% 
      distinct(),
    lty   = 'dashed',
    color = 'gray50',
    size  = .5,
    alpha = .5
  ) +
  annotate(
    geom = 'text',
    x = as_date('2013-05-19'),
    y = 1,
    label = 'Red Line South\nModernization Project',
    color = 'gray25',
    size  = 2,
    hjust = 0,
    vjust = 0
  ) +
  scale_x_date(
    breaks = seq.Date(as_date('2012-01-01'), as_date('2016-01-01'), by = '6 months'),
    date_labels = '%Y-%m'
  ) +
  scale_y_continuous(breaks = seq(0, 3e5, 5e4), limits = c(0, 250000)) +
  labs(x = '', y = '', subtitle = 'Daily Riders') +
  guides(x = guide_axis(n.dodge = 2)) +
  theme(
    axis.ticks.x = element_line(color = 'gray90')
  )
```


With the data ready to go, we are ready for modeling, so let's get started!



## Classical Time Series

### Intro

Classical times series from an econometrics perspective often considers a error model that accounts for the correlation a current observation has with past observations.  A traditional example is the so-called *autoregressive*, or *AR*, model, which let's a current observation be predicted by past observations up to a certain point. For example, would could start by just using the last observation to predict the current one. Next we extend this to predict the current based on the previous two observations, and so on. How many *lags* we use is part of the model exploration.

$$y_t = \alpha_1y_{t-1} + \dots +\alpha_{p}y_{t-p} + \varepsilon_t$$

We can extend this to include not just past observation but also past residuals, called a *moving average*.  So formally, our *ARMA* (p, q) model now looks like this for an observation $y$ at time $t$:

$$y_t = \alpha_1y_{t-1} + \dots +\alpha_{p}y_{t-p} + (\varepsilon_t + \theta_1 \varepsilon_{t-1} + \cdots +\theta_q \varepsilon_{t-q})$$

We can also use [differencing](https://otexts.com/fpp3/stationarity.html), for example subtracting previous time value from the current observation value for all values, to come to the final [*ARIMA* (p, d, q) model](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average). See @Hyndman2021 for more details.

### Model

Even base R comes with basic time series models such as this.  However, as mentioned, we typically don't know what to set the values of an ARIMA(p, d, q) to.  A quick way to explore this is via the <span class="" style = "">forecast</span> package, which will search over the various hyperparameters and select one based on AIC. Note that <span class="pack" style = "">fable</span>, a package we will be using later, will also allow such an approach, and if you'd like to go ahead and start using it I show some commented code below.

```{r arima-model}
model_arima = forecast::auto.arima(
  red_line_train$rides_scaled
)

# model_arima = red_line_train %>%
#   select(date, rides_scaled) %>%
#   tsibble::as_tsibble() %>%
#   fabletools::model(fable::ARIMA(
#     rides_scaled ~ 0 + PDQ(0,0,0),
#     stepwise = FALSE,
#     approximation = FALSE
#   ))
# fabletools::report(model_arima)
```


### Explore

In this case we have an selected AR of `r model_arima$arma[1]` and MA of `r model_arima$arma[2]` for the centered value.  But looking at the predictions, we can see this is an almost useless result for any number of days out, and does little better than guessing.



```{r arima-tidy, eval=FALSE}
broom::tidy(model_arima)
```

```{r arima-tidy-show, echo=FALSE}
broom::tidy(model_arima) %>% 
  kable_df()
```


```{r arima-explore}
# plot(acf(residuals(model_arima))) # weekly autocorrelation still exists

red_line_validate %>% 
  slice(1:30)  %>%
  mutate(pred = predict(model_arima, n.ahead = 30)$pred) %>%
  # mutate(pred = forecast(model_arima, h = 30)$.mean) %>%
  ggplot(aes(date, rides_scaled)) +
  geom_line(alpha = .25) +
  geom_line(aes(y = pred), alpha = .25, color = 'darkred')
```


We'll use <span class="pack" style = "">yardstick</span> to help us evaluate performance for this and subsequent models. In this case however, the visualization told us enough- a basic ARIMA isn't going cut it.

```{r arima-performance, eval=FALSE}
library(yardstick)

# this function will be used for all subsequent models!
metric_score = metric_set(rmse, mae, rsq) 

# validation
data.frame(
  pred = predict(model_arima, newdata = red_line_validate, n.ahead = 30)$pred,
  observed = red_line_validate$rides_scaled[1:30]
) %>%
  metric_score(truth = observed, estimate = pred)
```


```{r arima-performance-show, echo=FALSE}
library(yardstick)

metric_score = metric_set(rmse, mae, rsq)

data.frame(
  pred = predict(model_arima, newdata = red_line_validate, n.ahead = 30)$pred,
  observed = red_line_validate$rides_scaled[1:30]
) %>%
  metric_score(truth = observed, estimate = pred) %>% 
  kable_df()
```


One nice thing about the <span class="pack" style = "">forecast</span> package is that it can include additional covariates via the `xreg` argument, which is exactly what we need- additional information. Now our model looks something like this, where $X$ is our model matrix of covariates and $\beta$ their corresponding regression weights.

$$y_t = X_t\beta + \alpha_1y_{t-1} + \dots +\alpha_{p}y_{t-p} + (\varepsilon_t + \theta_1 \varepsilon_{t-1} + \cdots +\theta_q \varepsilon_{t-q})$$

Adding these is not exactly straightforward, since it requires a matrix rather than a data frame, but not a big deal once you are used to creating model matrices.


```{r arima-model-xreg}
mm = model.matrix(
  ~ . - 1, 
  data = red_line_train %>% 
    select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization)
)

model_arima_xreg = forecast::auto.arima(
  red_line_train$rides_scaled,
  max.p = 10,
  max.q = 10,
  xreg  = mm
)
```

```{r arima-model-xreg-show, echo=FALSE}
broom::tidy(model_arima_xreg) %>% 
  kable_df()
```


This is looking much better!  We can also see how notably different the ARMA structure is relative to the previous model.  We also see that weekends and holidays results in a huge drop in ridership as expected, while baseball games and good weather will lead to an increase.

In the following code, we create a model matrix similar to the training data that we can feed into the <span class="func" style = "">predict</span> function.  The forecast package also offers a <span class="func" style = "">glance</span> method if desired.

```{r arima-model-xreg-explore}
nd = red_line_validate %>%
  select(is_weekend:is_sox_game, tmax_scaled, prcp_scaled, red_line_modernization) %>% 
  model.matrix( ~ . - 1, data = .)

preds = predict(model_arima_xreg, newxreg = nd, n.ahead = nrow(red_line_validate))$pred
```

```{r armia-vis}
p_arima_red = red_line_validate %>% 
  mutate(pred = preds) %>% 
  ggplot(aes(date, rides_scaled)) +
  geom_line(alpha = .25) +
  geom_line(aes(y = pred), alpha = .25, color = 'red') +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'ARIMA')

p_arima_red
```


And here we can see performance is notably improved (restrict to first 30 obs for a direct comparison to the previous.

```{r arima-xreg-performance, echo=FALSE}
# validation
data.frame(
    pred = preds, 
    observed = red_line_validate$rides_scaled
  ) %>%
  metric_score(truth = observed, estimate = pred) %>% 
  kable_df()
```

```{r arima-model-save, echo=FALSE}
save(
  model_arima,
  model_arima_xreg,
  p_arima_red,
  file = 'data/time-series/arima-model.RData'
)
```



## Mixed model with AR Structure

### Intro

More generally, we can think of that original AR error as a random effect, such that after the linear predictor is constructed, we add a random effect based on the correlation structure desired, in this case, autoregressive.  In the mixed model setting, it is actually quite common to use an AR residual structure within a cluster or group, and here we can do so as well, as the data is naturally grouped by line.  

To make this a bit more clear, we can state the AR effect more formally as follows for a single line at time $t$:

$$z_t  \sim N(0, \Sigma_{ar})$$
$$\Sigma_{ar} = cov(z(s), z(t)) = \sigma^2\exp(-\theta|t-s|)$$

Where t,s are different time points, e.g. within a line.

If we were to simulate it for 4 time points, with autocovariance value of .5, we could do so as follows[^bolkercov].

[^bolkercov]: This follows [Bolker's demo](https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html).

```{r ar-sim}
n_clusters   = 1
n_timepoints = 4
mu  = 0
var = 1  # not actually used if the value is 1
S = .5^as.matrix(dist(1:n_timepoints))

S

z = MASS::mvrnorm(mu = rep(mu, n_timepoints), Sigma = S)

z
```


And here is our typical model with a single random effect, e.g. for line:

$$ y_{tl} \sim X\beta + z^{line}_{l} + e_{tl}$$
$$\textrm{z}_{l} \sim N(0, \sigma_l^2)$$
$$\epsilon \sim N(0, \sigma_e^2)$$

The X may be at either line or observation level, and potentially the $\beta$ could vary by line.

Putting it all together, we're just adding the AR random effect to the standard mixed model for a single line.

$$ y_{tl} \sim X\beta + z^{ar}_t +z^{line}_{l} + e_{tl}$$


### Data Prep

So let's try this! First some minor data prep to add holidays.

```{r mixed-ar-data}
df_train_mixed = df_train %>% 
  mutate(date = as.numeric(date)) %>% 
  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %>% 
  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))

df_validate_mixed = df_validate %>% 
  mutate(date = as.numeric(date)) %>% 
  left_join(holidays, by = c('date' = 'ds', 'year' = 'year')) %>% 
  mutate(holiday = factor(ifelse(is.na(holiday), '0', as.character(holiday))))
```

### Model

For the model, we can now easily think of it as we do other standard modeling scenarios.  Along with standard covariates, we'll add random effects for line, day, day x line interaction, etc.  Finally we also add an AR random effect. For each line, we have an autoregressive structure for days, such that days right next to each other are correlated, and this correlation tapers off as days are further apart.  This is not our only option, but seems straightforward to me. 


Depending on what you include in the model, you may have convergence issues, so feel free to reduce the complexity if needed.  For example, most of the day effect is captured by weekend vs. not, and a by line year trend wasn't really necessary. In addition, the way the AR random effect variance is estimated as noted above, this essentially captures the line intercept variance.

```{r mixed-ar-model}
model_mixed = 
  rides_scaled ~ 
  is_weekend +
  is_cubs_game +
  is_sox_game +
  tmax_scaled + 
  prcp_scaled + 
  snow_scaled +
  # year_day +
  ar1(0 + day|line) +     # the 0 + is a nuance of tmb's approach
  (1|holiday) +           # as RE with all holidays instead of just holiday vs. not
  (1|year) +     
  (1 | red_line_modernization:line) +  # the project shifted ridership from red to other lines
  # (1|day) #+ 
  # (1|line) +
  (1|day:line) #+
  # (1 + year_day|line)

library(glmmTMB)

fit_mixed = glmmTMB(model_mixed, data = df_train_mixed)
```


### Explore

The mixed model approach is nice because it is highly interpretable.  We get both standard regression coefficients, and variance components to help us understand how the rest of the variance breaks down.  For example, I would interpret the following that that line and weekend are the biggest contributors to the variability seen, and that we have high autocorrelation, as expected.

```{r mixed-ar-explore, message=TRUE, cache.rebuild=TRUE}
library(mixedup)

summarise_model(fit_mixed, digits = 4)

extract_cor_structure(fit_mixed, which_cor = 'ar1')
```


```{r mixed-vis, cache.rebuild=TRUE}
p_mixed = df_validate_mixed %>% 
  droplevels() %>% 
  mutate(pred = predict(fit_mixed, newdata = .)) %>%
  mutate(date = as_date(date)) %>% 
  ggplot(aes(date, rides_scaled)) +
  geom_line(alpha = .1) +
  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +
  facet_grid(rows = vars(line), scales = 'free_y') +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'Mixed')

p_mixed
```

```{r mixed-vis-save, echo=FALSE}
p_mixed_red = df_validate_mixed %>% 
  droplevels() %>% 
  filter(line == 'red', year == 2018) %>% 
  mutate(pred = predict(fit_mixed, newdata = .)) %>%
  mutate(date = as_date(date)) %>% 
  ggplot(aes(date, rides_scaled)) +
  geom_line(alpha = .1) +
  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'Mixed')
```



As before we can measure performance via yardstick.  This model does appears to do very well.


```{r mixed-ar-performance}
# validation
data.frame(
  pred = predict(fit_mixed, newdata = df_validate_mixed, allow.new.levels = TRUE),
  observed = df_validate_mixed$rides_scaled
) %>%
  metric_score(truth = observed, estimate = pred)
```

```{r mixed-model-save, echo=FALSE}
save(
  fit_mixed,
  p_mixed,
  p_mixed_red,
  file = 'data/time-series/mixed-model.RData'
)
```

For more on autocorrelation structure in the mixed model setting, [see my mixed model document here](https://m-clark.github.io/mixed-models-with-R/extensions.html#autocorrelation)[^brady]. 

[^brady]: I always appreciated the depiction of this topic in @west2014 quite a bit.



## Generalized Additive  Models

### Intro

We can generalize mixed models even further to incorporate nonlinear components, which may include cyclic or other effects. Such models are typically referred to as [generalized additive models](https://m-clark.github.io/generalized-additive-models/) (GAMs).
AR processes themselves can be seen as a special case of [gaussian processes](http://www.gaussianprocess.org/), which can potentially be approximated via GAMs.  As they can accommodate spatial, temporal, nonlinear, and other effects, they are sometimes more generally referred to as *structured additive regression models*, or STARs.


### Data Prep

The data prep for the GAM is the same as with the mixed model, so we'll just use that data.

```{r gam-data-prep}
df_train_gam = df_train_mixed

df_validate_gam = df_validate_mixed
```

### Model

With data in place we are ready to conduct the model. We have numerous options for how we'd like to take this.  However, as an example, I tried various smooths, but didn't really see much difference, which is actually a good thing. For any further improvements we'd likely have to tweak the core model itself.  I also use <span class="func" style = "">bam</span> for a quicker result, but this isn't really necessary, as it didn't even take a minute to run.  As with the mixed model, we will use holiday as a random effect.

```{r gam-model}
library(mgcv)

# for year, use year (numeric) or use year_fac, but for latter, it will not be
# able to predict any year not in the training data unless you use
# drop.unused.levels.
model_gam = 
  rides_scaled ~ 
  is_weekend +
  is_cubs_game +
  is_sox_game +
  s(tmax_scaled) + 
  s(prcp_scaled) + 
  s(snow_scaled) +
  s(red_line_modernization, line, bs = 're') +
  s(holiday,  bs = 're') +
  s(year_fac, bs = 're') +      
  s(day,  bs = 're') + 
  s(line, bs = 're') + 
  s(line, day, bs = 're') + 
  s(year_day, by = line, bs = c('ds', 'fs'))


# will take a while!
# fit_gam = gam(
#   model_gam, 
#   data     = df_train_gam,
#   drop.unused.levels = FALSE, 
#   method   = "REML"
# )

# fast even without parallel
fit_gam = bam(
  model_gam, 
  data     = df_train_gam,
  drop.unused.levels = FALSE, 
  method   = "fREML",
  # nthreads = 8,     # this option assumes a cluster is available. not necessary for this.
  discrete = TRUE
)
```

### Explore

As with <span class="pack" style = "">glmmTMB</span>, I use a custom function to summarize the model, or extract different components from it. From the initial glance we can see things that we expect (e.g. day/weekend effects are large).

```{r gam-explore, message=TRUE}
mixedup::summarise_model(fit_gam)
```

Now we can visualize test predictions broken about by line. The greater flexibility of the GAM allows it to follow the trends more easily than the standard linear mixed model approach.

```{r gam-vis, echo=FALSE}
# visualize training
# df_train_gam %>%
#   drop_na(rides) %>%
#   mutate(pred = fitted(fit_gam)) %>%
#   ggplot(aes(date, rides_scaled)) +
#   geom_line(alpha = .25) +
#   geom_line(aes(y = pred, color = I(colors)), alpha = .25) +
#   facet_grid(rows = vars(line), scales = 'free_y')

# gammit::plot_gam_check(fit_gam)

p_gam = df_validate_gam %>% 
  droplevels() %>% 
  mutate(pred = predict(fit_gam, newdata = .)) %>%
  mutate(date = as_date(date)) %>% 
  ggplot(aes(date, rides_scaled)) +
  geom_line(alpha = .1) +
  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +
  facet_grid(rows = vars(line), scales = 'free_y') +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'GAM')

p_gam


p_gam_red = df_validate_gam %>% 
  droplevels() %>% 
  filter(line == 'red', year == 2018) %>% 
  mutate(pred = predict(fit_gam, newdata = .)) %>%
  mutate(date = as_date(date)) %>% 
  ggplot(aes(date, rides_scaled)) +
  geom_line(alpha = .1) +
  geom_line(aes(y = pred, color = I(colors)), alpha = .25) +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'GAM')
```

We can see improvement over our standard mixed model approach.

```{r gam-performance, echo=FALSE}
# data.frame(
#   pred = predict(fit_gam, newdata = df_validate_gam %>% filter(line == 'red')),
#   observed = df_validate_gam %>% filter(line == 'red') %>% mutate(date = as.numeric(date)) %>% pull(rides_scaled)
# ) %>%
#   metric_score(truth = observed, estimate = pred) %>% 
#   kable_df()

data.frame(
  pred = predict(fit_gam, newdata = df_validate_gam),
  observed = df_validate_gam %>% mutate(date = as.numeric(date)) %>% pull(rides_scaled)
) %>%
  metric_score(truth = observed, estimate = pred) %>% 
  kable_df()
```

```{r gam-model-save, echo=FALSE}
save(
  fit_gam,
  p_gam,
  p_gam_red,
  file = 'data/time-series/fit_gam.RData'
)
```




## Prophet

### Intro

*Prophet* is an approach from Facebook that uses Stan to estimate a time series model taking various trends, seasonality, and other factors under consideration. By default, it only uses Stan for optimization (e.g. via 'BFGS'), but you can switch to fully Bayesian if desired, and take advantage of all that the Bayesian approach has to offer.

### Data Prep

The <span class="pack" style = "">prophet</span> package in R takes some getting used to.  We have to have specific names for our variables, and unfortunately have to do extra work to incorporate categorical predictors. We can  use <span class="pack" style = "">recipes</span> (like <span class="pack" style = "">yardstick</span>, part of the <span class="pack" style = "">tidymodels</span> 'verse) to set up the data (e.g. one-hot encoding).

```{r prophet-data-prep}
library(prophet)
library(recipes)

df_train_prophet = df_train %>% 
  arrange(date, line) %>% 
  rename(y  = rides_scaled,
         ds = date)

library(recipes)
rec = recipes::recipe(~., df_train_prophet)

df_train_prophet = rec %>% 
  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %>% 
  prep(training = df_train_prophet) %>% 
  bake(new_data = df_train_prophet) %>% 
  rename_with(.cols = starts_with('line_'), str_remove, 'line_')

df_validate_prophet = df_validate %>% 
  arrange(date, line)%>%
  rename(ds = date, y = rides_scaled)

rec = recipe(~., df_validate_prophet)

df_validate_prophet = rec %>% 
  step_dummy(line, one_hot = TRUE, keep_original_cols = TRUE) %>% 
  prep(training = df_validate_prophet) %>% 
  bake(new_data = df_validate_prophet) %>% 
  rename_with(.cols = starts_with('line_'), str_remove, 'line_')
```

### Model

With data in place, we are ready to build the model.  Note that later we will compare results to <span class="pack" style = "">fable.prophet</span>, which will mask some of the functions here, or vice versa depending on which you load first.  I would suggest only doing the prophet model, or only doing the fable model, rather than trying to do both at the same time, to avoid any mix-up.  

After setting up the model, you have to add additional predictors in separate steps.  Prophet has a nice way to incorporate holidays though. When you run this model, you may have to wait for a minute or so.

```{r prophet-model}
# use prophet::prophet in case you have fable.prophet loaded also
model_prophet = prophet::prophet(
  holidays = generated_holidays %>% filter(country == 'US'),
  yearly.seasonality = FALSE,
  seasonality.mode = "multiplicative",
  changepoint.prior.scale = .5
)

line_names = c(
  'blue',
  'brown',
  'green',
  'orange',
  'pink',
  'purple',
  'purple_express',
  'red',
  'yellow'
)

predictors = c(
  'is_weekend',
  'is_cubs_game',
  'is_sox_game',
  # 'is_holiday',
  'tmax_scaled',
  'prcp_scaled',
  'snow_scaled',
  line_names
)

for (i in predictors) {
  model_prophet = add_regressor(model_prophet, i, standardize = FALSE, mode = 'additive')
}

model_prophet = add_country_holidays(model_prophet, country_name = 'US')

fit_prophet = fit.prophet(model_prophet, df = df_train_prophet)

forecast = predict(fit_prophet, df_validate_prophet)
```

### Explore

We now visualize predictions as we did with the [GAM][Structured Additive Regression Models]. But one of the nice things with <span class="pack" style = "">prophet</span> is that you can plot the various parts of the model results via the <span class="func" style = "">plot
</span> method or <span class="func" style = "">prophet_plot_components</span> (not shown). Unfortunately, our baseline effort seems to undersmooth our more popular lines (blue, red), and overreacts to some of the others (purple, yellow).


```{r prophet-vis, echo=FALSE}
# prophet_plot_components(fit_prophet, forecast)
# plot(fit_prophet, forecast)

p_prophet = forecast %>%
  select(ds, yhat) %>%
  bind_cols(df_validate_prophet %>% select(-ds)) %>% 
  ggplot(aes(x = ds)) +
  geom_line(aes(y = y, group = colors), alpha = .1) +
  geom_path(aes(y = yhat, color = I(colors)), alpha = .25) +
  facet_grid(rows = vars(line), scales = 'free') +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'Prophet')

p_prophet

p_prophet_red = forecast %>%
  select(ds, yhat) %>%
  bind_cols(df_validate_prophet %>% select(-ds)) %>% 
  filter(line == 'red', year == 2018) %>% 
  ggplot(aes(x = ds)) +
  geom_line(aes(y = y, group = colors), alpha = .1) +
  geom_path(aes(y = yhat, color = I(colors)), alpha = .25) +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'Prophet')
```

We can also assess performance as before, but note that <span class="pack" style = "">prophet</span> has it's own cross-validation capabilities which would be better to utilize if this was your primary tool. Recall that we created the `ms` function previously. Between the previous visualization and these metrics, our first stab doesn't appear to do as well as the GAM, so you might like to go back and tweak things.

```{r prophet-performance, echo=FALSE}
# prophet::performance_metrics() # if you do cross-validation

data.frame(
  pred = forecast$yhat,
  observed = df_validate_prophet %>% arrange(ds, line) %>% pull(y)
) %>%
  metric_score(truth = observed, estimate = pred) %>% 
  kable_df()
```


```{r prophet-model-save, echo=FALSE}
save(
  fit_prophet,
  p_prophet,
  p_prophet_red,
  file = 'data/time-series/fit_prophet.RData'
)
```


## Fable

### Intro

I came across <span class="pack" style = "">fable.prophet</span> as a possibly easier way to engage prophet. It is an extension of <span class="pack" style = "">fable</span> and related packages, which are very useful for time series processing and analysis. Note that it is 0.1.0 version development, and hasn't had much done with it in the past year, so your mileage may vary with regard to utility by the time you read this.  But with it we can specify the model in more of an R fashion, and we now don't have as much data pre-processing. 

### Data Prep

One key difference using <span class="pack" style = "">fable.prophet</span> is that it works with `tsibble` objects, and thus must have unique date observations.  We can do this by setting `line` as the key[^holidaybug].

[holidaybug]: <span class="pack" style = "">fable.prohet</span> may have a bug enabling the holidays functionality with parallel, so you can just use the original holiday column if you do so (single core doesn't take too long). 


```{r fable-data-prep}
library(fable.prophet)

df_train_fable = df_train_prophet %>% 
  as_tsibble(index = ds, key = line)

df_validate_fable = df_validate_prophet %>% 
  as_tsibble(index = ds, key = line)

holidays_fable = holidays %>% 
  filter(country == 'US') %>% 
  mutate(ds = as_date(ds)) %>% 
  as_tsibble()
```

### Model

Beyond this we use functions within our formula to set the various components.  With line as the key, <span class="pack" style = "">fable</span> is actually running separate prophet models for each line, and we can do so in parallel if desired.


```{r fable-model}
model_prophet = fable.prophet::prophet(
  y ~ 
    growth('linear', changepoint_prior_scale = 0.5) +
    season("week", type = "multiplicative") +
    holiday(holidays_fable) +
    xreg(
      is_weekend,
      is_cubs_game,
      is_sox_game,
      # is_holiday,
      tmax_scaled,
      prcp_scaled,
      snow_scaled
    ) 
)

# library(future)
# plan(multisession)

# furrr is used under the hood, and though it wants a seed, it doesn't
# automatically use one so will give warnings. I don't think it can be passed
# via the model function, so expect to see ignorable warnings (suppressed here).

fit_fable = model(df_train_fable, mdl = model_prophet)

forecast_fable = fit_fable %>% 
  forecast(df_validate_fable) 

# plan(sequential)
```

### Explore

With <span class="pack" style = "">fable.prophet</span> visualization, we have the more automatic plots, but again we'll stick with the basic validation plot we've been doing.

```{r fable-model-components, eval=FALSE}
components(fit_fable)
components(fit_fable) %>%
  autoplot()
forecast_fable %>%
  autoplot(level = 95, color = '#ff5500')
```

This model does well, but we can see its limitations, for example, with the yellow line, and more recent ridership in general.

```{r fable-model-explore, echo = FALSE}
p = forecast_fable %>% 
  autoplot(level = 95, color = '#ff5500') 

# p

p_dat1 = layer_data(p, 1)  # CI
p_dat2 = layer_data(p, 3)  # main value

p_dat2 = p_dat2 %>%
  mutate(
    ds     = as_date(x),
    line   = df_validate_fable$line,
    colors = df_validate_fable$colors
  ) %>% 
  rename(yhat = y)

p_fable_prophet = df_validate_fable %>% 
  ggplot(aes(x = ds)) +
  geom_line(aes(y = y, group = colors), alpha = .1) +
  geom_line(aes(y = yhat, color = I(colors)), alpha = .25, data = p_dat2) +
  facet_grid(rows = vars(line), scales = 'free') +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'Fable Prophet')

p_fable_prophet

p_fable_prophet_red = df_validate_fable %>%
  filter(line == 'red', year == 2018) %>%
  ggplot(aes(x = ds)) +
  geom_line(aes(y = y, group = colors), alpha = .1) +
  geom_line(
    aes(y = yhat, color = I(colors)),
    alpha = .25,
    data = filter(p_dat2, line == 'red' & year(ds) == 2018)
  ) +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'Fable Prophet')
```

And we check performance as before. The <span class="pack" style = "">fable</span> model is doing as well as our [GAM][Structured Additive Regression Models] approach did.

```{r fable-performance, echo=FALSE}
# data.frame(
#   pred =  fitted(fit_fable) %>% pull(.fitted), 
#   observed = df_train_fable %>% pull(y)
# ) %>% 
#   metric_score(truth = observed, estimate = pred)

# data.frame(
#   pred = forecast_fable$.mean,
#   observed = df_validate_fable$y,
#   line = forecast_fable$line
# ) %>%
#   filter(line == 'red') %>% 
#   metric_score(truth = observed, estimate = pred) %>% 
#   kable_df()


# validation
data.frame(
  pred = forecast_fable$.mean,
  observed = df_validate_fable$y,
  line = forecast_fable$line
) %>%
  metric_score(truth = observed, estimate = pred) %>% 
  kable_df()
```

One nice thing about the <span class="pack" style = "">fable</span> approach is its internal performance metrics, which are easily obtained.  It will give us results for each line[^linexline], validation data results shown. We see that we have more error for the popular lines as before, but in terms of percentage error, the other lines are showing more difficulty.  You can find out more about the additional metrics available  [here](https://otexts.com/fpp3/accuracy.html).

[^linexline]: We can also do this with our previous method with a split-by-apply approach. You would obtain the same results, so this serves as a nice supplement to our 'overall' metrics.

```{r fable-performance2, eval=FALSE}
accuracy(fit_fable)
accuracy(forecast_fable, df_validate_fable)
```

```{r fable-performance2-show, echo=FALSE}
accuracy(forecast_fable, df_validate_fable) %>% 
  select(where(~!all(is.nan(.)))) %>% 
  kable_df()
```


The <span class="pack" style = "">fable</span> results suggests what we already knew from our [GAM][Structured Additive Regression Models] and [mixed model][Mixed Model with AR] approach, that interactions of the series with line are important.  We weren't easily able to do this with the default prophet (it would likely require adding time x line interaction regresssors)

```{r fable-model-save, echo=FALSE}
save(
  fit_fable,
  forecast_fable,
  p_fable_prophet,
  p_fable_prophet_red,
  file = 'data/time-series/fit_fable.RData'
)
```


## Torch

At this point we have a collection of models that are still relatively interpretable, and mostly within our standard regression model framework.  It's good to see them able to perform very well without too much complexity. However, we still have other methods available that would be more computationally demanding, are more opaque in operations, but which would potentially provide the most accurate forecasts.  For this we turn to using PyTorch, which is now available via the <span class="pack" style = "">torch</span> package in R[^torch].

[^torch]: For the basics of using PyTorch via R, including installation, see [the RStudio](https://blogs.rstudio.com/ai/posts/2020-09-29-introducing-torch-for-r/).

In using <span class="pack" style = "">torch</span>, we're going to follow the [demo series at the RStudio AI blog](https://blogs.rstudio.com/ai/posts/2021-03-19-forecasting-time-series-with-torch_4/) [^blog-v-repo].  It shows in four parts how to use a *recurrent neural network*.  In their example, they use a data set for a single series with (summarized) daily values, similar to our daily counts here.  We will use the final model demonstrated in the series a so-called *seq2seq* model that includes an *attention* mechanism.  More detail can be found [here](https://www.deeplearningbook.org/contents/rnn.html).  The conceptual gist of the model can be described as taking a set of time points to predict another set of future time points, and doing so for all points in the series.

[^blog-v-repo]: The blog code actually has several issues, but the [github repo](https://github.com/mlverse/torchbook_materials/blob/master/scripts/rnn_attention.R) should work fine and is what is followed for this demo.

To be clear, they only use a single series, no other information (e.g. additional regressors). So we will do the same, coming full circle to what we started out with, just looking at daily ridership- a single time series for the red line.


### Data

As usual we'll need some data prep, both for initial training-test split creation, but also specifically for usage with Torch.

```{r torch-data-prep}
library(tsibble)
library(lubridate)
library(torch)


df_train_torch = df_train %>%
  filter(line == 'red', year < 2017) %>%
  pull(rides_scaled) %>%
  as.matrix()

df_validate_torch = df_validate %>%
  filter(line == 'red', year >= 2017) %>%
  pull(rides_scaled) %>%
  as.matrix()

df_test_torch = df_validate %>%
  filter(line == 'red', date > '2017-12-24') %>%
  pull(rides_scaled) %>%
  as.matrix()

train_mean = mean(df_train_torch)
train_sd   = sd(df_train_torch)
```



### Torch data

For our data, we will use a week behind lag to predict the following week.  This seems appropriate for this problem, but for any particular problem we'd want to probably think hard about this and/or test different settings.

```{r torch-data-timestep-forecast}
n_timesteps = 7    # we use a week instead of 14 days in blog
n_forecast  = 7    # look ahead one week
```


```{r torch-data-prep2}
cta_dataset <- dataset(
  name = "cta_dataset",

  initialize = function(x, n_timesteps, sample_frac = 1) {

    self$n_timesteps <- n_timesteps
    self$x <- torch_tensor((x - train_mean) / train_sd)

    n <- length(self$x) - self$n_timesteps - 1

    self$starts <- sort(sample.int(
      n = n,
      size = n * sample_frac
    ))

  },

  .getitem = function(i) {

    start <- self$starts[i]
    end <- start + self$n_timesteps - 1
    lag <- 1

    list(
      x = self$x[start:end],
      y = self$x[(start+lag):(end+lag)]$squeeze(2)
    )

  },

  .length = function() {
    length(self$starts)
  }
)

batch_size = 32

train_ds = cta_dataset(df_train_torch, n_timesteps)
train_dl = dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)

valid_ds = cta_dataset(df_validate_torch, n_timesteps)
valid_dl = dataloader(valid_ds, batch_size = batch_size)

test_ds  = cta_dataset(df_test_torch, n_timesteps)
test_dl  = dataloader(test_ds, batch_size = 1)
```



### Model

I leave it to the [blog](https://blogs.rstudio.com/ai/posts/2021-03-19-forecasting-time-series-with-torch_4/#attention-module) for details, but briefly, there are four components to the model:

- **Encoder**: takes input, and produces outputs and states via RNN
- **Decoder**: takes the last predicted value as input and current context to make a new prediction
- **Seq2Seq**: essentially encodes once, and calls the decoder in a loop
- **Attention**: allows output from the encoder at a specific time point to provide 'context' for the decoder


```{r torch-net, eval=FALSE}
net =
  seq2seq_module(
    "gru",
    input_size     = 1,
    hidden_size    = 32,
    attention_type = "multiplicative",
    attention_size = 8,
    n_forecast     = n_forecast
  )

b = dataloader_make_iter(train_dl) %>% dataloader_next()

net(b$x, b$y, teacher_forcing_ratio = 1)
```


### Training

With data in place, we're ready to train the model. For the most part, not much is going on here that would be different from other deep learning situations, e.g. choosing an optimizer, number of epochs, etc.  We'll use mean squared error as our loss, and I create an object to store the validation loss over the epochs of training.  I played around with it a bit, and you're probably not going to see much after letting it go for 100 epochs.


```{r torch-training, eval=FALSE}
optimizer = optim_adam(net$parameters, lr = 0.001)

num_epochs = 100

train_batch <- function(b, teacher_forcing_ratio) {

  optimizer$zero_grad()
  output <- net(b$x, b$y, teacher_forcing_ratio)
  target <- b$y

  loss <- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])
  loss$backward()
  optimizer$step()

  loss$item()

}

valid_batch <- function(b, teacher_forcing_ratio = 0) {

  output <- net(b$x, b$y, teacher_forcing_ratio)
  target <- b$y

  loss <- nnf_mse_loss(output, target[ , 1:(dim(output)[2])])

  loss$item()

}


all_valid_loss = c()

for (epoch in 1:num_epochs) {

  net$train()
  train_loss <- c()

  coro::loop(for (b in train_dl) {
    loss <- train_batch(b, teacher_forcing_ratio = 0.0)
    train_loss <- c(train_loss, loss)
  })

  cat(sprintf("\nEpoch %d, training: loss: %3.5f \n", epoch, mean(train_loss)))

  net$eval()
  valid_loss <- c()

  coro::loop(for (b in valid_dl) {
    loss <- valid_batch(b)
    valid_loss <- c(valid_loss, loss)
  })
  
  all_valid_loss = c(all_valid_loss, mean(valid_loss))

  cat(sprintf("\nEpoch %d, validation: loss: %3.5f \n", epoch, mean(valid_loss)))
}
```


```{r torch-training-save, echo=FALSE, eval=FALSE}
torch_save(
  net,
  path = 'data/time-series/fit_torch.pt'
)
```



### Evaluations



```{r torch-load, eval=FALSE, echo=FALSE}
net = torch_load(path = 'data/time-series/fit_torch.pt')
```


```{r torch-eval, eval=FALSE}
net$eval()

test_preds = vector(mode = "list", length = length(test_dl))

i = 1

coro::loop(for (b in test_dl) {

  if (i %% 100 == 0)
    print(i)

  output <- net(b$x, b$y, teacher_forcing_ratio = 0)
  preds <- as.numeric(output)

  test_preds[[i]] <- preds
  i <<- i + 1
})
```

```{r torch-save-predictions, echo=FALSE, eval=FALSE}
# as expected, torch + knitr is not so great, and simply loading the model
# didn't work, and I don't care enough to sort it out.
save(
  test_preds,
  file = 'data/time-series/torch_preds.RData'
)
```


For this visualization, we do things a little different. In our current setup, we have `r n_timesteps` timesteps predicting `r n_forecast` day windows.  We started our test set at the beginning of December so that the first prediction is January first, and then proceeds accordingly.

```{r torch-plot1, echo = -(1:3)}
load('data/time-series/torch_preds.RData')
# n_timesteps = 7    # we use a week instead of 14 days in blog
# n_forecast  = 7    # look ahead one week

# same as test
df_eval_torch = df_validate %>%
  filter(line == 'red', date > '2017-12-01') %>%
  select(rides_scaled, date) %>%
  as_tsibble()

test_preds_plot = vector(mode = "list", length = length(test_preds))

for (i in 1:(length(test_preds_plot)-  n_forecast)) {
  test_preds_plot[[i]] =
    data.frame(
      pred = c(
        rep(NA, n_timesteps + (i - 1)),
        test_preds[[i]] * train_sd + train_mean,
        rep(NA, nrow(df_eval_torch) - (i - 1) - n_timesteps - n_forecast)
      )
    )
}

df_eval_torch_plot0 =
  bind_cols(df_eval_torch, bind_cols(test_preds_plot))
```

A visualization of the predictions makes this more clear. Each 30 day segment is making predictions for the next 14 days.

```{r torch-plot-window, echo=FALSE}
p_dat_window = df_eval_torch_plot0 %>%
  pivot_longer(-c(date, rides_scaled)) %>%
  mutate(name = str_replace(name, '\\.\\.\\.', '_')) %>%
  mutate(name = factor(name, levels = unique(name))) %>%
  drop_na() 

p_dat_window %>%
  ggplot(aes(date, name)) +
  geom_tile(aes(fill = rides_scaled)) +
  scale_fill_scico(palette = 'lajolla', direction = -1) +
  labs(x = '', y = '') +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )
```


So for our red line plot, we'll just use the average prediction at each date to make it comparable to the other plots. In general it looks to be doing okay, even armed with no contextual information.  Certainly better than the base ARIMA plot.

```{r torch-plot, echo = FALSE}
df_eval_torch_plot =
  bind_cols(df_eval_torch_plot0, value = rowMeans(bind_cols(test_preds_plot), na.rm = TRUE))

p_torch_red = df_eval_torch_plot %>%
  pivot_longer(-c(date, rides_scaled)) %>%
  drop_na() %>%
  # mutate(line = factor('red', levels = levels(df_train$line))) %>%
  ggplot(aes(date, value)) +
  geom_line(aes(y = rides_scaled), alpha = .1) +
  geom_line(alpha = .25, color = 'red') +
  labs(x = '', y = 'Rides (scaled)', subtitle = 'Torch') #+

p_torch_red

save(
  p_torch_red,
  file = 'data/time-series/torch.RData'
)
```

However, we can see that there is much information lost just adhering to the series alone.

```{r torch-performance, echo = FALSE}
# validation
p_dat_window %>%
  as_tibble() %>% 
  # split(.$name) %>%
  # map_dbl(function(x) rmse_vec(truth = x$rides_scaled, estimate = x$value)) %>%
  # plot(type = 'l')
  metric_score(truth = rides_scaled, estimate = value) %>%
  kable_df()
```




## All

```{r echo=FALSE, eval=FALSE}
# load all the objects
x = list.files('data/time-series/', pattern = 'RData', full.names = TRUE)
purrr::map(x, load, envir = .GlobalEnv)
```


```{r vis-all-lines, layout = 'l-page', echo=FALSE}
library(patchwork)

# (
#   p_mixed +
#     p_gam + labs(y = '') +
#     p_prophet + labs(y = '') +
#     p_fable_prophet + labs(y = '')
# )*
#   lims(y = c(-3, 3)) *
#   theme(
#     axis.text = element_text(size = 6),
#     axis.title.y = element_text(size = 6),
#     title = element_text(size = 10)
#   )

# best/simpler
(
  p_mixed /
    p_gam /
    p_fable_prophet
)*
  labs(y = '') *
  lims(y = c(-3, 3)) *
  theme(
    axis.text.x = element_text(size = 6),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.y = element_text(size = 6),
    strip.text.y = element_text(size = 6, hjust = 0),
    title = element_text(size = 10)
  )


ggsave(
  'img/time-series/model-test-comparison-all-lines.svg',
  width = 10,
  height = 7,
  bg = 'transparent',
  scale = .8
)
ggsave(
  'img/time-series/model-test-comparison-all-lines.png',
  width = 10,
  height = 7,
  bg = 'transparent',
  scale = .8
)
```




```{r vis-red-line, layout = 'l-page', echo=FALSE}
# (
#   p_mixed_red +
#     p_gam_red + labs(y = '') +
#     p_prophet_red + labs(y = '') +
#     p_fable_prophet_red + labs(y = '') +
#     p_torch_red  +
#     labs(y = '')
# )*
#   lims(y = c(-1, 3)) *
#   theme(
#     axis.text = element_text(size = 6),
#     axis.title.y = element_text(size = 6),
#     title = element_text(size = 10)
#   )

# best
(
  p_gam_red  /
    (p_fable_prophet_red + labs(y = '')) /
    (p_torch_red + labs(y = ''))
)*
  lims(y = c(-1, 3)) *
  theme(
    axis.text = element_text(size = 6),
    axis.title.y = element_text(size = 6),
    title = element_text(size = 10)
  )

ggsave(
  'img/time-series/model-test-comparison-red-line.svg',
  width = 10,
  height = 7,
  bg = 'transparent',
  scale = .8
)
ggsave(
  'img/time-series/model-test-comparison-red-line.png',
  width = 10,
  height = 7,
  bg = 'transparent',
  scale = .8
)
```

## Summary

- GAM: great, more viable than some might suspect, easy implementation
- Prophet: needs work out of the box
- Fable: saves you that work, require unique dates, did great in this situation via by-group models
- Torch: pretty good even with minimal information.

Other possibilities:
More deep learning e.g. tsai

To get some information on what Torch would do at the next level, i.e. adding additional covariates and other considerations, see Cody's post.

