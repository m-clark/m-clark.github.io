<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
  <title>This is definitely not all you need</title>

  <meta property="description" itemprop="description" content="A summary of findings regarding deep learning for tabular data."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2021-07-19"/>
  <meta property="article:created" itemprop="dateCreated" content="2021-07-19"/>
  <meta name="article:author" content="Michael Clark"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="This is definitely not all you need"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="A summary of findings regarding deep learning for tabular data."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="This is definitely not all you need"/>
  <meta property="twitter:description" content="A summary of findings regarding deep learning for tabular data."/>

  <!--/radix_placeholder_meta_tags-->
  
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","preview","output","bibliography","draft","tags","categories","nocite"]}},"value":[{"type":"character","attributes":{},"value":["This is definitely not all you need"]},{"type":"character","attributes":{},"value":["A summary of findings regarding deep learning for tabular data.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Michael Clark"]},{"type":"character","attributes":{},"value":["https://m-clark.github.io"]}]}]},{"type":"character","attributes":{},"value":["2021-07-19"]},{"type":"character","attributes":{},"value":["../../img/dl-for-tab/deep_nn.png"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","css"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["../../styles.css"]}]}]},{"type":"character","attributes":{},"value":["../../bibs/dl-tab.bib"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["deep learning","embeddings","TabNet","XGBoost","gradient boosting","SAINT"]},{"type":"character","attributes":{},"value":["deep learning","machine learning"]},{"type":"character","attributes":{},"value":["@gorishniy2021tabular, @kadra2021tabular, @shwartz2021tabular"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["dl-for-tabular_files/anchor-4.2.2/anchor.min.js","dl-for-tabular_files/bowser-1.9.3/bowser.min.js","dl-for-tabular_files/distill-2.2.21/template.v2.js","dl-for-tabular_files/header-attrs-2.11/header-attrs.js","dl-for-tabular_files/header-attrs-2.9/header-attrs.js","dl-for-tabular_files/jquery-1.11.3/jquery.min.js","dl-for-tabular_files/jquery-3.6.0/jquery-3.6.0.js","dl-for-tabular_files/jquery-3.6.0/jquery-3.6.0.min.js","dl-for-tabular_files/jquery-3.6.0/jquery-3.6.0.min.map","dl-for-tabular_files/popper-2.6.0/popper.min.js","dl-for-tabular_files/tippy-6.2.7/tippy-bundle.umd.min.js","dl-for-tabular_files/tippy-6.2.7/tippy-light-border.css","dl-for-tabular_files/tippy-6.2.7/tippy.css","dl-for-tabular_files/tippy-6.2.7/tippy.umd.min.js","dl-for-tabular_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          return "<p>" + $('#ref-' + ref).html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="dl-for-tabular_files/header-attrs-2.13/header-attrs.js"></script>
  <script src="dl-for-tabular_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="dl-for-tabular_files/popper-2.6.0/popper.min.js"></script>
  <link href="dl-for-tabular_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="dl-for-tabular_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="dl-for-tabular_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="dl-for-tabular_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="dl-for-tabular_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="dl-for-tabular_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="dl-for-tabular_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->

  <link rel="stylesheet" href="../../styles.css" type="text/css"/>

</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"This is definitely not all you need","description":"A summary of findings regarding deep learning for tabular data.","authors":[{"author":"Michael Clark","authorURL":"https://m-clark.github.io","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2021-07-19T00:00:00.000-04:00","citationText":"Clark, 2021"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>This is definitely not all you need</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt=tag">deep learning</div>
<div class="dt=tag">machine learning</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>A summary of findings regarding deep learning for tabular
data.</p></p>
</div>

<div class="d-byline">
  Michael Clark <a href="https://m-clark.github.io"
class="uri">https://m-clark.github.io</a> 
  
<br/>2021-07-19
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#goal">Goal</a></li>
<li><a href="#caveats">Caveats</a></li>
<li><a href="#quick-take">Quick Take</a></li>
<li><a href="#tabular-data-deep-learning-is-not-all-you-need">Tabular
Data: Deep Learning is Not All You Need</a>
<ul>
<li><a href="#paper-info">Paper Info</a></li>
<li><a href="#from-the-abstract">From the Abstract</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#models-explored">Models Explored</a></li>
<li><a href="#quick-summary">Quick Summary</a></li>
<li><a href="#other-stuff">Other stuff</a></li>
</ul></li>
<li><a
href="#regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data">Regularization
is all you Need: Simple Neural Nets can Excel on Tabular Data</a>
<ul>
<li><a href="#paper-info-1">Paper Info</a></li>
<li><a href="#from-the-abstract-1">From the Abstract</a></li>
<li><a href="#overview-1">Overview</a></li>
<li><a href="#data-1">Data</a></li>
<li><a href="#models-explored-1">Models Explored</a></li>
<li><a href="#quick-summary-1">Quick Summary</a></li>
<li><a href="#other-stuff-1">Other Stuff</a></li>
</ul></li>
<li><a
href="#revisiting-deep-learning-models-for-tabular-data">Revisiting Deep
Learning Models for Tabular Data</a>
<ul>
<li><a href="#paper-info-2">Paper Info</a></li>
<li><a href="#from-the-abstract-2">From the Abstract</a></li>
<li><a href="#overview-2">Overview</a></li>
<li><a href="#data-2">Data</a></li>
<li><a href="#models-explored-2">Models Explored</a></li>
<li><a href="#quick-summary-2">Quick Summary</a></li>
<li><a href="#other-stuff-2">Other Stuff</a></li>
</ul></li>
<li><a href="#overall-assessment">Overall Assessment</a></li>
</ul>
</nav>
</div>
<h2 id="motivation">Motivation</h2>
<p>I’ve been a little perplexed at the lack of attention of deep
learning (DL) toward what I consider to be ‘default’ data in my world,
often referred to as <em>tabular data</em>, where typically we have a
two dimensional input of observations (rows) and features (columns) and
inputs are of varying type, scale and source. Despite the ubiquity of
such data in data science generally, and despite momentous advances in
areas like computer vision and natural language processing, at this
time, it’s not very clear what the status of DL for tabular data is.</p>
<p>There have been developments in the area recently though, with some
modeling approaches, such as TabNet, gaining traction. In June of 2021,
I actually came across three papers on <a href="arxiv.org">Arxiv</a>
that were making related claims about the efficacy of DL for tabular
data. As in many academic and practical (and life) pursuits, results of
these studies are nuanced, so I thought I’d help myself and others by
summarizing here.</p>
<h2 id="goal">Goal</h2>
<p>I want to know if, e.g. time and/or resources are limited, whether it
will be worth diving into a DL model if I have a satisfactory
simpler/easier one ready to implement that does pretty well. Perhaps
this answer is already, ‘if it ain’t broke, don’t fix it’, but given the
advancements in other data domains, it would be good to assess what the
current state of DL with tabular data is.</p>
<h2 id="caveats">Caveats</h2>
<ul>
<li>I’m not going to do more than give a cursory summary of the
articles, and provide no in-depth explanation of the models. For more
detail, see the corresponding articles and references for the models
therein. You are not going to learn how to use TabNet, NODE,
transformers, etc., for tabular data.</li>
<li>There are other decent articles on the topic not covered here. Some
are referenced in these more recent offerings, so feel free to
peruse.</li>
</ul>
<h2 id="quick-take">Quick Take</h2>
<p>In case you don’t want any detail, here’s a quick summary based on my
impressions from these articles. Right now, if you want to use DL on
tabular data, don’t make a fuss of it. A simple architecture, even a
standard multi-layer perceptron, will likely do as well as more
complicated ones. In general though, the amount of effort put into
prep/tuning may not be worth it for many typical tabular data settings,
for example, relative to a suitably flexible statistical model
(e.g. GAMM) or a default fast boosting implementation like XGBoost.
However, DL models are already thinking ‘big data’, so for very large
data situations, a DL model might make a great choice, as others may not
be computationally very viable. It also will not be surprising at all
that in the near future some big hurdle may be overcome as we saw with
DL applications in other fields, in which case some form of DL may be
‘all you need’.</p>
<p>Now, on to the rest!</p>
<h2 id="tabular-data-deep-learning-is-not-all-you-need">Tabular Data:
Deep Learning is Not All You Need</h2>
<h3 id="paper-info">Paper Info</h3>
<ul>
<li><em>Who</em>: Shwartz-Ziv &amp; Armon</li>
<li><em>Where</em>: Intel</li>
<li><em>When</em>: 2021-06-21 V1</li>
<li><a href="https://arxiv.org/pdf/2106.03253v1.pdf">Arxiv Link</a></li>
</ul>
<h3 id="from-the-abstract">From the Abstract</h3>
<blockquote>
<p>We analyze the deep models proposed in four recent papers across
eleven datasets, nine of which were used in these papers, to answer
these questions. We show that in most cases, each model performs best on
the datasets used in its respective paper but significantly worse on
other datasets. Moreover, our study shows that XGBoost (Chen and
Guestrin, 2016) usually outperforms the deep models on these datasets.
Furthermore, we demonstrate that the hyperparameter search process was
much shorter for XGBoost.</p>
</blockquote>
<h3 id="overview">Overview</h3>
<p>For each model they used the data that was implemented in the
original model papers by the authors (e.g. the dataset used in the
TabNet article), and also used their suggested parameter settings. They
tested all the models against their own data, plus the other papers’
data, plus two additional data sets that were not used in any of the
original papers.</p>
<h3 id="data">Data</h3>
<p>They use eleven total datasets. Nine datasets are those used in the
original papers on TabNet, DNF-Net, and NODE, drawing three datasets
from each paper. Additionally, Shwartz-Ziv &amp; Armon use two Kaggle
datasets not used in any of those papers. Sample sizes ranged from 7k to
1M, 10-2000 features, with two being numeric targets, while the other
target variables ranged from 2-7 classes. Datasets are described in
detail in the paper along with links to the source (all publicly
available).</p>
<h3 id="models-explored">Models Explored</h3>
<p>Brief summaries of the DL models are found in the paper.</p>
<ul>
<li><em>XGBoost</em></li>
<li><em>TabNet</em></li>
<li><em>Neural Oblivious Decision Ensembles</em> (NODE)</li>
<li><em>DNF-Net</em></li>
<li><em>1D-CNN</em></li>
</ul>
<h3 id="quick-summary">Quick Summary</h3>
<h5 id="not-counting-the-ensemble-methods">Not counting the ensemble
methods…</h5>
<ul>
<li>TabNet did best on all of its own data sets, but was not the best
model on any other.</li>
<li>NODE each did best on 2 of its own 3 data sets, but not on any
other.</li>
<li>DNF-Net best on one of its own 3 data sets, but not on any
other.</li>
<li>XGBoost was best on the remaining 5 datasets.</li>
</ul>
<h5 id="counting-the-ensemble-methods">Counting the ensemble
methods…</h5>
<ul>
<li>TabNet did best on 2 of its own 3 data sets, but was not the best
model on any other.</li>
<li>DNF-Net and NODE each did best on one of its own 3 data sets, but
not on any other.</li>
<li>XGBoost was best on one dataset.</li>
</ul>
<p>Of those, XGB was notably better on ‘unseen’ data, and comparable to
the best performing ensemble. A simple ensemble was also very
performant. From the paper:</p>
<blockquote>
<p>The ensemble of all the models was the best model with 2.32% average
relative increase, XGBoost was the second best with 3.4%, 1D-CNN had
7.5%, TabNet had 10.5%, DNF-Net had 11.8% and NODE had 14.2% (see Tables
2 and 3 in the appendix for full results).</p>
</blockquote>
<p>As a side note, XGBoost + DL was best, but that defeats the purpose
in my opinion. Presumably any notably more complicated setting will be
potentially better with enough complexity, but unless there is an
obvious way on how to add such complexity, it’s mostly an academic
exercise. However, as the authors note, if search is automated, maybe
the complexity of combining the models is less of an issue.</p>
<h3 id="other-stuff">Other stuff</h3>
<h5 id="kudos">Kudos</h5>
<p>The authors cite the <a
href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">No Free Lunch
theorem</a> in the second paragraph, something that appears to be lost
on many (most?) of these types of papers touting small increases in
performance for some given modeling approach.</p>
<h5 id="issues">Issues</h5>
<p>There are always things like training process/settings that are
difficult to fully replicate. By the time authors publish any paper,
unless exact records are kept, the iterations (including discussions
that rule out various paths) are largely lost to time. This isn’t a
knock on this paper, just something to keep in mind.</p>
<h5 id="opinion">Opinion</h5>
<p>I liked this one in general. They start by giving the competing
models their best chance with their own settings and data, which was
processed and trained in the same way. Even then, those models still
either didn’t perform best, and/or performed relatively poorly on any
other dataset.</p>
<h2
id="regularization-is-all-you-need-simple-neural-nets-can-excel-on-tabular-data">Regularization
is all you Need: Simple Neural Nets can Excel on Tabular Data</h2>
<h3 id="paper-info-1">Paper Info</h3>
<ul>
<li><em>Who</em>: Kadra et al.</li>
<li><em>Where</em>: U of Freiburg, Leibniz U (Germany)</li>
<li><em>When</em>: 2021-06-06 V1</li>
<li><a href="https://arxiv.org/pdf/2106.11189.pdf4">Arxiv Link</a></li>
</ul>
<h3 id="from-the-abstract-1">From the Abstract</h3>
<blockquote>
<p>Tabular datasets are the last “unconquered castle” for deep learning…
In this paper, we hypothesize that the key to boosting the performance
of neural networks lies in rethinking the joint and simultaneous
application of a large set of modern regularization techniques. As a
result, we propose regularizing plain Multilayer Perceptron (MLP)
networks by searching for the optimal combination/cocktail of 13
regularization techniques for each dataset using a joint optimization
over the decision on which regularizers to apply and their subsidiary
hyperparameters.</p>
</blockquote>
<blockquote>
<p>We empirically assess the impact of these regularization cocktails
for MLPs on a large-scale empirical study comprising 40 tabular datasets
and demonstrate that (i) well-regularized plain MLPs significantly
outperform recent state-of-the-art specialized neural network
architectures, and (ii) they even outperform strong traditional ML
methods, such as XGBoost.</p>
</blockquote>
<blockquote>
<p>We emphasize that some of these publications claim to outperform
Gradient Boosted Decision Trees (GDBT) [1, 37], and other papers
explicitly stress that their neural networks do not outperform GBDT on
tabular datasets [38, 22]. In contrast, we do not propose a new kind of
neural architecture, but a novel paradigm for learning a combination of
regularization methods.**</p>
</blockquote>
<h3 id="overview-1">Overview</h3>
<p>This data is more about exploring regularization techniques
(e.g. data augmentation, model averaging via dropout) rather than
suggesting any particular model is superior. Even in the second
paragraph they state their results do not suggest a performance gain
over boosting methods. Their focus is on potentially improving DL for
tabular data through regularization with two hypotheses:</p>
<ul>
<li>Regularization cocktails outperform state-of-the-art deep learning
architectures on tabular datasets.</li>
<li>Regularization cocktails outperform Gradient-Boosted Decision Trees,
as the most commonly used traditional ML method for tabular data.</li>
</ul>
<h3 id="data-1">Data</h3>
<p>Forty total datasets ranging from as little as ~400 observations to
over 400k, and between 4 and 2000 features. All were categorical
targets, with about half binary. All available at openml.org with target
ID provided.</p>
<h3 id="models-explored-1">Models Explored</h3>
<p>Comparison models:</p>
<ul>
<li><em>TabNet</em>: (with author’s proposed defaults)</li>
<li><em>NODE</em>: (with author’s proposed defaults)</li>
<li><em>Autogluon</em>: Tabular: can use other techniques but restricted
to ensembles of neural nets for this demo</li>
<li><em>ASK-GBDT</em>: GB via Auto-sklearn (Note this tool comes from
one of the authors )</li>
<li><em>XGBoost</em>: Original implementation</li>
<li><em>MLP</em>: Multilayer Perceptron - 9 layers with 512 hidden units
each.</li>
<li><em>MLP+D</em>: MLP with Dropout</li>
<li><em>MLP+C</em>: MLP with regularization cocktail</li>
</ul>
<h3 id="quick-summary-1">Quick Summary</h3>
<ul>
<li>To begin, their regularization cocktail approach is the clear winner
on these datasets, having one outright on over 40% of them (based on
table 2).</li>
<li>Standard XGB performed best (or tied for best) 8 of the 40 data
sets, while it or ASK-GBDT were best for 12 datasets combined.</li>
<li>Simple MLP was best once, while MLP with dropout was best 5 times,
while the cocktail method was best in general, across 19 datasets.</li>
<li>The ‘fancy’ DL models were the worst performers across the board.
TabNet never performed best, and NODE only did once, but the latter also
repeatedly failed due to memory issues or run-time limitations (this
memory issue was mentioned in the previous paper also).</li>
<li>Head-to-head, the cocktail beat the standard XGB 26 out of 38 times
with three ties. So it wins 65% of the time against XGB, 70% against
ASK-GBDT, but 60% against either (i.e. some XGB approach).</li>
</ul>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<h3 id="other-stuff-1">Other Stuff</h3>
<h5 id="kudos-1">Kudos</h5>
<ul>
<li>Recognize that tabular data is understudied in mainstream DL
literature</li>
<li>They used a lot of datasets</li>
<li>They look at the simplest DL models for comparison</li>
</ul>
<h5 id="issues-1">Issues</h5>
<ul>
<li><p>I wonder why there was not a single numeric outcome among so many
datasets. Furthermore, some of the data are image classification
(e.g. Fashion-MNIST), so I’m not sure why they’re included. I wouldn’t
use a ‘tabular’ technique when standard computer vision approaches
already work so well.</p></li>
<li><p>I’m not familiar with the augmentation techniques they mention,
which were devised for image classification, but there have been some
used for tabular data for a couple decades at this point that were not
mentioned, including simple upsampling, or imputation methods
(e.g. SMOTE). That’s not a beef with the article at all, I’ve long
wondered why people haven’t been using data augmentation for tabular
data given it’s success elsewhere (including for tabular
data!).</p></li>
<li><p>They use a standard t-tests of ranks, but if we’re going to use
this sort of approach, we’d maybe want to adjust for all the tests done,
and probably for all pairwise comparisons (they show such a table for
the regularization methods). Depending on the approach and cutoff, the
XGB vs. Cocktail difference may not be significant.</p></li>
<li><p>Also, I couldn’t duplicate these p-values with R’s default
settings for Wilcoxon signed rank tests, and there does in fact seem to
be inconsistency between the detailed results and Wilcoxon summaries.
For example, in the regularization tests of Table 9,
<code>Cocktail</code> vs. <code>WD</code> and <code>DO</code> shows two
ties in the first four data sets, yet only 1 tie is reported in the
comparison chart for both (Figure 4). For the models, Table 2 show 3
ties of <code>XGB</code> &amp; the <code>Cocktail</code>, with 1 for
<code>ASK-G</code> and <code>Cocktail</code>, but 2 and 0 are reported
for their Wilcoxon tests. It’s not clear what they did for NODE with all
the NAs. I do not believe these discrepancies, nor adjusting for
multiple comparisons, will change the results (I re-did those
myself).</p></li>
</ul>
<h5 id="opinion-1">Opinion</h5>
<p>If we ignore the regularization focus and just look at the model
comparisons, I’m not overly convinced we have a straightforward victory
for cocktail vs. GB as implied in the conclusion. Results appear to be
in favor of their proposed method, but not enough to be a near-guarantee
in a particular setting, so we’re back to square one of just using the
easier/faster/better tool. I’m also not sure who was questioning the use
of regularization for neural networks or modeling in general, so the
comparison to any model without some form of regularization isn’t as
interesting to me. What is interesting to me is that we have another
round of evidence that the fancier DL models like TabNet do not perform
that well relative to GB or simpler DL architectures.</p>
<h2 id="revisiting-deep-learning-models-for-tabular-data">Revisiting
Deep Learning Models for Tabular Data</h2>
<h3 id="paper-info-2">Paper Info</h3>
<ul>
<li><em>Who</em>: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov,
Artem Babenko</li>
<li><em>Where</em>: Yandex (Russia)</li>
<li><em>When</em>: 2021-06-22</li>
<li><a href="https://arxiv.org/abs/2106.11959">Arxiv Link</a></li>
<li><a href="https://github.com/yandex-research/rtdl">Source
code</a></li>
</ul>
<h3 id="from-the-abstract-2">From the Abstract</h3>
<blockquote>
<p>The necessity of deep learning for tabular data is still an
unanswered question addressed by a large number of research efforts. The
recent literature on tabular DL proposes several deep architectures
reported to be superior to traditional “shallow” models like Gradient
Boosted Decision Trees. However, since existing works often use
different benchmarks and tuning protocols, it is unclear if the proposed
models universally outperform GBDT. Moreover, the models are often not
compared to each other, therefore, it is challenging to identify the
best deep model for practitioners.</p>
</blockquote>
<blockquote>
<p>In this work, we start from a thorough review of the main families of
DL models recently developed for tabular data. We carefully tune and
evaluate them on a wide range of datasets and reveal two significant
findings. First, we show that the choice between GBDT and DL models
highly depends on data and there is still no universally superior
solution. Second, we demonstrate that a simple ResNet-like architecture
is a surprisingly effective baseline, which outperforms most of the
sophisticated models from the DL literature. Finally, we design a simple
adaptation of the Transformer architecture for tabular data that becomes
a new strong DL baseline and reduces the gap between GBDT and DL models
on datasets where GBDT dominates.</p>
</blockquote>
<h3 id="overview-2">Overview</h3>
<p>This paper compares different models on a variety of datasets. They
are interested in the GB vs. DL debate, but like the previous paper,
also interested in how well a simpler DL architecture might perform, and
what steps might help the more complicated ones do better.</p>
<h3 id="data-2">Data</h3>
<p>They have 11 datasets with a mix of binary, multiclass and numeric
targets. Sizes range from 20K to 1M+. There appears to be some overlap
with the first paper (e.g. Higgs, Cover type).</p>
<h3 id="models-explored-2">Models Explored</h3>
<h5 id="baselines">‘Baselines’</h5>
<ul>
<li><em>XGBoost</em></li>
<li><em>CatBoost</em></li>
<li><em>MLP</em></li>
<li><em>ResNet</em></li>
</ul>
<h5 id="dl-comparisons">DL Comparisons</h5>
<ul>
<li><em>SNN</em></li>
<li><em>NODE</em></li>
<li><em>TabNet</em></li>
<li><em>GrowNet</em></li>
<li><em>DCN V2</em></li>
<li><em>AutoInt</em></li>
</ul>
<p>In addition, they look at ensembles of these models, but this is not
of interest to me for this post.</p>
<h3 id="quick-summary-2">Quick Summary</h3>
<p>Note that these refer to the ‘single model’ results, not the results
for ensembles.</p>
<ul>
<li><p>Some form of boosting performed best on 4 of the 11
datasets.</p></li>
<li><p>ResNet was best on four classification tasks, but not once for
numeric targets.</p></li>
<li><p>At this point you won’t be surprised at what doesn’t perform as
well- TabNet, NODE, and similar. TabNet, DCN, and GrowNet were never the
best performer, and the other three were best one time a piece.</p></li>
<li><p>MLP did not perform best on any data, however the authors note
that it ‘is often on par or even better than some of the recently
proposed DL models’.</p></li>
<li><p>They also looked at models with a ‘simple’ transformer
architecture. Their results suggest better performance than the other DL
models, and similar performance to ResNet.</p></li>
</ul>
<h3 id="other-stuff-2">Other Stuff</h3>
<h5 id="kudos-2">Kudos</h5>
<ul>
<li><p>Sharing the source code!</p></li>
<li><p>Recognizing that results at this point are complex at best given
the lack of standard datasets</p></li>
</ul>
<h5 id="issues-2">Issues</h5>
<ul>
<li>They note a distinction between <em>heterogeneous</em> vs. other
types of data. They call data heterogeneous if the predictors are of
mixed data types (e.g. categorical, numeric, count), while something
like pixel data would be <em>homogeneous</em> because all the columns
are essentially the same type. The latter isn’t as interesting to me for
this sort enterprise, and I think the former is what most are thinking
about for ‘tabular’ data, otherwise we’d just call it what it is
(e.g. image or text data), and modeling/estimation is generally quite a
bit easier when all the data is the same type. I do think it’s important
that they point out that GB is better with heterogeneous data, and I
think if you only look at such data, you’d likely see GB methods still
outperforming or at worst on par with the best DL methods.</li>
</ul>
<h5 id="opinion-2">Opinion</h5>
<p>These results seem consistent with others at this point. Complex DL
isn’t helping, and simpler architectures, even standard MLP show good
performance. In the end, we still don’t have any clear winner over GB
methods.</p>
<h2 id="overall-assessment">Overall Assessment</h2>
<p>These papers put together are helpful in painting a picture of where
we are at present with deep learning for tabular data, especially with
mixed data types. In this setting, it seems that more complicated DL
models do not seem to have any obvious gain over simpler architectures,
which themselves do not consistently beat boosting methods. It may also
be the case that for data of mixed data types/sources, boosting is still
the standard to beat.</p>
<p>Even though these articles are geared toward comparisons to
GB/XGBoost, in several settings I’ve applied them, I typically do not
necessarily have appreciably greater success compared to a default
setting random forest (e.g. from the <span class="pack"
style="">ranger</span> package in R), or sufficiently flexible
statistical model. Unfortunately this comparison is lacking from the
papers, and would have been nice to have, especially for smaller data
settings where such models are still very viable. I think a viable fast
model, preferably one without any tuning required (or which simply is
taken off the shelf) should be the baseline.</p>
<p>In that light, for tabular data I think one should maybe start with a
baseline of a penalized regression with appropriate interactions
(e.g. ridge/lasso), or a more flexible penalized approach (GAMM) as a
baseline, the latter especially, as it can at least automatically
incorporate nonlinear relationships, and tools like <span class="pack"
style="">mgcv</span> or <span class="pack" style="">gpboost</span> in R
can do so with very large data (1 million +) in a matter of seconds. In
settings of relatively higher dimensions, interactions and
nonlinearities should be prevalent enough such that basis function,
tree, and DL models should be superior. Whether they are practically so
is the key concern even in those settings. With smaller, noisier data of
less dimension, I suspect the tuning/time effort with present day DL
models for tabular data will likely not be worth it. This may change
very soon however, so such an assumption should be regularly
checked.</p>
<p><br> <br></p>
<p>last updated: 2022-05-02</p>
<p>Neural Net image source from <a
href="https://uc-r.github.io/2018/04/09/feedforward-deep-models/">UC
Business Analytics R Programming Guide</a></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-gorishniy2021tabular" class="csl-entry"
role="doc-biblioentry">
Gorishniy, Yuri, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.
2021. <span>“Revisiting Deep Learning Models for Tabular Data.”</span>
<em>arXiv Preprint arXiv:2106.11959</em>.
</div>
<div id="ref-kadra2021tabular" class="csl-entry" role="doc-biblioentry">
Kadra, Arlind, Marius Lindauer, Frank Hutter, and Josif Grabocka. 2021.
<span>“Regularization Is All You Need: Simple Neural Nets Can Excel on
Tabular Data.”</span> <em>arXiv Preprint arXiv:2106.11189</em>.
</div>
<div id="ref-shwartz2021tabular" class="csl-entry"
role="doc-biblioentry">
Shwartz-Ziv, Ravid, and Amitai Armon. 2021. <span>“Tabular Data: Deep
Learning Is Not All You Need.”</span> <em>arXiv Preprint
arXiv:2106.03253</em>.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
