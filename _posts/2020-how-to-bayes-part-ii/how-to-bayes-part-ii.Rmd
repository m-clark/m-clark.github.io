---
title: "Practical Bayes Part II"
description: |
  Practical steps you can take in your Stan journey to deal with issues and explore your models fully.
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/r_and_stan.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    highlight: pygments
    css: [../../styles.css, 'https://use.fontawesome.com/releases/v5.14.0/css/all.css']
draft: true
tags: [tags, taggy]
categories:
  - bayesian
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = T, 
  eval      = T, 
  message   = F, 
  warning   = F, 
  comment   = NA,
  R.options = list(width = 120),
  cache.rebuild = F,
  cache = T,
  fig.align = 'center',
  fig.asp = .7,
  dev = 'svg',
  dev.args = list(bg = 'transparent')
)

library(tidyverse)
library(broom)
library(kableExtra)
library(visibly)

kable_df <- function(..., digits=3) {
  kable(..., digits=digits) %>% 
    kable_styling(full_width = F)
}

rnd = tidyext::rnd
```


# Better Bayesian Approaches, Part II

We've talked about the basics one can do to run a Bayesian model, and what to do if there is a problem, which was the primary goal of [Part I]().  But it might be nice if we could avoid the problems in the first place, and our model might still be inadequate without any warnings.  So let's engage in some better practices you can use every time to help things run more smoothly, and get more from your models.


## Outline for Better Bayesian Analysis

<div style = 'text-align: center'>
<i class="fas fa-list-ol fa-5x" style="padding: 20px; color: #ff5500"></i>
</div>

<br>

<div style = 'text-align: center'>
<i class="fas fa-hat-wizard fa-5x" style="padding: 20px; color: #ff5500"></i>
</div>

<br>

We'll cover the following steps in more detail, but here is a general outline.

- First generate 'fake data' to assess prior viability
- With adequate priors, start with a simple, but plausible model
- For simple models you do not need many iterations
    - If you are doing standard GLM or simpler versions of common extensions, even the defaults are likely overkill.  For example, a basic linear regression should converge almost immediately.
- Problems (see [Part I]())
  - If Rhat/ESS is issue, run more iterations
  - If `max_treedepth` is issue, increase the default
  - If divergent transitions
      - Check the data, is there some problem that went undiscovered? 
      - Check the data, has it been scaled properly? 
      - Can something more be done about priors?
      - Use visualizations to aid diagnosis
      - Reparameterize model (unlikely if using a higher level package)
      - Get more/better data (unlikely in practice)
      - Get a better model
  - Issues with loo ([see below][Problems at the loo])
- Use posterior predictive checks
    - Nice, but what if it doesn't fit?
        - Get better data    (unlikely in practice)
        - Get a better model
        - If nothing to do, advise others how to get better data/models in the future.
- Explore a more viable model
  - Add interactions
  - Add nonlinear relations
  - Account for other structure (e.g. random effects)
- Compare and/or average models


We'll now demonstrate these steps.

# Example data


<div style = 'text-align: center'>
<i class="fas fa-database fa-5x" style="padding: 20px; color: #ff5500"></i>
</div>
<br>

NEED LINK

[As before](), I'm going to create some data for us to run some basic models with, the same as Part I. As a reminder, the true underlying model has categorical and continuous covariates, interactions, nonlinear relationships, random effects (observations are clustered in groups), and some variables are collinear.  

```{r create-data-setup, echo=FALSE}
library(tidyverse)

create_data <- function( N = 1000, ng = 100, seed = 1234) {
  
  set.seed(seed)
  
  X_mm = cbind(
    # a standard binary
    binary_1 = sample(0:1, N, replace = TRUE),                      
    # a relatively rare categorical
    binary_2 = sample(0:1, N, replace = TRUE, prob = c(.05, .95)),  
    # two partly collinear numeric
    mvtnorm::rmvnorm(N,
                     mean = rep(0, 3),
                     sigma = lazerhawk::create_corr(runif(3, max = .6)))
  )
  
  X_mm = cbind(
    # intercept
    1,
    X_mm,
    # a quadratic effect
    scale(poly(X_mm[,5], 3))[,2:3],
    # interaction of binary variables
    X_mm[,1]*X_mm[,2], 
    # interaction of binary 2 with numeric 1
    X_mm[,2]*X_mm[,3]
  )
  
  # add intercept
  colnames(X_mm) = c(
    'Intercept',
    'b1',
    'b2',
    'x1',
    'x2',
    'x3',
    'x3_sq',
    'x3_cub',
    'b1_b2',
    'b2_x1'
  )
  
  # coefficients
  beta = c(
    3.0,   # intercept
     .3,   # b1
    -.3,   # b2
     .5,   # x1
     .0,   # x2
     .3 ,  # x3 
     .3,   # x3_sq
    -.2,   # x3_cub
     .5,   # b1_b2 
    -.5    # b2_x1
  )
  
  # create target variable/linear predictor
  y = X_mm %*% beta
  
  # add random effect
  groups = sort(sample(1:ng, N, replace = T))
  
  re = rnorm(ng, sd = 1)[groups]  # re sd = 1
  
  # add re and residual noise
  y = y + re + rnorm(N)
  y = cbind(y, groups)
  colnames(y) = c('y', 'group')
  
  as_tibble(cbind(X_mm, y))
}


```

```{r save-true-parameters, echo=FALSE}
true_params = data.frame(
  parameter = c(
    'intercept',
    'b1',
    'b2',
    'x1',
    'x2',
    'x3 ',
    'x3_sq',
    'x3_cub',
    'b1_b2 ',
    'b2_x1',
    're sd',
    'sigma '
  ), 
    value = c(
    3.0,
     .3,
    -.3,
     .5,
     .0,
     .3,
     .3,
    -.2,
     .5,
    -.5,
     .5,
    1.0
  )
)
```



For our purposes so we'll create a data frame with the total sample size of 1000.  

```{r create-data}
# create the primary data frame

main_df = 
  create_data(N = 1000) %>% 
  as_tibble() %>% 
  select(group, b1:x3, y) %>% 
  mutate(
    b1 = factor(b1),   # will help with visuals
    b2 = factor(b2)
  )
```

# Simulate from priors

A first step is to produce some viable priors.  But the obvious question is, what priors should we choose?  Thankfully, for standard models there is not much guesswork involved. Bayesian analysis has been around a long time, so the bulk of the work has been done for you.  Even default settings should not affect things much, especially for rstanarm, which has some basic defaults that are informed by the data.  However, due to the flexibility of the brms modeling functions, some priors are unspecified and left flat (i.e. uniform), which is something we definitely don't want. And even defaults could still cause problems for more complex situations.  So how might we choose better ones?


The basic idea here is to generate parameters (e.g. regression coefficients) based on the prior distributions for those parameters, predict data based on those prior draws, and then compare the predictions to our observed target variable.  The brms package makes this very easy to do.  We will check the following types of priors.

```{r prior-table, echo=FALSE}
tibble(
  ` ` = c('Note:', 'Regression coefficients:', 'Intercept:', 'Variances:'),
  `Prior set 0` = c('similar to brms', 'uniform', 'default', 'default'),
  `Prior set 1` = c('similar to rstanarm', 'normal, diffuse', 'default', 'default'),
  `Prior set 2` = c('similar to rstanarm', 'normal', 'default', 'default'),
  `Prior set 3` = c('If data is standardized, this would be very reasonable', 'Normal(0, 1)', 'default', 'default'),
  `Prior set 4` = c('restricts range of intercept to more plausible values', 'Normal(0, 1)', 'mean of `y` (~3)', 'default'),
  `Prior set 5` = c('restricts range of sigma to more plausible values', 'Normal(0, 1)', 'based on mean of `y` (~3)', 'based on sd of `y` (~1)')
) %>% 
  DT::datatable(options = list(
    dom = 't',
    autoWidth = T,
    columnDefs = list(list(
      targets = '', orderable = FALSE
    )),
    scrollX = TRUE
  ), rownames = FALSE) %>% 
  DT::formatStyle(0:6, "white-space"="nowrap") %>% 
  DT::formatStyle(columns = 1, fontWeight = 'bold', `text-align` = 'left')
```


We can use <span class="func" style = "">pp_check</span> to examine the prior-generated data versus the observed target `y`, but I wait to show them all together at the end.

```{r sample-prior, results='hide'}
library(brms)

pr_uniform = prior(uniform(-100, 100), lb = -100, ub = 100, 'b')

model_default_prior = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_uniform
)

# pp_check(model_default_prior, nsamples = 50)

# diffuse normal for reg
pr_norm_b_0_10 = prior(normal(0, 10), 'b')

model_0_norm_b_0_10 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_0_10
)

# pp_check(model_0_norm_b_0_10, nsamples = 50)

# rstanarm-like prior
pr_auto = sjstats::auto_prior(
  y ~ b1 + b2 + x1 + x2 + x3,
  data = main_df,
  gaussian = TRUE
)

model_auto_prior = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_auto
)

# pp_check(model_auto_prior, nsamples = 50)


pr_norm_b_0_1 = prior(normal(0, 1), 'b')

model_0_norm_b_0_1 = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_0_1
)

# pp_check(model_0_norm_b_0_1, nsamples = 50)

pr_norm_b_norm_int = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept')#,
)

model_0_norm_b_0_1_norm_Int = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_norm_int
)

# pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50)


pr_norm_b_norm_int_t_sigma = c(
  prior(normal(0, 1), class = 'b'),
  prior(normal(3, 1), class = 'Intercept'),
  prior(student_t(10, 1, 1), class = 'sigma')
)

model_0_norm_b_0_1_norm_Int_sigma = brm(
  y ~ b1 + b2 + x1 + x2 + x3, 
  data = main_df,
  iter = 1000,
  sample_prior = 'only',
  prior = pr_norm_b_norm_int_t_sigma
)

# pp_check(model_0_norm_b_0_1_norm_Int_sigma, nsamples = 50)
```

The following plot shows the model predictions based on priors only. We restrict the range of values for display purposes, so note that some of the priors would generate more extreme results.  For example, the default prior setting could generate values into the $\pm$ 500 and beyond. I also mark the boundaries of the observed target variable. 

```{r proposed-priors-plot, echo=FALSE}
# can't do alpha here
library(bayesplot)
library(patchwork)
color_scheme_set(scico::scico(6, palette = 'batlow', begin = .1, end = .9, direction = -1)) 

{
  {
    pp_check(model_default_prior, nsamples = 50, alpha = .2) +
      labs(title = 'Defaults')
  }/
    # theme(axis.text.x = element_text(size= 6)) +
    {
      pp_check(model_0_norm_b_0_10, nsamples = 50, alpha = .2) +
        labs(title = 'Normal b (diffuse)')
    }/
    {
      pp_check(model_auto_prior, nsamples = 50, alpha = .2) +
        labs(title = 'Auto prior')
    }
} * 
  geom_vline(xintercept = c(min(main_df$y), max(main_df$y)), color = '#00aaff80') * 
  scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50))  *
  lims(y = c(0,.35)) |
  
{
  {
    pp_check(model_0_norm_b_0_1, nsamples = 50, alpha = .2) +
    labs(title = 'Normal b tighter')
  } /
{
  pp_check(model_0_norm_b_0_1_norm_Int, nsamples = 50, alpha = .2) +
    labs(title = 'Add Int prior') 
  } /
    pp_check(model_0_norm_b_0_1_norm_Int_sigma, nsamples = 50, alpha = .2) +
    labs(title = 'Add sigma prior')
} * 
  geom_vline(xintercept = c(min(main_df$y), max(main_df$y)), color = '#00aaff80') * 
  scale_x_continuous(breaks = seq(-25, 25, by = 10), limits = c(-50, 50)) *
  lims(y = c(0,.35))
```


So given that our target variable is between `r round(min(main_df$y))` and `r round(max(main_df$y))`, it seems that just adding some basic, data-informed information to our priors resulted in more plausible results.  This will generally help our models be more efficient and better behaved. Note that if all else fails, you can use a convenience function like <span class="func" style = "">auto_prior</span> demonstrated above.


# Run and Summarize the Baseline Model

Now let's run a baseline model, one that's simple but plausible.  Given that there will eventually be additional complexities, I'll go ahead and add some iterations, and increase max tree depth and adapt delta now to make the code reusable.

```{r model-baseline, results='hide'}
library(brms)

pr = c(
  prior(normal(0, 1), class = 'b'),
  prior(student_t(10, 1, 1), class = 'sigma'),
  prior(student_t(10, 1, 1), class = 'sd')  # prior for random intercept std dev
)

model_baseline = brm(
  y ~ b1 + b2 + x1 + x2 + x3 + (1 | group), 
  data    = main_df,
  warmup  = 5000,
  iter    = 6000,
  thin    = 4,
  prior   = pr, 
  cores   = 4,
  control = list(
    adapt_delta   = .95,
    max_treedepth = 15
  ),
  save_pars = save_pars(all = TRUE)  # potentially allows for more  more post-processing functionality
)
```

```{r model-baseline-summary}
summary(model_baseline)
```

For reporting purposes all you need are the `Estimate` and lower and upper bounds.  If you want a visual approach, you can use something like the following types of plots.

```{r model-baseline-summary-plots}
mcmc_plot(model_baseline, type = 'areas')
mcmc_plot(model_baseline, type = 'intervals')
```

The tidybayes package offers some nice options as well.

```{r tidybayesdemo}
library(tidybayes)

# get_variables(model_baseline) %>% as_tibble() # to see variable names as required for plotting.

# grab fixed effects - intercept
tidy_plot_data_fe = model_baseline %>%
  spread_draws(`^b_(b|x).*`, regex = TRUE) %>% 
  pivot_longer(b_b11:b_x3, names_to = 'coefficient')

tidy_plot_data_fe %>%
  ggplot(aes(y = rev(coefficient), x = value)) +
  geom_vline(xintercept = c(-.25, .25), color = 'gray92', size = .5) +
  stat_dotsinterval(
    aes(fill = stat(abs(x) < .25)),
    quantiles = 40,
    point_color = '#b2001d',
    interval_color = '#b2001d',
    interval_alpha = .6
  ) +
  scico::scale_fill_scico_d(begin = .2, end = .6) +
  labs(y = '') +
  guides(fill = 'none') +
  theme_clean()
```


```{r tidybayesdemo-re, echo=FALSE, eval=FALSE}
# grab random effects
# tidy_plot_data_re = model_baseline %>%
#   spread_draws(r_group[group, term]) 
# tidy_plot_data_re %>%
#   ggplot(aes(y = group, x = r_group, slab_colour = stat(abs(x) < .25))) +
#   stat_dotsinterval() +
#   geom_vline(xintercept = c(-.25, .25), linetype = "dashed") +
#   scale_fill_manual(values = c("gray80", "skyblue")) +
#   scale_color_manual(values = c("gray80", "skyblue")) +
#   labs(y = 'Coefficient') +
#   guides(slab_color = 'none') +
#   theme_clean()
```



## Check priors


We might still be concerned about how influential our priors were.  So how can we check whether our priors were informative? The following will do a simple check of whether the posterior standard deviation is greater than 10% of the prior standard deviation[^lakeland].  Having an informative prior isn't really a problem in my opinion, unless it's more informative than you wanted.  For example, shrinkage of a coefficient towards zero will generally help avoid overfitting.



```{r informative-priors}
prior_summary(model_baseline)
bayestestR::check_prior(model_baseline)
```

These results suggest that we might be more informative, but for the intercept, which we largely care less about, and for the factor that is highly unbalanced, which has no obvious solution.  I would be fine with this result.



## Explore and visualize effects

We can plot effects easily with brms. The <span class="func" style = "">conditional_effects</span> function is what we want here.  Without interactions or other things going, on they aren't very interesting, but it's a useful tool nonetheless.


```{r model-start-explore}
conditional_effects(model_baseline, 'b2')
```

We can also use the hypothesis function to test for specific types of effects.  By default they provide a one-sided probability and uncertainty interval.  For starters,  we can just duplicate what we saw in the previous summary for the `b2` effect.  The only benefit is to easily obtain the one-sided p-value (that `b2` is less than zero) and the corresponding *evidence ratio*, which is just `p/(1-p)`.

```{r model-start-hype}
hypothesis(model_baseline, 'b21 < 0')
```

But we can really try anything, which is the power of this function. The following tests whether the combined effect of our categorical covariates is greater than zero.

```{r model-start-hype2}
hypothesis(model_baseline, 'b11 + abs(b21) > 0')
```


## Model Effectiveness

A natural question to ask is how useful our model actually is, which then suggests we need to know how to define such utility.  Such an assessment definitely cannot be made with something like 'statistical significance'. Science of any kind is nothing without prediction, so we we can start there.

### Posterior predictive checks

Posterior predictive checks are a key component of bayesian analysis.  The prior checks we did before are just a special case of this.  Here we actually use the posterior distributions of parameters to generate the data to compare what the model implies and what we actually observe. Doing so can give insight to where the model fails.

```{r model-baseline-ppcheck}
pp_check(model_baseline, nsamples = 100)
pp_check(model_baseline, nsamples = 10, type ='error_scatter_avg', alpha = .1)
```

In this case we see good alignment between model and data, and no obvious pattern to the types of errors we are getting. We do see that the model does not capture the most extreme value well, but that's not terribly surprising.

Consider predictions with and without random effects.  You have to look closely, but the random effect predictions appear to do better with the predicting more in the tails.

```{r model-baseline-ppcheck-re, echo=FALSE}
library(patchwork)
{pp_check(model_baseline, nsamples = 50, re_formula = NA) + labs(title = 'No RE') + guides(color = 'none') +
pp_check(model_baseline, nsamples = 50) + labs(title = 'With RE')}  * lims(x = c(-3,10)) 
```



We can use the same approach to look at specific statistical measures.  The following suggests our model is okay with capturing the minimum value, but typically underestimates the maximum value, which we noted earlier.

```{r model-baseline-ppcheck-med-max}
pp_check(model_baseline, nsamples = 100, type ='stat', stat='median')
pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'max')
```


We can define any function to use for our posterior predictive check.  The following shows how to examine the 10th and 90th quantiles.  Minimum and maximum values are unlikely to be captured very well due to their inherent variability, so looking at less extreme quantiles (e.g. 10^th^ or 90^th^ percentile) might be a better way to assess whether the model captures the tails of a distribution.

```{r model-baseline-ppcheck-qs, echo=1:6, eval=FALSE}
q10 = function(y) quantile(y, 0.1)
q90 = function(y) quantile(y, 0.9)

pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'q90')

pp_check(model_baseline, nsamples = 100, type ='stat_2d', stat = c('q10', 'q90'))

# library(patchwork)
# {pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'min') +
#     pp_check(model_baseline, nsamples = 100, type ='stat', stat = 'max')}  /
#   {pp_check(model_baseline, nsamples = 100, type ='stat_grouped', stat = 'min', group = 'b2') + 
#       guides(fill = 'none') +
#     pp_check(model_baseline, nsamples = 100, type ='stat_grouped', stat = 'max', group = 'b2') + 
#       guides(fill = 'none')}
```

### Bayes R-squared

In this scenario, we can examine the amount of variance accounted for in the target variable by the covariates. I don't really recommend this beyond linear models assuming a normal distribution for the target, but people like to report it. Conceptually, it is simply a (squared) correlation of fitted values to the observed, so can be seen as descriptive statistic.  Since we are Bayesians, we also get a ready-made interval for it, as it is based on the posterior predictive distribution.  But to stress the complexity in trying to assess this, in this mixed model we can obtain the result with the random effect included (conditional) or without (unconditional).  Both are reasonable ways to express the statistic, but the one including the group effect naturally will be superior, assuming the variance is notable in the first place.

https://avehtari.github.io/bayes_R2/bayes_R2.html

Andrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari (2018). R-squared for Bayesian regression models. The American Statistician, doi:10.1080/00031305.2018.1549100. [Online Preprint](http://www.stat.columbia.edu/~gelman/research/unpublished/bayes_R2_v3.pdf).

```{r model-baseline-r2}
bayes_R2(model_baseline)                   # random effects included
bayes_R2(model_baseline, re_formula = NA)  # random effects not included
# performance::r2_bayes(model_baseline)    # alternative provides both
```

To show the limitation of R^2^, I rerun the model using a restrictive prior on the intercept. Intercepts for the resulting models are different but the other fixed effects are basically the same.  The R^2^ suggests equal fit.

```{r r2-not-pp, echo=FALSE}
model_r2_vs_pp = update(
  model_baseline, 
  cores = 4,
  prior = c(
    prior(normal(0, 1), class = 'b'),
    prior(normal(5, .1), class = 'Intercept'),
    prior(student_t(10, 1, 1), class = 'sigma'),
    prior(student_t(10, 1, 1), class = 'sd')  # prior for random intercept
  ),
  save_all_pars = TRUE
)

tibble(
  model = c('baseline', 'modified'),
  rbind(as_tibble(bayes_R2(model_baseline, re_formula = NA)),
        as_tibble(bayes_R2(model_r2_vs_pp, re_formula = NA)))
) %>% 
  kable_df()
```

However, a posterior predictive check shows clearly the failure of the modified model to capture the data.

```{r r2-not-pp-check, echo=FALSE}
{pp_check(model_baseline, re_formula = NA) + labs(subtitle = 'Baseline Model')} /
  {pp_check(model_r2_vs_pp, re_formula = NA) + labs(subtitle = 'Modified Model')}
```

A variant of R^2^, the 'LOO' R^2^, is also available via the <span class="func" style = "">loo_R2</span> function. LOO stands for *leave-one-out*, as in leave-one-out cross-validation.  It's based on the residuals from the leave one out predictions.  You can think of it as a better 'adjusted R^2^'. The results suggests that the LOO R^2^ actually picks up the difference in models, and would be lower for the modified model, even if we included the random effects

```{r looR2, echo=FALSE}
tibble(
  model = c('baseline', 'modified'),
  rbind(as_tibble(loo_R2(model_baseline, re_formula = NA)),
        as_tibble(loo_R2(model_r2_vs_pp, re_formula = NA)))
) %>% 
  rename(`LOO R2` = value) %>% 
  kable_df()
```



<!-- ### Model comparison -->

<!-- We will use estimates like WAIC and loo for model comparison later.  They are essentially used as you would AIC, or root mean squared error to compare models in a predictive fashion.  In addition We only really have our  -->

<!-- Even now, we can use loo as a diagnostic to possibly discover problematic observations, -->

<!-- ```{r} -->
<!-- loo(model_baseline) -->
<!-- WAIC(model_baseline) -->
<!-- post_prob(model_baseline, model_r2_vs_pp) -->
<!-- ``` -->

<!-- The interesting thing here is that we have a grossly inefficient model, yet none we have nothing that notes any issues -->









# Prediction & Model Comparison

In general, a model is judged most by whether it has practical value.  Even if we think a model is effective, there still might be another model that can do better.  So, if we're doing what we should, we generally will have a couple models to compare with one another. And one of the best ways to compare them is via prediction, especially by predicting on data the model wasn't trained on to begin with.

For our demonstration, we will add two new models.  The first adds interactions, the second adds a nonlinear relationship for one of the variables to that model, and is the closest to the underlying data generating mechanism.


```{r model-complex, results='hide'}
model_interact = update(
  model_baseline,
  . ~ . + b1:b2 + b2:x1,
  cores = 4
)

model_interact_nonlin = update(
  model_interact,
  . ~ . + s(x3),
  cores = 4
)
```


## Basic prediction

With models in hand, let's look at our basic predictive capabilities.  We can get fitted values which include 'confidence' intervals, or predictions, which include 'prediction' intervals that include the uncertainty for a new observation.  We can specify these as follows.  First we create a small data set to make some predictions on.  It will include both values for of the binary covariates, and the means of the numeric (0).

```{r fitted-predict}
prediction_data = crossing(
  b1 = 0:1,
  b2 = 0:1,
  x1 = 0,
  x2 = 0,
  x3 = 0
)

# fitted values
head(fitted(model_baseline))  

# new predictions
predict(model_baseline, newdata = prediction_data, re_formula = NA)
```

In general, we'd always like to visualize the predictions.  We can do so as we did before with the <span class="func" style = "">conditional_effects</span> function.  For the third plot of the nonlinear effect, I  modify the basic conditional effects plot that brms provides for a slightly cleaner visualization.

```{r conditional-effects-model-comparison, echo=1:3}
conditional_effects(model_baseline, effects = 'x2')
conditional_effects(model_interact, effects = 'x1:b2')
init = conditional_effects(model_interact_nonlin, effects = 'x3', spaghetti = T)

# plot(
#   init,
#   # points = T, # this (nor rug) argument doesn't really make sense nor is consistent with other options
#   spaghetti_args = list(colour = alpha('#ff5500', .05)),
#   line_args = list(colour = alpha('#00aaff', .75)),
#   theme = theme_clean()
# ) + 
#   geom_point(aes(x = x3, y = y), data = main_df) +
#   theme_clean()
```


```{r conditional-effects-model-comparison-show, echo=F, layout='l-body-outset'}
library(modelr)

# main_df %>%
#   group_by(b2) %>%
#   data_grid(
#     x1 = seq_range(x1, n = 101),
#     b1 = 0,
#     b2 = factor(0:1),
#     x2 = 0,
#     x3 = 0
#   ) %>%
#   add_fitted_draws(model_interact, n = 100, re_formula = NA) %>%
#   ggplot(aes(x = x1, color = b2)) +
#   geom_line(aes(y = .value, group = paste(b2, .draw)), alpha = .1) +
#   geom_point(aes(y = y), data = main_df, alpha = .25) +
#   scale_color_brewer(palette = "Dark2") +
#   theme_clean()

plot_dat = main_df %>%
  group_by(b2) %>%
  data_grid(
    x3 = seq_range(x3, n = 101),
    b1 = 0,
    b2 = 0,
    x2 = 0,
    x1 = 0
  ) %>%
  add_fitted_draws(model_interact_nonlin, n = 100, re_formula = NA) 

plot_dat %>%
  ggplot(aes(x = x3)) +
  # geom_point(aes(y = y), data = main_df,  alpha = .05) +
  geom_line(aes(
    y = .value,
    color = x3,
    group = paste(b2, .draw)
  ),
  alpha = .1,
  show.legend = FALSE) +
  geom_line(aes(
    y = .value
  ),
  color = '#b0012d88',
  show.legend = FALSE, data = plot_dat %>% group_by(x3) %>% summarise(.value = mean(.value))) +
  scico::scale_color_scico() +
  theme_clean()
```



## Model Comparison

In typical situations it is good to have competing models, attempting to see if improvements can be made in one way or another.  In the Bayesian approach, we can use estimates like *WAIC* and *LOOIC* for model comparison, much like you would AIC to compare models in traditional frameworks.  The values themselves don't tell us much, but in comparing models, lower means less predictive error for 'ic' metrics, which is what we want. Like in the traditional framework, LOOIC is ~ -2\*expected log posterior density (*ELPD*)[^bayesaic], and since we're Bayesian, we even have estimates of uncertainty for these values as well.

[^bayesaic]: Much like how AIC works with the -2*log likelihood.

However, we will have to consider things a little differently in the Bayesian context.  Consider the following two thoughts.


>The general issue is that with *unregularized* estimation such as least squares or maximum likelihood, adding parameters to a model (or making a model more complex) leads to overfitting. With regularized estimation such as multilevel modeling, Bayesian inference, lasso, deep learning, etc., the regularization adds complexity but in a way that reduces the problem of overfitting. So traditional notions of model complexity and tradeoffs are overturned. ~ Andrew Gelman

> Sometimes a simple model will outperform a more complex model... Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.  ~ Radford Neal

The take-home message here is that simpler is not always better.  And to be frank, using penalized approaches in the non-Bayesian approach should probably be our default model.  Such approaches approximate a Bayesian one with specific priors.  In our fully Bayesian situation, we will have to 


With our new models in place, we can now make some comparisons using <span class="func" style = "">loo_compare</span>.  It shows the 'best', i.e. lowest valued, model first followed by the others.  The result regards the total expected log probability for the leave-one-out observations.  We also get stuff like `p_loo`, which is the effective number of parameters, and `Pareto k values`, which we'll talk about soon.

```{r loo-compare}
model_baseline = add_criterion(model_baseline,  'loo')
model_interact = add_criterion(model_interact, 'loo')
model_interact_nonlin = add_criterion(model_interact_nonlin, 'loo')

# example
loo(model_baseline) 

loo_compare(
  model_baseline, 
  model_interact,
  model_interact_nonlin
)
```

Let's compare several metrics available to us.  In this particular setting, all are generally in agreement in the rank order of the models.  However, there is no meaningful difference between the baseline and interaction models, while information criteria would more clearly prefer the latter.

```{r loo-compare2, echo=FALSE}
mods = list(
  baseline = model_baseline,
  interact = model_interact,
  interact_nonlin = model_interact_nonlin
)

map_df(mods, function(x) data.frame(R2 = bayes_R2(x)[,'Estimate']), .id = 'model') %>% 
  left_join(map_df(mods, function(x) data.frame(loo_R2 = loo_R2(x)[1]), .id = 'model')) %>% 
  left_join(map_df(mods, function(x) data.frame(WAIC = WAIC(x)$estimates[3]), .id = 'model')) %>% 
  left_join(map_df(mods, function(x) data.frame(LOOIC = loo(x)$estimates[3], 
                                                ELPD  = loo(x)$estimates[1]), .id = 'model')) %>% 
  mutate(
    weight = loo_model_weights(model_baseline, model_interact, model_interact_nonlin)) %>% 
  kable_df(digits = 2) %>% 
  add_footnote('Odds are relative to baseline')
```


While we have several metrics, for model comparison, we'd like to stick to the IC type appoach. As far as using WAIC vs. LOOIC, the latter has better diagnostics for noting whether there are potential problems in using it.  In practice, it likely wouldn't differ much.  As we noted, LOOIC reflects the ELPD, which is used in constructing the model weights[^waic_weights], the model weights can then be used in making final predictions.

[^waic_weights]: Technically we can use WAIC to produce weights like we do with AIC, e.g. `exp(waic) / sum(exp(all_waics))`, but this isn't recommended.  The stacking approach allows similar models to share their weight, while more unique models will more or less keep their weight as additional models are added.




### Problems at the loo

https://mc-stan.org/loo/articles/loo2-weights.html


issues- overfitting, time, complexity is more accurate portrayal of nature even if not 'best'

After the model warnings discussed in [Part I](), the next most common point of confusion I see with clients is with model comparison.  Part of the reason is that this is an area of ongoing research and development, and most of the tools and documentation are notably technical.  Another reason is that these are not perfect tools.  They can fail to show notable problems for models that are definitely misspecified, and flag models that are essentially okay.  Sometimes they flag models that other indicators may suggest are better models relatively speaking, which actually isn't a contradiction, but which may indicate an overfit situation.  Another thing to think about from Gelman:



So what should we do here? As an example, let's start with our complex model and get the leave-one-out criterion measure.  I will avoid as much technical jargon as possible so that the applied modeler can get on with things.  The first part are the stats that are used in the previous model comparison and weighting, particularly the elpd_loo[^looic].  We also get  influence statistics for our observed data.  

```{r loo-basic}
loo(model_baseline)
loo(model_interact_nonlin)
```


```{r pareto-compare, echo=FALSE}
pareto_probs = loo(model_interact_nonlin)

# str(pareto_probs)

loo_pit_interact_nonlin = pp_check(model_interact_nonlin, type = 'loo_pit')
loo_pit_baseline = pp_check(model_baseline, type = 'loo_pit')

# bind_rows(
#   loo_pit_baseline %>% layer_data(),
#   loo_pit_interact_nonlin %>% layer_data(),
#   .id = 'model'
# ) %>% 
#   mutate(
#     model = factor(model, labels = c('baseline', 'complex')),
#     alpha = rep(c(1, .5), e = 1000)
#     ) %>% 
#   rename(Uniform = x, `LOO-PIT` = y) %>% 
#   ggplot(aes(Uniform, `LOO-PIT`)) +
#   geom_line(aes(color = model, alpha = I(alpha)), size = 2) +
#   geom_line(aes(y = theoretical), color = '#990021', alpha = .5) +
#   scico::scale_color_scico_d(begin = .25, end = .75) +
#   theme_clean()

# main_df %>% 
#   slice(which(pareto_probs$diagnostics$pareto_k > .5)) %>% 
#   bind_cols(pareto_probs$diagnostics %>%
#               as_tibble() %>%
#               slice(which(pareto_probs$diagnostics$pareto_k > .5))) %>% 
#   bind_cols(pareto_probs$pointwise %>%
#               as_tibble() %>%
#               slice(which(pareto_probs$diagnostics$pareto_k > .5)))
```

- Resource: [The LOO glossary](https://mc-stan.org/loo/reference/loo-glossary.html)

#### Pareto values

If you happen to see Pareto values in the 'bad' or 'very bad' group, what does it mean?  You can read the definition provided [here](https://mc-stan.org/loo/reference/loo-glossary.htmll#pareto-k-estimates), but I doubt it will help many due to the background knowledge needed to parse it.  However you can just understand it as an *outlier* diagnostic, and if it is a problem, it means your LOOIC may not be good for comparing models.  As in the standard model setting setting, outliers indicate *model incompetence*, or in other words, its inability to understand such observations.  Unless you have reason to suspect something wrong in the data (e.g. an incorrect value/typo), an outlier is a sign that your model is not able to capture the data fully.  It definitely is not a reason to remove the observation.

If you have values > .7, you may recalculate LOOIC with the options provided by the <span class="func" style = "">loo</span> function, getting a better estimate that could then be used in, for example, model stacking for prediction.  If you don't have many discovered outliers, it probably won't make much difference in your final estimates and conclusions.   The output for Pareto values doesn't even save the row identifying information that would make it easy to find which observations are the problem, but you can do something like the following.


```{r pareto-obs}
problems = pareto_probs$pointwise %>% 
  data.frame() %>% 
  rowid_to_column() %>% 
  group_by(flag = influence_pareto_k > .5) %>% 
  filter(flag) %>% 
  pull(rowid)

model_interact_nonlin$data %>% 
  slice(problems)
```

As we might have expected, the observations with the more extreme target values are likely to be  problems, but for some of these, there is nothing to suggest why they might be difficult, and it's even harder to speculate in typical modeling situations with more predictors and complexity. Furthermore, outside of additional interactions, which might then become difficult to interpret, there is little we can do about this, or at least, is not obvious.


#### The rabbit hole of model comparsion

If you start to look more into this, there are numerous technical articles, whole websites, and various discussions regarding how to go about these results.  I'm guessing most don't want to try and parse technical information only to still feel confused about what to actually do with the results.  Furthermore, it's not clear to me that issues regarding specific statistics for model comparison should be a basis for altering a model, unless there is an obvious path for doing so (which begs the question of why you weren't already exploring that model).  The suggestions typically amount to 'Your model is misspecified' no matter what the outcome.  The are also suggestions that one should use posterior predictive checks (PPCs) are somehow to be used to detect the problem, but PPC don't 'detect' anything in and of themselves, nor have any standard metric to report. , so this is all kind of a mess here.



https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s


The usual cases are

- misspecified models MC: all models are misspecified, so how will knowing this help us finish the project?
- models with parameters which see the information only from one observation each (e.g. 'random' effect models)  MC: sorry, but this is reality for many such models.  Assuming we can't increase this, which is practically every situation, what are we to do about it?
- otherwise flexible models  MC: how is 'flexible' defined? Overly vague priors?
- [A quick note what I infer from p_loo and Pareto km values](https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446)
- [Recommendations for what to do when k exceeds 0.5 in the loo package?](https://discourse.mc-stan.org/t/recommendations-for-what-to-do-when-k-exceeds-0-5-in-the-loo-package/3417)
- [Improve model with some observations Pareto >0.7](https://discourse.mc-stan.org/t/improve-model-with-some-observations-pareto-0-7/17500)
- [Bayesian data analysis - roaches cross-validation demo](https://rawgit.com/avehtari/modelselection_tutorial/master/roaches.html#22_cross-validation_checking)
- [16 What to do if I have many high Pareto k’s?](https://avehtari.github.io/modelselection/CV-FAQ.html#16_What_to_do_if_I_have_many_high_Pareto_(k)%E2%80%99s)
- [Pareto K for outlier detection 1](https://discourse.mc-stan.org/t/pareto-k-for-outlier-detection/12177/9)





> These measures are not independent. If there are many high Pareto k values as in case of model 4, then elpd_loo (or looic) can’t be trusted. Even if there would be no high Pareto k values, R^2 can’t be trusted if p_loo is relatively high compared to the total number of parameters or the number of observations as in case of model 2-4. So there is no contradiction here, but you need to take into account if diagnostics tell you that some other measures can’t be used. ~ [Aki Vehtari](https://discourse.mc-stan.org/t/good-pp-check-and-r-square-but-large-pareto-k-values/17678)


### Solutions with Model Comparison

- Avoid the problem and fit the model that includes everything of interest, assuming you have a decent data size to do so.
- If some application performance is obvious and available to assess, pick a model that does best there.
- If trying to select among many competing models, e.g. feature selection, you should consider why you are in this situation.  If you don't have much data, then the usual model selection criteria may lead you notably astray.  If you have a lot of data, consider why you need to select a subset and not merely use all available.  If you are somewhere in between, note that you'll likely spend a lot more time here and still not be confident in the results.  However, there are approaches, such as those in the <span class="pack" style = "">projpred</span> package, that might be useful, but likely will only work for simpler models.
- Use the metrics noted above, e.g. LOOIC, 

https://avehtari.github.io/modelselection/CV-FAQ.html#5_How_to_use_cross-validation_for_model_selection


```{r eval=FALSE, echo=FALSE}
library(projpred)

test_mod = update(model_interact, .~. - (1|group), cores = 4)
ref_model = get_refmodel(test_mod)
res = cv_varsel(ref_model)
res
suggest_size(res)
# plot predictive performance on training data
varsel_plot(res, stats = c('elpd', 'rmse'))
```


## Averaging models

With the previous statistics for model comparison we can obtain relative model weights, using the <span class="func" style = "">model_weights</span> function. This essentially spreads the total probability of the models across all those being compared. These weights in turn allow us to obtain (weighted) average predictions.  The key idea being that we do not select a 'best' model, but rather combine their results for predictive purposes[^bma].

[^bma]: Some might be familiar with Bayesian model averaging. Conceptually we aren't changing much, but BMA assumes that one of our models is the true model, while the stacking approach underlying these weights does not. It is also different from conventional stacking in machine learning in that we are trying to average posterior predictive distributions, rather than merely point estimates.

We can start by comparing the first two models.  Adding the interactions helped, and comparing the weights suggests that it would be contributing most to averaged predictions.


```{r posterior-prob, echo=FALSE}
loo_model_weights(model_baseline, model_interact)
```

If we compare the baseline to our most complex model, almost the entirety of the weight is placed on the latter.

```{r posterior-prob-nonlin}
loo_model_weights(model_baseline, model_interact_nonlin)
```

Now we compare all three, with roughly the same conclusion.


```{r posterior-prob-all, echo=FALSE}
model_weights(model_baseline,
              model_interact,
              model_interact_nonlin) %>%
  t() %>%
  as_tibble() %>%
  kable_df(digits = 5)
```

Now what about those average predictions?  Let's create a data frame that sets the continuous covariates at their means, and at each level of the categorical covariates.  We then will make average predictions for those observations using <span class="func" style = "">pp_average</span>.


```{r ppaverage}
prediction_data = crossing(
  b1 = 0:1,
  b2 = 0:1,
  x1 = 0,
  x2 = 0,
  x3 = 0
)

average_predictions = pp_average(
  model_baseline,
  model_interact,
  model_interact_nonlin,
  newdata = prediction_data,
  re_formula = NA
)
```


```{r ppaverage-show, echo=FALSE}
average_predictions %>% 
  as_tibble() %>% 
  bind_cols(prediction_data, .) %>% 
  kable_df()
```





## Cross-validation


```{r cv, eval = F}
library(future)
plan(multiprocess)
model_interact_nonlin_cv = kfold(model_interact_nonlin, K = 5, chains = 1, save_fits = TRUE)
plan(sequential)

str(model_interact_nonlin_cv, 0)
test = kfold_predict(model_interact_nonlin_cv, newdata = prediction_data)

rmse <- function(y, yrep) {
  yrep_mean <- colMeans(yrep)
  sqrt(mean((yrep_mean - y)^2))
}
rmse(y = test$y, yrep = test$yrep)
```

















# Resources

[Gelman's prior check approach](https://statmodeling.stat.columbia.edu/2019/08/10/)

https://mc-stan.org/docs/2_24/stan-users-guide/problematic-posteriors-chapter.html

Vehtari, A., Gelman, A., and Gabry, J. (2017a). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. 27(5), 1413–1432. doi:10.1007/s11222-016-9696-4 ([journal version](http://link.springer.com/article/10.1007%2Fs11222-016-9696-4), [preprint arXiv:1507.04544](https://arxiv.org/abs/1507.04544)).

Yao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018) Using stacking to average Bayesian predictive distributions. Bayesian Analysis, advance publication, doi:10.1214/17-BA1091. ([online](https://projecteuclid.org/euclid.ba/1516093227)).

Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2019). Pareto smoothed importance sampling. [preprint arXiv:1507.02646](https://arxiv.org/abs/1507.02646/)

Gabry, J. , Simpson, D. , Vehtari, A. , Betancourt, M. and Gelman, A. (2019), Visualization in Bayesian workflow. J. R. Stat. Soc. A, 182: 389-402. doi:10.1111/rssa.12378. (journal version, arXiv preprint, code on GitHub)

[Aki Vehtari's A quick note what I infer from p_loo and Pareto k values](https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446)

https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

https://mc-stan.org/docs/2_24/reference-manual/divergent-transitions.html

https://discourse.mc-stan.org/t/divergent-transitions-a-primer/17099

https://mc-stan.org/docs/2_24/reference-manual/effective-sample-size-section.html

https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html

[Use CmdStan to save memory](http://mc-stan.org/cmdstanr/articles/cmdstanr.html)

- [Jeffrey Arnold's Bayesian Notes](https://jrnold.github.io/bayesian_notes/) has nice examples of many models and good summaries otherwise

[^betanwhat]: Betancourt, whose work I admire greatly, typically makes my head spin.

[^parcoord]: I swear sometimes we're taking a step back in time with some of these plots.  All of the diagnostic plots defaults appear to be styles you'd find in Tukey's EDA from 1950.  You don't necessarily need to get fancy, but surely the defaults could be better.

[^funnelcloud]: And I say this as someone with  experience detecting funnel clouds.

[^adapt_delta]: In my experience, there isn't a need to guess between .80 and .99 as the time differences are typically negligble.  If it doesn't work at .99, it won't at .9999 either.

[^bigobjects]: With more posterior samples comes slower visualizations and possibly other computations.

[^mac]: I will spare the details my true opinions, which are almost entirely negative.

[^lakeland]: In the same post, as a comment, Daniel Lakeland proposes an alternative approach is whether the posterior estimate falls within the 95% highest density interval of the prior.  This is available via the method argument:  `bayestestR::check_prior(model_baseline, method = 'lakeland')`.

[^looic]: The `looic` is just -2*`elpd_loo`, as we often use -2*log likelihood (a.k.a. deviance) in standard approaches for AIC.  In this case, `elpd_loo`, is a leave-one-out density.  `p_loo` is the 'effective number of parameters', which users of penalized regression and mixed models will have some familiarity with.

[^reparameterize]: It is often suggested in the Stan world to reparameterize models.  However, this advice doesn't really apply in the case of using rstanarm or brms (i.e. where you aren't writing Stan code directly), and it assumes a level of statistical expertise many would not have, or even if they do, route to respecifying the model may not be obvious.