---
title: "Title"
description: |
  blah blah
author:
  - name: Michael Clark
    url: https://m-clark.github.io
date: '`r format(Sys.Date(), "%B %d, %Y")`'
preview: ../../img/198R.png   # apparently no way to change the size displayed via css (ignored) or file (stretched)
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: ../../styles.css
draft: true
tags: [tags, taggy]
categories:
  - ?
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo      = TRUE, 
  eval      = TRUE, 
  message   = FALSE,
  warning   = FALSE,
  comment   = NA,
  R.options = list(width = 120),
  cache         = FALSE,
  cache.rebuild = FALSE,
  cache.lazy    = FALSE,
  fig.align = 'center',
  fig.asp   = .7,
  dev       = 'png',
  dev.args  = list(bg = 'transparent')
)


library(tidyverse)

theme_clean <- function (
  font_size = 12,
  font_family = "",
  center_axis_labels = FALSE
) {
  
  if (center_axis_labels) {
    haxis_just_x <- 0.5
    vaxis_just_y <- 0.5
    v_rotation_x <- 0
    v_rotation_y <- 0
  }
  else {
    haxis_just_x <- 0
    vaxis_just_y <- 1
    v_rotation_x <- 0
    v_rotation_y <- 0
  }
  
  ggplot2::theme(
    text = ggplot2::element_text(
      family = font_family,
      face   = "plain",
      color  = "gray30",
      size   = font_size,
      hjust  = 0.5,
      vjust  = 0.5,
      angle  = 0,
      lineheight = 0.9,
      margin = ggplot2::margin(),
      debug  = FALSE
    ),
    axis.title.x = ggplot2::element_text(
      hjust = haxis_just_x,
      angle = v_rotation_x,
      size  = 0.8 * font_size
    ),
    axis.title.y = ggplot2::element_text(
      vjust = vaxis_just_y,
      hjust = 0,
      angle = v_rotation_y,
      size  = 0.8 * font_size
    ),
    axis.ticks        = ggplot2::element_line(color = "gray30"),
    title             = ggplot2::element_text(color = "gray30", size = font_size * 1.25),
    plot.subtitle     = ggplot2::element_text(color = "gray30", size = font_size * .75, hjust = 0),
    plot.caption      = ggplot2::element_text(color = "gray30", size = font_size * .5, hjust = 0),
    legend.position   = 'bottom', 
    legend.key        = ggplot2::element_rect(fill = "transparent", color = NA),
    legend.background = ggplot2::element_rect(fill = "transparent", color = NA),
    legend.title      = ggplot2::element_blank(), 
    panel.background  = ggplot2::element_blank(),
    panel.grid        = ggplot2::element_blank(),
    strip.background  = ggplot2::element_blank(),
    plot.background   = ggplot2::element_rect(fill = "transparent", color = NA),
  )
}

# set the theme as default
theme_set(theme_clean())

# set other point/line default colors; in most cases, we can use the color from
# default discrete scale for more consistency across plots.
# paletteer::palettes_d$colorblindr$OkabeIto
update_geom_defaults('vline',   list(color = 'gray25',  alpha = .25))  # vlines and hlines are typically not attention grabbers so set alpha
update_geom_defaults('hline',   list(color = 'gray25',  alpha = .25))  # usually a zero marker
update_geom_defaults('point',   list(color = '#E69F00', alpha = .5))   # alpha as usually there are many points
update_geom_defaults('smooth',  list(color = '#56B4E9', alpha = .15))
update_geom_defaults('line',    list(color = '#56B4E9', alpha = .5))
update_geom_defaults('bar',     list(color = '#E69F00', fill = '#E69F00'))  
update_geom_defaults('col',     list(color = '#E69F00', fill = '#E69F00'))
update_geom_defaults('dotplot', list(color = '#E69F00', fill = '#E69F00'))

# use colorblind safe colors for categories; if you supply a continuous value to
# color you'll get an error, but you just have to use `myplot +
# scale_color_continous()` or whatever to override this; likewise you can always
# override this scale for categorical schemes if desired also. Note that this
# will apply for both color and fill, which is usually what we want.

okabe_ito = c(
  '#E69F00',
  '#56B4E9',
  '#009E73',
  '#F0E442',
  '#0072B2',
  '#D55E00',
  '#CC79A7',
  '#999999'
)

ggplot <- function(...) ggplot2::ggplot(...) + 
  # okabe ito colorblind safe scheme
  scale_color_manual(
    values = okabe_ito,
    drop = FALSE,
    aesthetics = c('color', 'fill')
  )

gt <- function(..., decimals = 2, title = NULL, subtitle = NULL) {
  gt::gt(...) %>% 
    gt::fmt_number(
      columns = where(is.numeric),
      decimals = decimals
    ) %>% 
    gt::tab_header(title = title, subtitle = subtitle) %>% 
    gtExtras::gt_theme_nytimes()
}

gt_theme <-   
  list(
    # report median (IQR) and n (percent) as default stats in `tbl_summary()`
    "tbl_summary-str:continuous_stat" = "{mean} ({sd})",
    "tbl_summary-str:categorical_stat" = "{n} ({p})"
  )

rnd = tidyext::rnd
```


```{r pysetup, include=FALSE}
library(reticulate)
# use_python('/Users/micl/Library/r-miniconda-arm64/bin/python3.9')
# use_python('/Users/micl/opt/anaconda3/envs/xgb_lgb/bin/python')
# to use, set the project python to the above, it seems not to work with use_python
```


## Introduction

This post gives a by-hand example of linear regression using pytorch.  Why would anyone do this? Well, for one thing, it serves as a stepping stone for those who know basic statistical methodology like regression, but want to get into deep learning in a conceptual manner.  Another is to just seem some pytorch basics in a simple setting.  And one last reason is that maybe you want to incorporate a standard statistical modeling approach into some other deep learning endeavor.

We'll use [an example by fastai](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch), which is a great resource for [getting started with deep learning](https://course.fast.ai/).  Their example serves as a basis, though I will generalize the functionality so that you can play around with the settings and try other data examples.  In addition, this post will assume you know things like why you would dummy code features and linear regression basics, and will use some other naming conventions[^nobigdeal].  

[^nobigdeal]: I'm also not going to go into broadcasting, submitting to Kaggle, and many other things that I don't think are necessary for our purposes here.
[^nofastai]: I won't actually use fastai, since they aren't up to supporting M1/2 Macs very well.  I think it was only used for the train/test data split anyway.  I would rant a bit about this, but a lot of fastai is geared toward non-local computing, and the fault is really with Apple and NVidia.


## Getting Started

Let's get the primary packages loaded first.


```{python, imports}
import pandas as pd
import numpy as np
import torch
```

Next, we'll use the well-known [titanic dataset](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch/data), and get a sense of what we're dealing with. The basic idea is that we'd like to predict survival based on key features like sex, age, ticket class and more.

```{python, read-titanic}
df_titanic_train = pd.read_csv('data/dl-linear-regression/titanic/train.csv')
# df_titanic_train
```


```{python, describe}
df_titanic_train.describe()
```


## Initial Data Processing

The data is not ready for modeling as is, so we'll do some additional processing to get it ready. We'll check out the missing values and replace them with modes[^missingmode].


[^missingmode]: Just as an aside, this sort of approach to impute has largely been frowned upon in the statistical world for decades for numerous (and valid) reasons, but we just want something quick and dirty here.

```{python show-na}
df_titanic_train.isna().sum()
```


```{python replace-na}
modes = df_titanic_train.mode().iloc[0]

df_titanic_train.fillna(modes, inplace = True)

df_titanic_train.describe(include = (np.number))
```


Sometimes it is worthwhile to log transform data to help deal with heteroscedasticity[^heterosced], or simply to allow for potentially more efficient optimization search.  Since we have zeros, we add 1 before taking the log.

[^heterosced]: It really shouldn't be done to deal with outliers and/or a misunderstanding of the normality assumption.  For the former, there are better ways (e.g. using a better model, different distribution, different loss function), and the latter is only a concern if we're actually interested in uncertainty of the coefficients, and even then, it is not a big issue, especially with large data.


```{python hist-fare, fig.show='hide'}
df_titanic_train['Fare'].hist()
```

![](../../img/dl-linreg/fare-hist.png){width=50%}


Now the transformed data looks a little more manageable.

```{python l1p-fare, fig.show='hide'}
df_titanic_train['LogFare'] = np.log1p(df_titanic_train['Fare'])

df_titanic_train['LogFare'].hist()
```

![](../../img/dl-linreg/fare-hist-log.png){width=50%}

The Pclass (passenger class) feature is actually categorical.

```{python inspect-cats}
pclasses = sorted(df_titanic_train.Pclass.unique())
pclasses
```

Here are the other categorical features.

```{python inspect-more-cats}
df_titanic_train.describe(include = [object])
```

In order to use categorical variables, they need to be changed to numbers[^caterrors], so we dummy code them here. There are other coding schemes, and for most deep learning approaches people will often use embeddings, particularly for things that have lots of unique categories.

[^caterrors]: Even though every modeling tool requires this, strangely very few in the Python world offer automatic handling of such things.

```{python dummify}
df_titanic_train = pd.get_dummies(df_titanic_train, columns = ["Sex", "Pclass", "Embarked"])
df_titanic_train.columns
```

Let's take a look at our data now.

```{python show-ready-data}
df_titanic_train.head()
```


## Getting Started with Pytorch

### Setup

Now we are ready to prep things for specific use with pytorch.  I will not use the same terminology as in Jeremy's original post, so for us, target = dependent variable and X is our feature matrix[^indep]. Both of these will be pytorch *tensors*, which for our purposes is just another word for an array of arbitrary size.


[^indep]: I'll not perpetuate calling features/predicotr variables that are clearly not independent as independent. That really only works under randomized experiments, and that is definitely not the case here.


```{python target-tensor}
from torch import tensor

target = tensor(df_titanic_train.Survived)
```


```{python dummy-features}
dummies = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']
all_features = ['Age', 'SibSp', 'Parch', 'LogFare'] + dummies 

X = tensor(df_titanic_train[all_features].values, dtype = torch.float)
X.shape
```


## Setting up a linear model

We have our feature matrix and target fariable prepped.  Next step is to map the features to the target by means of predicted values.  In linear regression, we typically call the weights that produce the predictions *coefficients*, but in standard deep/machine learning terminology, they are usually called feature *weights*, or more generally, *parameters*.  Here, we generate some random values between -.5 and .5 to get started[^other_starts]:.  

[^other_starts]: You could use <span class="func">torch.randn</span> to get standard normal values, and often times we'll even start with just zeros.

```{python random-coeffs}
torch.manual_seed(442)

n_coeff = X.shape[1]
coeffs = torch.rand(n_coeff) - 0.5  # default would produce values from 0 to 1
coeffs
```

The original post did a form of min-max scaling to the features, basically putting everything on a potentially [0, 1] scale. Here we'll use standardization as an alternative, giving each feature a mean of zero and standard deviation of 1.

```{python scale-X}
# vals,indices = X.max(dim=0)
# X = X / vals
X_means = X.mean(dim = 0, keepdim = True)
X_sds   = X.std(dim = 0)

X_sc = (X - X_means) / X_sds

# X_sc.mean(dim = 0)  # all means = 0 
# X_sc.std(dim = 0)   # all sd = 1
```

As noted in the original post and worth iterating here for our statistical modeling crowd, we don't estimate an intercept for this model and keep all the dummy coded features.  The following takes our coefficients, multiplies them by their respective feature, and sums them.

```{python init-preds}
preds = (X_sc * coeffs).sum(axis = 1)
preds[:10]
```

We can calculate our *loss*, the difference in our predictions versus the expected values, in many ways. Here we get the mean squared error.  Later on we'll use a specific function for this.

```{python init-loss}
loss = torch.square(preds - target).mean()
loss
```


Now we'll create functions that do the previous steps, and finally, give it a test run!  In the original fastai formulation, they use mean absolute error for the loss, which actually is just the `L1loss` that is available in torch. For a change of pace, we'll keep our mean squared error, which is sometimes called `L2` loss (this will create different results from the original notebook). I create the option within the function for you to do either.  Also note that the functions we create here will take inputs generally, rather than being specific to the objects we create, so you can try this stuff out with other data.

```{python func-calc_predsloss-init}
def calc_preds(X, weights):
    return((X * weights).sum(axis = 1))

def calc_loss(X, weights, target, which = 'l2'):
    preds = calc_preds(X, weights)
    
    # torch.abs(calc_preds(X, coeffs)-target).mean()  # original

    if which == 'l2':
      loss = torch.nn.MSELoss()
    else: 
      loss = torch.nn.L1Loss()
      
    L = loss(preds, target.float())
      
    return(L)

calc_loss(X_sc, coeffs, target), calc_loss(X_sc, coeffs, target, which = 'l1')
```




### Doing a Gradient Descent Step


We can continue our journey onward to actually estimating the weights rather than specifying them. This is an iterative process where we start with an initial guess, then, at each step, refine our guess in a way that lowers our loss. For neural networks we call these steps *epochs*, and getting our next guess requires calculating what's called a *gradient*.  Here are some resources:

- [How Does a Neural Net Really Work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work): great intro by Jeremy Howard
- [Some by-hand code using gradient descent for linear regression](https://m-clark.github.io/models-by-example/gradient-descent.html): By yours truly


In any case, this is a basic part of pytorch, and it will keep track of each step taken.

```{python require-grad}
coeffs.requires_grad_()
```

```{python redo-loss}
loss = calc_loss(X_sc, coeffs, target)
loss
```


We use <span class="func">backward</span> to calculate the gradients and inspect them.

```{python backward-step}
loss.backward()

coeffs.grad
```

Each time backward is called, the gradients are added to the previous values.  We can see here that they've now doubled.

```{python another-round}
loss = calc_loss(X_sc, coeffs, target)

loss.backward()

coeffs.grad
```

What we want instead is to set them back to zero after they are used for our estimation step.  The following does this.


```{python zero-grad}
loss = calc_loss(X_sc, coeffs, target)

loss.backward()

with torch.no_grad():
    coeffs.sub_(coeffs.grad * 0.1)     # sub subtracts in place
    coeffs.grad.zero_()                # zeros out in place
    print(calc_loss(X, coeffs, target))

```


### Training the Linear Model

We typically would typically split our data into training and test. We can do so here, or keep this data as training and import `test.csv` for the test set.  The latter is actually used for the Kaggle submission, but that's not a goal here.  We'll use <span class="pack">scikit-learn</span> for the splitting. 


```{python train-test-split}
from sklearn.model_selection import train_test_split

train_x, valid_x, train_y, valid_y = train_test_split(X_sc, target.float(), test_size = 0.2) # in keeping with RandomSplitter default

len(train_x), len(valid_x) # might be one off of the original notebook
```


As before, we'll create functions to help automate our steps:

- one to initialize the weights
- a function to update weights
- one to do a full epoch (using weights to calculate loss, updating weights)
- one to train the entire model (run multiple times/epochs)

As mentioned, the approach here is to create functions are general enough to take any X or target, so they look a little different from the original notebook. I also add in a verbosity option so you can see the loss at each `verbose` value epoch (e.g. `verbose = 10` means you'll see the latest loss value every 10 epochs), so you can watch the iterations for as long as you like without it printing constantly (not to big a deal depending on your IDE). 

```{python fun-linmod-training}
def init_weights(n_wts): 
    return (torch.rand(n_wts) - 0.5).requires_grad_()

def update_weights(weights, lr):
    weights.sub_(weights.grad * lr)
    weights.grad.zero_()

def one_epoch(X, weights, target, lr, verbose = 1, i = 1):
    loss = calc_loss(X, weights, target)
    loss.backward()
    
    with torch.no_grad(): update_weights(weights, lr)
    
    if verbose != 0:
        if i % verbose == 0:
            print(f'{loss: 3f}', end = '\n ')

def train_model(X, target, epochs = 30, lr = 1e-3, verbose = 1):
    torch.manual_seed(442)
    coeffs = init_weights(X.shape[1])
    
    for i in range(epochs): 
        one_epoch(X, coeffs, target, lr = lr, i = i, verbose = verbose)
    return coeffs

```


Try out the functions if you like (not shown).

```{python calc-loss-linmod, eval=FALSE}
calc_loss(X_sc, init_weights(X_sc.shape[1]), target).backward()
```

```{python one-epoch-linmod, eval=FALSE}
one_epoch(train_x, init_weights(train_x.shape[1]), train_y, .01)
```


Now train the model for multiple epochs.

```{python train-linmod}
coeffs_est = train_model(train_x, train_y, epochs = 50, verbose = 5, lr = .2)
```


Let's create a function to show our estimated parameters/weights/coefficients in a pretty fashion.

```{python fun-show_coeffs}
def show_coeffs(estimates): 
  coef_dict = dict(zip(all_features, estimates.requires_grad_(False).numpy()))
  return pd.DataFrame(coef_dict, index = ['value']).T

show_coeffs(coeffs_est)
```


### Measuring Accuracy

It's one thing to get accuracy on the trained data, but a better estimate of model performance is to measure it on our test/validation data.  The following function will convert our estimates to a binary value like our target.  Depending on how you did your training setup, it might be pretty bad or at least better than guessing.

```{python init-accuracy}
def acc(X, weights, target): 
    return (target.bool() == (calc_preds(X, weights) > 0.5)).float().mean()

acc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)
```



#### Using sigmoid

The sigmoid function[^plogis]  allows us to transform our predctions to values between 0 and 1, i.e. probabilities in this context, and in particular, the probability of survival. Otherwise you can inspect them and might see values are above 1 or below zero (`python np.min(calc_preds(train_x, coeffs_est).numpy())`), which we generally don't want[^linprob].

[^plogis]: Always boggled me that R folks still write an explicit function for this rather than using <span class="func">plogis</span> from base R.
[^linprob]: Unless you are an economist, in which case you call it a *linear probability model* and ignore the ridiculous predictions because you have very fine standard errors.


```{python use-sigmoid}
def calc_preds(X, weights):
    return torch.sigmoid((X*weights).sum(axis = 1))
```

We also will do more iterations, and fiddle with the learning rate (a.k.a. step size)

```{python retrain-with-sigmoid}
coeffs_est = train_model(
  train_x,
  train_y,
  epochs = 500,
  lr = 1,
  verbose = 100
)
```


Not too shabby!

```{python acc-with-sigmoid}
acc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y)
```

```{python coeffs-with-sigmoid}
show_coeffs(coeffs_est)
```

In implementing the sigmoid, let's go ahead and optimize how we calculate the predictions using a matrix shorthand for getting the predictions (which is also much more efficient/faster).  To do this, the coefficients will need to be a column vector, so we change our <span class="func" style = "">init_coeffs</span> function slightly[^coltarget].   


[^coltarget]: The fastai demo also changes the target to a column vector, but this isn't actually necessary.



```{python mat-mult}
def calc_preds(X, weights): 
    return torch.sigmoid(X@weights)

def init_coeffs(n_wts): 
    return (torch.rand(n_wts, 1) * 0.1).requires_grad_()

```

Now our functions are more like the mathematical notation we'd usually see for linear regression.

$$\hat{y} = X\beta$$



### Compare to Linear/Logistic Regression

Before getting too excited, let's compare our results to basic linear and logistic regression.  Depending on your settings, the logistic regression is probably doing better at this point.

```{python lin-prob-model}
from sklearn import linear_model
from sklearn.metrics import accuracy_score


reg = linear_model.LinearRegression()
reg.fit(train_x, train_y)

acc(valid_x, coeffs_est, valid_y), acc(valid_x, reg.coef_.T, valid_y)
```


```{python acc-torch-vs-logreg}
reg = linear_model.LogisticRegression()
reg.fit(train_x, train_y)

accuracy_score(valid_y, reg.predict(valid_x)).round(4)
```

```{python logreg-coefs}
show_coeffs(coeffs_est).assign(logreg = np.round(reg.coef_.T, 4))
```



## A Neural Network


```{python fun-init_weights_nn}
def init_weights(n_wts, n_hidden=20):
    layer1 = (torch.rand(n_wts, n_hidden)-0.5)/n_hidden
    layer2 = torch.rand(n_hidden, 1)-0.3
    const = torch.rand(1)[0]
    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()
```


```{python fun-calc_preds-nn}
import torch.nn.functional as F

def calc_preds(X, weights):
    l1,l2,const = weights
    res = F.mish(X@l1)
    res = res@l2 + const
    return torch.sigmoid(res).flatten()
```

```{python fun-update_weights-nn}
def update_weights(weights, lr):
    for layer in weights:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()
```

```{python coeffs-est-nn}
coeffs_est = train_model(train_x, train_y, epochs = 500, lr = 1, verbose = 50)
```


```{python acc-compare-nn}
acc(train_x, coeffs_est, train_y), acc(valid_x, coeffs_est, valid_y), accuracy_score(valid_y, reg.predict(valid_x)).round(4)

```


## Deep Learning


```{python funs-dl}
def init_weights(n_wts, hiddens):  
    sizes = [n_wts] + hiddens + [1]
    n = len(sizes)
    layers = [(torch.rand(sizes[i], sizes[i + 1]) - 0.3)/sizes[i + 1] * 4 for i in range(n - 1)]
    consts = [(torch.rand(1)[0] - 0.5)*0.1 for i in range(n - 1)]
    for l in layers+consts: l.requires_grad_()
    return layers,consts

def calc_preds(X, weights):
    layers,consts = weights
    n = len(layers)
    res = X
    
    for i,l in enumerate(layers):
        res = res@l + consts[i]
    
    if i!=n-1: res = F.mish(res)
    
    return torch.sigmoid(res).flatten()

def update_weights(weights, lr):
    layers,consts = weights
    for layer in layers + consts:
        layer.sub_(layer.grad * lr)
        layer.grad.zero_()

def train_model(X, target, hiddens = [10, 10], epochs = 30, lr = 1e-3, verbose = 1):
    torch.manual_seed(442)
    coeffs = init_weights(X.shape[1], hiddens)
    
    for i in range(epochs): 
        if verbose != 0:
            one_epoch(X, coeffs, target, lr = lr, verbose = verbose, i = i)
    
    return coeffs
```


```{python estimate-dl}
coeffs_est = train_model(
  train_x,
  train_y,
  hiddens = [10, 10],
  epochs  = 500,
  lr      = 4,
  verbose = 50
)
```


```{python compare-dl}
pd.DataFrame({
    'acc_train': acc(train_x, coeffs_est, train_y).flatten(), 
    'acc_test': acc(valid_x, , valid_y).flatten(), 
    'acc_test_glm': accuracy_score(valid_y, (reg.predict(valid_x) > .5).astype(int)).round(6)
}, index=['value'])
```


## Summary

This was a lot of work to do slightly better than a logistic regression!  However, there is a lot going on with a typical DL model that would likely prove even better.  But it also serves as a reminder to have a suitable baseline.

